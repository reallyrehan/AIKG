{"url": "http://arxiv.org/abs/1801.07829v2", "updated": "2019-06-11T06:11:21Z", "published": "2018-01-24T01:14:04Z", "title": "Dynamic Graph CNN for Learning on Point Clouds", "summary": "  Point clouds provide a flexible geometric representation suitable forcountless applications in computer graphics; they also comprise the raw outputof most 3D data acquisition devices. While hand-designed features on pointclouds have long been proposed in graphics and vision, however, the recentoverwhelming success of convolutional neural networks (CNNs) for image analysissuggests the value of adapting insight from CNN to the point cloud world. Pointclouds inherently lack topological information so designing a model to recovertopology can enrich the representation power of point clouds. To this end, wepropose a new neural network module dubbed EdgeConv suitable for CNN-basedhigh-level tasks on point clouds including classification and segmentation.EdgeConv acts on graphs dynamically computed in each layer of the network. Itis differentiable and can be plugged into existing architectures. Compared toexisting modules operating in extrinsic space or treating each pointindependently, EdgeConv has several appealing properties: It incorporates localneighborhood information; it can be stacked applied to learn global shapeproperties; and in multi-layer systems affinity in feature space capturessemantic characteristics over potentially long distances in the originalembedding. We show the performance of our model on standard benchmarksincluding ModelNet40, ShapeNetPart, and S3DIS.", "authors": ["Yue Wang", "Yongbin Sun", "Ziwei Liu", "Sanjay E. Sarma", "Michael M. Bronstein", "Justin M. Solomon"], "categories": ["cs.CV"]}
{"url": "http://arxiv.org/abs/1705.01583v1", "updated": "2017-05-03T19:13:23Z", "published": "2017-05-03T19:13:23Z", "title": "VNect: Real-time 3D Human Pose Estimation with a Single RGB Camera", "summary": "  We present the first real-time method to capture the full global 3D skeletalpose of a human in a stable, temporally consistent manner using a single RGBcamera. Our method combines a new convolutional neural network (CNN) based poseregressor with kinematic skeleton fitting. Our novel fully-convolutional poseformulation regresses 2D and 3D joint positions jointly in real time and doesnot require tightly cropped input frames. A real-time kinematic skeletonfitting method uses the CNN output to yield temporally stable 3D global posereconstructions on the basis of a coherent kinematic skeleton. This makes ourapproach the first monocular RGB method usable in real-time applications suchas 3D character control---thus far, the only monocular methods for suchapplications employed specialized RGB-D cameras. Our method's accuracy isquantitatively on par with the best offline 3D monocular RGB pose estimationmethods. Our results are qualitatively comparable to, and sometimes betterthan, results from monocular RGB-D approaches, such as the Kinect. However, weshow that our approach is more broadly applicable than RGB-D solutions, i.e. itworks for outdoor scenes, community videos, and low quality commodity RGBcameras.", "authors": ["Dushyant Mehta", "Srinath Sridhar", "Oleksandr Sotnychenko", "Helge Rhodin", "Mohammad Shafiei", "Hans-Peter Seidel", "Weipeng Xu", "Dan Casas", "Christian Theobalt"], "categories": ["cs.CV", "cs.GR"]}
{"url": "http://arxiv.org/abs/1712.01537v1", "updated": "2017-12-05T09:25:19Z", "published": "2017-12-05T09:25:19Z", "title": "O-CNN: Octree-based Convolutional Neural Networks for 3D Shape Analysis", "summary": "  We present O-CNN, an Octree-based Convolutional Neural Network (CNN) for 3Dshape analysis. Built upon the octree representation of 3D shapes, our methodtakes the average normal vectors of a 3D model sampled in the finest leafoctants as input and performs 3D CNN operations on the octants occupied by the3D shape surface. We design a novel octree data structure to efficiently storethe octant information and CNN features into the graphics memory and executethe entire O-CNN training and evaluation on the GPU. O-CNN supports various CNNstructures and works for 3D shapes in different representations. By restrainingthe computations on the octants occupied by 3D surfaces, the memory andcomputational costs of the O-CNN grow quadratically as the depth of the octreeincreases, which makes the 3D CNN feasible for high-resolution 3D models. Wecompare the performance of the O-CNN with other existing 3D CNN solutions anddemonstrate the efficiency and efficacy of O-CNN in three shape analysis tasks,including object classification, shape retrieval, and shape segmentation.", "authors": ["Peng-Shuai Wang", "Yang Liu", "Yu-Xiao Guo", "Chun-Yu Sun", "Xin Tong"], "categories": ["cs.CV"]}
{"url": "http://arxiv.org/abs/1609.02974v1", "updated": "2016-09-09T23:33:38Z", "published": "2016-09-09T23:33:38Z", "title": "Learning-Based View Synthesis for Light Field Cameras", "summary": "  With the introduction of consumer light field cameras, light field imaginghas recently become widespread. However, there is an inherent trade-off betweenthe angular and spatial resolution, and thus, these cameras often sparselysample in either spatial or angular domain. In this paper, we use machinelearning to mitigate this trade-off. Specifically, we propose a novellearning-based approach to synthesize new views from a sparse set of inputviews. We build upon existing view synthesis techniques and break down theprocess into disparity and color estimation components. We use two sequentialconvolutional neural networks to model these two components and train bothnetworks simultaneously by minimizing the error between the synthesized andground truth images. We show the performance of our approach using only fourcorner sub-aperture views from the light fields captured by the Lytro Illumcamera. Experimental results show that our approach synthesizes high-qualityimages that are superior to the state-of-the-art techniques on a variety ofchallenging real-world scenes. We believe our method could potentially decreasethe required angular resolution of consumer light field cameras, which allowstheir spatial resolution to increase.", "authors": ["Nima Khademi Kalantari", "Ting-Chun Wang", "Ravi Ramamoorthi"], "categories": ["cs.CV", "cs.GR", "I.4.1"]}
{"url": "http://arxiv.org/abs/1804.02717v3", "updated": "2018-07-27T03:44:10Z", "published": "2018-04-08T17:04:58Z", "title": "DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based  Character Skills", "summary": "  A longstanding goal in character animation is to combine data-drivenspecification of behavior with a system that can execute a similar behavior ina physical simulation, thus enabling realistic responses to perturbations andenvironmental variation. We show that well-known reinforcement learning (RL)methods can be adapted to learn robust control policies capable of imitating abroad range of example motion clips, while also learning complex recoveries,adapting to changes in morphology, and accomplishing user-specified goals. Ourmethod handles keyframed motions, highly-dynamic actions such asmotion-captured flips and spins, and retargeted motions. By combining amotion-imitation objective with a task objective, we can train characters thatreact intelligently in interactive settings, e.g., by walking in a desireddirection or throwing a ball at a user-specified target. This approach thuscombines the convenience and motion quality of using motion clips to define thedesired style and appearance, with the flexibility and generality afforded byRL methods and physics-based animation. We further explore a number of methodsfor integrating multiple clips into the learning process to developmulti-skilled agents capable of performing a rich repertoire of diverse skills.We demonstrate results using multiple characters (human, Atlas robot, bipedaldinosaur, dragon) and a large variety of skills, including locomotion,acrobatics, and martial arts.", "authors": ["Xue Bin Peng", "Pieter Abbeel", "Sergey Levine", "Michiel van de Panne"], "categories": ["cs.GR", "cs.AI", "cs.LG"]}
{"url": "http://arxiv.org/abs/1705.01088v2", "updated": "2017-06-06T15:16:19Z", "published": "2017-05-02T17:44:01Z", "title": "Visual Attribute Transfer through Deep Image Analogy", "summary": "  We propose a new technique for visual attribute transfer across images thatmay have very different appearance but have perceptually similar semanticstructure. By visual attribute transfer, we mean transfer of visual information(such as color, tone, texture, and style) from one image to another. Forexample, one image could be that of a painting or a sketch while the other is aphoto of a real scene, and both depict the same type of scene.  Our technique finds semantically-meaningful dense correspondences between twoinput images. To accomplish this, it adapts the notion of \"image analogy\" withfeatures extracted from a Deep Convolutional Neutral Network for matching; wecall our technique Deep Image Analogy. A coarse-to-fine strategy is used tocompute the nearest-neighbor field for generating the results. We validate theeffectiveness of our proposed method in a variety of cases, includingstyle/texture transfer, color/style swap, sketch/painting to photo, and timelapse.", "authors": ["Jing Liao", "Yuan Yao", "Lu Yuan", "Gang Hua", "Sing Bing Kang"], "categories": ["cs.CV"]}
{"url": "http://arxiv.org/abs/1804.03619v2", "updated": "2018-08-09T21:22:37Z", "published": "2018-04-10T16:28:59Z", "title": "Looking to Listen at the Cocktail Party: A Speaker-Independent  Audio-Visual Model for Speech Separation", "summary": "  We present a joint audio-visual model for isolating a single speech signalfrom a mixture of sounds such as other speakers and background noise. Solvingthis task using only audio as input is extremely challenging and does notprovide an association of the separated speech signals with speakers in thevideo. In this paper, we present a deep network-based model that incorporatesboth visual and auditory signals to solve this task. The visual features areused to \"focus\" the audio on desired speakers in a scene and to improve thespeech separation quality. To train our joint audio-visual model, we introduceAVSpeech, a new dataset comprised of thousands of hours of video segments fromthe Web. We demonstrate the applicability of our method to classic speechseparation tasks, as well as real-world scenarios involving heated interviews,noisy bars, and screaming children, only requiring the user to specify the faceof the person in the video whose speech they want to isolate. Our method showsclear advantage over state-of-the-art audio-only speech separation in cases ofmixed speech. In addition, our model, which is speaker-independent (trainedonce, applicable to any speaker), produces better results than recentaudio-visual speech separation methods that are speaker-dependent (requiretraining a separate model for each speaker of interest).", "authors": ["Ariel Ephrat", "Inbar Mosseri", "Oran Lang", "Tali Dekel", "Kevin Wilson", "Avinatan Hassidim", "William T. Freeman", "Michael Rubinstein"], "categories": ["cs.SD", "cs.CV", "eess.AS"]}
{"url": "http://arxiv.org/abs/1705.02999v1", "updated": "2017-05-08T17:58:11Z", "published": "2017-05-08T17:58:11Z", "title": "Real-Time User-Guided Image Colorization with Learned Deep Priors", "summary": "  We propose a deep learning approach for user-guided image colorization. Thesystem directly maps a grayscale image, along with sparse, local user \"hints\"to an output colorization with a Convolutional Neural Network (CNN). Ratherthan using hand-defined rules, the network propagates user edits by fusinglow-level cues along with high-level semantic information, learned fromlarge-scale data. We train on a million images, with simulated user inputs. Toguide the user towards efficient input selection, the system recommends likelycolors based on the input image and current user inputs. The colorization isperformed in a single feed-forward pass, enabling real-time use. Even withrandomly simulated user inputs, we show that the proposed system helps noviceusers quickly create realistic colorizations, and offers large improvements incolorization quality with just a minute of use. In addition, we demonstratethat the framework can incorporate other user \"hints\" to the desiredcolorization, showing an application to color histogram transfer. Our code andmodels are available at https://richzhang.github.io/ideepcolor.", "authors": ["Richard Zhang", "Jun-Yan Zhu", "Phillip Isola", "Xinyang Geng", "Angela S. Lin", "Tianhe Yu", "Alexei A. Efros"], "categories": ["cs.CV", "cs.GR"]}
{"url": "http://arxiv.org/abs/1707.02880v2", "updated": "2017-08-22T19:26:08Z", "published": "2017-07-10T14:34:06Z", "title": "Deep Bilateral Learning for Real-Time Image Enhancement", "summary": "  Performance is a critical challenge in mobile image processing. Given areference imaging pipeline, or even human-adjusted pairs of images, we seek toreproduce the enhancements and enable real-time evaluation. For this, weintroduce a new neural network architecture inspired by bilateral gridprocessing and local affine color transforms. Using pairs of input/outputimages, we train a convolutional neural network to predict the coefficients ofa locally-affine model in bilateral space. Our architecture learns to makelocal, global, and content-dependent decisions to approximate the desired imagetransformation. At runtime, the neural network consumes a low-resolutionversion of the input image, produces a set of affine transformations inbilateral space, upsamples those transformations in an edge-preserving fashionusing a new slicing node, and then applies those upsampled transformations tothe full-resolution image. Our algorithm processes high-resolution images on asmartphone in milliseconds, provides a real-time viewfinder at 1080presolution, and matches the quality of state-of-the-art approximationtechniques on a large class of image operators. Unlike previous work, our modelis trained off-line from data and therefore does not require access to theoriginal operator at runtime. This allows our model to learn complex,scene-dependent transformations for which no reference implementation isavailable, such as the photographic edits of a human retoucher.", "authors": ["Micha\u00ebl Gharbi", "Jiawen Chen", "Jonathan T. Barron", "Samuel W. Hasinoff", "Fr\u00e9do Durand"], "categories": ["cs.GR", "cs.CV"]}
{"url": "http://arxiv.org/abs/1412.7725v2", "updated": "2015-05-16T03:49:35Z", "published": "2014-12-24T17:51:17Z", "title": "Automatic Photo Adjustment Using Deep Neural Networks", "summary": "  Photo retouching enables photographers to invoke dramatic visual impressionsby artistically enhancing their photos through stylistic color and toneadjustments. However, it is also a time-consuming and challenging task thatrequires advanced skills beyond the abilities of casual photographers. Using anautomated algorithm is an appealing alternative to manual work but such analgorithm faces many hurdles. Many photographic styles rely on subtleadjustments that depend on the image content and even its semantics. Further,these adjustments are often spatially varying. Because of thesecharacteristics, existing automatic algorithms are still limited and cover onlya subset of these challenges. Recently, deep machine learning has shown uniqueabilities to address hard problems that resisted machine algorithms for long.This motivated us to explore the use of deep learning in the context of photoediting. In this paper, we explain how to formulate the automatic photoadjustment problem in a way suitable for this approach. We also introduce animage descriptor that accounts for the local semantics of an image. Ourexperiments demonstrate that our deep learning formulation applied using thesedescriptors successfully capture sophisticated photographic styles. Inparticular and unlike previous techniques, it can model local adjustments thatdepend on the image semantics. We show on several examples that this yieldsresults that are qualitatively and quantitatively better than previous work.", "authors": ["Zhicheng Yan", "Hao Zhang", "Baoyuan Wang", "Sylvain Paris", "Yizhou Yu"], "categories": ["cs.CV", "cs.GR", "cs.LG", "eess.IV"]}
{"url": "http://arxiv.org/abs/1603.06188v1", "updated": "2016-03-20T07:37:01Z", "published": "2016-03-20T07:37:01Z", "title": "An angular momentum conserving Affine-Particle-In-Cell method", "summary": "  We present a new technique for transferring momentum and velocity betweenparticles and grid with Particle-In-Cell (PIC) calculations which we callAffine-Particle-In-Cell (APIC). APIC represents particle velocities as locallyaffine, rather than locally constant as in traditional PIC. We show that thisrepresentation allows APIC to conserve linear and angular momentum acrosstransfers while also dramatically reducing numerical diffusion usuallyassociated with PIC. Notably, conservation is achieved with lumped mass, asopposed to the more commonly used Fluid Implicit Particle (FLIP) transferswhich require a 'full' mass matrix for exact conservation. Furthermore, unlikeFLIP, APIC retains a filtering property of the original PIC and thus does notaccumulate velocity modes on particles as FLIP does. In particular, wedemonstrate that APIC does not experience velocity instabilities that arecharacteristic of FLIP in a number of Material Point Method (MPM)hyperelasticity calculations. Lastly, we demonstrate that when combined withthe midpoint rule for implicit update of grid momentum that linear and angularmomentum are exactly conserved.", "authors": ["Chenfanfu Jiang", "Craig Schroeder", "Joseph Teran"], "categories": ["physics.comp-ph", "math.NA"]}
{"url": "http://arxiv.org/abs/1805.11714v1", "updated": "2018-05-29T21:31:14Z", "published": "2018-05-29T21:31:14Z", "title": "Deep Video Portraits", "summary": "  We present a novel approach that enables photo-realistic re-animation ofportrait videos using only an input video. In contrast to existing approachesthat are restricted to manipulations of facial expressions only, we are thefirst to transfer the full 3D head position, head rotation, face expression,eye gaze, and eye blinking from a source actor to a portrait video of a targetactor. The core of our approach is a generative neural network with a novelspace-time architecture. The network takes as input synthetic renderings of aparametric face model, based on which it predicts photo-realistic video framesfor a given target actor. The realism in this rendering-to-video transfer isachieved by careful adversarial training, and as a result, we can createmodified target videos that mimic the behavior of the synthetically-createdinput. In order to enable source-to-target video re-animation, we render asynthetic target video with the reconstructed head animation parameters from asource video, and feed it into the trained network -- thus taking full controlof the target. With the ability to freely recombine source and targetparameters, we are able to demonstrate a large variety of video rewriteapplications without explicitly modeling hair, body or background. Forinstance, we can reenact the full head using interactive user-controlledediting, and realize high-fidelity visual dubbing. To demonstrate the highquality of our output, we conduct an extensive series of experiments andevaluations, where for instance a user study shows that our video edits arehard to detect.", "authors": ["Hyeongwoo Kim", "Pablo Garrido", "Ayush Tewari", "Weipeng Xu", "Justus Thies", "Matthias Nie\u00dfner", "Patrick P\u00e9rez", "Christian Richardt", "Michael Zollh\u00f6fer", "Christian Theobalt"], "categories": ["cs.CV", "cs.AI", "cs.GR"]}
{"url": "http://arxiv.org/abs/1710.07480v1", "updated": "2017-10-20T10:48:22Z", "published": "2017-10-20T10:48:22Z", "title": "HDR image reconstruction from a single exposure using deep CNNs", "summary": "  Camera sensors can only capture a limited range of luminance simultaneously,and in order to create high dynamic range (HDR) images a set of differentexposures are typically combined. In this paper we address the problem ofpredicting information that have been lost in saturated image areas, in orderto enable HDR reconstruction from a single exposure. We show that this problemis well-suited for deep learning algorithms, and propose a deep convolutionalneural network (CNN) that is specifically designed taking into account thechallenges in predicting HDR values. To train the CNN we gather a large datasetof HDR images, which we augment by simulating sensor saturation for a range ofcameras. To further boost robustness, we pre-train the CNN on a simulated HDRdataset created from a subset of the MIT Places database. We demonstrate thatour approach can reconstruct high-resolution visually convincing HDR results ina wide range of situations, and that it generalizes well to reconstruction ofimages captured with arbitrary and low-end cameras that use unknown cameraresponse functions and post-processing. Furthermore, we compare to existingmethods for HDR expansion, and show high quality results also for image basedlighting. Finally, we evaluate the results in a subjective experiment performedon an HDR display. This shows that the reconstructed HDR images are visuallyconvincing, with large improvements as compared to existing methods.", "authors": ["Gabriel Eilertsen", "Joel Kronander", "Gyorgy Denes", "Rafa\u0142 K. Mantiuk", "Jonas Unger"], "categories": ["cs.CV", "cs.GR", "cs.LG"]}
{"url": "http://arxiv.org/abs/1705.02090v2", "updated": "2017-05-13T04:49:23Z", "published": "2017-05-05T05:45:10Z", "title": "GRASS: Generative Recursive Autoencoders for Shape Structures", "summary": "  We introduce a novel neural network architecture for encoding and synthesisof 3D shapes, particularly their structures. Our key insight is that 3D shapesare effectively characterized by their hierarchical organization of parts,which reflects fundamental intra-shape relationships such as adjacency andsymmetry. We develop a recursive neural net (RvNN) based autoencoder to map aflat, unlabeled, arbitrary part layout to a compact code. The code effectivelycaptures hierarchical structures of man-made 3D objects of varying structuralcomplexities despite being fixed-dimensional: an associated decoder maps a codeback to a full hierarchy. The learned bidirectional mapping is further tunedusing an adversarial setup to yield a generative model of plausible structures,from which novel structures can be sampled. Finally, our structure synthesisframework is augmented by a second trained module that produces fine-grainedpart geometry, conditioned on global and local structural context, leading to afull generative pipeline for 3D shapes. We demonstrate that withoutsupervision, our network learns meaningful structural hierarchies adhering toperceptual grouping principles, produces compact codes which enableapplications such as shape classification and partial matching, and supportsshape synthesis and interpolation with significant variations in topology andgeometry.", "authors": ["Jun Li", "Kai Xu", "Siddhartha Chaudhuri", "Ersin Yumer", "Hao Zhang", "Leonidas Guibas"], "categories": ["cs.GR", "cs.CV"]}
{"url": "http://arxiv.org/abs/1803.10091v1", "updated": "2018-03-27T14:06:16Z", "published": "2018-03-27T14:06:16Z", "title": "Point Convolutional Neural Networks by Extension Operators", "summary": "  This paper presents Point Convolutional Neural Networks (PCNN): a novelframework for applying convolutional neural networks to point clouds. Theframework consists of two operators: extension and restriction, mapping pointcloud functions to volumetric functions and vise-versa. A point cloudconvolution is defined by pull-back of the Euclidean volumetric convolution viaan extension-restriction mechanism.  The point cloud convolution is computationally efficient, invariant to theorder of points in the point cloud, robust to different samplings and varyingdensities, and translation invariant, that is the same convolution kernel isused at all points. PCNN generalizes image CNNs and allows readily adaptingtheir architectures to the point cloud setting.  Evaluation of PCNN on three central point cloud learning benchmarksconvincingly outperform competing point cloud learning methods, and the vastmajority of methods working with more informative shape representations such assurfaces and/or normals.", "authors": ["Matan Atzmon", "Haggai Maron", "Yaron Lipman"], "categories": ["cs.CV"]}
{"url": "http://arxiv.org/abs/1506.06668v1", "updated": "2015-06-22T16:20:34Z", "published": "2015-06-22T16:20:34Z", "title": "Fairy Lights in Femtoseconds: Aerial and Volumetric Graphics Rendered by  Focused Femtosecond Laser Combined with Computational Holographic Fields", "summary": "  We present a method of rendering aerial and volumetric graphics usingfemtosecond lasers. A high-intensity laser excites a physical matter to emitlight at an arbitrary 3D position. Popular applications can then be exploredespecially since plasma induced by a femtosecond laser is safer than thatgenerated by a nanosecond laser. There are two methods of rendering graphicswith a femtosecond laser in air: Producing holograms using spatial lightmodulation technology, and scanning of a laser beam by a galvano mirror. Theholograms and workspace of the system proposed here occupy a volume of up to 1cm^3; however, this size is scalable depending on the optical devices and theirsetup. This paper provides details of the principles, system setup, andexperimental evaluation, and discussions on scalability, design space, andapplications of this system. We tested two laser sources: an adjustable (30-100fs) laser which projects up to 1,000 pulses per second at energy up to 7 mJ perpulse, and a 269-fs laser which projects up to 200,000 pulses per second at anenergy up to 50 uJ per pulse. We confirmed that the spatiotemporal resolutionof volumetric displays, implemented with these laser sources, is 4,000 and200,000 dots per second. Although we focus on laser-induced plasma in air, thediscussion presented here is also applicable to other rendering principles suchas fluorescence and microbubble in solid/liquid materials.", "authors": ["Yoichi Ochiai", "Kota Kumagai", "Takayuki Hoshi", "Jun Rekimoto", "Satoshi Hasegawa", "Yoshio Hayasaki"], "categories": ["cs.GR", "cs.HC", "physics.optics"]}
{"url": "http://arxiv.org/abs/1805.09817v1", "updated": "2018-05-24T17:58:02Z", "published": "2018-05-24T17:58:02Z", "title": "Stereo Magnification: Learning View Synthesis using Multiplane Images", "summary": "  The view synthesis problem--generating novel views of a scene from knownimagery--has garnered recent attention due in part to compelling applicationsin virtual and augmented reality. In this paper, we explore an intriguingscenario for view synthesis: extrapolating views from imagery captured bynarrow-baseline stereo cameras, including VR cameras and now-widespreaddual-lens camera phones. We call this problem stereo magnification, and proposea learning framework that leverages a new layered representation that we callmultiplane images (MPIs). Our method also uses a massive new data source forlearning view extrapolation: online videos on YouTube. Using data mined fromsuch videos, we train a deep network that predicts an MPI from an input stereoimage pair. This inferred MPI can then be used to synthesize a range of novelviews of the scene, including views that extrapolate significantly beyond theinput baseline. We show that our method compares favorably with several recentview synthesis methods, and demonstrate applications in magnifyingnarrow-baseline stereo images.", "authors": ["Tinghui Zhou", "Richard Tucker", "John Flynn", "Graham Fyffe", "Noah Snavely"], "categories": ["cs.CV", "cs.GR"]}
{"url": "http://arxiv.org/abs/1604.07043v3", "updated": "2016-05-04T08:17:19Z", "published": "2016-04-24T15:53:22Z", "title": "Towards Better Analysis of Deep Convolutional Neural Networks", "summary": "  Deep convolutional neural networks (CNNs) have achieved breakthroughperformance in many pattern recognition tasks such as image classification.However, the development of high-quality deep models typically relies on asubstantial amount of trial-and-error, as there is still no clear understandingof when and why a deep model works. In this paper, we present a visualanalytics approach for better understanding, diagnosing, and refining deepCNNs. We formulate a deep CNN as a directed acyclic graph. Based on thisformulation, a hybrid visualization is developed to disclose the multiplefacets of each neuron and the interactions between them. In particular, weintroduce a hierarchical rectangle packing algorithm and a matrix reorderingalgorithm to show the derived features of a neuron cluster. We also propose abiclustering-based edge bundling method to reduce visual clutter caused by alarge number of connections between neurons. We evaluated our method on a setof CNNs and the results are generally favorable.", "authors": ["Mengchen Liu", "Jiaxin Shi", "Zhen Li", "Chongxuan Li", "Jun Zhu", "Shixia Liu"], "categories": ["cs.CV"]}
{"url": "http://arxiv.org/abs/1801.06889v3", "updated": "2018-05-14T04:59:24Z", "published": "2018-01-21T20:13:07Z", "title": "Visual Analytics in Deep Learning: An Interrogative Survey for the Next  Frontiers", "summary": "  Deep learning has recently seen rapid development and received significantattention due to its state-of-the-art performance on previously-thought hardproblems. However, because of the internal complexity and nonlinear structureof deep neural networks, the underlying decision making processes for why thesemodels are achieving such performance are challenging and sometimes mystifyingto interpret. As deep learning spreads across domains, it is of paramountimportance that we equip users of deep learning with tools for understandingwhen a model works correctly, when it fails, and ultimately how to improve itsperformance. Standardized toolkits for building neural networks have helpeddemocratize deep learning; visual analytics systems have now been developed tosupport model explanation, interpretation, debugging, and improvement. Wepresent a survey of the role of visual analytics in deep learning research,which highlights its short yet impactful history and thoroughly summarizes thestate-of-the-art using a human-centered interrogative framework, focusing onthe Five W's and How (Why, Who, What, How, When, and Where). We conclude byhighlighting research directions and open research problems. This survey helpsresearchers and practitioners in both visual analytics and deep learning toquickly learn key aspects of this young and rapidly growing body of research,whose impact spans a diverse range of domains.", "authors": ["Fred Hohman", "Minsuk Kahng", "Robert Pienta", "Duen Horng Chau"], "categories": ["cs.HC", "cs.AI", "cs.LG", "stat.ML", "H.5.2; I.5.1.d; I.6.9.c; I.6.9.f; I.2.6.g"]}
{"url": "http://arxiv.org/abs/1608.04366v2", "updated": "2016-11-19T16:24:27Z", "published": "2016-08-15T19:02:30Z", "title": "Infill Optimization for Additive Manufacturing -- Approaching Bone-like  Porous Structures", "summary": "  Porous structures such as trabecular bone are widely seen in nature. Thesestructures exhibit superior mechanical properties whilst being lightweight. Inthis paper, we present a method to generate bone-like porous structures aslightweight infill for additive manufacturing. Our method builds upon andextends voxel-wise topology optimization. In particular, for the purpose ofgenerating sparse yet stable structures distributed in the interior of a givenshape, we propose upper bounds on the localized material volume in theproximity of each voxel in the design domain. We then aggregate the localper-voxel constraints by their p-norm into an equivalent global constraint, inorder to facilitate an efficient optimization process. Implemented on ahigh-resolution topology optimization framework, our results demonstratemechanically optimized, detailed porous structures which mimic those found innature. We further show variants of the optimized structures subject todifferent design specifications, and analyze the optimality and robustness ofthe obtained structures.", "authors": ["Jun Wu", "Niels Aage", "Ruediger Westermann", "Ole Sigmund"], "categories": ["cs.GR"]}
{"url": "http://arxiv.org/abs/1705.04058v7", "updated": "2018-10-30T09:48:05Z", "published": "2017-05-11T08:08:44Z", "title": "Neural Style Transfer: A Review", "summary": "  The seminal work of Gatys et al. demonstrated the power of ConvolutionalNeural Networks (CNNs) in creating artistic imagery by separating andrecombining image content and style. This process of using CNNs to render acontent image in different styles is referred to as Neural Style Transfer(NST). Since then, NST has become a trending topic both in academic literatureand industrial applications. It is receiving increasing attention and a varietyof approaches are proposed to either improve or extend the original NSTalgorithm. In this paper, we aim to provide a comprehensive overview of thecurrent progress towards NST. We first propose a taxonomy of current algorithmsin the field of NST. Then, we present several evaluation methods and comparedifferent NST algorithms both qualitatively and quantitatively. The reviewconcludes with a discussion of various applications of NST and open problemsfor future research. A list of papers discussed in this review, correspondingcodes, pre-trained models and more comparison results are publicly available athttps://github.com/ycjing/Neural-Style-Transfer-Papers.", "authors": ["Yongcheng Jing", "Yezhou Yang", "Zunlei Feng", "Jingwen Ye", "Yizhou Yu", "Mingli Song"], "categories": ["cs.CV", "cs.NE", "eess.IV", "stat.ML"]}
{"url": "http://arxiv.org/abs/1606.07461v2", "updated": "2017-10-30T15:11:54Z", "published": "2016-06-23T20:20:39Z", "title": "LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in  Recurrent Neural Networks", "summary": "  Recurrent neural networks, and in particular long short-term memory (LSTM)networks, are a remarkably effective tool for sequence modeling that learn adense black-box hidden representation of their sequential input. Researchersinterested in better understanding these models have studied the changes inhidden state representations over time and noticed some interpretable patternsbut also significant noise. In this work, we present LSTMVIS, a visual analysistool for recurrent neural networks with a focus on understanding these hiddenstate dynamics. The tool allows users to select a hypothesis input range tofocus on local state changes, to match these states changes to similar patternsin a large data set, and to align these results with structural annotationsfrom their domain. We show several use cases of the tool for analyzing specifichidden state properties on dataset containing nesting, phrase structure, andchord progressions, and demonstrate how the tool can be used to isolatepatterns for further statistical analysis. We characterize the domain, thedifferent stakeholders, and their goals and tasks.", "authors": ["Hendrik Strobelt", "Sebastian Gehrmann", "Hanspeter Pfister", "Alexander M. Rush"], "categories": ["cs.CL", "cs.AI", "cs.NE"]}
{"url": "http://arxiv.org/abs/1512.01655v3", "updated": "2016-06-16T09:36:40Z", "published": "2015-12-05T12:05:52Z", "title": "Approximated and User Steerable tSNE for Progressive Visual Analytics", "summary": "  Progressive Visual Analytics aims at improving the interactivity in existinganalytics techniques by means of visualization as well as interaction withintermediate results. One key method for data analysis is dimensionalityreduction, for example, to produce 2D embeddings that can be visualized andanalyzed efficiently. t-Distributed Stochastic Neighbor Embedding (tSNE) is awell-suited technique for the visualization of several high-dimensional data.tSNE can create meaningful intermediate results but suffers from a slowinitialization that constrains its application in Progressive Visual Analytics.We introduce a controllable tSNE approximation (A-tSNE), which trades off speedand accuracy, to enable interactive data exploration. We offer real-timevisualization techniques, including a density-based solution and a Magic Lensto inspect the degree of approximation. With this feedback, the user can decideon local refinements and steer the approximation level during the analysis. Wedemonstrate our technique with several datasets, in a real-world researchscenario and for the real-time analysis of high-dimensional streams toillustrate its effectiveness for interactive data analysis.", "authors": ["Nicola Pezzotti", "Boudewijn P. F. Lelieveldt", "Laurens van der Maaten", "Thomas H\u00f6llt", "Elmar Eisemann", "Anna Vilanova"], "categories": ["cs.CV", "cs.LG"]}
{"url": "http://arxiv.org/abs/1710.06501v1", "updated": "2017-10-17T21:02:59Z", "published": "2017-10-17T21:02:59Z", "title": "Do Convolutional Neural Networks Learn Class Hierarchy?", "summary": "  Convolutional Neural Networks (CNNs) currently achieve state-of-the-artaccuracy in image classification. With a growing number of classes, theaccuracy usually drops as the possibilities of confusion increase.Interestingly, the class confusion patterns follow a hierarchical structureover the classes. We present visual-analytics methods to reveal and analyzethis hierarchy of similar classes in relation with CNN-internal data. We foundthat this hierarchy not only dictates the confusion patterns between theclasses, it furthermore dictates the learning behavior of CNNs. In particular,the early layers in these networks develop feature detectors that can separatehigh-level groups of classes quite well, even after a few training epochs. Incontrast, the latter layers require substantially more epochs to developspecialized feature detectors that can separate individual classes. Wedemonstrate how these insights are key to significant improvement in accuracyby designing hierarchy-aware CNNs that accelerate model convergence andalleviate overfitting. We further demonstrate how our methods help inidentifying various quality issues in the training data.", "authors": ["Bilal Alsallakh", "Amin Jourabloo", "Mao Ye", "Xiaoming Liu", "Liu Ren"], "categories": ["cs.CV", "I.4; I.5"]}
{"url": "http://arxiv.org/abs/1802.04233v1", "updated": "2018-02-12T18:31:24Z", "published": "2018-02-12T18:31:24Z", "title": "Embedding Complexity In the Data Representation Instead of In the Model:  A Case Study Using Heterogeneous Medical Data", "summary": "  Electronic Health Records have become popular sources of data for secondaryresearch, but their use is hampered by the amount of effort it takes toovercome the sparsity, irregularity, and noise that they contain. Modernlearning architectures can remove the need for expert-driven featureengineering, but not the need for expert-driven preprocessing to abstract awaythe inherent messiness of clinical data. This preprocessing effort is often thedominant component of a typical clinical prediction project. In this work wepropose using semantic embedding methods to directly couple the raw, messyclinical data to downstream learning architectures with truly minimalpreprocessing. We examine this step from the perspective of capturing andencoding complex data dependencies in the data representation instead of in themodel, which has the nice benefit of allowing downstream processing to be donewith fast, lightweight, and simple models accessible to researchers withoutmachine learning expertise. We demonstrate with three typical clinicalprediction tasks that the highly compressed, embedded data representationscapture a large amount of useful complexity, although in some cases thecompression is not completely lossless.", "authors": ["Jacek M. Bajor", "Diego A. Mesa", "Travis J. Osterman", "Thomas A. Lasko"], "categories": ["stat.AP"]}
{"url": "http://arxiv.org/abs/1506.05274v2", "updated": "2015-12-22T12:57:25Z", "published": "2015-06-17T10:47:20Z", "title": "Partial Functional Correspondence", "summary": "  In this paper, we propose a method for computing partial functionalcorrespondence between non-rigid shapes. We use perturbation analysis to showhow removal of shape parts changes the Laplace-Beltrami eigenfunctions, andexploit it as a prior on the spectral representation of the correspondence.Corresponding parts are optimization variables in our problem and are used toweight the functional correspondence; we are looking for the largest and mostregular (in the Mumford-Shah sense) parts that minimize correspondencedistortion. We show that our approach can cope with very challengingcorrespondence settings.", "authors": ["Emanuele Rodol\u00e0", "Luca Cosmo", "Michael M. Bronstein", "Andrea Torsello", "Daniel Cremers"], "categories": ["cs.CV"]}
{"url": "http://arxiv.org/abs/1802.07954v1", "updated": "2018-02-22T09:48:56Z", "published": "2018-02-22T09:48:56Z", "title": "The State of the Art in Integrating Machine Learning into Visual  Analytics", "summary": "  Visual analytics systems combine machine learning or other analytictechniques with interactive data visualization to promote sensemaking andanalytical reasoning. It is through such techniques that people can make senseof large, complex data. While progress has been made, the tactful combinationof machine learning and data visualization is still under-explored. Thisstate-of-the-art report presents a summary of the progress that has been madeby highlighting and synthesizing select research advances. Further, it presentsopportunities and challenges to enhance the synergy between machine learningand visual analytics for impactful future research directions.", "authors": ["A. Endert", "W. Ribarsky", "C. Turkay", "W Wong", "I. Nabney", "I D\u00edaz Blanco", "Fabrice Rossi"], "categories": ["stat.ML", "cs.HC", "cs.LG"]}
{"url": "http://arxiv.org/abs/1703.08014v2", "updated": "2017-03-24T08:24:07Z", "published": "2017-03-23T11:35:41Z", "title": "Sparse Inertial Poser: Automatic 3D Human Pose Estimation from Sparse  IMUs", "summary": "  We address the problem of making human motion capture in the wild morepractical by using a small set of inertial sensors attached to the body. Sincethe problem is heavily under-constrained, previous methods either use a largenumber of sensors, which is intrusive, or they require additional video input.We take a different approach and constrain the problem by: (i) making use of arealistic statistical body model that includes anthropometric constraints and(ii) using a joint optimization framework to fit the model to orientation andacceleration measurements over multiple frames. The resulting tracker SparseInertial Poser (SIP) enables 3D human pose estimation using only 6 sensors(attached to the wrists, lower legs, back and head) and works for arbitraryhuman motions. Experiments on the recently released TNT15 dataset show that,using the same number of sensors, SIP achieves higher accuracy than the datasetbaseline without using any video data. We further demonstrate the effectivenessof SIP on newly recorded challenging motions in outdoor scenarios such asclimbing or jumping over a wall.", "authors": ["Timo von Marcard", "Bodo Rosenhahn", "Michael J. Black", "Gerard Pons-Moll"], "categories": ["cs.CV", "cs.GR"]}
{"url": "http://arxiv.org/abs/1502.06686v1", "updated": "2015-02-24T04:30:43Z", "published": "2015-02-24T04:30:43Z", "title": "Data-Driven Shape Analysis and Processing", "summary": "  Data-driven methods play an increasingly important role in discoveringgeometric, structural, and semantic relationships between 3D shapes incollections, and applying this analysis to support intelligent modeling,editing, and visualization of geometric data. In contrast to traditionalapproaches, a key feature of data-driven approaches is that they aggregateinformation from a collection of shapes to improve the analysis and processingof individual shapes. In addition, they are able to learn models that reasonabout properties and relationships of shapes without relying on hard-codedrules or explicitly programmed instructions. We provide an overview of the mainconcepts and components of these techniques, and discuss their application toshape classification, segmentation, matching, reconstruction, modeling andexploration, as well as scene analysis and synthesis, through reviewing theliterature and relating the existing works with both qualitative and numericalcomparisons. We conclude our report with ideas that can inspire future researchin data-driven shape analysis and processing.", "authors": ["Kai Xu", "Vladimir G. Kim", "Qixing Huang", "Evangelos Kalogerakis"], "categories": ["cs.GR"]}
{"url": "http://arxiv.org/abs/1710.04954v4", "updated": "2018-06-19T13:35:22Z", "published": "2017-10-13T15:02:18Z", "title": "PCPNET: Learning Local Shape Properties from Raw Point Clouds", "summary": "  In this paper, we propose PCPNet, a deep-learning based approach forestimating local 3D shape properties in point clouds. In contrast to themajority of prior techniques that concentrate on global or mid-levelattributes, e.g., for shape classification or semantic labeling, we suggest apatch-based learning method, in which a series of local patches at multiplescales around each point is encoded in a structured manner. Our approach isespecially well-adapted for estimating local shape properties such as normals(both unoriented and oriented) and curvature from raw point clouds in thepresence of strong noise and multi-scale features. Our main contributionsinclude both a novel multi-scale variant of the recently proposed PointNetarchitecture with emphasis on local shape information, and a series of novelapplications in which we demonstrate how learning from training data arisingfrom well-structured triangle meshes, and applying the trained model to noisypoint clouds can produce superior results compared to specializedstate-of-the-art techniques. Finally, we demonstrate the utility of ourapproach in the context of shape reconstruction, by showing how it can be usedto extract normal orientation information from point clouds.", "authors": ["Paul Guerrero", "Yanir Kleiman", "Maks Ovsjanikov", "Niloy J. Mitra"], "categories": ["cs.CG"]}
{"url": "http://arxiv.org/abs/1705.03811v2", "updated": "2017-09-21T09:35:30Z", "published": "2017-05-10T15:12:14Z", "title": "From 3D Models to 3D Prints: an Overview of the Processing Pipeline", "summary": "  Due to the wide diffusion of 3D printing technologies, geometric algorithmsfor Additive Manufacturing are being invented at an impressive speed. Eachsingle step, in particular along the Process Planning pipeline, can now counton dozens of methods that prepare the 3D model for fabrication, while analysingand optimizing geometry and machine instructions for various objectives. Thisreport provides a classification of this huge state of the art, and elicits therelation between each single algorithm and a list of desirable objectivesduring Process Planning. The objectives themselves are listed and discussed,along with possible needs for tradeoffs. Additive Manufacturing technologiesare broadly categorized to explicitly relate classes of devices and supportedfeatures. Finally, this report offers an analysis of the state of the art whilediscussing open and challenging problems from both an academic and anindustrial perspective.", "authors": ["Marco Livesu", "Stefano Ellero", "Jon\u00e1s Mart\u00ecnez", "Sylvain Lefebvre", "Marco Attene"], "categories": ["cs.GR"]}
{"url": "http://arxiv.org/abs/1806.02071v2", "updated": "2019-02-01T14:44:57Z", "published": "2018-06-06T08:57:18Z", "title": "Deep Fluids: A Generative Network for Parameterized Fluid Simulations", "summary": "  This paper presents a novel generative model to synthesize fluid simulationsfrom a set of reduced parameters. A convolutional neural network is trained ona collection of discrete, parameterizable fluid simulation velocity fields. Dueto the capability of deep learning architectures to learn representativefeatures of the data, our generative model is able to accurately approximatethe training data set, while providing plausible interpolated in-betweens. Theproposed generative model is optimized for fluids by a novel loss function thatguarantees divergence-free velocity fields at all times. In addition, wedemonstrate that we can handle complex parameterizations in reduced spaces, andadvance simulations in time by integrating in the latent space with a secondnetwork. Our method models a wide variety of fluid behaviors, thus enablingapplications such as fast construction of simulations, interpolation of fluidswith different parameters, time re-sampling, latent space simulations, andcompression of fluid simulation data. Reconstructed velocity fields aregenerated up to 700x faster than re-simulating the data with the underlying CPUsolver, while achieving compression rates of up to 1300x.", "authors": ["Byungsoo Kim", "Vinicius C. Azevedo", "Nils Thuerey", "Theodore Kim", "Markus Gross", "Barbara Solenthaler"], "categories": ["cs.LG", "cs.GR", "physics.comp-ph", "physics.flu-dyn", "stat.ML"]}
{"url": "http://arxiv.org/abs/1910.02696v1", "updated": "2019-10-07T09:48:38Z", "published": "2019-10-07T09:48:38Z", "title": "Hierarchical stochastic neighbor embedding as a tool for visualizing the  encoding capability of magnetic resonance fingerprinting dictionaries", "summary": "  In Magnetic Resonance Fingerprinting (MRF) the quality of the estimatedparameter maps depends on the encoding capability of the variable flip angletrain. In this work we show how the dimensionality reduction techniqueHierarchical Stochastic Neighbor Embedding (HSNE) can be used to obtain insightinto the encoding capability of different MRF sequences. Embeddinghigh-dimensional MRF dictionaries into a lower-dimensional space andvisualizing them with colors, being a surrogate for location in low-dimensionalspace, provides a comprehensive overview of particular dictionaries and, inaddition, enables comparison of different sequences. Dictionaries for varioussequences and sequence lengths were compared to each other, and the effect oftransmit field variations on the encoding capability was assessed. Cleardifferences in encoding capability were observed between different sequences,and HSNE results accurately reflect those obtained from an MRF matchingsimulation.", "authors": ["Kirsten Koolstra", "Peter B\u00f6rnert", "Boudewijn Lelieveldt", "Andrew Webb", "Oleh Dzyubachyk"], "categories": ["eess.IV", "cs.CV"]}
{"url": "http://arxiv.org/abs/1802.10123v3", "updated": "2019-03-05T09:58:44Z", "published": "2018-02-27T19:18:43Z", "title": "Latent-space Physics: Towards Learning the Temporal Evolution of Fluid  Flow", "summary": "  We propose a method for the data-driven inference of temporal evolutions ofphysical functions with deep learning. More specifically, we target fluidflows, i.e. Navier-Stokes problems, and we propose a novel LSTM-based approachto predict the changes of pressure fields over time. The central challenge inthis context is the high dimensionality of Eulerian space-time data sets. Wedemonstrate for the first time that dense 3D+time functions of physics systemcan be predicted within the latent spaces of neural networks, and we arrive ata neural-network based simulation algorithm with significant practicalspeed-ups. We highlight the capabilities of our method with a series of complexliquid simulations, and with a set of single-phase buoyancy simulations. With aset of trained networks, our method is more than two orders of magnitudesfaster than a traditional pressure solver. Additionally, we present and discussa series of detailed evaluations for the different components of our algorithm.", "authors": ["Steffen Wiewel", "Moritz Becher", "Nils Thuerey"], "categories": ["cs.LG", "cs.GR"]}
{"url": "http://arxiv.org/abs/1806.04942v1", "updated": "2018-06-13T10:48:33Z", "published": "2018-06-13T10:48:33Z", "title": "Convolutional Sparse Coding for High Dynamic Range Imaging", "summary": "  Current HDR acquisition techniques are based on either (i) fusingmultibracketed, low dynamic range (LDR) images, (ii) modifying existinghardware and capturing different exposures simultaneously with multiplesensors, or (iii) reconstructing a single image with spatially-varying pixelexposures. In this paper, we propose a novel algorithm to recover high-qualityHDRI images from a single, coded exposure. The proposed reconstruction methodbuilds on recently-introduced ideas of convolutional sparse coding (CSC); thispaper demonstrates how to make CSC practical for HDR imaging. We demonstratethat the proposed algorithm achieves higher-quality reconstructions thanalternative methods, we evaluate optical coding schemes, analyze algorithmicparameters, and build a prototype coded HDR camera that demonstrates theutility of convolutional sparse HDRI coding with a custom hardware platform.", "authors": ["Ana Serrano", "Felix Heide", "Diego Gutierrez", "Gordon Wetzstein", "Belen Masia"], "categories": ["cs.CV", "cs.GR", "eess.IV"]}
{"url": "http://arxiv.org/abs/1909.07316v1", "updated": "2019-09-16T16:20:10Z", "published": "2019-09-16T16:20:10Z", "title": "Situational Awareness Enhanced through Social Media Analytics: A Survey  of First Responders", "summary": "  Social media data has been increasingly used to facilitate situationalawareness during events and emergencies such as natural disasters. Whileresearchers have investigated several methods to summarize, visualize or minethe data for analysis, first responders have not been able to fully leverageresearch advancements largely due to the gap between academic research anddeployed, functional systems. In this paper, we explore the opportunities andbarriers for the effective use of social media data from first responders'perspective. We present the summary of several detailed interviews with firstresponders on their use of social media for situational awareness. We furtherassess the impact of SMART-a social media visual analytics system-on firstresponder operations.", "authors": ["Luke S. Snyder", "Morteza Karimzadeh", "Christina Stober", "David S. Ebert"], "categories": ["cs.SI", "cs.CY"]}
{"url": "http://arxiv.org/abs/2001.00892v1", "updated": "2020-01-03T17:06:58Z", "published": "2020-01-03T17:06:58Z", "title": "Exploration of Interaction Techniques for Graph-based Modelling in  Virtual Reality", "summary": "  Editing and manipulating graph-based models within immersive environments islargely unexplored and certain design activities could benefit from using thosetechnologies. For example, in the case study of architectural modelling, the 3Dcontext of Virtual Reality naturally matches the intended output product, i.e.a 3D architectural geometry. Since both the state of the art and the state ofthe practice are lacking, we explore the field of VR-based interactivemodelling, and provide insights as to how to implement proper interactions inthat context, with broadly available devices. We consequently produce severalopen-source software prototypes for manipulating graph-based models in VR.", "authors": ["Adrien Coppens", "Berat Bicer", "Naz Yilmaz", "Serhat Aras"], "categories": ["cs.HC"]}
{"url": "http://arxiv.org/abs/1808.10587v1", "updated": "2018-08-31T03:33:21Z", "published": "2018-08-31T03:33:21Z", "title": "Geometric algebra and singularities of ruled and developable surfaces", "summary": "  Any ruled surface in Euclidean 3-space is described as a curve of unit dualvectors in the algebra of dual quaternions (=the even Clifford algebra of type(0,3,1)). Combining this classical framework and Singularity Theory, wecharacterize local diffeomorphic types of singular ruled surfaces in terms ofgeometric invariants. In particular, using a theorem of G. Ishikawa, we showthat local topological type of singular (non-cylindrical) developable surfacesis completely determined by vanishing order of the dual torsion, thatgeneralizes an old result of D. Mond for tangent developables of non-singularspace curves. Our approach would be useful for analysis on singularitiesarising in differential line geometry related with several applications such asrobotics, vision theory and architectural geometry.", "authors": ["Junki Tanaka", "Toru Ohmoto"], "categories": ["math.DG"]}
{"url": "http://arxiv.org/abs/1707.05738v2", "updated": "2017-10-26T20:52:51Z", "published": "2017-07-18T16:50:11Z", "title": "Engineering Er3+ placement and emission through chemically-synthesized  self-aligned SiC:Ox nanowire photonic crystal structures", "summary": "  High precision placement and integration of color centers in a silicon-basednanosystem, such as a nanowire (NW) array, exhibiting high integrationfunctionality and high photoluminescence (PL) yield can serve as a criticalbuilding block towards the practical realization of devices in the emergingfield of quantum technologies. Herein, we report on an innovative synthesisroute for realizing ultrathin silicon carbide (SiC) NW arrays doped with andwithout oxygen (SiC:Ox), and also erbium (Er). The arrays of thedeterministically positioned NWs are grown in a self-aligned manner throughchemical-vapor-deposition (CVD). A key enabler of this synthesis route is thatSiC:Ox NW photonic crystal (PC) nanostructures are engineered with tailoredgeometry in precise locations during nanofabrication. These ultrathin NW PCstructures not only facilitate the on-demand positioning of Er3+ ions but arepivotal in engineering the emission properties of these color centers. Througha combinational and systematic micro-PL (uPL) and power-dependence PL (PDPL)spectroscopy, PC architecture geometry effects on Er3+-related 1538 nmemission, which is the telecommunication wavelength used in optical fibers,were studied. Approximately 60-fold and 30-fold enhancements for, respectively,the room-temperature Er3+ PL emission and lifetime in the NW PC sample wereobserved compared to its thin-film analog. Furthermore, the 1538 nm emission inSiC:Ox NW PC was found to be modulated linearly with the PC lattice periodicityof the structure. The observed characteristics reveal the efficientEr3+-emission extraction from the technologically-friendly SiC:Ox NW PCstructures.", "authors": ["Natasha Tabassum", "Vasileios Nikas", "Brian Ford", "Edward Crawford", "Spyros Gallis"], "categories": ["cond-mat.mes-hall"]}
{"url": "http://arxiv.org/abs/1703.05318v1", "updated": "2017-03-15T17:28:40Z", "published": "2017-03-15T17:28:40Z", "title": "Smooth polyhedral surfaces", "summary": "  Polyhedral surfaces are fundamental objects in architectural geometry andindustrial design. Whereas closeness of a given mesh to a smooth referencesurface and its suitability for numerical simulations were already studiedextensively, the aim of our work is to find and to discuss suitable assessmentsof smoothness of polyhedral surfaces that only take the geometry of thepolyhedral surface itself into account. Motivated by analogies to classicaldifferential geometry, we propose a theory of smoothness of polyhedral surfacesincluding suitable notions of normal vectors, tangent planes, asymptoticdirections, and parabolic curves that are invariant under projectivetransformations. It is remarkable that seemingly mild conditions significantlylimit the shapes of faces of a smooth polyhedral surface. Besides being oftheoretical interest, we believe that smoothness of polyhedral surfaces is ofinterest in the architectural context, where vertices and edges of polyhedralsurfaces are highly visible.", "authors": ["Felix G\u00fcnther", "Caigui Jiang", "Helmut Pottmann"], "categories": ["math.MG", "math.DG", "52B70, 53A05"]}
{"url": "http://arxiv.org/abs/1710.02322v1", "updated": "2017-10-06T09:27:44Z", "published": "2017-10-06T09:27:44Z", "title": "Human Pose Regression by Combining Indirect Part Detection and  Contextual Information", "summary": "  In this paper, we propose an end-to-end trainable regression approach forhuman pose estimation from still images. We use the proposed Soft-argmaxfunction to convert feature maps directly to joint coordinates, resulting in afully differentiable framework. Our method is able to learn heat mapsrepresentations indirectly, without additional steps of artificial ground truthgeneration. Consequently, contextual information can be included to the posepredictions in a seamless way. We evaluated our method on two very challengingdatasets, the Leeds Sports Poses (LSP) and the MPII Human Pose datasets,reaching the best performance among all the existing regression methods andcomparable results to the state-of-the-art detection based approaches.", "authors": ["Diogo C. Luvizon", "Hedi Tabia", "David Picard"], "categories": ["cs.CV"]}
{"url": "http://arxiv.org/abs/1710.02634v1", "updated": "2017-10-07T04:55:17Z", "published": "2017-10-07T04:55:17Z", "title": "Notions of optimal transport theory and how to implement them on a  computer", "summary": "  This article gives an introduction to optimal transport, a mathematicaltheory that makes it possible to measure distances between functions (ordistances between more general objects), to interpolate between objects or toenforce mass/volume conservation in certain computational physics simulations.Optimal transport is a rich scientific domain, with active researchcommunities, both on its theoretical aspects and on more applicativeconsiderations, such as geometry processing and machine learning. This articleaims at explaining the main principles behind the theory of optimal transport,introduce the different involved notions, and more importantly, how theyrelate, to let the reader grasp an intuition of the elegant theory thatstructures them. Then we will consider a specific setting, calledsemi-discrete, where a continuous function is transported to a discrete sum ofDirac masses. Studying this specific setting naturally leads to an efficientcomputational algorithm, that uses classical notions of computational geometry,such as a generalization of Voronoi diagrams called Laguerre diagrams.", "authors": ["Bruno Levy", "Erica Schwindt"], "categories": ["math.AP", "math.NA", "49M15, 35J96, 65D18"]}
{"url": "http://arxiv.org/abs/1702.08675v3", "updated": "2018-05-26T01:45:21Z", "published": "2017-02-28T07:26:55Z", "title": "3D Shape Segmentation via Shape Fully Convolutional Networks", "summary": "  We desgin a novel fully convolutional network architecture for shapes,denoted by Shape Fully Convolutional Networks (SFCN). 3D shapes are representedas graph structures in the SFCN architecture, based on novel graph convolutionand pooling operations, which are similar to convolution and pooling operationsused on images. Meanwhile, to build our SFCN architecture in the original imagesegmentation fully convolutional network (FCN) architecture, we also design andimplement a generating operation} with bridging function. This ensures that theconvolution and pooling operation we have designed can be successfully appliedin the original FCN architecture. In this paper, we also present a new shapesegmentation approach based on SFCN. Furthermore, we allow more general andchallenging input, such as mixed datasets of different categories of shapes}which can prove the ability of our generalisation. In our approach, SFCNs aretrained triangles-to-triangles by using three low-level geometric features asinput. Finally, the feature voting-based multi-label graph cuts is adopted tooptimise the segmentation results obtained by SFCN prediction. The experimentresults show that our method can effectively learn and predict mixed shapedatasets of either similar or different characteristics, and achieve excellentsegmentation results.", "authors": ["Pengyu Wang", "Yuan Gan", "Panpan Shui", "Fenggen Yu", "Yan Zhang", "Songle Chen", "Zhengxing Sun"], "categories": ["cs.CV"]}
{"url": "http://arxiv.org/abs/1804.02527v1", "updated": "2018-04-07T07:52:04Z", "published": "2018-04-07T07:52:04Z", "title": "Visual Analytics for Explainable Deep Learning", "summary": "  Recently, deep learning has been advancing the state of the art in artificialintelligence to a new level, and humans rely on artificial intelligencetechniques more than ever. However, even with such unprecedented advancements,the lack of explanation regarding the decisions made by deep learning modelsand absence of control over their internal processes act as major drawbacks incritical decision-making processes, such as precision medicine and lawenforcement. In response, efforts are being made to make deep learninginterpretable and controllable by humans. In this paper, we review visualanalytics, information visualization, and machine learning perspectivesrelevant to this aim, and discuss potential challenges and future researchdirections.", "authors": ["Jaegul Choo", "Shixia Liu"], "categories": ["cs.HC", "cs.LG", "stat.ML", "I.6.9.c"]}
{"url": "http://arxiv.org/abs/1407.6175v4", "updated": "2014-12-13T22:51:56Z", "published": "2014-07-23T11:11:11Z", "title": "Analysis-suitable adaptive T-mesh refinement with linear complexity", "summary": "  We present an efficient adaptive refinement procedure that preservesanalysis-suitability of the T-mesh, this is, the linear independence of theT-spline blending functions. We prove analysis-suitability of the overlays andboundedness of their cardinalities, nestedness of the generated T-splinespaces, and linear computational complexity of the refinement procedure interms of the number of marked and generated mesh elements.", "authors": ["Philipp Morgenstern", "Daniel Peterseim"], "categories": ["math.NA", "65D17, 65N30, 65N50"]}
{"url": "http://arxiv.org/abs/1410.2729v1", "updated": "2014-10-10T10:30:14Z", "published": "2014-10-10T10:30:14Z", "title": "Convergence of univariate non-stationary subdivision schemes via  asymptotical similarity", "summary": "  A new equivalence notion between non-stationary subdivision schemes, termedasymptotical similarity, which is weaker than asymptotical equivalence, isintroduced and studied. It is known that asymptotical equivalence between anon-stationary subdivision scheme and a convergent stationary scheme guaranteesthe convergence of the non-stationary scheme. We show that for non-stationaryschemes reproducing constants, the condition of asymptotical equivalence can berelaxed to asymptotical similarity. This result applies to a wide class ofnon-stationary schemes of importance in theory and applications.", "authors": ["Costanza Conti", "Nira Dyn", "Carla Manni", "Marie-Laurence Mazure"], "categories": ["math.NA"]}
{"url": "http://arxiv.org/abs/1710.05488v2", "updated": "2017-12-19T04:28:31Z", "published": "2017-10-16T03:30:09Z", "title": "A Geometric View of Optimal Transportation and Generative Model", "summary": "  In this work, we show the intrinsic relations between optimal transportationand convex geometry, especially the variational approach to solve Alexandrovproblem: constructing a convex polytope with prescribed face normals andvolumes. This leads to a geometric interpretation to generative models, andleads to a novel framework for generative models. By using the optimaltransportation view of GAN model, we show that the discriminator computes theKantorovich potential, the generator calculates the transportation map. For alarge class of transportation costs, the Kantorovich potential can give theoptimal transportation map by a close-form formula. Therefore, it is sufficientto solely optimize the discriminator. This shows the adversarial competitioncan be avoided, and the computational architecture can be simplified.Preliminary experimental results show the geometric method outperforms WGAN forapproximating probability measures with multiple clusters in low dimensionalspace.", "authors": ["Na Lei", "Kehua Su", "Li Cui", "Shing-Tung Yau", "David Xianfeng Gu"], "categories": ["cs.LG", "stat.ML"]}
{"url": "http://arxiv.org/abs/1509.05566v1", "updated": "2015-09-18T09:59:14Z", "published": "2015-09-18T09:59:14Z", "title": "Complexity of hierarchical refinement for a class of admissible mesh  configurations", "summary": "  An adaptive isogeometric method based on $d$-variate hierarchical splineconstructions can be derived by considering a refine module that preserves acertain class of admissibility between two consecutive steps of the adaptiveloop [6]. In this paper we provide a complexity estimate, i.e., an estimate onhow the number of mesh elements grows with respect to the number of elementsthat are marked for refinement by the adaptive strategy. Our estimate is in theline of the similar ones proved in the finite element context, [3,24].", "authors": ["Annalisa Buffa", "Carlotta Giannelli", "Philipp Morgenstern", "Daniel Peterseim"], "categories": ["math.NA"]}
{"url": "http://arxiv.org/abs/1609.06536v2", "updated": "2017-06-02T13:54:51Z", "published": "2016-09-21T12:55:59Z", "title": "Production-Level Facial Performance Capture Using Deep Convolutional  Neural Networks", "summary": "  We present a real-time deep learning framework for video-based facialperformance capture -- the dense 3D tracking of an actor's face given amonocular video. Our pipeline begins with accurately capturing a subject usinga high-end production facial capture pipeline based on multi-view stereotracking and artist-enhanced animations. With 5-10 minutes of captured footage,we train a convolutional neural network to produce high-quality output,including self-occluded regions, from a monocular video sequence of thatsubject. Since this 3D facial performance capture is fully automated, oursystem can drastically reduce the amount of labor involved in the developmentof modern narrative-driven video games or films involving realistic digitaldoubles of actors and potentially hours of animated dialogue per character. Wecompare our results with several state-of-the-art monocular real-time facialcapture techniques and demonstrate compelling animation inference inchallenging areas such as eyes and lips.", "authors": ["Samuli Laine", "Tero Karras", "Timo Aila", "Antti Herva", "Shunsuke Saito", "Ronald Yu", "Hao Li", "Jaakko Lehtinen"], "categories": ["cs.CV", "cs.GR"]}
{"url": "http://arxiv.org/abs/1611.01055v1", "updated": "2016-11-03T15:15:00Z", "published": "2016-11-03T15:15:00Z", "title": "Learning Locomotion Skills Using DeepRL: Does the Choice of Action Space  Matter?", "summary": "  The use of deep reinforcement learning allows for high-dimensional statedescriptors, but little is known about how the choice of action representationimpacts the learning difficulty and the resulting performance. We compare theimpact of four different action parameterizations (torques, muscle-activations,target joint angles, and target joint-angle velocities) in terms of learningtime, policy robustness, motion quality, and policy query rates. Our resultsare evaluated on a gait-cycle imitation task for multiple planar articulatedfigures and multiple gaits. We demonstrate that the local feedback provided byhigher-level action parameterizations can significantly impact the learning,robustness, and quality of the resulting policies.", "authors": ["Xue Bin Peng", "Michiel van de Panne"], "categories": ["cs.LG", "cs.GR", "cs.RO"]}
{"url": "http://arxiv.org/abs/1802.02673v2", "updated": "2018-02-20T01:58:15Z", "published": "2018-02-07T23:37:20Z", "title": "Position-Based Multi-Agent Dynamics for Real-Time Crowd Simulation (MiG  paper)", "summary": "  Exploiting the efficiency and stability of Position-Based Dynamics (PBD), weintroduce a novel crowd simulation method that runs at interactive rates forhundreds of thousands of agents. Our method enables the detailed modeling ofper-agent behavior in a Lagrangian formulation. We model short-range andlong-range collision avoidance to simulate both sparse and dense crowds. On theparticles representing agents, we formulate a set of positional constraintsthat can be readily integrated into a standard PBD solver. We augment thetentative particle motions with planning velocities to determine the preferredvelocities of agents, and project the positions onto the constraint manifold toeliminate colliding configurations. The local short-range interaction isrepresented with collision and frictional contact between agents, as in thediscrete simulation of granular materials. We incorporate a cohesion model formodeling collective behaviors and propose a new constraint for dealing withpotential future collisions. Our new method is suitable for use in interactivegames.", "authors": ["Tomer Weiss", "Alan Litteneker", "Chenfanfu Jiang", "Demetri Terzopoulos"], "categories": ["cs.GR"]}
{"url": "http://arxiv.org/abs/1802.02731v1", "updated": "2018-02-08T07:35:43Z", "published": "2018-02-08T07:35:43Z", "title": "Topologically Controlled Lossy Compression", "summary": "  This paper presents a new algorithm for the lossy compression of scalar datadefined on 2D or 3D regular grids, with topological control. Certain techniquesallow users to control the pointwise error induced by the compression. However,in many scenarios it is desirable to control in a similar way the preservationof higher-level notions, such as topological features , in order to provideguarantees on the outcome of post-hoc data analyses. This paper presents thefirst compression technique for scalar data which supports a strictlycontrolled loss of topological features. It provides users with specificguarantees both on the preservation of the important features and on the sizeof the smaller features destroyed during compression. In particular, we presenta simple compression strategy based on a topologically adaptive quantization ofthe range. Our algorithm provides strong guarantees on the bottleneck distancebetween persistence diagrams of the input and decompressed data, specificallythose associated with extrema. A simple extension of our strategy additionallyenables a control on the pointwise error. We also show how to combine ourapproach with state-of-the-art compressors, to further improve the geometricalreconstruction. Extensive experiments, for comparable compression rates,demonstrate the superiority of our algorithm in terms of the preservation oftopological features. We show the utility of our approach by illustrating thecompatibility between the output of post-hoc topological data analysispipelines, executed on the input and decompressed data, for simulated oracquired data sets. We also provide a lightweight VTK-based C++ implementationof our approach for reproduction purposes.", "authors": ["Maxime Soler", "Melanie Plainchault", "Bruno Conche", "Julien Tierny"], "categories": ["eess.IV", "cs.CG", "cs.CV", "cs.GR"]}
{"url": "http://arxiv.org/abs/1705.01968v3", "updated": "2017-10-01T22:24:17Z", "published": "2017-05-04T18:24:38Z", "title": "A Workflow for Visual Diagnostics of Binary Classifiers using  Instance-Level Explanations", "summary": "  Human-in-the-loop data analysis applications necessitate greater transparencyin machine learning models for experts to understand and trust their decisions.To this end, we propose a visual analytics workflow to help data scientists anddomain experts explore, diagnose, and understand the decisions made by a binaryclassifier. The approach leverages \"instance-level explanations\", measures oflocal feature relevance that explain single instances, and uses them to build aset of visual representations that guide the users in their investigation. Theworkflow is based on three main visual representations and steps: one based onaggregate statistics to see how data distributes across correct / incorrectdecisions; one based on explanations to understand which features are used tomake these decisions; and one based on raw data, to derive insights onpotential root causes for the observed patterns. The workflow is derived from along-term collaboration with a group of machine learning and healthcareprofessionals who used our method to make sense of machine learning models theydeveloped. The case study from this collaboration demonstrates that theproposed workflow helps experts derive useful knowledge about the model and thephenomena it describes, thus experts can generate useful hypotheses on how amodel can be improved.", "authors": ["Josua Krause", "Aritra Dasgupta", "Jordan Swartz", "Yindalon Aphinyanaphongs", "Enrico Bertini"], "categories": ["stat.ML", "cs.AI"]}
{"url": "http://arxiv.org/abs/1504.02218v2", "updated": "2017-01-02T20:27:53Z", "published": "2015-04-09T07:58:01Z", "title": "Evaluating Cartogram Effectiveness", "summary": "  Cartograms are maps in which areas of geographic regions (countries, states)appear in proportion to some variable of interest (population, income).Cartograms are popular visualizations for geo-referenced data that have beenused for over a century and that make it possible to gain insight into patternsand trends in the world around us. Despite the popularity of cartograms and thelarge number of cartogram types, there are few studies evaluating theeffectiveness of cartograms in conveying information. Based on a recent tasktaxonomy for cartograms, we evaluate four major different types of cartograms:contiguous, non-contiguous, rectangular, and Dorling cartograms. Specifically,we evaluate the effectiveness of these cartograms by quantitative performanceanalysis, as well as by subjective preferences. We analyze the results of ourstudy in the context of some prevailing assumptions in the literature ofcartography and cognitive science. Finally, we make recommendations for the useof different types of cartograms for different tasks and settings.", "authors": ["Sabrina Nusrat", "Md. Jawaherul Alam", "Stephen G. Kobourov"], "categories": ["cs.HC"]}
{"url": "http://arxiv.org/abs/1705.11050v2", "updated": "2018-02-01T13:29:19Z", "published": "2017-05-31T12:10:32Z", "title": "3D Mesh Segmentation via Multi-branch 1D Convolutional Neural Networks", "summary": "  There is an increasing interest in applying deep learning to 3D meshsegmentation. We observe that 1) existing feature-based techniques are oftenslow or sensitive to feature resizing, 2) there are minimal comparative studiesand 3) techniques often suffer from reproducibility issue. This studycontributes in two ways. First, we propose a novel convolutional neural network(CNN) for mesh segmentation. It uses 1D data, filters and a multi-brancharchitecture for separate training of multi-scale features. Together with anovel way of computing conformal factor (CF), our technique clearlyout-performs existing work. Secondly, we publicly provide implementations ofseveral deep learning techniques, namely, neural networks (NNs), autoencoders(AEs) and CNNs, whose architectures are at least two layers deep. Thesignificance of this study is that it proposes a robust form of CF, offers anovel and accurate CNN technique, and a comprehensive study of several deeplearning techniques for baseline comparison.", "authors": ["David George", "Xianghua Xie", "Gary KL Tam"], "categories": ["cs.GR"]}
{"url": "http://arxiv.org/abs/1505.01214v1", "updated": "2015-05-05T22:59:32Z", "published": "2015-05-05T22:59:32Z", "title": "Learning Style Similarity for Searching Infographics", "summary": "  Infographics are complex graphic designs integrating text, images, charts andsketches. Despite the increasing popularity of infographics and the rapidgrowth of online design portfolios, little research investigates how we cantake advantage of these design resources. In this paper we present a method formeasuring the style similarity between infographics. Based on human perceptiondata collected from crowdsourced experiments, we use computer vision andmachine learning algorithms to learn a style similarity metric for infographicdesigns. We evaluate different visual features and learning algorithms and findthat a combination of color histograms and Histograms-of-Gradients (HoG)features is most effective in characterizing the style of infographics. Wedemonstrate our similarity metric on a preliminary image retrieval test.", "authors": ["Babak Saleh", "Mira Dontcheva", "Aaron Hertzmann", "Zhicheng Liu"], "categories": ["cs.GR", "cs.CV", "cs.HC", "cs.IR", "cs.MM"]}
{"url": "http://arxiv.org/abs/1611.00939v1", "updated": "2016-11-03T10:11:10Z", "published": "2016-11-03T10:11:10Z", "title": "Recent Advances in Transient Imaging: A Computer Graphics and Vision  Perspective", "summary": "  Transient imaging has recently made a huge impact in the computer graphicsand computer vision fields. By capturing, reconstructing, or simulating lighttransport at extreme temporal resolutions, researchers have proposed noveltechniques to show movies of light in motion, see around corners, detectobjects in highly-scattering media, or infer material properties from adistance, to name a few. The key idea is to leverage the wealth of informationin the temporal domain at the pico or nanosecond resolution, informationusually lost during the capture-time temporal integration. This paper presentsrecent advances in this field of transient imaging from a graphics and visionperspective, including capture techniques, analysis, applications andsimulation.", "authors": ["Adrian Jarabo", "Belen Masia", "Julio Marco", "Diego Gutierrez"], "categories": ["cs.CV", "cs.GR"]}
{"url": "http://arxiv.org/abs/1709.08774v1", "updated": "2017-09-26T01:15:26Z", "published": "2017-09-26T01:15:26Z", "title": "Exploring the Design Space of Immersive Urban Analytics", "summary": "  Recent years have witnessed the rapid development and wide adoption ofimmersive head-mounted devices, such as HTC VIVE, Oculus Rift, and MicrosoftHoloLens. These immersive devices have the potential to significantly extendthe methodology of urban visual analytics by providing critical 3D contextinformation and creating a sense of presence. In this paper, we propose antheoretical model to characterize the visualizations in immersive urbananalytics. Further more, based on our comprehensive and concise model, wecontribute a typology of combination methods of 2D and 3D visualizations thatdistinguish between linked views, embedded views, and mixed views. We alsopropose a supporting guideline to assist users in selecting a proper view undercertain circumstances by considering visual geometry and spatial distributionof the 2D and 3D visualizations. Finally, based on existing works, possiblefuture research opportunities are explored and discussed.", "authors": ["Zhutian Chen", "Yifang Wang", "Tianchen Sun", "Xiang Gao", "Wei Chen", "Zhigeng Pan", "Huamin Qu", "Yingcai Wu"], "categories": ["cs.GR", "cs.HC"]}
{"url": "http://arxiv.org/abs/1809.00270v1", "updated": "2018-09-02T00:03:54Z", "published": "2018-09-02T00:03:54Z", "title": "Exploring the Limits of Complexity: A Survey of Empirical Studies on  Graph Visualisation", "summary": "  For decades, researchers in information visualisation and graph drawing havefocused on developing techniques for the layout and display of very large andcomplex networks. Experiments involving human participants have also exploredthe readability of different styles of layout and representations for suchnetworks. In both bodies of literature, networks are frequently referred to asbeing 'large' or 'complex', yet these terms are relative. From a human-centred,experiment point-of-view, what constitutes 'large' (for example) depends onseveral factors, such as data complexity, visual complexity, and the technologyused. In this paper, we survey the literature on human-centred experiments tounderstand how, in practice, different features and characteristics ofnode-link diagrams affect visual complexity.", "authors": ["Vahan Yoghourdjian", "Daniel Archambault", "Stephan Diehl", "Tim Dwyer", "Karsten Klein", "Helen C. Purchase", "Hsiang-Yun Wu"], "categories": ["cs.HC"]}
{"url": "http://arxiv.org/abs/1711.06363v2", "updated": "2018-03-10T18:12:27Z", "published": "2017-11-17T00:58:53Z", "title": "3D Reconstruction of Incomplete Archaeological Objects Using a  Generative Adversarial Network", "summary": "  We introduce a data-driven approach to aid the repairing and conservation ofarchaeological objects: ORGAN, an object reconstruction generative adversarialnetwork (GAN). By using an encoder-decoder 3D deep neural network on a GANarchitecture, and combining two loss objectives: a completion loss and anImproved Wasserstein GAN loss, we can train a network to effectively predictthe missing geometry of damaged objects. As archaeological objects can greatlydiffer between them, the network is conditioned on a variable, which can be aculture, a region or any metadata of the object. In our results, we show thatour method can recover most of the information from damaged objects, even incases where more than half of the voxels are missing, without producing manyerrors.", "authors": ["Renato Hermoza", "Ivan Sipiran"], "categories": ["cs.CV", "cs.AI"]}
{"url": "http://arxiv.org/abs/1207.3502v2", "updated": "2017-07-22T22:23:49Z", "published": "2012-07-15T13:06:26Z", "title": "A Simple and Correct Even-Odd Algorithm for the Point-in-Polygon Problem  for Complex Polygons", "summary": "  Determining if a point is in a polygon or not is used by a lot ofapplications in computer graphics, computer games and geoinformatics.Implementing this check is error-prone since there are many special cases to beconsidered. This holds true in particular for complex polygons whose edgesintersect each other creating holes. In this paper we present a simple even-oddalgorithm to solve this problem for complex polygons in linear time and proveits correctness for all possible points and polygons. We furthermore provideexamples and implementation notes for this algorithm.", "authors": ["Michael Galetzka", "Patrick O. Glauner"], "categories": ["cs.CG"]}
{"url": "http://arxiv.org/abs/1801.07829v2", "updated": "2019-06-11T06:11:21Z", "published": "2018-01-24T01:14:04Z", "title": "Dynamic Graph CNN for Learning on Point Clouds", "summary": "  Point clouds provide a flexible geometric representation suitable forcountless applications in computer graphics; they also comprise the raw outputof most 3D data acquisition devices. While hand-designed features on pointclouds have long been proposed in graphics and vision, however, the recentoverwhelming success of convolutional neural networks (CNNs) for image analysissuggests the value of adapting insight from CNN to the point cloud world. Pointclouds inherently lack topological information so designing a model to recovertopology can enrich the representation power of point clouds. To this end, wepropose a new neural network module dubbed EdgeConv suitable for CNN-basedhigh-level tasks on point clouds including classification and segmentation.EdgeConv acts on graphs dynamically computed in each layer of the network. Itis differentiable and can be plugged into existing architectures. Compared toexisting modules operating in extrinsic space or treating each pointindependently, EdgeConv has several appealing properties: It incorporates localneighborhood information; it can be stacked applied to learn global shapeproperties; and in multi-layer systems affinity in feature space capturessemantic characteristics over potentially long distances in the originalembedding. We show the performance of our model on standard benchmarksincluding ModelNet40, ShapeNetPart, and S3DIS.", "authors": ["Yue Wang", "Yongbin Sun", "Ziwei Liu", "Sanjay E. Sarma", "Michael M. Bronstein", "Justin M. Solomon"], "categories": ["cs.CV"]}
{"url": "http://arxiv.org/abs/1705.01583v1", "updated": "2017-05-03T19:13:23Z", "published": "2017-05-03T19:13:23Z", "title": "VNect: Real-time 3D Human Pose Estimation with a Single RGB Camera", "summary": "  We present the first real-time method to capture the full global 3D skeletalpose of a human in a stable, temporally consistent manner using a single RGBcamera. Our method combines a new convolutional neural network (CNN) based poseregressor with kinematic skeleton fitting. Our novel fully-convolutional poseformulation regresses 2D and 3D joint positions jointly in real time and doesnot require tightly cropped input frames. A real-time kinematic skeletonfitting method uses the CNN output to yield temporally stable 3D global posereconstructions on the basis of a coherent kinematic skeleton. This makes ourapproach the first monocular RGB method usable in real-time applications suchas 3D character control---thus far, the only monocular methods for suchapplications employed specialized RGB-D cameras. Our method's accuracy isquantitatively on par with the best offline 3D monocular RGB pose estimationmethods. Our results are qualitatively comparable to, and sometimes betterthan, results from monocular RGB-D approaches, such as the Kinect. However, weshow that our approach is more broadly applicable than RGB-D solutions, i.e. itworks for outdoor scenes, community videos, and low quality commodity RGBcameras.", "authors": ["Dushyant Mehta", "Srinath Sridhar", "Oleksandr Sotnychenko", "Helge Rhodin", "Mohammad Shafiei", "Hans-Peter Seidel", "Weipeng Xu", "Dan Casas", "Christian Theobalt"], "categories": ["cs.CV", "cs.GR"]}
{"url": "http://arxiv.org/abs/1712.01537v1", "updated": "2017-12-05T09:25:19Z", "published": "2017-12-05T09:25:19Z", "title": "O-CNN: Octree-based Convolutional Neural Networks for 3D Shape Analysis", "summary": "  We present O-CNN, an Octree-based Convolutional Neural Network (CNN) for 3Dshape analysis. Built upon the octree representation of 3D shapes, our methodtakes the average normal vectors of a 3D model sampled in the finest leafoctants as input and performs 3D CNN operations on the octants occupied by the3D shape surface. We design a novel octree data structure to efficiently storethe octant information and CNN features into the graphics memory and executethe entire O-CNN training and evaluation on the GPU. O-CNN supports various CNNstructures and works for 3D shapes in different representations. By restrainingthe computations on the octants occupied by 3D surfaces, the memory andcomputational costs of the O-CNN grow quadratically as the depth of the octreeincreases, which makes the 3D CNN feasible for high-resolution 3D models. Wecompare the performance of the O-CNN with other existing 3D CNN solutions anddemonstrate the efficiency and efficacy of O-CNN in three shape analysis tasks,including object classification, shape retrieval, and shape segmentation.", "authors": ["Peng-Shuai Wang", "Yang Liu", "Yu-Xiao Guo", "Chun-Yu Sun", "Xin Tong"], "categories": ["cs.CV"]}
{"url": "http://arxiv.org/abs/1609.02974v1", "updated": "2016-09-09T23:33:38Z", "published": "2016-09-09T23:33:38Z", "title": "Learning-Based View Synthesis for Light Field Cameras", "summary": "  With the introduction of consumer light field cameras, light field imaginghas recently become widespread. However, there is an inherent trade-off betweenthe angular and spatial resolution, and thus, these cameras often sparselysample in either spatial or angular domain. In this paper, we use machinelearning to mitigate this trade-off. Specifically, we propose a novellearning-based approach to synthesize new views from a sparse set of inputviews. We build upon existing view synthesis techniques and break down theprocess into disparity and color estimation components. We use two sequentialconvolutional neural networks to model these two components and train bothnetworks simultaneously by minimizing the error between the synthesized andground truth images. We show the performance of our approach using only fourcorner sub-aperture views from the light fields captured by the Lytro Illumcamera. Experimental results show that our approach synthesizes high-qualityimages that are superior to the state-of-the-art techniques on a variety ofchallenging real-world scenes. We believe our method could potentially decreasethe required angular resolution of consumer light field cameras, which allowstheir spatial resolution to increase.", "authors": ["Nima Khademi Kalantari", "Ting-Chun Wang", "Ravi Ramamoorthi"], "categories": ["cs.CV", "cs.GR", "I.4.1"]}
{"url": "http://arxiv.org/abs/1804.02717v3", "updated": "2018-07-27T03:44:10Z", "published": "2018-04-08T17:04:58Z", "title": "DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based  Character Skills", "summary": "  A longstanding goal in character animation is to combine data-drivenspecification of behavior with a system that can execute a similar behavior ina physical simulation, thus enabling realistic responses to perturbations andenvironmental variation. We show that well-known reinforcement learning (RL)methods can be adapted to learn robust control policies capable of imitating abroad range of example motion clips, while also learning complex recoveries,adapting to changes in morphology, and accomplishing user-specified goals. Ourmethod handles keyframed motions, highly-dynamic actions such asmotion-captured flips and spins, and retargeted motions. By combining amotion-imitation objective with a task objective, we can train characters thatreact intelligently in interactive settings, e.g., by walking in a desireddirection or throwing a ball at a user-specified target. This approach thuscombines the convenience and motion quality of using motion clips to define thedesired style and appearance, with the flexibility and generality afforded byRL methods and physics-based animation. We further explore a number of methodsfor integrating multiple clips into the learning process to developmulti-skilled agents capable of performing a rich repertoire of diverse skills.We demonstrate results using multiple characters (human, Atlas robot, bipedaldinosaur, dragon) and a large variety of skills, including locomotion,acrobatics, and martial arts.", "authors": ["Xue Bin Peng", "Pieter Abbeel", "Sergey Levine", "Michiel van de Panne"], "categories": ["cs.GR", "cs.AI", "cs.LG"]}
{"url": "http://arxiv.org/abs/1705.01088v2", "updated": "2017-06-06T15:16:19Z", "published": "2017-05-02T17:44:01Z", "title": "Visual Attribute Transfer through Deep Image Analogy", "summary": "  We propose a new technique for visual attribute transfer across images thatmay have very different appearance but have perceptually similar semanticstructure. By visual attribute transfer, we mean transfer of visual information(such as color, tone, texture, and style) from one image to another. Forexample, one image could be that of a painting or a sketch while the other is aphoto of a real scene, and both depict the same type of scene.  Our technique finds semantically-meaningful dense correspondences between twoinput images. To accomplish this, it adapts the notion of \"image analogy\" withfeatures extracted from a Deep Convolutional Neutral Network for matching; wecall our technique Deep Image Analogy. A coarse-to-fine strategy is used tocompute the nearest-neighbor field for generating the results. We validate theeffectiveness of our proposed method in a variety of cases, includingstyle/texture transfer, color/style swap, sketch/painting to photo, and timelapse.", "authors": ["Jing Liao", "Yuan Yao", "Lu Yuan", "Gang Hua", "Sing Bing Kang"], "categories": ["cs.CV"]}
{"url": "http://arxiv.org/abs/1804.03619v2", "updated": "2018-08-09T21:22:37Z", "published": "2018-04-10T16:28:59Z", "title": "Looking to Listen at the Cocktail Party: A Speaker-Independent  Audio-Visual Model for Speech Separation", "summary": "  We present a joint audio-visual model for isolating a single speech signalfrom a mixture of sounds such as other speakers and background noise. Solvingthis task using only audio as input is extremely challenging and does notprovide an association of the separated speech signals with speakers in thevideo. In this paper, we present a deep network-based model that incorporatesboth visual and auditory signals to solve this task. The visual features areused to \"focus\" the audio on desired speakers in a scene and to improve thespeech separation quality. To train our joint audio-visual model, we introduceAVSpeech, a new dataset comprised of thousands of hours of video segments fromthe Web. We demonstrate the applicability of our method to classic speechseparation tasks, as well as real-world scenarios involving heated interviews,noisy bars, and screaming children, only requiring the user to specify the faceof the person in the video whose speech they want to isolate. Our method showsclear advantage over state-of-the-art audio-only speech separation in cases ofmixed speech. In addition, our model, which is speaker-independent (trainedonce, applicable to any speaker), produces better results than recentaudio-visual speech separation methods that are speaker-dependent (requiretraining a separate model for each speaker of interest).", "authors": ["Ariel Ephrat", "Inbar Mosseri", "Oran Lang", "Tali Dekel", "Kevin Wilson", "Avinatan Hassidim", "William T. Freeman", "Michael Rubinstein"], "categories": ["cs.SD", "cs.CV", "eess.AS"]}
{"url": "http://arxiv.org/abs/1705.02999v1", "updated": "2017-05-08T17:58:11Z", "published": "2017-05-08T17:58:11Z", "title": "Real-Time User-Guided Image Colorization with Learned Deep Priors", "summary": "  We propose a deep learning approach for user-guided image colorization. Thesystem directly maps a grayscale image, along with sparse, local user \"hints\"to an output colorization with a Convolutional Neural Network (CNN). Ratherthan using hand-defined rules, the network propagates user edits by fusinglow-level cues along with high-level semantic information, learned fromlarge-scale data. We train on a million images, with simulated user inputs. Toguide the user towards efficient input selection, the system recommends likelycolors based on the input image and current user inputs. The colorization isperformed in a single feed-forward pass, enabling real-time use. Even withrandomly simulated user inputs, we show that the proposed system helps noviceusers quickly create realistic colorizations, and offers large improvements incolorization quality with just a minute of use. In addition, we demonstratethat the framework can incorporate other user \"hints\" to the desiredcolorization, showing an application to color histogram transfer. Our code andmodels are available at https://richzhang.github.io/ideepcolor.", "authors": ["Richard Zhang", "Jun-Yan Zhu", "Phillip Isola", "Xinyang Geng", "Angela S. Lin", "Tianhe Yu", "Alexei A. Efros"], "categories": ["cs.CV", "cs.GR"]}
{"url": "http://arxiv.org/abs/1707.02880v2", "updated": "2017-08-22T19:26:08Z", "published": "2017-07-10T14:34:06Z", "title": "Deep Bilateral Learning for Real-Time Image Enhancement", "summary": "  Performance is a critical challenge in mobile image processing. Given areference imaging pipeline, or even human-adjusted pairs of images, we seek toreproduce the enhancements and enable real-time evaluation. For this, weintroduce a new neural network architecture inspired by bilateral gridprocessing and local affine color transforms. Using pairs of input/outputimages, we train a convolutional neural network to predict the coefficients ofa locally-affine model in bilateral space. Our architecture learns to makelocal, global, and content-dependent decisions to approximate the desired imagetransformation. At runtime, the neural network consumes a low-resolutionversion of the input image, produces a set of affine transformations inbilateral space, upsamples those transformations in an edge-preserving fashionusing a new slicing node, and then applies those upsampled transformations tothe full-resolution image. Our algorithm processes high-resolution images on asmartphone in milliseconds, provides a real-time viewfinder at 1080presolution, and matches the quality of state-of-the-art approximationtechniques on a large class of image operators. Unlike previous work, our modelis trained off-line from data and therefore does not require access to theoriginal operator at runtime. This allows our model to learn complex,scene-dependent transformations for which no reference implementation isavailable, such as the photographic edits of a human retoucher.", "authors": ["Micha\u00ebl Gharbi", "Jiawen Chen", "Jonathan T. Barron", "Samuel W. Hasinoff", "Fr\u00e9do Durand"], "categories": ["cs.GR", "cs.CV"]}
{"url": "http://arxiv.org/abs/1412.7725v2", "updated": "2015-05-16T03:49:35Z", "published": "2014-12-24T17:51:17Z", "title": "Automatic Photo Adjustment Using Deep Neural Networks", "summary": "  Photo retouching enables photographers to invoke dramatic visual impressionsby artistically enhancing their photos through stylistic color and toneadjustments. However, it is also a time-consuming and challenging task thatrequires advanced skills beyond the abilities of casual photographers. Using anautomated algorithm is an appealing alternative to manual work but such analgorithm faces many hurdles. Many photographic styles rely on subtleadjustments that depend on the image content and even its semantics. Further,these adjustments are often spatially varying. Because of thesecharacteristics, existing automatic algorithms are still limited and cover onlya subset of these challenges. Recently, deep machine learning has shown uniqueabilities to address hard problems that resisted machine algorithms for long.This motivated us to explore the use of deep learning in the context of photoediting. In this paper, we explain how to formulate the automatic photoadjustment problem in a way suitable for this approach. We also introduce animage descriptor that accounts for the local semantics of an image. Ourexperiments demonstrate that our deep learning formulation applied using thesedescriptors successfully capture sophisticated photographic styles. Inparticular and unlike previous techniques, it can model local adjustments thatdepend on the image semantics. We show on several examples that this yieldsresults that are qualitatively and quantitatively better than previous work.", "authors": ["Zhicheng Yan", "Hao Zhang", "Baoyuan Wang", "Sylvain Paris", "Yizhou Yu"], "categories": ["cs.CV", "cs.GR", "cs.LG", "eess.IV"]}
{"url": "http://arxiv.org/abs/1603.06188v1", "updated": "2016-03-20T07:37:01Z", "published": "2016-03-20T07:37:01Z", "title": "An angular momentum conserving Affine-Particle-In-Cell method", "summary": "  We present a new technique for transferring momentum and velocity betweenparticles and grid with Particle-In-Cell (PIC) calculations which we callAffine-Particle-In-Cell (APIC). APIC represents particle velocities as locallyaffine, rather than locally constant as in traditional PIC. We show that thisrepresentation allows APIC to conserve linear and angular momentum acrosstransfers while also dramatically reducing numerical diffusion usuallyassociated with PIC. Notably, conservation is achieved with lumped mass, asopposed to the more commonly used Fluid Implicit Particle (FLIP) transferswhich require a 'full' mass matrix for exact conservation. Furthermore, unlikeFLIP, APIC retains a filtering property of the original PIC and thus does notaccumulate velocity modes on particles as FLIP does. In particular, wedemonstrate that APIC does not experience velocity instabilities that arecharacteristic of FLIP in a number of Material Point Method (MPM)hyperelasticity calculations. Lastly, we demonstrate that when combined withthe midpoint rule for implicit update of grid momentum that linear and angularmomentum are exactly conserved.", "authors": ["Chenfanfu Jiang", "Craig Schroeder", "Joseph Teran"], "categories": ["physics.comp-ph", "math.NA"]}
{"url": "http://arxiv.org/abs/1805.11714v1", "updated": "2018-05-29T21:31:14Z", "published": "2018-05-29T21:31:14Z", "title": "Deep Video Portraits", "summary": "  We present a novel approach that enables photo-realistic re-animation ofportrait videos using only an input video. In contrast to existing approachesthat are restricted to manipulations of facial expressions only, we are thefirst to transfer the full 3D head position, head rotation, face expression,eye gaze, and eye blinking from a source actor to a portrait video of a targetactor. The core of our approach is a generative neural network with a novelspace-time architecture. The network takes as input synthetic renderings of aparametric face model, based on which it predicts photo-realistic video framesfor a given target actor. The realism in this rendering-to-video transfer isachieved by careful adversarial training, and as a result, we can createmodified target videos that mimic the behavior of the synthetically-createdinput. In order to enable source-to-target video re-animation, we render asynthetic target video with the reconstructed head animation parameters from asource video, and feed it into the trained network -- thus taking full controlof the target. With the ability to freely recombine source and targetparameters, we are able to demonstrate a large variety of video rewriteapplications without explicitly modeling hair, body or background. Forinstance, we can reenact the full head using interactive user-controlledediting, and realize high-fidelity visual dubbing. To demonstrate the highquality of our output, we conduct an extensive series of experiments andevaluations, where for instance a user study shows that our video edits arehard to detect.", "authors": ["Hyeongwoo Kim", "Pablo Garrido", "Ayush Tewari", "Weipeng Xu", "Justus Thies", "Matthias Nie\u00dfner", "Patrick P\u00e9rez", "Christian Richardt", "Michael Zollh\u00f6fer", "Christian Theobalt"], "categories": ["cs.CV", "cs.AI", "cs.GR"]}
{"url": "http://arxiv.org/abs/1710.07480v1", "updated": "2017-10-20T10:48:22Z", "published": "2017-10-20T10:48:22Z", "title": "HDR image reconstruction from a single exposure using deep CNNs", "summary": "  Camera sensors can only capture a limited range of luminance simultaneously,and in order to create high dynamic range (HDR) images a set of differentexposures are typically combined. In this paper we address the problem ofpredicting information that have been lost in saturated image areas, in orderto enable HDR reconstruction from a single exposure. We show that this problemis well-suited for deep learning algorithms, and propose a deep convolutionalneural network (CNN) that is specifically designed taking into account thechallenges in predicting HDR values. To train the CNN we gather a large datasetof HDR images, which we augment by simulating sensor saturation for a range ofcameras. To further boost robustness, we pre-train the CNN on a simulated HDRdataset created from a subset of the MIT Places database. We demonstrate thatour approach can reconstruct high-resolution visually convincing HDR results ina wide range of situations, and that it generalizes well to reconstruction ofimages captured with arbitrary and low-end cameras that use unknown cameraresponse functions and post-processing. Furthermore, we compare to existingmethods for HDR expansion, and show high quality results also for image basedlighting. Finally, we evaluate the results in a subjective experiment performedon an HDR display. This shows that the reconstructed HDR images are visuallyconvincing, with large improvements as compared to existing methods.", "authors": ["Gabriel Eilertsen", "Joel Kronander", "Gyorgy Denes", "Rafa\u0142 K. Mantiuk", "Jonas Unger"], "categories": ["cs.CV", "cs.GR", "cs.LG"]}
{"url": "http://arxiv.org/abs/1705.02090v2", "updated": "2017-05-13T04:49:23Z", "published": "2017-05-05T05:45:10Z", "title": "GRASS: Generative Recursive Autoencoders for Shape Structures", "summary": "  We introduce a novel neural network architecture for encoding and synthesisof 3D shapes, particularly their structures. Our key insight is that 3D shapesare effectively characterized by their hierarchical organization of parts,which reflects fundamental intra-shape relationships such as adjacency andsymmetry. We develop a recursive neural net (RvNN) based autoencoder to map aflat, unlabeled, arbitrary part layout to a compact code. The code effectivelycaptures hierarchical structures of man-made 3D objects of varying structuralcomplexities despite being fixed-dimensional: an associated decoder maps a codeback to a full hierarchy. The learned bidirectional mapping is further tunedusing an adversarial setup to yield a generative model of plausible structures,from which novel structures can be sampled. Finally, our structure synthesisframework is augmented by a second trained module that produces fine-grainedpart geometry, conditioned on global and local structural context, leading to afull generative pipeline for 3D shapes. We demonstrate that withoutsupervision, our network learns meaningful structural hierarchies adhering toperceptual grouping principles, produces compact codes which enableapplications such as shape classification and partial matching, and supportsshape synthesis and interpolation with significant variations in topology andgeometry.", "authors": ["Jun Li", "Kai Xu", "Siddhartha Chaudhuri", "Ersin Yumer", "Hao Zhang", "Leonidas Guibas"], "categories": ["cs.GR", "cs.CV"]}
{"url": "http://arxiv.org/abs/1803.10091v1", "updated": "2018-03-27T14:06:16Z", "published": "2018-03-27T14:06:16Z", "title": "Point Convolutional Neural Networks by Extension Operators", "summary": "  This paper presents Point Convolutional Neural Networks (PCNN): a novelframework for applying convolutional neural networks to point clouds. Theframework consists of two operators: extension and restriction, mapping pointcloud functions to volumetric functions and vise-versa. A point cloudconvolution is defined by pull-back of the Euclidean volumetric convolution viaan extension-restriction mechanism.  The point cloud convolution is computationally efficient, invariant to theorder of points in the point cloud, robust to different samplings and varyingdensities, and translation invariant, that is the same convolution kernel isused at all points. PCNN generalizes image CNNs and allows readily adaptingtheir architectures to the point cloud setting.  Evaluation of PCNN on three central point cloud learning benchmarksconvincingly outperform competing point cloud learning methods, and the vastmajority of methods working with more informative shape representations such assurfaces and/or normals.", "authors": ["Matan Atzmon", "Haggai Maron", "Yaron Lipman"], "categories": ["cs.CV"]}
{"url": "http://arxiv.org/abs/1506.06668v1", "updated": "2015-06-22T16:20:34Z", "published": "2015-06-22T16:20:34Z", "title": "Fairy Lights in Femtoseconds: Aerial and Volumetric Graphics Rendered by  Focused Femtosecond Laser Combined with Computational Holographic Fields", "summary": "  We present a method of rendering aerial and volumetric graphics usingfemtosecond lasers. A high-intensity laser excites a physical matter to emitlight at an arbitrary 3D position. Popular applications can then be exploredespecially since plasma induced by a femtosecond laser is safer than thatgenerated by a nanosecond laser. There are two methods of rendering graphicswith a femtosecond laser in air: Producing holograms using spatial lightmodulation technology, and scanning of a laser beam by a galvano mirror. Theholograms and workspace of the system proposed here occupy a volume of up to 1cm^3; however, this size is scalable depending on the optical devices and theirsetup. This paper provides details of the principles, system setup, andexperimental evaluation, and discussions on scalability, design space, andapplications of this system. We tested two laser sources: an adjustable (30-100fs) laser which projects up to 1,000 pulses per second at energy up to 7 mJ perpulse, and a 269-fs laser which projects up to 200,000 pulses per second at anenergy up to 50 uJ per pulse. We confirmed that the spatiotemporal resolutionof volumetric displays, implemented with these laser sources, is 4,000 and200,000 dots per second. Although we focus on laser-induced plasma in air, thediscussion presented here is also applicable to other rendering principles suchas fluorescence and microbubble in solid/liquid materials.", "authors": ["Yoichi Ochiai", "Kota Kumagai", "Takayuki Hoshi", "Jun Rekimoto", "Satoshi Hasegawa", "Yoshio Hayasaki"], "categories": ["cs.GR", "cs.HC", "physics.optics"]}
{"url": "http://arxiv.org/abs/1805.09817v1", "updated": "2018-05-24T17:58:02Z", "published": "2018-05-24T17:58:02Z", "title": "Stereo Magnification: Learning View Synthesis using Multiplane Images", "summary": "  The view synthesis problem--generating novel views of a scene from knownimagery--has garnered recent attention due in part to compelling applicationsin virtual and augmented reality. In this paper, we explore an intriguingscenario for view synthesis: extrapolating views from imagery captured bynarrow-baseline stereo cameras, including VR cameras and now-widespreaddual-lens camera phones. We call this problem stereo magnification, and proposea learning framework that leverages a new layered representation that we callmultiplane images (MPIs). Our method also uses a massive new data source forlearning view extrapolation: online videos on YouTube. Using data mined fromsuch videos, we train a deep network that predicts an MPI from an input stereoimage pair. This inferred MPI can then be used to synthesize a range of novelviews of the scene, including views that extrapolate significantly beyond theinput baseline. We show that our method compares favorably with several recentview synthesis methods, and demonstrate applications in magnifyingnarrow-baseline stereo images.", "authors": ["Tinghui Zhou", "Richard Tucker", "John Flynn", "Graham Fyffe", "Noah Snavely"], "categories": ["cs.CV", "cs.GR"]}
{"url": "http://arxiv.org/abs/1604.07043v3", "updated": "2016-05-04T08:17:19Z", "published": "2016-04-24T15:53:22Z", "title": "Towards Better Analysis of Deep Convolutional Neural Networks", "summary": "  Deep convolutional neural networks (CNNs) have achieved breakthroughperformance in many pattern recognition tasks such as image classification.However, the development of high-quality deep models typically relies on asubstantial amount of trial-and-error, as there is still no clear understandingof when and why a deep model works. In this paper, we present a visualanalytics approach for better understanding, diagnosing, and refining deepCNNs. We formulate a deep CNN as a directed acyclic graph. Based on thisformulation, a hybrid visualization is developed to disclose the multiplefacets of each neuron and the interactions between them. In particular, weintroduce a hierarchical rectangle packing algorithm and a matrix reorderingalgorithm to show the derived features of a neuron cluster. We also propose abiclustering-based edge bundling method to reduce visual clutter caused by alarge number of connections between neurons. We evaluated our method on a setof CNNs and the results are generally favorable.", "authors": ["Mengchen Liu", "Jiaxin Shi", "Zhen Li", "Chongxuan Li", "Jun Zhu", "Shixia Liu"], "categories": ["cs.CV"]}
{"url": "http://arxiv.org/abs/1801.06889v3", "updated": "2018-05-14T04:59:24Z", "published": "2018-01-21T20:13:07Z", "title": "Visual Analytics in Deep Learning: An Interrogative Survey for the Next  Frontiers", "summary": "  Deep learning has recently seen rapid development and received significantattention due to its state-of-the-art performance on previously-thought hardproblems. However, because of the internal complexity and nonlinear structureof deep neural networks, the underlying decision making processes for why thesemodels are achieving such performance are challenging and sometimes mystifyingto interpret. As deep learning spreads across domains, it is of paramountimportance that we equip users of deep learning with tools for understandingwhen a model works correctly, when it fails, and ultimately how to improve itsperformance. Standardized toolkits for building neural networks have helpeddemocratize deep learning; visual analytics systems have now been developed tosupport model explanation, interpretation, debugging, and improvement. Wepresent a survey of the role of visual analytics in deep learning research,which highlights its short yet impactful history and thoroughly summarizes thestate-of-the-art using a human-centered interrogative framework, focusing onthe Five W's and How (Why, Who, What, How, When, and Where). We conclude byhighlighting research directions and open research problems. This survey helpsresearchers and practitioners in both visual analytics and deep learning toquickly learn key aspects of this young and rapidly growing body of research,whose impact spans a diverse range of domains.", "authors": ["Fred Hohman", "Minsuk Kahng", "Robert Pienta", "Duen Horng Chau"], "categories": ["cs.HC", "cs.AI", "cs.LG", "stat.ML", "H.5.2; I.5.1.d; I.6.9.c; I.6.9.f; I.2.6.g"]}
{"url": "http://arxiv.org/abs/1608.04366v2", "updated": "2016-11-19T16:24:27Z", "published": "2016-08-15T19:02:30Z", "title": "Infill Optimization for Additive Manufacturing -- Approaching Bone-like  Porous Structures", "summary": "  Porous structures such as trabecular bone are widely seen in nature. Thesestructures exhibit superior mechanical properties whilst being lightweight. Inthis paper, we present a method to generate bone-like porous structures aslightweight infill for additive manufacturing. Our method builds upon andextends voxel-wise topology optimization. In particular, for the purpose ofgenerating sparse yet stable structures distributed in the interior of a givenshape, we propose upper bounds on the localized material volume in theproximity of each voxel in the design domain. We then aggregate the localper-voxel constraints by their p-norm into an equivalent global constraint, inorder to facilitate an efficient optimization process. Implemented on ahigh-resolution topology optimization framework, our results demonstratemechanically optimized, detailed porous structures which mimic those found innature. We further show variants of the optimized structures subject todifferent design specifications, and analyze the optimality and robustness ofthe obtained structures.", "authors": ["Jun Wu", "Niels Aage", "Ruediger Westermann", "Ole Sigmund"], "categories": ["cs.GR"]}
{"url": "http://arxiv.org/abs/1705.04058v7", "updated": "2018-10-30T09:48:05Z", "published": "2017-05-11T08:08:44Z", "title": "Neural Style Transfer: A Review", "summary": "  The seminal work of Gatys et al. demonstrated the power of ConvolutionalNeural Networks (CNNs) in creating artistic imagery by separating andrecombining image content and style. This process of using CNNs to render acontent image in different styles is referred to as Neural Style Transfer(NST). Since then, NST has become a trending topic both in academic literatureand industrial applications. It is receiving increasing attention and a varietyof approaches are proposed to either improve or extend the original NSTalgorithm. In this paper, we aim to provide a comprehensive overview of thecurrent progress towards NST. We first propose a taxonomy of current algorithmsin the field of NST. Then, we present several evaluation methods and comparedifferent NST algorithms both qualitatively and quantitatively. The reviewconcludes with a discussion of various applications of NST and open problemsfor future research. A list of papers discussed in this review, correspondingcodes, pre-trained models and more comparison results are publicly available athttps://github.com/ycjing/Neural-Style-Transfer-Papers.", "authors": ["Yongcheng Jing", "Yezhou Yang", "Zunlei Feng", "Jingwen Ye", "Yizhou Yu", "Mingli Song"], "categories": ["cs.CV", "cs.NE", "eess.IV", "stat.ML"]}
{"url": "http://arxiv.org/abs/1606.07461v2", "updated": "2017-10-30T15:11:54Z", "published": "2016-06-23T20:20:39Z", "title": "LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in  Recurrent Neural Networks", "summary": "  Recurrent neural networks, and in particular long short-term memory (LSTM)networks, are a remarkably effective tool for sequence modeling that learn adense black-box hidden representation of their sequential input. Researchersinterested in better understanding these models have studied the changes inhidden state representations over time and noticed some interpretable patternsbut also significant noise. In this work, we present LSTMVIS, a visual analysistool for recurrent neural networks with a focus on understanding these hiddenstate dynamics. The tool allows users to select a hypothesis input range tofocus on local state changes, to match these states changes to similar patternsin a large data set, and to align these results with structural annotationsfrom their domain. We show several use cases of the tool for analyzing specifichidden state properties on dataset containing nesting, phrase structure, andchord progressions, and demonstrate how the tool can be used to isolatepatterns for further statistical analysis. We characterize the domain, thedifferent stakeholders, and their goals and tasks.", "authors": ["Hendrik Strobelt", "Sebastian Gehrmann", "Hanspeter Pfister", "Alexander M. Rush"], "categories": ["cs.CL", "cs.AI", "cs.NE"]}
{"url": "http://arxiv.org/abs/1512.01655v3", "updated": "2016-06-16T09:36:40Z", "published": "2015-12-05T12:05:52Z", "title": "Approximated and User Steerable tSNE for Progressive Visual Analytics", "summary": "  Progressive Visual Analytics aims at improving the interactivity in existinganalytics techniques by means of visualization as well as interaction withintermediate results. One key method for data analysis is dimensionalityreduction, for example, to produce 2D embeddings that can be visualized andanalyzed efficiently. t-Distributed Stochastic Neighbor Embedding (tSNE) is awell-suited technique for the visualization of several high-dimensional data.tSNE can create meaningful intermediate results but suffers from a slowinitialization that constrains its application in Progressive Visual Analytics.We introduce a controllable tSNE approximation (A-tSNE), which trades off speedand accuracy, to enable interactive data exploration. We offer real-timevisualization techniques, including a density-based solution and a Magic Lensto inspect the degree of approximation. With this feedback, the user can decideon local refinements and steer the approximation level during the analysis. Wedemonstrate our technique with several datasets, in a real-world researchscenario and for the real-time analysis of high-dimensional streams toillustrate its effectiveness for interactive data analysis.", "authors": ["Nicola Pezzotti", "Boudewijn P. F. Lelieveldt", "Laurens van der Maaten", "Thomas H\u00f6llt", "Elmar Eisemann", "Anna Vilanova"], "categories": ["cs.CV", "cs.LG"]}
{"url": "http://arxiv.org/abs/1710.06501v1", "updated": "2017-10-17T21:02:59Z", "published": "2017-10-17T21:02:59Z", "title": "Do Convolutional Neural Networks Learn Class Hierarchy?", "summary": "  Convolutional Neural Networks (CNNs) currently achieve state-of-the-artaccuracy in image classification. With a growing number of classes, theaccuracy usually drops as the possibilities of confusion increase.Interestingly, the class confusion patterns follow a hierarchical structureover the classes. We present visual-analytics methods to reveal and analyzethis hierarchy of similar classes in relation with CNN-internal data. We foundthat this hierarchy not only dictates the confusion patterns between theclasses, it furthermore dictates the learning behavior of CNNs. In particular,the early layers in these networks develop feature detectors that can separatehigh-level groups of classes quite well, even after a few training epochs. Incontrast, the latter layers require substantially more epochs to developspecialized feature detectors that can separate individual classes. Wedemonstrate how these insights are key to significant improvement in accuracyby designing hierarchy-aware CNNs that accelerate model convergence andalleviate overfitting. We further demonstrate how our methods help inidentifying various quality issues in the training data.", "authors": ["Bilal Alsallakh", "Amin Jourabloo", "Mao Ye", "Xiaoming Liu", "Liu Ren"], "categories": ["cs.CV", "I.4; I.5"]}
{"url": "http://arxiv.org/abs/1802.04233v1", "updated": "2018-02-12T18:31:24Z", "published": "2018-02-12T18:31:24Z", "title": "Embedding Complexity In the Data Representation Instead of In the Model:  A Case Study Using Heterogeneous Medical Data", "summary": "  Electronic Health Records have become popular sources of data for secondaryresearch, but their use is hampered by the amount of effort it takes toovercome the sparsity, irregularity, and noise that they contain. Modernlearning architectures can remove the need for expert-driven featureengineering, but not the need for expert-driven preprocessing to abstract awaythe inherent messiness of clinical data. This preprocessing effort is often thedominant component of a typical clinical prediction project. In this work wepropose using semantic embedding methods to directly couple the raw, messyclinical data to downstream learning architectures with truly minimalpreprocessing. We examine this step from the perspective of capturing andencoding complex data dependencies in the data representation instead of in themodel, which has the nice benefit of allowing downstream processing to be donewith fast, lightweight, and simple models accessible to researchers withoutmachine learning expertise. We demonstrate with three typical clinicalprediction tasks that the highly compressed, embedded data representationscapture a large amount of useful complexity, although in some cases thecompression is not completely lossless.", "authors": ["Jacek M. Bajor", "Diego A. Mesa", "Travis J. Osterman", "Thomas A. Lasko"], "categories": ["stat.AP"]}
{"url": "http://arxiv.org/abs/1506.05274v2", "updated": "2015-12-22T12:57:25Z", "published": "2015-06-17T10:47:20Z", "title": "Partial Functional Correspondence", "summary": "  In this paper, we propose a method for computing partial functionalcorrespondence between non-rigid shapes. We use perturbation analysis to showhow removal of shape parts changes the Laplace-Beltrami eigenfunctions, andexploit it as a prior on the spectral representation of the correspondence.Corresponding parts are optimization variables in our problem and are used toweight the functional correspondence; we are looking for the largest and mostregular (in the Mumford-Shah sense) parts that minimize correspondencedistortion. We show that our approach can cope with very challengingcorrespondence settings.", "authors": ["Emanuele Rodol\u00e0", "Luca Cosmo", "Michael M. Bronstein", "Andrea Torsello", "Daniel Cremers"], "categories": ["cs.CV"]}
{"url": "http://arxiv.org/abs/1802.07954v1", "updated": "2018-02-22T09:48:56Z", "published": "2018-02-22T09:48:56Z", "title": "The State of the Art in Integrating Machine Learning into Visual  Analytics", "summary": "  Visual analytics systems combine machine learning or other analytictechniques with interactive data visualization to promote sensemaking andanalytical reasoning. It is through such techniques that people can make senseof large, complex data. While progress has been made, the tactful combinationof machine learning and data visualization is still under-explored. Thisstate-of-the-art report presents a summary of the progress that has been madeby highlighting and synthesizing select research advances. Further, it presentsopportunities and challenges to enhance the synergy between machine learningand visual analytics for impactful future research directions.", "authors": ["A. Endert", "W. Ribarsky", "C. Turkay", "W Wong", "I. Nabney", "I D\u00edaz Blanco", "Fabrice Rossi"], "categories": ["stat.ML", "cs.HC", "cs.LG"]}
{"url": "http://arxiv.org/abs/1703.08014v2", "updated": "2017-03-24T08:24:07Z", "published": "2017-03-23T11:35:41Z", "title": "Sparse Inertial Poser: Automatic 3D Human Pose Estimation from Sparse  IMUs", "summary": "  We address the problem of making human motion capture in the wild morepractical by using a small set of inertial sensors attached to the body. Sincethe problem is heavily under-constrained, previous methods either use a largenumber of sensors, which is intrusive, or they require additional video input.We take a different approach and constrain the problem by: (i) making use of arealistic statistical body model that includes anthropometric constraints and(ii) using a joint optimization framework to fit the model to orientation andacceleration measurements over multiple frames. The resulting tracker SparseInertial Poser (SIP) enables 3D human pose estimation using only 6 sensors(attached to the wrists, lower legs, back and head) and works for arbitraryhuman motions. Experiments on the recently released TNT15 dataset show that,using the same number of sensors, SIP achieves higher accuracy than the datasetbaseline without using any video data. We further demonstrate the effectivenessof SIP on newly recorded challenging motions in outdoor scenarios such asclimbing or jumping over a wall.", "authors": ["Timo von Marcard", "Bodo Rosenhahn", "Michael J. Black", "Gerard Pons-Moll"], "categories": ["cs.CV", "cs.GR"]}
{"url": "http://arxiv.org/abs/1502.06686v1", "updated": "2015-02-24T04:30:43Z", "published": "2015-02-24T04:30:43Z", "title": "Data-Driven Shape Analysis and Processing", "summary": "  Data-driven methods play an increasingly important role in discoveringgeometric, structural, and semantic relationships between 3D shapes incollections, and applying this analysis to support intelligent modeling,editing, and visualization of geometric data. In contrast to traditionalapproaches, a key feature of data-driven approaches is that they aggregateinformation from a collection of shapes to improve the analysis and processingof individual shapes. In addition, they are able to learn models that reasonabout properties and relationships of shapes without relying on hard-codedrules or explicitly programmed instructions. We provide an overview of the mainconcepts and components of these techniques, and discuss their application toshape classification, segmentation, matching, reconstruction, modeling andexploration, as well as scene analysis and synthesis, through reviewing theliterature and relating the existing works with both qualitative and numericalcomparisons. We conclude our report with ideas that can inspire future researchin data-driven shape analysis and processing.", "authors": ["Kai Xu", "Vladimir G. Kim", "Qixing Huang", "Evangelos Kalogerakis"], "categories": ["cs.GR"]}
{"url": "http://arxiv.org/abs/1710.04954v4", "updated": "2018-06-19T13:35:22Z", "published": "2017-10-13T15:02:18Z", "title": "PCPNET: Learning Local Shape Properties from Raw Point Clouds", "summary": "  In this paper, we propose PCPNet, a deep-learning based approach forestimating local 3D shape properties in point clouds. In contrast to themajority of prior techniques that concentrate on global or mid-levelattributes, e.g., for shape classification or semantic labeling, we suggest apatch-based learning method, in which a series of local patches at multiplescales around each point is encoded in a structured manner. Our approach isespecially well-adapted for estimating local shape properties such as normals(both unoriented and oriented) and curvature from raw point clouds in thepresence of strong noise and multi-scale features. Our main contributionsinclude both a novel multi-scale variant of the recently proposed PointNetarchitecture with emphasis on local shape information, and a series of novelapplications in which we demonstrate how learning from training data arisingfrom well-structured triangle meshes, and applying the trained model to noisypoint clouds can produce superior results compared to specializedstate-of-the-art techniques. Finally, we demonstrate the utility of ourapproach in the context of shape reconstruction, by showing how it can be usedto extract normal orientation information from point clouds.", "authors": ["Paul Guerrero", "Yanir Kleiman", "Maks Ovsjanikov", "Niloy J. Mitra"], "categories": ["cs.CG"]}
{"url": "http://arxiv.org/abs/1705.03811v2", "updated": "2017-09-21T09:35:30Z", "published": "2017-05-10T15:12:14Z", "title": "From 3D Models to 3D Prints: an Overview of the Processing Pipeline", "summary": "  Due to the wide diffusion of 3D printing technologies, geometric algorithmsfor Additive Manufacturing are being invented at an impressive speed. Eachsingle step, in particular along the Process Planning pipeline, can now counton dozens of methods that prepare the 3D model for fabrication, while analysingand optimizing geometry and machine instructions for various objectives. Thisreport provides a classification of this huge state of the art, and elicits therelation between each single algorithm and a list of desirable objectivesduring Process Planning. The objectives themselves are listed and discussed,along with possible needs for tradeoffs. Additive Manufacturing technologiesare broadly categorized to explicitly relate classes of devices and supportedfeatures. Finally, this report offers an analysis of the state of the art whilediscussing open and challenging problems from both an academic and anindustrial perspective.", "authors": ["Marco Livesu", "Stefano Ellero", "Jon\u00e1s Mart\u00ecnez", "Sylvain Lefebvre", "Marco Attene"], "categories": ["cs.GR"]}
{"url": "http://arxiv.org/abs/1806.02071v2", "updated": "2019-02-01T14:44:57Z", "published": "2018-06-06T08:57:18Z", "title": "Deep Fluids: A Generative Network for Parameterized Fluid Simulations", "summary": "  This paper presents a novel generative model to synthesize fluid simulationsfrom a set of reduced parameters. A convolutional neural network is trained ona collection of discrete, parameterizable fluid simulation velocity fields. Dueto the capability of deep learning architectures to learn representativefeatures of the data, our generative model is able to accurately approximatethe training data set, while providing plausible interpolated in-betweens. Theproposed generative model is optimized for fluids by a novel loss function thatguarantees divergence-free velocity fields at all times. In addition, wedemonstrate that we can handle complex parameterizations in reduced spaces, andadvance simulations in time by integrating in the latent space with a secondnetwork. Our method models a wide variety of fluid behaviors, thus enablingapplications such as fast construction of simulations, interpolation of fluidswith different parameters, time re-sampling, latent space simulations, andcompression of fluid simulation data. Reconstructed velocity fields aregenerated up to 700x faster than re-simulating the data with the underlying CPUsolver, while achieving compression rates of up to 1300x.", "authors": ["Byungsoo Kim", "Vinicius C. Azevedo", "Nils Thuerey", "Theodore Kim", "Markus Gross", "Barbara Solenthaler"], "categories": ["cs.LG", "cs.GR", "physics.comp-ph", "physics.flu-dyn", "stat.ML"]}
{"url": "http://arxiv.org/abs/1910.02696v1", "updated": "2019-10-07T09:48:38Z", "published": "2019-10-07T09:48:38Z", "title": "Hierarchical stochastic neighbor embedding as a tool for visualizing the  encoding capability of magnetic resonance fingerprinting dictionaries", "summary": "  In Magnetic Resonance Fingerprinting (MRF) the quality of the estimatedparameter maps depends on the encoding capability of the variable flip angletrain. In this work we show how the dimensionality reduction techniqueHierarchical Stochastic Neighbor Embedding (HSNE) can be used to obtain insightinto the encoding capability of different MRF sequences. Embeddinghigh-dimensional MRF dictionaries into a lower-dimensional space andvisualizing them with colors, being a surrogate for location in low-dimensionalspace, provides a comprehensive overview of particular dictionaries and, inaddition, enables comparison of different sequences. Dictionaries for varioussequences and sequence lengths were compared to each other, and the effect oftransmit field variations on the encoding capability was assessed. Cleardifferences in encoding capability were observed between different sequences,and HSNE results accurately reflect those obtained from an MRF matchingsimulation.", "authors": ["Kirsten Koolstra", "Peter B\u00f6rnert", "Boudewijn Lelieveldt", "Andrew Webb", "Oleh Dzyubachyk"], "categories": ["eess.IV", "cs.CV"]}
{"url": "http://arxiv.org/abs/1802.10123v3", "updated": "2019-03-05T09:58:44Z", "published": "2018-02-27T19:18:43Z", "title": "Latent-space Physics: Towards Learning the Temporal Evolution of Fluid  Flow", "summary": "  We propose a method for the data-driven inference of temporal evolutions ofphysical functions with deep learning. More specifically, we target fluidflows, i.e. Navier-Stokes problems, and we propose a novel LSTM-based approachto predict the changes of pressure fields over time. The central challenge inthis context is the high dimensionality of Eulerian space-time data sets. Wedemonstrate for the first time that dense 3D+time functions of physics systemcan be predicted within the latent spaces of neural networks, and we arrive ata neural-network based simulation algorithm with significant practicalspeed-ups. We highlight the capabilities of our method with a series of complexliquid simulations, and with a set of single-phase buoyancy simulations. With aset of trained networks, our method is more than two orders of magnitudesfaster than a traditional pressure solver. Additionally, we present and discussa series of detailed evaluations for the different components of our algorithm.", "authors": ["Steffen Wiewel", "Moritz Becher", "Nils Thuerey"], "categories": ["cs.LG", "cs.GR"]}
{"url": "http://arxiv.org/abs/1806.04942v1", "updated": "2018-06-13T10:48:33Z", "published": "2018-06-13T10:48:33Z", "title": "Convolutional Sparse Coding for High Dynamic Range Imaging", "summary": "  Current HDR acquisition techniques are based on either (i) fusingmultibracketed, low dynamic range (LDR) images, (ii) modifying existinghardware and capturing different exposures simultaneously with multiplesensors, or (iii) reconstructing a single image with spatially-varying pixelexposures. In this paper, we propose a novel algorithm to recover high-qualityHDRI images from a single, coded exposure. The proposed reconstruction methodbuilds on recently-introduced ideas of convolutional sparse coding (CSC); thispaper demonstrates how to make CSC practical for HDR imaging. We demonstratethat the proposed algorithm achieves higher-quality reconstructions thanalternative methods, we evaluate optical coding schemes, analyze algorithmicparameters, and build a prototype coded HDR camera that demonstrates theutility of convolutional sparse HDRI coding with a custom hardware platform.", "authors": ["Ana Serrano", "Felix Heide", "Diego Gutierrez", "Gordon Wetzstein", "Belen Masia"], "categories": ["cs.CV", "cs.GR", "eess.IV"]}
{"url": "http://arxiv.org/abs/1909.07316v1", "updated": "2019-09-16T16:20:10Z", "published": "2019-09-16T16:20:10Z", "title": "Situational Awareness Enhanced through Social Media Analytics: A Survey  of First Responders", "summary": "  Social media data has been increasingly used to facilitate situationalawareness during events and emergencies such as natural disasters. Whileresearchers have investigated several methods to summarize, visualize or minethe data for analysis, first responders have not been able to fully leverageresearch advancements largely due to the gap between academic research anddeployed, functional systems. In this paper, we explore the opportunities andbarriers for the effective use of social media data from first responders'perspective. We present the summary of several detailed interviews with firstresponders on their use of social media for situational awareness. We furtherassess the impact of SMART-a social media visual analytics system-on firstresponder operations.", "authors": ["Luke S. Snyder", "Morteza Karimzadeh", "Christina Stober", "David S. Ebert"], "categories": ["cs.SI", "cs.CY"]}
{"url": "http://arxiv.org/abs/2001.00892v1", "updated": "2020-01-03T17:06:58Z", "published": "2020-01-03T17:06:58Z", "title": "Exploration of Interaction Techniques for Graph-based Modelling in  Virtual Reality", "summary": "  Editing and manipulating graph-based models within immersive environments islargely unexplored and certain design activities could benefit from using thosetechnologies. For example, in the case study of architectural modelling, the 3Dcontext of Virtual Reality naturally matches the intended output product, i.e.a 3D architectural geometry. Since both the state of the art and the state ofthe practice are lacking, we explore the field of VR-based interactivemodelling, and provide insights as to how to implement proper interactions inthat context, with broadly available devices. We consequently produce severalopen-source software prototypes for manipulating graph-based models in VR.", "authors": ["Adrien Coppens", "Berat Bicer", "Naz Yilmaz", "Serhat Aras"], "categories": ["cs.HC"]}
{"url": "http://arxiv.org/abs/1808.10587v1", "updated": "2018-08-31T03:33:21Z", "published": "2018-08-31T03:33:21Z", "title": "Geometric algebra and singularities of ruled and developable surfaces", "summary": "  Any ruled surface in Euclidean 3-space is described as a curve of unit dualvectors in the algebra of dual quaternions (=the even Clifford algebra of type(0,3,1)). Combining this classical framework and Singularity Theory, wecharacterize local diffeomorphic types of singular ruled surfaces in terms ofgeometric invariants. In particular, using a theorem of G. Ishikawa, we showthat local topological type of singular (non-cylindrical) developable surfacesis completely determined by vanishing order of the dual torsion, thatgeneralizes an old result of D. Mond for tangent developables of non-singularspace curves. Our approach would be useful for analysis on singularitiesarising in differential line geometry related with several applications such asrobotics, vision theory and architectural geometry.", "authors": ["Junki Tanaka", "Toru Ohmoto"], "categories": ["math.DG"]}
{"url": "http://arxiv.org/abs/1707.05738v2", "updated": "2017-10-26T20:52:51Z", "published": "2017-07-18T16:50:11Z", "title": "Engineering Er3+ placement and emission through chemically-synthesized  self-aligned SiC:Ox nanowire photonic crystal structures", "summary": "  High precision placement and integration of color centers in a silicon-basednanosystem, such as a nanowire (NW) array, exhibiting high integrationfunctionality and high photoluminescence (PL) yield can serve as a criticalbuilding block towards the practical realization of devices in the emergingfield of quantum technologies. Herein, we report on an innovative synthesisroute for realizing ultrathin silicon carbide (SiC) NW arrays doped with andwithout oxygen (SiC:Ox), and also erbium (Er). The arrays of thedeterministically positioned NWs are grown in a self-aligned manner throughchemical-vapor-deposition (CVD). A key enabler of this synthesis route is thatSiC:Ox NW photonic crystal (PC) nanostructures are engineered with tailoredgeometry in precise locations during nanofabrication. These ultrathin NW PCstructures not only facilitate the on-demand positioning of Er3+ ions but arepivotal in engineering the emission properties of these color centers. Througha combinational and systematic micro-PL (uPL) and power-dependence PL (PDPL)spectroscopy, PC architecture geometry effects on Er3+-related 1538 nmemission, which is the telecommunication wavelength used in optical fibers,were studied. Approximately 60-fold and 30-fold enhancements for, respectively,the room-temperature Er3+ PL emission and lifetime in the NW PC sample wereobserved compared to its thin-film analog. Furthermore, the 1538 nm emission inSiC:Ox NW PC was found to be modulated linearly with the PC lattice periodicityof the structure. The observed characteristics reveal the efficientEr3+-emission extraction from the technologically-friendly SiC:Ox NW PCstructures.", "authors": ["Natasha Tabassum", "Vasileios Nikas", "Brian Ford", "Edward Crawford", "Spyros Gallis"], "categories": ["cond-mat.mes-hall"]}
{"url": "http://arxiv.org/abs/1703.05318v1", "updated": "2017-03-15T17:28:40Z", "published": "2017-03-15T17:28:40Z", "title": "Smooth polyhedral surfaces", "summary": "  Polyhedral surfaces are fundamental objects in architectural geometry andindustrial design. Whereas closeness of a given mesh to a smooth referencesurface and its suitability for numerical simulations were already studiedextensively, the aim of our work is to find and to discuss suitable assessmentsof smoothness of polyhedral surfaces that only take the geometry of thepolyhedral surface itself into account. Motivated by analogies to classicaldifferential geometry, we propose a theory of smoothness of polyhedral surfacesincluding suitable notions of normal vectors, tangent planes, asymptoticdirections, and parabolic curves that are invariant under projectivetransformations. It is remarkable that seemingly mild conditions significantlylimit the shapes of faces of a smooth polyhedral surface. Besides being oftheoretical interest, we believe that smoothness of polyhedral surfaces is ofinterest in the architectural context, where vertices and edges of polyhedralsurfaces are highly visible.", "authors": ["Felix G\u00fcnther", "Caigui Jiang", "Helmut Pottmann"], "categories": ["math.MG", "math.DG", "52B70, 53A05"]}
{"url": "http://arxiv.org/abs/1710.02322v1", "updated": "2017-10-06T09:27:44Z", "published": "2017-10-06T09:27:44Z", "title": "Human Pose Regression by Combining Indirect Part Detection and  Contextual Information", "summary": "  In this paper, we propose an end-to-end trainable regression approach forhuman pose estimation from still images. We use the proposed Soft-argmaxfunction to convert feature maps directly to joint coordinates, resulting in afully differentiable framework. Our method is able to learn heat mapsrepresentations indirectly, without additional steps of artificial ground truthgeneration. Consequently, contextual information can be included to the posepredictions in a seamless way. We evaluated our method on two very challengingdatasets, the Leeds Sports Poses (LSP) and the MPII Human Pose datasets,reaching the best performance among all the existing regression methods andcomparable results to the state-of-the-art detection based approaches.", "authors": ["Diogo C. Luvizon", "Hedi Tabia", "David Picard"], "categories": ["cs.CV"]}
{"url": "http://arxiv.org/abs/1710.02634v1", "updated": "2017-10-07T04:55:17Z", "published": "2017-10-07T04:55:17Z", "title": "Notions of optimal transport theory and how to implement them on a  computer", "summary": "  This article gives an introduction to optimal transport, a mathematicaltheory that makes it possible to measure distances between functions (ordistances between more general objects), to interpolate between objects or toenforce mass/volume conservation in certain computational physics simulations.Optimal transport is a rich scientific domain, with active researchcommunities, both on its theoretical aspects and on more applicativeconsiderations, such as geometry processing and machine learning. This articleaims at explaining the main principles behind the theory of optimal transport,introduce the different involved notions, and more importantly, how theyrelate, to let the reader grasp an intuition of the elegant theory thatstructures them. Then we will consider a specific setting, calledsemi-discrete, where a continuous function is transported to a discrete sum ofDirac masses. Studying this specific setting naturally leads to an efficientcomputational algorithm, that uses classical notions of computational geometry,such as a generalization of Voronoi diagrams called Laguerre diagrams.", "authors": ["Bruno Levy", "Erica Schwindt"], "categories": ["math.AP", "math.NA", "49M15, 35J96, 65D18"]}
{"url": "http://arxiv.org/abs/1702.08675v3", "updated": "2018-05-26T01:45:21Z", "published": "2017-02-28T07:26:55Z", "title": "3D Shape Segmentation via Shape Fully Convolutional Networks", "summary": "  We desgin a novel fully convolutional network architecture for shapes,denoted by Shape Fully Convolutional Networks (SFCN). 3D shapes are representedas graph structures in the SFCN architecture, based on novel graph convolutionand pooling operations, which are similar to convolution and pooling operationsused on images. Meanwhile, to build our SFCN architecture in the original imagesegmentation fully convolutional network (FCN) architecture, we also design andimplement a generating operation} with bridging function. This ensures that theconvolution and pooling operation we have designed can be successfully appliedin the original FCN architecture. In this paper, we also present a new shapesegmentation approach based on SFCN. Furthermore, we allow more general andchallenging input, such as mixed datasets of different categories of shapes}which can prove the ability of our generalisation. In our approach, SFCNs aretrained triangles-to-triangles by using three low-level geometric features asinput. Finally, the feature voting-based multi-label graph cuts is adopted tooptimise the segmentation results obtained by SFCN prediction. The experimentresults show that our method can effectively learn and predict mixed shapedatasets of either similar or different characteristics, and achieve excellentsegmentation results.", "authors": ["Pengyu Wang", "Yuan Gan", "Panpan Shui", "Fenggen Yu", "Yan Zhang", "Songle Chen", "Zhengxing Sun"], "categories": ["cs.CV"]}
{"url": "http://arxiv.org/abs/1804.02527v1", "updated": "2018-04-07T07:52:04Z", "published": "2018-04-07T07:52:04Z", "title": "Visual Analytics for Explainable Deep Learning", "summary": "  Recently, deep learning has been advancing the state of the art in artificialintelligence to a new level, and humans rely on artificial intelligencetechniques more than ever. However, even with such unprecedented advancements,the lack of explanation regarding the decisions made by deep learning modelsand absence of control over their internal processes act as major drawbacks incritical decision-making processes, such as precision medicine and lawenforcement. In response, efforts are being made to make deep learninginterpretable and controllable by humans. In this paper, we review visualanalytics, information visualization, and machine learning perspectivesrelevant to this aim, and discuss potential challenges and future researchdirections.", "authors": ["Jaegul Choo", "Shixia Liu"], "categories": ["cs.HC", "cs.LG", "stat.ML", "I.6.9.c"]}
{"url": "http://arxiv.org/abs/1407.6175v4", "updated": "2014-12-13T22:51:56Z", "published": "2014-07-23T11:11:11Z", "title": "Analysis-suitable adaptive T-mesh refinement with linear complexity", "summary": "  We present an efficient adaptive refinement procedure that preservesanalysis-suitability of the T-mesh, this is, the linear independence of theT-spline blending functions. We prove analysis-suitability of the overlays andboundedness of their cardinalities, nestedness of the generated T-splinespaces, and linear computational complexity of the refinement procedure interms of the number of marked and generated mesh elements.", "authors": ["Philipp Morgenstern", "Daniel Peterseim"], "categories": ["math.NA", "65D17, 65N30, 65N50"]}
{"url": "http://arxiv.org/abs/1410.2729v1", "updated": "2014-10-10T10:30:14Z", "published": "2014-10-10T10:30:14Z", "title": "Convergence of univariate non-stationary subdivision schemes via  asymptotical similarity", "summary": "  A new equivalence notion between non-stationary subdivision schemes, termedasymptotical similarity, which is weaker than asymptotical equivalence, isintroduced and studied. It is known that asymptotical equivalence between anon-stationary subdivision scheme and a convergent stationary scheme guaranteesthe convergence of the non-stationary scheme. We show that for non-stationaryschemes reproducing constants, the condition of asymptotical equivalence can berelaxed to asymptotical similarity. This result applies to a wide class ofnon-stationary schemes of importance in theory and applications.", "authors": ["Costanza Conti", "Nira Dyn", "Carla Manni", "Marie-Laurence Mazure"], "categories": ["math.NA"]}
{"url": "http://arxiv.org/abs/1710.05488v2", "updated": "2017-12-19T04:28:31Z", "published": "2017-10-16T03:30:09Z", "title": "A Geometric View of Optimal Transportation and Generative Model", "summary": "  In this work, we show the intrinsic relations between optimal transportationand convex geometry, especially the variational approach to solve Alexandrovproblem: constructing a convex polytope with prescribed face normals andvolumes. This leads to a geometric interpretation to generative models, andleads to a novel framework for generative models. By using the optimaltransportation view of GAN model, we show that the discriminator computes theKantorovich potential, the generator calculates the transportation map. For alarge class of transportation costs, the Kantorovich potential can give theoptimal transportation map by a close-form formula. Therefore, it is sufficientto solely optimize the discriminator. This shows the adversarial competitioncan be avoided, and the computational architecture can be simplified.Preliminary experimental results show the geometric method outperforms WGAN forapproximating probability measures with multiple clusters in low dimensionalspace.", "authors": ["Na Lei", "Kehua Su", "Li Cui", "Shing-Tung Yau", "David Xianfeng Gu"], "categories": ["cs.LG", "stat.ML"]}
{"url": "http://arxiv.org/abs/1509.05566v1", "updated": "2015-09-18T09:59:14Z", "published": "2015-09-18T09:59:14Z", "title": "Complexity of hierarchical refinement for a class of admissible mesh  configurations", "summary": "  An adaptive isogeometric method based on $d$-variate hierarchical splineconstructions can be derived by considering a refine module that preserves acertain class of admissibility between two consecutive steps of the adaptiveloop [6]. In this paper we provide a complexity estimate, i.e., an estimate onhow the number of mesh elements grows with respect to the number of elementsthat are marked for refinement by the adaptive strategy. Our estimate is in theline of the similar ones proved in the finite element context, [3,24].", "authors": ["Annalisa Buffa", "Carlotta Giannelli", "Philipp Morgenstern", "Daniel Peterseim"], "categories": ["math.NA"]}
{"url": "http://arxiv.org/abs/1609.06536v2", "updated": "2017-06-02T13:54:51Z", "published": "2016-09-21T12:55:59Z", "title": "Production-Level Facial Performance Capture Using Deep Convolutional  Neural Networks", "summary": "  We present a real-time deep learning framework for video-based facialperformance capture -- the dense 3D tracking of an actor's face given amonocular video. Our pipeline begins with accurately capturing a subject usinga high-end production facial capture pipeline based on multi-view stereotracking and artist-enhanced animations. With 5-10 minutes of captured footage,we train a convolutional neural network to produce high-quality output,including self-occluded regions, from a monocular video sequence of thatsubject. Since this 3D facial performance capture is fully automated, oursystem can drastically reduce the amount of labor involved in the developmentof modern narrative-driven video games or films involving realistic digitaldoubles of actors and potentially hours of animated dialogue per character. Wecompare our results with several state-of-the-art monocular real-time facialcapture techniques and demonstrate compelling animation inference inchallenging areas such as eyes and lips.", "authors": ["Samuli Laine", "Tero Karras", "Timo Aila", "Antti Herva", "Shunsuke Saito", "Ronald Yu", "Hao Li", "Jaakko Lehtinen"], "categories": ["cs.CV", "cs.GR"]}
{"url": "http://arxiv.org/abs/1611.01055v1", "updated": "2016-11-03T15:15:00Z", "published": "2016-11-03T15:15:00Z", "title": "Learning Locomotion Skills Using DeepRL: Does the Choice of Action Space  Matter?", "summary": "  The use of deep reinforcement learning allows for high-dimensional statedescriptors, but little is known about how the choice of action representationimpacts the learning difficulty and the resulting performance. We compare theimpact of four different action parameterizations (torques, muscle-activations,target joint angles, and target joint-angle velocities) in terms of learningtime, policy robustness, motion quality, and policy query rates. Our resultsare evaluated on a gait-cycle imitation task for multiple planar articulatedfigures and multiple gaits. We demonstrate that the local feedback provided byhigher-level action parameterizations can significantly impact the learning,robustness, and quality of the resulting policies.", "authors": ["Xue Bin Peng", "Michiel van de Panne"], "categories": ["cs.LG", "cs.GR", "cs.RO"]}
{"url": "http://arxiv.org/abs/1802.02673v2", "updated": "2018-02-20T01:58:15Z", "published": "2018-02-07T23:37:20Z", "title": "Position-Based Multi-Agent Dynamics for Real-Time Crowd Simulation (MiG  paper)", "summary": "  Exploiting the efficiency and stability of Position-Based Dynamics (PBD), weintroduce a novel crowd simulation method that runs at interactive rates forhundreds of thousands of agents. Our method enables the detailed modeling ofper-agent behavior in a Lagrangian formulation. We model short-range andlong-range collision avoidance to simulate both sparse and dense crowds. On theparticles representing agents, we formulate a set of positional constraintsthat can be readily integrated into a standard PBD solver. We augment thetentative particle motions with planning velocities to determine the preferredvelocities of agents, and project the positions onto the constraint manifold toeliminate colliding configurations. The local short-range interaction isrepresented with collision and frictional contact between agents, as in thediscrete simulation of granular materials. We incorporate a cohesion model formodeling collective behaviors and propose a new constraint for dealing withpotential future collisions. Our new method is suitable for use in interactivegames.", "authors": ["Tomer Weiss", "Alan Litteneker", "Chenfanfu Jiang", "Demetri Terzopoulos"], "categories": ["cs.GR"]}
{"url": "http://arxiv.org/abs/1802.02731v1", "updated": "2018-02-08T07:35:43Z", "published": "2018-02-08T07:35:43Z", "title": "Topologically Controlled Lossy Compression", "summary": "  This paper presents a new algorithm for the lossy compression of scalar datadefined on 2D or 3D regular grids, with topological control. Certain techniquesallow users to control the pointwise error induced by the compression. However,in many scenarios it is desirable to control in a similar way the preservationof higher-level notions, such as topological features , in order to provideguarantees on the outcome of post-hoc data analyses. This paper presents thefirst compression technique for scalar data which supports a strictlycontrolled loss of topological features. It provides users with specificguarantees both on the preservation of the important features and on the sizeof the smaller features destroyed during compression. In particular, we presenta simple compression strategy based on a topologically adaptive quantization ofthe range. Our algorithm provides strong guarantees on the bottleneck distancebetween persistence diagrams of the input and decompressed data, specificallythose associated with extrema. A simple extension of our strategy additionallyenables a control on the pointwise error. We also show how to combine ourapproach with state-of-the-art compressors, to further improve the geometricalreconstruction. Extensive experiments, for comparable compression rates,demonstrate the superiority of our algorithm in terms of the preservation oftopological features. We show the utility of our approach by illustrating thecompatibility between the output of post-hoc topological data analysispipelines, executed on the input and decompressed data, for simulated oracquired data sets. We also provide a lightweight VTK-based C++ implementationof our approach for reproduction purposes.", "authors": ["Maxime Soler", "Melanie Plainchault", "Bruno Conche", "Julien Tierny"], "categories": ["eess.IV", "cs.CG", "cs.CV", "cs.GR"]}
{"url": "http://arxiv.org/abs/1705.01968v3", "updated": "2017-10-01T22:24:17Z", "published": "2017-05-04T18:24:38Z", "title": "A Workflow for Visual Diagnostics of Binary Classifiers using  Instance-Level Explanations", "summary": "  Human-in-the-loop data analysis applications necessitate greater transparencyin machine learning models for experts to understand and trust their decisions.To this end, we propose a visual analytics workflow to help data scientists anddomain experts explore, diagnose, and understand the decisions made by a binaryclassifier. The approach leverages \"instance-level explanations\", measures oflocal feature relevance that explain single instances, and uses them to build aset of visual representations that guide the users in their investigation. Theworkflow is based on three main visual representations and steps: one based onaggregate statistics to see how data distributes across correct / incorrectdecisions; one based on explanations to understand which features are used tomake these decisions; and one based on raw data, to derive insights onpotential root causes for the observed patterns. The workflow is derived from along-term collaboration with a group of machine learning and healthcareprofessionals who used our method to make sense of machine learning models theydeveloped. The case study from this collaboration demonstrates that theproposed workflow helps experts derive useful knowledge about the model and thephenomena it describes, thus experts can generate useful hypotheses on how amodel can be improved.", "authors": ["Josua Krause", "Aritra Dasgupta", "Jordan Swartz", "Yindalon Aphinyanaphongs", "Enrico Bertini"], "categories": ["stat.ML", "cs.AI"]}
{"url": "http://arxiv.org/abs/1504.02218v2", "updated": "2017-01-02T20:27:53Z", "published": "2015-04-09T07:58:01Z", "title": "Evaluating Cartogram Effectiveness", "summary": "  Cartograms are maps in which areas of geographic regions (countries, states)appear in proportion to some variable of interest (population, income).Cartograms are popular visualizations for geo-referenced data that have beenused for over a century and that make it possible to gain insight into patternsand trends in the world around us. Despite the popularity of cartograms and thelarge number of cartogram types, there are few studies evaluating theeffectiveness of cartograms in conveying information. Based on a recent tasktaxonomy for cartograms, we evaluate four major different types of cartograms:contiguous, non-contiguous, rectangular, and Dorling cartograms. Specifically,we evaluate the effectiveness of these cartograms by quantitative performanceanalysis, as well as by subjective preferences. We analyze the results of ourstudy in the context of some prevailing assumptions in the literature ofcartography and cognitive science. Finally, we make recommendations for the useof different types of cartograms for different tasks and settings.", "authors": ["Sabrina Nusrat", "Md. Jawaherul Alam", "Stephen G. Kobourov"], "categories": ["cs.HC"]}
{"url": "http://arxiv.org/abs/1705.11050v2", "updated": "2018-02-01T13:29:19Z", "published": "2017-05-31T12:10:32Z", "title": "3D Mesh Segmentation via Multi-branch 1D Convolutional Neural Networks", "summary": "  There is an increasing interest in applying deep learning to 3D meshsegmentation. We observe that 1) existing feature-based techniques are oftenslow or sensitive to feature resizing, 2) there are minimal comparative studiesand 3) techniques often suffer from reproducibility issue. This studycontributes in two ways. First, we propose a novel convolutional neural network(CNN) for mesh segmentation. It uses 1D data, filters and a multi-brancharchitecture for separate training of multi-scale features. Together with anovel way of computing conformal factor (CF), our technique clearlyout-performs existing work. Secondly, we publicly provide implementations ofseveral deep learning techniques, namely, neural networks (NNs), autoencoders(AEs) and CNNs, whose architectures are at least two layers deep. Thesignificance of this study is that it proposes a robust form of CF, offers anovel and accurate CNN technique, and a comprehensive study of several deeplearning techniques for baseline comparison.", "authors": ["David George", "Xianghua Xie", "Gary KL Tam"], "categories": ["cs.GR"]}
{"url": "http://arxiv.org/abs/1505.01214v1", "updated": "2015-05-05T22:59:32Z", "published": "2015-05-05T22:59:32Z", "title": "Learning Style Similarity for Searching Infographics", "summary": "  Infographics are complex graphic designs integrating text, images, charts andsketches. Despite the increasing popularity of infographics and the rapidgrowth of online design portfolios, little research investigates how we cantake advantage of these design resources. In this paper we present a method formeasuring the style similarity between infographics. Based on human perceptiondata collected from crowdsourced experiments, we use computer vision andmachine learning algorithms to learn a style similarity metric for infographicdesigns. We evaluate different visual features and learning algorithms and findthat a combination of color histograms and Histograms-of-Gradients (HoG)features is most effective in characterizing the style of infographics. Wedemonstrate our similarity metric on a preliminary image retrieval test.", "authors": ["Babak Saleh", "Mira Dontcheva", "Aaron Hertzmann", "Zhicheng Liu"], "categories": ["cs.GR", "cs.CV", "cs.HC", "cs.IR", "cs.MM"]}
{"url": "http://arxiv.org/abs/1611.00939v1", "updated": "2016-11-03T10:11:10Z", "published": "2016-11-03T10:11:10Z", "title": "Recent Advances in Transient Imaging: A Computer Graphics and Vision  Perspective", "summary": "  Transient imaging has recently made a huge impact in the computer graphicsand computer vision fields. By capturing, reconstructing, or simulating lighttransport at extreme temporal resolutions, researchers have proposed noveltechniques to show movies of light in motion, see around corners, detectobjects in highly-scattering media, or infer material properties from adistance, to name a few. The key idea is to leverage the wealth of informationin the temporal domain at the pico or nanosecond resolution, informationusually lost during the capture-time temporal integration. This paper presentsrecent advances in this field of transient imaging from a graphics and visionperspective, including capture techniques, analysis, applications andsimulation.", "authors": ["Adrian Jarabo", "Belen Masia", "Julio Marco", "Diego Gutierrez"], "categories": ["cs.CV", "cs.GR"]}
{"url": "http://arxiv.org/abs/1709.08774v1", "updated": "2017-09-26T01:15:26Z", "published": "2017-09-26T01:15:26Z", "title": "Exploring the Design Space of Immersive Urban Analytics", "summary": "  Recent years have witnessed the rapid development and wide adoption ofimmersive head-mounted devices, such as HTC VIVE, Oculus Rift, and MicrosoftHoloLens. These immersive devices have the potential to significantly extendthe methodology of urban visual analytics by providing critical 3D contextinformation and creating a sense of presence. In this paper, we propose antheoretical model to characterize the visualizations in immersive urbananalytics. Further more, based on our comprehensive and concise model, wecontribute a typology of combination methods of 2D and 3D visualizations thatdistinguish between linked views, embedded views, and mixed views. We alsopropose a supporting guideline to assist users in selecting a proper view undercertain circumstances by considering visual geometry and spatial distributionof the 2D and 3D visualizations. Finally, based on existing works, possiblefuture research opportunities are explored and discussed.", "authors": ["Zhutian Chen", "Yifang Wang", "Tianchen Sun", "Xiang Gao", "Wei Chen", "Zhigeng Pan", "Huamin Qu", "Yingcai Wu"], "categories": ["cs.GR", "cs.HC"]}
{"url": "http://arxiv.org/abs/1809.00270v1", "updated": "2018-09-02T00:03:54Z", "published": "2018-09-02T00:03:54Z", "title": "Exploring the Limits of Complexity: A Survey of Empirical Studies on  Graph Visualisation", "summary": "  For decades, researchers in information visualisation and graph drawing havefocused on developing techniques for the layout and display of very large andcomplex networks. Experiments involving human participants have also exploredthe readability of different styles of layout and representations for suchnetworks. In both bodies of literature, networks are frequently referred to asbeing 'large' or 'complex', yet these terms are relative. From a human-centred,experiment point-of-view, what constitutes 'large' (for example) depends onseveral factors, such as data complexity, visual complexity, and the technologyused. In this paper, we survey the literature on human-centred experiments tounderstand how, in practice, different features and characteristics ofnode-link diagrams affect visual complexity.", "authors": ["Vahan Yoghourdjian", "Daniel Archambault", "Stephan Diehl", "Tim Dwyer", "Karsten Klein", "Helen C. Purchase", "Hsiang-Yun Wu"], "categories": ["cs.HC"]}
{"url": "http://arxiv.org/abs/1711.06363v2", "updated": "2018-03-10T18:12:27Z", "published": "2017-11-17T00:58:53Z", "title": "3D Reconstruction of Incomplete Archaeological Objects Using a  Generative Adversarial Network", "summary": "  We introduce a data-driven approach to aid the repairing and conservation ofarchaeological objects: ORGAN, an object reconstruction generative adversarialnetwork (GAN). By using an encoder-decoder 3D deep neural network on a GANarchitecture, and combining two loss objectives: a completion loss and anImproved Wasserstein GAN loss, we can train a network to effectively predictthe missing geometry of damaged objects. As archaeological objects can greatlydiffer between them, the network is conditioned on a variable, which can be aculture, a region or any metadata of the object. In our results, we show thatour method can recover most of the information from damaged objects, even incases where more than half of the voxels are missing, without producing manyerrors.", "authors": ["Renato Hermoza", "Ivan Sipiran"], "categories": ["cs.CV", "cs.AI"]}
{"url": "http://arxiv.org/abs/1207.3502v2", "updated": "2017-07-22T22:23:49Z", "published": "2012-07-15T13:06:26Z", "title": "A Simple and Correct Even-Odd Algorithm for the Point-in-Polygon Problem  for Complex Polygons", "summary": "  Determining if a point is in a polygon or not is used by a lot ofapplications in computer graphics, computer games and geoinformatics.Implementing this check is error-prone since there are many special cases to beconsidered. This holds true in particular for complex polygons whose edgesintersect each other creating holes. In this paper we present a simple even-oddalgorithm to solve this problem for complex polygons in linear time and proveits correctness for all possible points and polygons. We furthermore provideexamples and implementation notes for this algorithm.", "authors": ["Michael Galetzka", "Patrick O. Glauner"], "categories": ["cs.CG"]}
