{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "proof-acquisition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import rltk\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-source",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hired-satisfaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC = 'vision'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-landscape",
   "metadata": {},
   "source": [
    "**Arxiv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "spare-arrest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_arxiv.shape pre  deduplucation: (1153, 7)\n",
      "df_arxiv.shape post deduplucation: (1151, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>updated</th>\n",
       "      <th>published</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>authors</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/abs/1409.4842v1</td>\n",
       "      <td>2014-09-17T01:03:11Z</td>\n",
       "      <td>2014-09-17T01:03:11Z</td>\n",
       "      <td>Going Deeper with Convolutions</td>\n",
       "      <td>We propose a deep convolutional neural netwo...</td>\n",
       "      <td>[Christian Szegedy, Wei Liu, Yangqing Jia, Pie...</td>\n",
       "      <td>[cs.CV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/abs/2011.06825v2</td>\n",
       "      <td>2020-12-03T19:50:31Z</td>\n",
       "      <td>2020-11-13T09:33:03Z</td>\n",
       "      <td>LULC classification by semantic segmentation o...</td>\n",
       "      <td>This paper analyses how well a Fast Fully Co...</td>\n",
       "      <td>[Md. Saif Hassan Onim, Aiman Rafeed Ehtesham, ...</td>\n",
       "      <td>[cs.CV, cs.LG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/abs/2003.11213v1</td>\n",
       "      <td>2020-03-25T04:27:01Z</td>\n",
       "      <td>2020-03-25T04:27:01Z</td>\n",
       "      <td>A New Multiple Max-pooling Integration Module ...</td>\n",
       "      <td>To better retain the deep features of an ima...</td>\n",
       "      <td>[Hongfeng You, Shengwei Tian, Long Yu, Xiang M...</td>\n",
       "      <td>[cs.CV, cs.LG, eess.IV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://arxiv.org/abs/1907.09194v2</td>\n",
       "      <td>2020-04-30T10:39:17Z</td>\n",
       "      <td>2019-07-22T09:19:05Z</td>\n",
       "      <td>FD-FCN: 3D Fully Dense and Fully Convolutional...</td>\n",
       "      <td>In this paper, a 3D patch-based fully dense ...</td>\n",
       "      <td>[Binbin Yang, Weiwei Zhang]</td>\n",
       "      <td>[eess.IV, cs.CV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://arxiv.org/abs/1907.06915v1</td>\n",
       "      <td>2019-07-16T09:41:13Z</td>\n",
       "      <td>2019-07-16T09:41:13Z</td>\n",
       "      <td>Mango Tree Net -- A fully convolutional networ...</td>\n",
       "      <td>This work presents a method for semantic seg...</td>\n",
       "      <td>[Vikas Agaradahalli Gurumurthy, Ramesh Kestur,...</td>\n",
       "      <td>[cs.CV]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 url               updated  \\\n",
       "0   http://arxiv.org/abs/1409.4842v1  2014-09-17T01:03:11Z   \n",
       "1  http://arxiv.org/abs/2011.06825v2  2020-12-03T19:50:31Z   \n",
       "2  http://arxiv.org/abs/2003.11213v1  2020-03-25T04:27:01Z   \n",
       "3  http://arxiv.org/abs/1907.09194v2  2020-04-30T10:39:17Z   \n",
       "4  http://arxiv.org/abs/1907.06915v1  2019-07-16T09:41:13Z   \n",
       "\n",
       "              published                                              title  \\\n",
       "0  2014-09-17T01:03:11Z                     Going Deeper with Convolutions   \n",
       "1  2020-11-13T09:33:03Z  LULC classification by semantic segmentation o...   \n",
       "2  2020-03-25T04:27:01Z  A New Multiple Max-pooling Integration Module ...   \n",
       "3  2019-07-22T09:19:05Z  FD-FCN: 3D Fully Dense and Fully Convolutional...   \n",
       "4  2019-07-16T09:41:13Z  Mango Tree Net -- A fully convolutional networ...   \n",
       "\n",
       "                                             summary  \\\n",
       "0    We propose a deep convolutional neural netwo...   \n",
       "1    This paper analyses how well a Fast Fully Co...   \n",
       "2    To better retain the deep features of an ima...   \n",
       "3    In this paper, a 3D patch-based fully dense ...   \n",
       "4    This work presents a method for semantic seg...   \n",
       "\n",
       "                                             authors               categories  \n",
       "0  [Christian Szegedy, Wei Liu, Yangqing Jia, Pie...                  [cs.CV]  \n",
       "1  [Md. Saif Hassan Onim, Aiman Rafeed Ehtesham, ...           [cs.CV, cs.LG]  \n",
       "2  [Hongfeng You, Shengwei Tian, Long Yu, Xiang M...  [cs.CV, cs.LG, eess.IV]  \n",
       "3                        [Binbin Yang, Weiwei Zhang]         [eess.IV, cs.CV]  \n",
       "4  [Vikas Agaradahalli Gurumurthy, Ramesh Kestur,...                  [cs.CV]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_arxiv = pd.read_json(f'crawler558/arxiv_crawler_{TOPIC}.jl', lines=True)\n",
    "print(f'df_arxiv.shape pre  deduplucation: {df_arxiv.shape}')\n",
    "df_arxiv = df_arxiv.drop_duplicates(subset='title')\n",
    "print(f'df_arxiv.shape post deduplucation: {df_arxiv.shape}')\n",
    "df_arxiv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "determined-quarter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1151 entries, 0 to 1150\n",
      "Data columns (total 8 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   ID          1151 non-null   object\n",
      " 1   url         1151 non-null   object\n",
      " 2   updated     1151 non-null   object\n",
      " 3   published   1151 non-null   object\n",
      " 4   title       1151 non-null   object\n",
      " 5   summary     1151 non-null   object\n",
      " 6   authors     1151 non-null   object\n",
      " 7   categories  1151 non-null   object\n",
      "dtypes: object(8)\n",
      "memory usage: 72.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Generate an id column for RLTK to use\n",
    "df_arxiv.reset_index(inplace=True)\n",
    "df_arxiv['index'] = df_arxiv['index'].astype('str')\n",
    "df_arxiv.rename(columns={'index':'ID'}, inplace=True)\n",
    "df_arxiv.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-soundtrack",
   "metadata": {},
   "source": [
    "**Google Scholar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "numerous-people",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_gscholar.shape pre  deduplication: (746, 6)\n",
      "df_gscholar.shape post deduplication: (738, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>citations</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Going Deeper With Convolutions</td>\n",
       "      <td>http://scholar.google.com/scholar?oi=bibs&amp;clus...</td>\n",
       "      <td>[C Szegedy, W Liu, Y Jia, P Sermanet, S Reed, ...</td>\n",
       "      <td>Proceedings of the IEEE Conference on Computer...</td>\n",
       "      <td>22434</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fully Convolutional Networks for Semantic Segm...</td>\n",
       "      <td>http://scholar.google.com/scholar?oi=bibs&amp;clus...</td>\n",
       "      <td>[J Long, E Shelhamer, T Darrell]</td>\n",
       "      <td>Proceedings of the IEEE Conference on Computer...</td>\n",
       "      <td>16664</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You Only Look Once: Unified, Real-Time Object ...</td>\n",
       "      <td>http://scholar.google.com/scholar?oi=bibs&amp;clus...</td>\n",
       "      <td>[J Redmon, S Divvala, R Girshick, A Farhadi]</td>\n",
       "      <td>Proceedings of the IEEE Conference on Computer...</td>\n",
       "      <td>9772</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Densely Connected Convolutional Networks</td>\n",
       "      <td>http://scholar.google.com/scholar?oi=bibs&amp;clus...</td>\n",
       "      <td>[G Huang, Z Liu, L van der Maaten, KQ Weinberger]</td>\n",
       "      <td>Proceedings of the IEEE Conference on Computer...</td>\n",
       "      <td>9733</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rethinking the Inception Architecture for Comp...</td>\n",
       "      <td>http://scholar.google.com/scholar?oi=bibs&amp;clus...</td>\n",
       "      <td>[C Szegedy, V Vanhoucke, S Ioffe, J Shlens, Z ...</td>\n",
       "      <td>Proceedings of the IEEE Conference on Computer...</td>\n",
       "      <td>8499</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                     Going Deeper With Convolutions   \n",
       "1  Fully Convolutional Networks for Semantic Segm...   \n",
       "2  You Only Look Once: Unified, Real-Time Object ...   \n",
       "3           Densely Connected Convolutional Networks   \n",
       "4  Rethinking the Inception Architecture for Comp...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  http://scholar.google.com/scholar?oi=bibs&clus...   \n",
       "1  http://scholar.google.com/scholar?oi=bibs&clus...   \n",
       "2  http://scholar.google.com/scholar?oi=bibs&clus...   \n",
       "3  http://scholar.google.com/scholar?oi=bibs&clus...   \n",
       "4  http://scholar.google.com/scholar?oi=bibs&clus...   \n",
       "\n",
       "                                             authors  \\\n",
       "0  [C Szegedy, W Liu, Y Jia, P Sermanet, S Reed, ...   \n",
       "1                   [J Long, E Shelhamer, T Darrell]   \n",
       "2       [J Redmon, S Divvala, R Girshick, A Farhadi]   \n",
       "3  [G Huang, Z Liu, L van der Maaten, KQ Weinberger]   \n",
       "4  [C Szegedy, V Vanhoucke, S Ioffe, J Shlens, Z ...   \n",
       "\n",
       "                                             journal  citations  year  \n",
       "0  Proceedings of the IEEE Conference on Computer...      22434  2015  \n",
       "1  Proceedings of the IEEE Conference on Computer...      16664  2015  \n",
       "2  Proceedings of the IEEE Conference on Computer...       9772  2016  \n",
       "3  Proceedings of the IEEE Conference on Computer...       9733  2017  \n",
       "4  Proceedings of the IEEE Conference on Computer...       8499  2016  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gscholar = pd.read_json(f'Google_Scholar/articles_{TOPIC}.json')\n",
    "print(f'df_gscholar.shape pre  deduplication: {df_gscholar.shape}')\n",
    "df_gscholar = df_gscholar.drop_duplicates(subset='title')\n",
    "print(f'df_gscholar.shape post deduplication: {df_gscholar.shape}')\n",
    "\n",
    "# Fix the wrong URLs\n",
    "df_gscholar['url'] = df_gscholar['url'].apply(lambda x: x[27:])\n",
    "\n",
    "df_gscholar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "third-clause",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 738 entries, 0 to 737\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   ID         738 non-null    object\n",
      " 1   title      738 non-null    object\n",
      " 2   url        738 non-null    object\n",
      " 3   authors    738 non-null    object\n",
      " 4   journal    738 non-null    object\n",
      " 5   citations  738 non-null    object\n",
      " 6   year       738 non-null    object\n",
      "dtypes: object(7)\n",
      "memory usage: 40.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# Generate an id column for RLTK to use\n",
    "df_gscholar.reset_index(inplace=True)\n",
    "df_gscholar['index'] = df_gscholar['index'].astype('str')\n",
    "df_gscholar.rename(columns={'index':'ID'}, inplace=True)\n",
    "\n",
    "# Also set all columns to string type\n",
    "df_gscholar['citations'] = df_gscholar['citations'].astype('str')\n",
    "df_gscholar['year'] = df_gscholar['year'].astype('str')\n",
    "df_gscholar.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-reporter",
   "metadata": {},
   "source": [
    "### Naïve Data Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "opposite-lawyer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234\n"
     ]
    }
   ],
   "source": [
    "# How many matches can be found with a naÏve identical string approach?\n",
    "arxiv_titles = df_arxiv['title']\n",
    "gscholar_titles = df_gscholar['title']\n",
    "print(len([1 for w in arxiv_titles.values if w in gscholar_titles.values]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "protective-manual",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_merged.shape: (1655, 14)\n",
      "Number of match found with pd.merge: 234\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_x</th>\n",
       "      <th>url_x</th>\n",
       "      <th>updated</th>\n",
       "      <th>published</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>authors_x</th>\n",
       "      <th>categories</th>\n",
       "      <th>ID_y</th>\n",
       "      <th>url_y</th>\n",
       "      <th>authors_y</th>\n",
       "      <th>journal</th>\n",
       "      <th>citations</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>http://arxiv.org/abs/1409.4842v1</td>\n",
       "      <td>2014-09-17T01:03:11Z</td>\n",
       "      <td>2014-09-17T01:03:11Z</td>\n",
       "      <td>Going Deeper with Convolutions</td>\n",
       "      <td>We propose a deep convolutional neural netwo...</td>\n",
       "      <td>[Christian Szegedy, Wei Liu, Yangqing Jia, Pie...</td>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>http://arxiv.org/abs/2011.06825v2</td>\n",
       "      <td>2020-12-03T19:50:31Z</td>\n",
       "      <td>2020-11-13T09:33:03Z</td>\n",
       "      <td>LULC classification by semantic segmentation o...</td>\n",
       "      <td>This paper analyses how well a Fast Fully Co...</td>\n",
       "      <td>[Md. Saif Hassan Onim, Aiman Rafeed Ehtesham, ...</td>\n",
       "      <td>[cs.CV, cs.LG]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>http://arxiv.org/abs/2003.11213v1</td>\n",
       "      <td>2020-03-25T04:27:01Z</td>\n",
       "      <td>2020-03-25T04:27:01Z</td>\n",
       "      <td>A New Multiple Max-pooling Integration Module ...</td>\n",
       "      <td>To better retain the deep features of an ima...</td>\n",
       "      <td>[Hongfeng You, Shengwei Tian, Long Yu, Xiang M...</td>\n",
       "      <td>[cs.CV, cs.LG, eess.IV]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID_x                              url_x               updated  \\\n",
       "0    0   http://arxiv.org/abs/1409.4842v1  2014-09-17T01:03:11Z   \n",
       "1    1  http://arxiv.org/abs/2011.06825v2  2020-12-03T19:50:31Z   \n",
       "2    2  http://arxiv.org/abs/2003.11213v1  2020-03-25T04:27:01Z   \n",
       "\n",
       "              published                                              title  \\\n",
       "0  2014-09-17T01:03:11Z                     Going Deeper with Convolutions   \n",
       "1  2020-11-13T09:33:03Z  LULC classification by semantic segmentation o...   \n",
       "2  2020-03-25T04:27:01Z  A New Multiple Max-pooling Integration Module ...   \n",
       "\n",
       "                                             summary  \\\n",
       "0    We propose a deep convolutional neural netwo...   \n",
       "1    This paper analyses how well a Fast Fully Co...   \n",
       "2    To better retain the deep features of an ima...   \n",
       "\n",
       "                                           authors_x               categories  \\\n",
       "0  [Christian Szegedy, Wei Liu, Yangqing Jia, Pie...                  [cs.CV]   \n",
       "1  [Md. Saif Hassan Onim, Aiman Rafeed Ehtesham, ...           [cs.CV, cs.LG]   \n",
       "2  [Hongfeng You, Shengwei Tian, Long Yu, Xiang M...  [cs.CV, cs.LG, eess.IV]   \n",
       "\n",
       "  ID_y url_y authors_y journal citations year  \n",
       "0  NaN   NaN       NaN     NaN       NaN  NaN  \n",
       "1  NaN   NaN       NaN     NaN       NaN  NaN  \n",
       "2  NaN   NaN       NaN     NaN       NaN  NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge both datasets by \"SQL\" join\n",
    "df_merged = df_arxiv.merge(df_gscholar, on='title', how='outer')\n",
    "print(f'df_merged.shape: {df_merged.shape}')\n",
    "\n",
    "# Number of match found with pd.merge:\n",
    "print(f'Number of match found with pd.merge: {df_arxiv.shape[0] + df_gscholar.shape[0] - df_merged.shape[0]}')\n",
    "\n",
    "df_merged.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-count",
   "metadata": {},
   "source": [
    "## RLTK Data Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "divine-formula",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RLTK Tokenizer\n",
    "tokenizer = rltk.CrfTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-lighting",
   "metadata": {},
   "source": [
    "**Arxiv Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "informal-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArxivRecord(rltk.Record):\n",
    "    def __init__(self, raw_object):\n",
    "        super().__init__(raw_object)\n",
    "        self.name = 'ArxivRecord'\n",
    "        \n",
    "    @property\n",
    "    def id(self):\n",
    "        return self.raw_object['ID']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def authors_string(self):\n",
    "        return self.raw_object['authors']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def title_string(self):\n",
    "        return self.raw_object['title']\n",
    "        \n",
    "    @rltk.cached_property\n",
    "    def summary_string(self):\n",
    "        return self.raw_object['summary']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def categories_string(self):\n",
    "        return self.raw_object['categories']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def published_string(self):\n",
    "        return self.raw_object['published']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def updated_string(self):\n",
    "        return self.raw_object['updated']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def url_string(self):\n",
    "        return self.raw_object['url']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def blocking_tokens(self):\n",
    "        tokens = ' '.join([self.title_string])\n",
    "        tokens = re.sub(r'\\bThe\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bthe\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bof\\b', '', tokens)\n",
    "        tokens = re.sub(r\"\\b's\\b\", '', tokens)\n",
    "        tokens = re.sub(r'\\band\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bI\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bA\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bin\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bfor\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bon\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bwith\\b', '', tokens)\n",
    "        return set(tokenizer.tokenize(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "marked-marketing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'rltk.dataset.Dataset'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>authors_string</th>\n",
       "      <th>title_string</th>\n",
       "      <th>summary_string</th>\n",
       "      <th>categories_string</th>\n",
       "      <th>published_string</th>\n",
       "      <th>updated_string</th>\n",
       "      <th>url_string</th>\n",
       "      <th>blocking_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Christian Szegedy, Wei Liu, Yangqing Jia, Pie...</td>\n",
       "      <td>Going Deeper with Convolutions</td>\n",
       "      <td>We propose a deep convolutional neural netwo...</td>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>2014-09-17T01:03:11Z</td>\n",
       "      <td>2014-09-17T01:03:11Z</td>\n",
       "      <td>http://arxiv.org/abs/1409.4842v1</td>\n",
       "      <td>{Convolutions, Going, Deeper}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Md. Saif Hassan Onim, Aiman Rafeed Ehtesham, ...</td>\n",
       "      <td>LULC classification by semantic segmentation o...</td>\n",
       "      <td>This paper analyses how well a Fast Fully Co...</td>\n",
       "      <td>[cs.CV, cs.LG]</td>\n",
       "      <td>2020-11-13T09:33:03Z</td>\n",
       "      <td>2020-12-03T19:50:31Z</td>\n",
       "      <td>http://arxiv.org/abs/2011.06825v2</td>\n",
       "      <td>{images, by, satellite, semantic, using, segme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[Hongfeng You, Shengwei Tian, Long Yu, Xiang M...</td>\n",
       "      <td>A New Multiple Max-pooling Integration Module ...</td>\n",
       "      <td>To better retain the deep features of an ima...</td>\n",
       "      <td>[cs.CV, cs.LG, eess.IV]</td>\n",
       "      <td>2020-03-25T04:27:01Z</td>\n",
       "      <td>2020-03-25T04:27:01Z</td>\n",
       "      <td>http://arxiv.org/abs/2003.11213v1</td>\n",
       "      <td>{Multiple, Segmentation, New, -, Module, Decon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                     authors_string  \\\n",
       "0  0  [Christian Szegedy, Wei Liu, Yangqing Jia, Pie...   \n",
       "1  1  [Md. Saif Hassan Onim, Aiman Rafeed Ehtesham, ...   \n",
       "2  2  [Hongfeng You, Shengwei Tian, Long Yu, Xiang M...   \n",
       "\n",
       "                                        title_string  \\\n",
       "0                     Going Deeper with Convolutions   \n",
       "1  LULC classification by semantic segmentation o...   \n",
       "2  A New Multiple Max-pooling Integration Module ...   \n",
       "\n",
       "                                      summary_string        categories_string  \\\n",
       "0    We propose a deep convolutional neural netwo...                  [cs.CV]   \n",
       "1    This paper analyses how well a Fast Fully Co...           [cs.CV, cs.LG]   \n",
       "2    To better retain the deep features of an ima...  [cs.CV, cs.LG, eess.IV]   \n",
       "\n",
       "       published_string        updated_string  \\\n",
       "0  2014-09-17T01:03:11Z  2014-09-17T01:03:11Z   \n",
       "1  2020-11-13T09:33:03Z  2020-12-03T19:50:31Z   \n",
       "2  2020-03-25T04:27:01Z  2020-03-25T04:27:01Z   \n",
       "\n",
       "                          url_string  \\\n",
       "0   http://arxiv.org/abs/1409.4842v1   \n",
       "1  http://arxiv.org/abs/2011.06825v2   \n",
       "2  http://arxiv.org/abs/2003.11213v1   \n",
       "\n",
       "                                     blocking_tokens  \n",
       "0                      {Convolutions, Going, Deeper}  \n",
       "1  {images, by, satellite, semantic, using, segme...  \n",
       "2  {Multiple, Segmentation, New, -, Module, Decon...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_arxiv = rltk.Dataset(reader=rltk.DataFrameReader(df_arxiv), record_class=ArxivRecord, adapter=rltk.MemoryKeyValueAdapter())\n",
    "print(type(ds_arxiv))\n",
    "ds_arxiv.generate_dataframe().head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-steps",
   "metadata": {},
   "source": [
    "**Google Scholar Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "august-sellers",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GScholarRecord(rltk.Record):\n",
    "    def __init__(self, raw_object):\n",
    "        super().__init__(raw_object)\n",
    "        self.name = 'GScholarRecord'\n",
    "        \n",
    "    @property\n",
    "    def id(self):\n",
    "        return self.raw_object['ID']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def authors_string(self):\n",
    "        return self.raw_object['authors']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def title_string(self):\n",
    "        return self.raw_object['title']\n",
    "        \n",
    "    @rltk.cached_property\n",
    "    def journal_string(self):\n",
    "        return self.raw_object['journal']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def citations_string(self):\n",
    "        return self.raw_object['citations']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def year_string(self):\n",
    "        return self.raw_object['year']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def url_string(self):\n",
    "        return self.raw_object['url']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def blocking_tokens(self):\n",
    "        tokens = ' '.join([self.title_string])\n",
    "        tokens = re.sub(r'\\bThe\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bthe\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bof\\b', '', tokens)\n",
    "        tokens = re.sub(r\"\\b's\\b\", '', tokens)\n",
    "        tokens = re.sub(r'\\band\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bI\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bA\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bin\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bfor\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bon\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bwith\\b', '', tokens)\n",
    "        return set(tokenizer.tokenize(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "embedded-tournament",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'rltk.dataset.Dataset'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>authors_string</th>\n",
       "      <th>title_string</th>\n",
       "      <th>journal_string</th>\n",
       "      <th>citations_string</th>\n",
       "      <th>year_string</th>\n",
       "      <th>url_string</th>\n",
       "      <th>blocking_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[C Szegedy, W Liu, Y Jia, P Sermanet, S Reed, ...</td>\n",
       "      <td>Going Deeper With Convolutions</td>\n",
       "      <td>Proceedings of the IEEE Conference on Computer...</td>\n",
       "      <td>22434</td>\n",
       "      <td>2015</td>\n",
       "      <td>http://scholar.google.com/scholar?oi=bibs&amp;clus...</td>\n",
       "      <td>{With, Convolutions, Going, Deeper}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[J Long, E Shelhamer, T Darrell]</td>\n",
       "      <td>Fully Convolutional Networks for Semantic Segm...</td>\n",
       "      <td>Proceedings of the IEEE Conference on Computer...</td>\n",
       "      <td>16664</td>\n",
       "      <td>2015</td>\n",
       "      <td>http://scholar.google.com/scholar?oi=bibs&amp;clus...</td>\n",
       "      <td>{Fully, Segmentation, Networks, Semantic, Conv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[J Redmon, S Divvala, R Girshick, A Farhadi]</td>\n",
       "      <td>You Only Look Once: Unified, Real-Time Object ...</td>\n",
       "      <td>Proceedings of the IEEE Conference on Computer...</td>\n",
       "      <td>9772</td>\n",
       "      <td>2016</td>\n",
       "      <td>http://scholar.google.com/scholar?oi=bibs&amp;clus...</td>\n",
       "      <td>{Detection, -, Look, You, :, Time, Only, Unifi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                     authors_string  \\\n",
       "0  0  [C Szegedy, W Liu, Y Jia, P Sermanet, S Reed, ...   \n",
       "1  1                   [J Long, E Shelhamer, T Darrell]   \n",
       "2  2       [J Redmon, S Divvala, R Girshick, A Farhadi]   \n",
       "\n",
       "                                        title_string  \\\n",
       "0                     Going Deeper With Convolutions   \n",
       "1  Fully Convolutional Networks for Semantic Segm...   \n",
       "2  You Only Look Once: Unified, Real-Time Object ...   \n",
       "\n",
       "                                      journal_string citations_string  \\\n",
       "0  Proceedings of the IEEE Conference on Computer...            22434   \n",
       "1  Proceedings of the IEEE Conference on Computer...            16664   \n",
       "2  Proceedings of the IEEE Conference on Computer...             9772   \n",
       "\n",
       "  year_string                                         url_string  \\\n",
       "0        2015  http://scholar.google.com/scholar?oi=bibs&clus...   \n",
       "1        2015  http://scholar.google.com/scholar?oi=bibs&clus...   \n",
       "2        2016  http://scholar.google.com/scholar?oi=bibs&clus...   \n",
       "\n",
       "                                     blocking_tokens  \n",
       "0                {With, Convolutions, Going, Deeper}  \n",
       "1  {Fully, Segmentation, Networks, Semantic, Conv...  \n",
       "2  {Detection, -, Look, You, :, Time, Only, Unifi...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_gscholar = rltk.Dataset(reader=rltk.DataFrameReader(df_gscholar), record_class=GScholarRecord, adapter=rltk.MemoryKeyValueAdapter())\n",
    "print(type(ds_gscholar))\n",
    "ds_gscholar.generate_dataframe().head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-fraud",
   "metadata": {},
   "source": [
    "### Blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dramatic-disabled",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'rltk.blocking.block.Block'>\n"
     ]
    }
   ],
   "source": [
    "# Generate blocks from tokens\n",
    "token_blocker = rltk.TokenBlockGenerator()\n",
    "blocks = token_blocker.generate(\n",
    "    token_blocker.block(ds_arxiv, property_='blocking_tokens'),\n",
    "    token_blocker.block(ds_gscholar, property_='blocking_tokens'))\n",
    "print(type(blocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fifth-dispatch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction Ratio: 0.45906\n"
     ]
    }
   ],
   "source": [
    "# Extract all record pairs from the block\n",
    "record_pairs = rltk.get_record_pairs(ds_arxiv, ds_gscholar, block=blocks)\n",
    "\n",
    "# Get the total number of record pairs generated\n",
    "compared_pairs = len(list(record_pairs))\n",
    "\n",
    "# Get the number of elements in each rltk.Dataset\n",
    "tally_imdb = ds_arxiv.generate_dataframe().shape[0]\n",
    "tally_tmd = ds_gscholar.generate_dataframe().shape[0]\n",
    "\n",
    "# Calculate the total number of pairs if both datasets were to be compared without any blocking (eg: a double for loop)\n",
    "tally_unblocked = tally_imdb * tally_tmd\n",
    "\n",
    "# Calculate how much smaller the blocked pairings are\n",
    "reduction_ratio = compared_pairs / tally_unblocked\n",
    "\n",
    "# Calculate the reduction ratio (the inverse of the )\n",
    "reduction_ratio = 1 - reduction_ratio\n",
    "print(f'Reduction Ratio: {reduction_ratio:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-public",
   "metadata": {},
   "source": [
    "### Matching Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "automated-penny",
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_similarity(arxiv_tuple, gscholar_tuple):\n",
    "    arxiv_title = arxiv_tuple.title_string.strip().lower()\n",
    "    gscholar_title = gscholar_tuple.title_string.strip().lower()\n",
    "    similarity = SequenceMatcher(None, arxiv_title, gscholar_title).ratio()\n",
    "\n",
    "    penalties = sum([len(arxiv_title)<=6,\n",
    "                     len(gscholar_title)<=6])\n",
    "\n",
    "    return similarity * (0.9**penalties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "together-terminology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def author_similarity(arxiv_tuple, gscholar_tuple):\n",
    "    arxiv_author = ' '.join(arxiv_tuple.authors_string).strip().lower()\n",
    "    gscholar_author = ' '.join(gscholar_tuple.authors_string).strip().lower()\n",
    "    similarity = SequenceMatcher(None, arxiv_author, gscholar_author).ratio() \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "wooden-powder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_similarity(arxiv_tuple, gscholar_tuple):\n",
    "    arxiv_year = int(float(arxiv_tuple.updated_string[0:4]))\n",
    "    gscholar_year = int(float(gscholar_tuple.year_string))\n",
    "    similarity = 1 /(1 + abs(arxiv_year-gscholar_year))\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "wireless-tenant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elementwise_similarity(arxiv_tuple, gscholar_tuple, match_threshold=0.75):\n",
    "    sim_title = title_similarity(arxiv_tuple, gscholar_tuple)\n",
    "    sim_author = author_similarity(arxiv_tuple, gscholar_tuple)\n",
    "    sim_year = year_similarity(arxiv_tuple, gscholar_tuple)\n",
    "\n",
    "    element_similarity = (0.70 * sim_title) + (0.15 * sim_author) + (0.15 * sim_year)\n",
    "\n",
    "    return element_similarity > match_threshold, element_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "straight-vertex",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arxiv samples: 1151\n",
      "GScholar samples: 738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 21/21 [1:48:31<00:00, 310.09s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Matches</th>\n",
       "      <th>Set_A Size</th>\n",
       "      <th>Set_B Size</th>\n",
       "      <th>Duplicates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.00</th>\n",
       "      <td>342675.0</td>\n",
       "      <td>1144.0</td>\n",
       "      <td>342675.0</td>\n",
       "      <td>683468.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.05</th>\n",
       "      <td>342675.0</td>\n",
       "      <td>1144.0</td>\n",
       "      <td>342675.0</td>\n",
       "      <td>683468.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.10</th>\n",
       "      <td>342578.0</td>\n",
       "      <td>1144.0</td>\n",
       "      <td>342578.0</td>\n",
       "      <td>683274.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.15</th>\n",
       "      <td>339952.0</td>\n",
       "      <td>1144.0</td>\n",
       "      <td>339952.0</td>\n",
       "      <td>678022.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.20</th>\n",
       "      <td>323173.0</td>\n",
       "      <td>1144.0</td>\n",
       "      <td>323173.0</td>\n",
       "      <td>644464.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>273317.0</td>\n",
       "      <td>1144.0</td>\n",
       "      <td>273317.0</td>\n",
       "      <td>544752.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.30</th>\n",
       "      <td>187785.0</td>\n",
       "      <td>1139.0</td>\n",
       "      <td>187785.0</td>\n",
       "      <td>373693.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.35</th>\n",
       "      <td>105151.0</td>\n",
       "      <td>1107.0</td>\n",
       "      <td>105151.0</td>\n",
       "      <td>208457.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.40</th>\n",
       "      <td>50316.0</td>\n",
       "      <td>1037.0</td>\n",
       "      <td>50316.0</td>\n",
       "      <td>98857.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.45</th>\n",
       "      <td>20727.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>20727.0</td>\n",
       "      <td>39819.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>7687.0</td>\n",
       "      <td>776.0</td>\n",
       "      <td>7687.0</td>\n",
       "      <td>13887.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.55</th>\n",
       "      <td>2855.0</td>\n",
       "      <td>681.0</td>\n",
       "      <td>2855.0</td>\n",
       "      <td>4374.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.60</th>\n",
       "      <td>1243.0</td>\n",
       "      <td>630.0</td>\n",
       "      <td>1243.0</td>\n",
       "      <td>1234.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.65</th>\n",
       "      <td>745.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>745.0</td>\n",
       "      <td>301.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>620.0</td>\n",
       "      <td>576.0</td>\n",
       "      <td>620.0</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>573.0</td>\n",
       "      <td>564.0</td>\n",
       "      <td>573.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.80</th>\n",
       "      <td>570.0</td>\n",
       "      <td>563.0</td>\n",
       "      <td>570.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.85</th>\n",
       "      <td>553.0</td>\n",
       "      <td>550.0</td>\n",
       "      <td>553.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>449.0</td>\n",
       "      <td>449.0</td>\n",
       "      <td>449.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.95</th>\n",
       "      <td>372.0</td>\n",
       "      <td>372.0</td>\n",
       "      <td>372.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Matches  Set_A Size  Set_B Size  Duplicates\n",
       "0.00  342675.0      1144.0    342675.0    683468.0\n",
       "0.05  342675.0      1144.0    342675.0    683468.0\n",
       "0.10  342578.0      1144.0    342578.0    683274.0\n",
       "0.15  339952.0      1144.0    339952.0    678022.0\n",
       "0.20  323173.0      1144.0    323173.0    644464.0\n",
       "0.25  273317.0      1144.0    273317.0    544752.0\n",
       "0.30  187785.0      1139.0    187785.0    373693.0\n",
       "0.35  105151.0      1107.0    105151.0    208457.0\n",
       "0.40   50316.0      1037.0     50316.0     98857.0\n",
       "0.45   20727.0       900.0     20727.0     39819.0\n",
       "0.50    7687.0       776.0      7687.0     13887.0\n",
       "0.55    2855.0       681.0      2855.0      4374.0\n",
       "0.60    1243.0       630.0      1243.0      1234.0\n",
       "0.65     745.0       594.0       745.0       301.0\n",
       "0.70     620.0       576.0       620.0        89.0\n",
       "0.75     573.0       564.0       573.0        16.0\n",
       "0.80     570.0       563.0       570.0        12.0\n",
       "0.85     553.0       550.0       553.0         3.0\n",
       "0.90     449.0       449.0       449.0         0.0\n",
       "0.95     372.0       372.0       372.0         0.0\n",
       "1.00       0.0         0.0         0.0         0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict matches for all pairs in the blocked data \n",
    "print(f'Arxiv samples: {df_arxiv.shape[0]}')\n",
    "print(f'GScholar samples: {df_gscholar.shape[0]}')\n",
    "\n",
    "summary_df = pd.DataFrame()\n",
    "THRESHOLDS = [T/100 for T in range(0, 101, 5)]\n",
    "\n",
    "# Iterate through various thresholds to find the most matches without any duplicates\n",
    "for T in tqdm(THRESHOLDS):\n",
    "\n",
    "    # Set to store pairs of IDs matched\n",
    "    ids_matched = set()\n",
    "    \n",
    "    # Iterate through candidates on the block\n",
    "    for block_id, arxiv_id, gscholar_id in blocks.pairwise(ds_arxiv, ds_gscholar):\n",
    "        \n",
    "        # Find similarity at a given threshold\n",
    "        match , similarity = elementwise_similarity(ds_arxiv.get_record(arxiv_id),\n",
    "                                                    ds_gscholar.get_record(gscholar_id),\n",
    "                                                    match_threshold=T)\n",
    "        # If a match is found, add to the set of matches\n",
    "        if match:\n",
    "            ids_matched.add((arxiv_id, gscholar_id))\n",
    "    \n",
    "    # Count the number of unique elements derived from each source\n",
    "    set_a = set()\n",
    "    set_b = set()\n",
    "    for tp in ids_matched:\n",
    "        set_a.add(tp[0])\n",
    "        set_b.add(tp[1])\n",
    "    \n",
    "    summary_df.at[T, 'Matches'] = int(len(ids_matched))\n",
    "    summary_df.at[T, 'Set_A Size'] = int(len(set_a))\n",
    "    summary_df.at[T, 'Set_B Size'] = int(len(ids_matched))\n",
    "    summary_df.at[T, 'Duplicates'] = int((len(ids_matched)-len(set_a)) + (len(ids_matched)-len(set_b)))\n",
    "    \n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afraid-anaheim",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the lowest threshold which gives no duplicates\n",
    "optimal_threshold = summary_df[summary_df['Duplicates']==0].index[0]\n",
    "optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "printable-blowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually set threshold to 0.85 to trade 104 extra True Positives for 3 extra False Positive\n",
    "optimal_threshold = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "recovered-florence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arxiv samples: 1151\n",
      "GScholar samples: 738\n",
      "\n",
      "Matches: 553\n",
      "Non-Matches Arxiv: 594\n",
      "Non-Matches GScholar: 185\n"
     ]
    }
   ],
   "source": [
    "# Generate matches based on the optimal (no-duplicate) threshold\n",
    "print(f'Arxiv samples: {df_arxiv.shape[0]}')\n",
    "print(f'GScholar samples: {df_gscholar.shape[0]}')\n",
    "\n",
    "# Store tuples of matches IDs, as well as singletons witouth a match\n",
    "ids_matched = set()\n",
    "singles_arxiv = set()\n",
    "singles_gscholar = set()\n",
    "\n",
    "# Write matches (and non-matches) to a CSV\n",
    "with open(f'Matches_{TOPIC}.csv', 'w') as predictions_full:\n",
    "    for block_id, arxiv_id, gscholar_id in blocks.pairwise(ds_arxiv, ds_gscholar):\n",
    "\n",
    "        match , similarity = elementwise_similarity(ds_arxiv.get_record(arxiv_id),\n",
    "                                                    ds_gscholar.get_record(gscholar_id),\n",
    "                                                    match_threshold=optimal_threshold)\n",
    "\n",
    "        if match:\n",
    "            ids_matched.add((arxiv_id, gscholar_id))\n",
    "        else:\n",
    "            singles_arxiv.add(arxiv_id)\n",
    "            singles_gscholar.add(gscholar_id)\n",
    "    \n",
    "    # After finding all matches, write them to a csv\n",
    "    for match_pair in ids_matched:\n",
    "        predictions_full.write(f'{match_pair[0]},{match_pair[1]},1\\n')\n",
    "        # And ensure that no item in the matches is counted as a single\n",
    "        try:\n",
    "            singles_arxiv.remove(match_pair[0])\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            singles_gscholar.remove(match_pair[1])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Then write all the singles which didn't find a match\n",
    "    NULL = None\n",
    "    for arxiv_id in singles_arxiv:\n",
    "        predictions_full.write(f'{arxiv_id},{NULL},0\\n')\n",
    "    for gscholar_id in singles_gscholar:\n",
    "        predictions_full.write(f'{NULL},{gscholar_id},0\\n')        \n",
    "        \n",
    "print()\n",
    "print(f'Matches: {len(ids_matched)}')\n",
    "print(f'Non-Matches Arxiv: {len(singles_arxiv)}')\n",
    "print(f'Non-Matches GScholar: {len(singles_gscholar)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-clause",
   "metadata": {},
   "source": [
    "### Create Merged Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "basic-banking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib import URIRef, Literal, Namespace\n",
    "from rdflib.namespace import RDF, RDFS, XSD\n",
    "\n",
    "MYNS = Namespace('http://inf558.org/myfakenamespace#')\n",
    "SCHEMA = Namespace(\"https://schema.org/\")\n",
    "\n",
    "# Initliaze the graph\n",
    "g = rdflib.Graph()\n",
    "\n",
    "# Bind namespace and prefixes\n",
    "g.bind('my_ns', MYNS)\n",
    "g.bind('schema', SCHEMA)\n",
    "g.bind('rdf', RDF)\n",
    "g.bind('rdfs', RDFS)\n",
    "g.bind('xsd', XSD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "assumed-delta",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions_df.shape: (1332, 3)\n",
      "predicted matches: 553  [41.52 %]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARXIV_ID</th>\n",
       "      <th>GSCHOLAR_ID</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>364</td>\n",
       "      <td>156</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1120</td>\n",
       "      <td>713</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1049</td>\n",
       "      <td>630</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>949</td>\n",
       "      <td>533</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>527</td>\n",
       "      <td>275</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ARXIV_ID GSCHOLAR_ID  LABEL\n",
       "0      364         156      1\n",
       "1     1120         713      1\n",
       "2     1049         630      1\n",
       "3      949         533      1\n",
       "4      527         275      1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load predictions to be used in populating the RDF\n",
    "predictions_df = pd.read_csv(f'Matches_{TOPIC}.csv', header=None, names=['ARXIV_ID', 'GSCHOLAR_ID', 'LABEL'])\n",
    "print(f'predictions_df.shape: {predictions_df.shape}')\n",
    "predicted_matches = predictions_df['LABEL'].sum()\n",
    "print(f'predicted matches: {predicted_matches}  [{100*predicted_matches/predictions_df.shape[0]:.2f} %]')\n",
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "funded-azerbaijan",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1332/1332 [00:16<00:00, 81.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# Dataframe to store the merged datasets\n",
    "df_merged = pd.DataFrame(columns = ['ID', 'title', 'authors', 'published', 'updated', \n",
    "                                    'abstract', 'categories', 'citations', 'arxiv_url', 'gscholar_url'],\n",
    "                         dtype='object')\n",
    "json_merged = {}\n",
    "\n",
    "NEW_ID = 0\n",
    "\n",
    "# Populate the RDF with predictions with a positive (1) label\n",
    "for idx, row in tqdm(predictions_df.iterrows(), total=predictions_df.shape[0]):\n",
    "    \n",
    "    # Populate the json object\n",
    "    json_merged[NEW_ID] = {}\n",
    "    \n",
    "    ### URI ###\n",
    "    node_uri = URIRef(str(NEW_ID))\n",
    "    g.add((node_uri, RDF.type, SCHEMA.ScholarlyArticle))\n",
    "    df_merged.at[NEW_ID, 'ID'] = NEW_ID\n",
    "    json_merged[NEW_ID]['ID'] = NEW_ID\n",
    "\n",
    "    \n",
    "    ### Title ###\n",
    "    try:\n",
    "        title_arxiv = str(df_arxiv[df_arxiv['ID'] == str(row['ARXIV_ID'])]['title'].values[0])\n",
    "    except:\n",
    "        title_arxiv = '<___>'\n",
    "    try:\n",
    "        title_gscholar = str(df_gscholar[df_gscholar['ID'] == str(row['GSCHOLAR_ID'])]['title'].values[0])\n",
    "    except:\n",
    "        title_gscholar = '<___>'\n",
    "    title = title_arxiv if title_arxiv != '<___>' else title_gscholar if title_gscholar != '<___>' else None\n",
    "    g.add((node_uri, SCHEMA.headline, Literal(title, datatype=SCHEMA.Text)))\n",
    "    df_merged.at[NEW_ID, 'title'] = title\n",
    "    json_merged[NEW_ID]['title'] = title\n",
    "\n",
    "    \n",
    "    ### Author(s) ###\n",
    "    try:\n",
    "        author_arxiv = df_arxiv[df_arxiv['ID'] == str(row['ARXIV_ID'])]['authors'].values[0]\n",
    "        author_arxiv = [name.strip() for name in author_arxiv if name != '<___>']\n",
    "    except:\n",
    "        author_arxiv = '<___>'\n",
    "    try:\n",
    "        author_gscholar = df_gscholar[df_gscholar['ID'] == str(row['GSCHOLAR_ID'])]['authors'].values[0]\n",
    "        author_gscholar = [name.strip() for name in author_gscholar if name != '<___>']\n",
    "    except:\n",
    "        author_gscholar = '<___>'\n",
    "    if author_arxiv != '<___>':\n",
    "        authors = list(set(author_arxiv))\n",
    "    else:\n",
    "        authors = list(set(author_gscholar))           \n",
    "    [g.add((node_uri, SCHEMA.author, Literal(author, datatype=SCHEMA.Person))) for author in authors]\n",
    "    df_merged.at[NEW_ID, 'authors'] = authors\n",
    "    json_merged[NEW_ID]['authors'] = authors\n",
    "                       \n",
    "    ### Published ###\n",
    "    try:\n",
    "        published = str(df_arxiv[df_arxiv['ID'] == str(row['ARXIV_ID'])]['published'].values[0])\n",
    "        g.add((node_uri, SCHEMA.datePublished, Literal(published, datatype=SCHEMA.DateTime)))\n",
    "        df_merged.at[NEW_ID, 'published'] = published\n",
    "        json_merged[NEW_ID]['published'] = published\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "                       \n",
    "    ### Updated ###\n",
    "    try:\n",
    "        updated = str(df_arxiv[df_arxiv['ID'] == str(row['ARXIV_ID'])]['updated'].values[0])\n",
    "        g.add((node_uri, SCHEMA.dateModified, Literal(updated, datatype=SCHEMA.DateTime)))\n",
    "        df_merged.at[NEW_ID, 'updated'] = updated\n",
    "        json_merged[NEW_ID]['updated'] = updated\n",
    "    except:\n",
    "        pass\n",
    "          \n",
    "                       \n",
    "    ### Abstract ###\n",
    "    try:\n",
    "        abstract = str(df_arxiv[df_arxiv['ID'] == str(row['ARXIV_ID'])]['summary'].values[0]).strip()\n",
    "        g.add((node_uri, SCHEMA.abstract, Literal(abstract, datatype=SCHEMA.Text)))\n",
    "        df_merged.at[NEW_ID, 'abstract'] = abstract\n",
    "        json_merged[NEW_ID]['abstract'] = abstract\n",
    "    except:\n",
    "        pass\n",
    "       \n",
    "                       \n",
    "    ### Categories ###\n",
    "    try:\n",
    "        categories = df_arxiv[df_arxiv['ID'] == str(row['ARXIV_ID'])]['categories'].values[0]\n",
    "        categories = [name.strip() for name in categories if name != '<___>']\n",
    "        [g.add((node_uri, SCHEMA.genre, Literal(category, datatype=SCHEMA.Text))) for category in categories]\n",
    "        df_merged.at[NEW_ID, 'categories'] = categories\n",
    "        json_merged[NEW_ID]['categories'] = categories\n",
    "    except:\n",
    "        pass\n",
    "          \n",
    "                       \n",
    "    ### Journal ###\n",
    "    try:\n",
    "        journal = str(df_gscholar[df_gscholar['ID'] == str(row['GSCHOLAR_ID'])]['journal'].values[0])\n",
    "        g.add((node_uri, SCHEMA.publisher, Literal(journal, datatype=SCHEMA.Periodical))) #datatype=SCHEMA.Organisation\n",
    "        df_merged.at[NEW_ID, 'journal'] = journal\n",
    "        json_merged[NEW_ID]['journal'] = journal\n",
    "    except:\n",
    "        pass\n",
    "     \n",
    "                       \n",
    "    ### Citations ###\n",
    "    try:\n",
    "        citations = str(df_gscholar[df_gscholar['ID'] == str(row['GSCHOLAR_ID'])]['citations'].values[0])\n",
    "        g.add((node_uri, SCHEMA.commentCount, Literal(citations, datatype=SCHEMA.Integer)))\n",
    "        df_merged.at[NEW_ID, 'citations'] = citations\n",
    "        json_merged[NEW_ID]['citations'] = citations\n",
    "    except:\n",
    "        pass\n",
    "            \n",
    "                       \n",
    "    ### Arxiv URL ###\n",
    "    try:\n",
    "        arxiv_url = str(df_arxiv[df_arxiv['ID'] == str(row['ARXIV_ID'])]['url'].values[0])\n",
    "        g.add((node_uri, SCHEMA.url, Literal(arxiv_url, datatype=SCHEMA.URL)))\n",
    "        df_merged.at[NEW_ID, 'arxiv_url'] = arxiv_url    \n",
    "        json_merged[NEW_ID]['arxiv_url'] = arxiv_url\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "                       \n",
    "    ### Google Scholar URL ###\n",
    "    try:\n",
    "        gscholar_url = str(df_gscholar[df_gscholar['ID'] == str(row['GSCHOLAR_ID'])]['url'].values[0])\n",
    "        g.add((node_uri, SCHEMA.url, Literal(gscholar_url, datatype=SCHEMA.URL)))\n",
    "        df_merged.at[NEW_ID, 'gscholar_url'] = gscholar_url   \n",
    "        json_merged[NEW_ID]['gscholar_url'] = gscholar_url\n",
    "    except:\n",
    "        pass\n",
    "             \n",
    "                       \n",
    "    NEW_ID += 1\n",
    "    \n",
    "# Save to disk using turtle format\n",
    "g.serialize(f'Triples_{TOPIC}.ttl.', format=\"turtle\")\n",
    "\n",
    "# And save the merged DataFrame as CSV\n",
    "df_merged.to_csv(f'Merged_{TOPIC}.csv', index=False)\n",
    "\n",
    "# Also save as Json, just because\n",
    "with open(f'Json_{TOPIC}.json', 'w') as fout:\n",
    "    json.dump(json_merged, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "active-skill",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>published</th>\n",
       "      <th>updated</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "      <th>citations</th>\n",
       "      <th>arxiv_url</th>\n",
       "      <th>gscholar_url</th>\n",
       "      <th>journal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>HyperNet: Towards Accurate Region Proposal Gen...</td>\n",
       "      <td>[Fuchun Sun, Anbang Yao, Yurong Chen, Tao Kong]</td>\n",
       "      <td>2016-04-03T06:52:14Z</td>\n",
       "      <td>2016-04-03T06:52:14Z</td>\n",
       "      <td>Almost all of the current top-performing objec...</td>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>485</td>\n",
       "      <td>http://arxiv.org/abs/1604.00600v1</td>\n",
       "      <td>http://scholar.google.com/scholar?oi=bibs&amp;clus...</td>\n",
       "      <td>Proceedings of the IEEE Conference on Computer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Nested Hierarchical Dirichlet Processes</td>\n",
       "      <td>[Michael I. Jordan, David M. Blei, Chong Wang,...</td>\n",
       "      <td>2012-10-25T04:25:00Z</td>\n",
       "      <td>2014-05-02T16:36:57Z</td>\n",
       "      <td>We develop a nested hierarchical Dirichlet pro...</td>\n",
       "      <td>[stat.ML, cs.LG]</td>\n",
       "      <td>174</td>\n",
       "      <td>http://arxiv.org/abs/1210.6738v4</td>\n",
       "      <td>http://scholar.google.com/scholar?oi=bibs&amp;clus...</td>\n",
       "      <td>IEEE Transactions on Pattern Analysis &amp; Machin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>An End-to-End Trainable Neural Network for Ima...</td>\n",
       "      <td>[Cong Yao, Xiang Bai, Baoguang Shi]</td>\n",
       "      <td>2015-07-21T06:26:32Z</td>\n",
       "      <td>2015-07-21T06:26:32Z</td>\n",
       "      <td>Image-based sequence recognition has been a lo...</td>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>807</td>\n",
       "      <td>http://arxiv.org/abs/1507.05717v1</td>\n",
       "      <td>http://scholar.google.com/scholar?oi=bibs&amp;clus...</td>\n",
       "      <td>IEEE Transactions on Pattern Analysis and Mach...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID                                              title  \\\n",
       "0  0  HyperNet: Towards Accurate Region Proposal Gen...   \n",
       "1  1            Nested Hierarchical Dirichlet Processes   \n",
       "2  2  An End-to-End Trainable Neural Network for Ima...   \n",
       "\n",
       "                                             authors             published  \\\n",
       "0    [Fuchun Sun, Anbang Yao, Yurong Chen, Tao Kong]  2016-04-03T06:52:14Z   \n",
       "1  [Michael I. Jordan, David M. Blei, Chong Wang,...  2012-10-25T04:25:00Z   \n",
       "2                [Cong Yao, Xiang Bai, Baoguang Shi]  2015-07-21T06:26:32Z   \n",
       "\n",
       "                updated                                           abstract  \\\n",
       "0  2016-04-03T06:52:14Z  Almost all of the current top-performing objec...   \n",
       "1  2014-05-02T16:36:57Z  We develop a nested hierarchical Dirichlet pro...   \n",
       "2  2015-07-21T06:26:32Z  Image-based sequence recognition has been a lo...   \n",
       "\n",
       "         categories citations                          arxiv_url  \\\n",
       "0           [cs.CV]       485  http://arxiv.org/abs/1604.00600v1   \n",
       "1  [stat.ML, cs.LG]       174   http://arxiv.org/abs/1210.6738v4   \n",
       "2           [cs.CV]       807  http://arxiv.org/abs/1507.05717v1   \n",
       "\n",
       "                                        gscholar_url  \\\n",
       "0  http://scholar.google.com/scholar?oi=bibs&clus...   \n",
       "1  http://scholar.google.com/scholar?oi=bibs&clus...   \n",
       "2  http://scholar.google.com/scholar?oi=bibs&clus...   \n",
       "\n",
       "                                             journal  \n",
       "0  Proceedings of the IEEE Conference on Computer...  \n",
       "1  IEEE Transactions on Pattern Analysis & Machin...  \n",
       "2  IEEE Transactions on Pattern Analysis and Mach...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "expensive-workstation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1332 entries, 0 to 1331\n",
      "Data columns (total 11 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   ID            1332 non-null   object\n",
      " 1   title         1332 non-null   object\n",
      " 2   authors       1332 non-null   object\n",
      " 3   published     1147 non-null   object\n",
      " 4   updated       1147 non-null   object\n",
      " 5   abstract      1147 non-null   object\n",
      " 6   categories    1147 non-null   object\n",
      " 7   citations     738 non-null    object\n",
      " 8   arxiv_url     1147 non-null   object\n",
      " 9   gscholar_url  738 non-null    object\n",
      " 10  journal       738 non-null    object\n",
      "dtypes: object(11)\n",
      "memory usage: 157.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "welsh-company",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'ID': 0,\n",
       "  'title': 'HyperNet: Towards Accurate Region Proposal Generation and Joint Object  Detection',\n",
       "  'authors': ['Fuchun Sun', 'Anbang Yao', 'Yurong Chen', 'Tao Kong'],\n",
       "  'published': '2016-04-03T06:52:14Z',\n",
       "  'updated': '2016-04-03T06:52:14Z',\n",
       "  'abstract': 'Almost all of the current top-performing object detection networks employregion proposals to guide the search for object instances. State-of-the-artregion proposal methods usually need several thousand proposals to get highrecall, thus hurting the detection efficiency. Although the latest RegionProposal Network method gets promising detection accuracy with several hundredproposals, it still struggles in small-size object detection and preciselocalization (e.g., large IoU thresholds), mainly due to the coarseness of itsfeature maps. In this paper, we present a deep hierarchical network, namelyHyperNet, for handling region proposal generation and object detection jointly.Our HyperNet is primarily based on an elaborately designed Hyper Feature whichaggregates hierarchical feature maps first and then compresses them into auniform space. The Hyper Features well incorporate deep but highly semantic,intermediate but really complementary, and shallow but naturallyhigh-resolution features of the image, thus enabling us to construct HyperNetby sharing them both in generating proposals and detecting objects via anend-to-end joint training strategy. For the deep VGG16 model, our methodachieves completely leading recall and state-of-the-art object detectionaccuracy on PASCAL VOC 2007 and 2012 using only 100 proposals per image. Itruns with a speed of 5 fps (including all steps) on a GPU, thus having thepotential for real-time processing.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '485',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1604.00600v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=417595952743722908&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 1: {'ID': 1,\n",
       "  'title': 'Nested Hierarchical Dirichlet Processes',\n",
       "  'authors': ['Michael I. Jordan',\n",
       "   'David M. Blei',\n",
       "   'Chong Wang',\n",
       "   'John Paisley'],\n",
       "  'published': '2012-10-25T04:25:00Z',\n",
       "  'updated': '2014-05-02T16:36:57Z',\n",
       "  'abstract': 'We develop a nested hierarchical Dirichlet process (nHDP) for hierarchicaltopic modeling. The nHDP is a generalization of the nested Chinese restaurantprocess (nCRP) that allows each word to follow its own path to a topic nodeaccording to a document-specific distribution on a shared tree. This alleviatesthe rigid, single-path formulation of the nCRP, allowing a document to moreeasily express thematic borrowings as a random effect. We derive a stochasticvariational inference algorithm for the model, in addition to a greedy subtreeselection method for each document, which allows for efficient inference usingmassive collections of text documents. We demonstrate our algorithm on 1.8million documents from The New York Times and 3.3 million documents fromWikipedia.',\n",
       "  'categories': ['stat.ML', 'cs.LG'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis & Machine Intelligence, 256-270',\n",
       "  'citations': '174',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1210.6738v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=9514991989258462685&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 2: {'ID': 2,\n",
       "  'title': 'An End-to-End Trainable Neural Network for Image-based Sequence  Recognition and Its Application to Scene Text Recognition',\n",
       "  'authors': ['Cong Yao', 'Xiang Bai', 'Baoguang Shi'],\n",
       "  'published': '2015-07-21T06:26:32Z',\n",
       "  'updated': '2015-07-21T06:26:32Z',\n",
       "  'abstract': 'Image-based sequence recognition has been a long-standing research topic incomputer vision. In this paper, we investigate the problem of scene textrecognition, which is among the most important and challenging tasks inimage-based sequence recognition. A novel neural network architecture, whichintegrates feature extraction, sequence modeling and transcription into aunified framework, is proposed. Compared with previous systems for scene textrecognition, the proposed architecture possesses four distinctive properties:(1) It is end-to-end trainable, in contrast to most of the existing algorithmswhose components are separately trained and tuned. (2) It naturally handlessequences in arbitrary lengths, involving no character segmentation orhorizontal scale normalization. (3) It is not confined to any predefinedlexicon and achieves remarkable performances in both lexicon-free andlexicon-based scene text recognition tasks. (4) It generates an effective yetmuch smaller model, which is more practical for real-world applicationscenarios. The experiments on standard benchmarks, including the IIIT-5K,Street View Text and ICDAR datasets, demonstrate the superiority of theproposed algorithm over the prior arts. Moreover, the proposed algorithmperforms well in the task of image-based music score recognition, whichevidently verifies the generality of it.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 39 (11), 2298\\xa0…',\n",
       "  'citations': '807',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1507.05717v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=18010691661893682162&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 3: {'ID': 3,\n",
       "  'title': \"What's the Point: Semantic Segmentation with Point Supervision\",\n",
       "  'authors': ['Amy Bearman',\n",
       "   'Olga Russakovsky',\n",
       "   'Li Fei-Fei',\n",
       "   'Vittorio Ferrari'],\n",
       "  'published': '2015-06-06T02:45:48Z',\n",
       "  'updated': '2016-07-23T17:41:43Z',\n",
       "  'abstract': 'The semantic image segmentation task presents a trade-off between test timeaccuracy and training-time annotation cost. Detailed per-pixel annotationsenable training accurate models but are very time-consuming to obtain,image-level class labels are an order of magnitude cheaper but result in lessaccurate models. We take a natural step from image-level annotation towardsstronger supervision: we ask annotators to point to an object if one exists. Weincorporate this point supervision along with a novel objectness potential inthe training loss function of a CNN model. Experimental results on the PASCALVOC 2012 benchmark reveal that the combined effect of point-level supervisionand objectness potential yields an improvement of 12.9% mIOU over image-levelsupervision. Further, we demonstrate that models trained with point-levelsupervision are more accurate than models trained with image-level,squiggle-level or full supervision given a fixed annotation budget.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (7), 549-565',\n",
       "  'citations': '324',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1506.02106v5',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6373567135027563764&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 4: {'ID': 4,\n",
       "  'title': 'Inverting Visual Representations with Convolutional Networks',\n",
       "  'authors': ['Alexey Dosovitskiy', 'Thomas Brox'],\n",
       "  'published': '2015-06-09T02:31:40Z',\n",
       "  'updated': '2016-04-26T23:30:11Z',\n",
       "  'abstract': 'Feature representations, both hand-designed and learned ones, are often hardto analyze and interpret, even when they are extracted from visual data. Wepropose a new approach to study image representations by inverting them with anup-convolutional neural network. We apply the method to shallow representations(HOG, SIFT, LBP), as well as to deep networks. For shallow representations ourapproach provides significantly better reconstructions than existing methods,revealing that there is surprisingly rich information contained in thesefeatures. Inverting a deep network trained on ImageNet provides severalinsights into the properties of the feature representation learned by thenetwork. Most strikingly, the colors and the rough contours of an image can bereconstructed from activations in higher network layers and even from thepredicted class probabilities.',\n",
       "  'categories': ['cs.NE', 'cs.CV', 'cs.LG'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '319',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1506.02753v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=13363611608288523650&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 5: {'ID': 5,\n",
       "  'title': 'CosFace: Large Margin Cosine Loss for Deep Face Recognition',\n",
       "  'authors': ['Jingchao Zhou',\n",
       "   'Zhifeng Li',\n",
       "   'Zheng Zhou',\n",
       "   'Wei Liu',\n",
       "   'Xing Ji',\n",
       "   'Hao Wang',\n",
       "   'Yitong Wang',\n",
       "   'Dihong Gong'],\n",
       "  'published': '2018-01-29T09:23:55Z',\n",
       "  'updated': '2018-04-03T11:15:57Z',\n",
       "  'abstract': 'Face recognition has made extraordinary progress owing to the advancement ofdeep convolutional neural networks (CNNs). The central task of facerecognition, including face verification and identification, involves facefeature discrimination. However, the traditional softmax loss of deep CNNsusually lacks the power of discrimination. To address this problem, recentlyseveral loss functions such as center loss, large margin softmax loss, andangular softmax loss have been proposed. All these improved losses share thesame idea: maximizing inter-class variance and minimizing intra-class variance.In this paper, we propose a novel loss function, namely large margin cosineloss (LMCL), to realize this idea from a different perspective. Morespecifically, we reformulate the softmax loss as a cosine loss by $L_2$normalizing both features and weight vectors to remove radial variations, basedon which a cosine margin term is introduced to further maximize the decisionmargin in the angular space. As a result, minimum intra-class variance andmaximum inter-class variance are achieved by virtue of normalization and cosinedecision margin maximization. We refer to our model trained with LMCL asCosFace. Extensive experimental evaluations are conducted on the most popularpublic-domain face recognition datasets such as MegaFace Challenge, YoutubeFaces (YTF) and Labeled Face in the Wild (LFW). We achieve the state-of-the-artperformance on these benchmarks, which confirms the effectiveness of ourproposed approach.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '445',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1801.09414v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=18180463842880004109&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 6: {'ID': 6,\n",
       "  'title': 'XNOR-Net: ImageNet Classification Using Binary Convolutional Neural  Networks',\n",
       "  'authors': ['Ali Farhadi',\n",
       "   'Vicente Ordonez',\n",
       "   'Mohammad Rastegari',\n",
       "   'Joseph Redmon'],\n",
       "  'published': '2016-03-16T20:56:21Z',\n",
       "  'updated': '2016-08-02T20:59:14Z',\n",
       "  'abstract': 'We propose two efficient approximations to standard convolutional neuralnetworks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks,the filters are approximated with binary values resulting in 32x memory saving.In XNOR-Networks, both the filters and the input to convolutional layers arebinary. XNOR-Networks approximate convolutions using primarily binaryoperations. This results in 58x faster convolutional operations and 32x memorysavings. XNOR-Nets offer the possibility of running state-of-the-art networkson CPUs (rather than GPUs) in real-time. Our binary networks are simple,accurate, efficient, and work on challenging visual tasks. We evaluate ourapproach on the ImageNet classification task. The classification accuracy witha Binary-Weight-Network version of AlexNet is only 2.9% less than thefull-precision AlexNet (in top-1 measure). We compare our method with recentnetwork binarization methods, BinaryConnect and BinaryNets, and outperformthese methods by large margins on ImageNet, more than 16% in top-1 accuracy.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (4), 525-542',\n",
       "  'citations': '2203',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1603.05279v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=12438338552609254233&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 7: {'ID': 7,\n",
       "  'title': 'Fully-Convolutional Siamese Networks for Object Tracking',\n",
       "  'authors': ['Jack Valmadre',\n",
       "   'Philip H. S. Torr',\n",
       "   'Luca Bertinetto',\n",
       "   'João F. Henriques',\n",
       "   'Andrea Vedaldi'],\n",
       "  'published': '2016-06-30T16:00:43Z',\n",
       "  'updated': '2016-09-14T11:48:29Z',\n",
       "  'abstract': \"The problem of arbitrary object tracking has traditionally been tackled bylearning a model of the object's appearance exclusively online, using as soletraining data the video itself. Despite the success of these methods, theironline-only approach inherently limits the richness of the model they canlearn. Recently, several attempts have been made to exploit the expressivepower of deep convolutional networks. However, when the object to track is notknown beforehand, it is necessary to perform Stochastic Gradient Descent onlineto adapt the weights of the network, severely compromising the speed of thesystem. In this paper we equip a basic tracking algorithm with a novelfully-convolutional Siamese network trained end-to-end on the ILSVRC15 datasetfor object detection in video. Our tracker operates at frame-rates beyondreal-time and, despite its extreme simplicity, achieves state-of-the-artperformance in multiple benchmarks.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV Workshops (2), 850-865',\n",
       "  'citations': '1326',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1606.09549v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=506606431503058331&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 8: {'ID': 8,\n",
       "  'title': 'AttnGAN: Fine-Grained Text to Image Generation with Attentional  Generative Adversarial Networks',\n",
       "  'authors': ['Tao Xu',\n",
       "   'Xiaodong He',\n",
       "   'Qiuyuan Huang',\n",
       "   'Pengchuan Zhang',\n",
       "   'Han Zhang',\n",
       "   'Xiaolei Huang',\n",
       "   'Zhe Gan'],\n",
       "  'published': '2017-11-28T18:59:50Z',\n",
       "  'updated': '2017-11-28T18:59:50Z',\n",
       "  'abstract': 'In this paper, we propose an Attentional Generative Adversarial Network(AttnGAN) that allows attention-driven, multi-stage refinement for fine-grainedtext-to-image generation. With a novel attentional generative network, theAttnGAN can synthesize fine-grained details at different subregions of theimage by paying attentions to the relevant words in the natural languagedescription. In addition, a deep attentional multimodal similarity model isproposed to compute a fine-grained image-text matching loss for training thegenerator. The proposed AttnGAN significantly outperforms the previous state ofthe art, boosting the best reported inception score by 14.14% on the CUBdataset and 170.25% on the more challenging COCO dataset. A detailed analysisis also performed by visualizing the attention layers of the AttnGAN. It forthe first time shows that the layered attentional GAN is able to automaticallyselect the condition at the word level for generating different parts of theimage.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '337',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1711.10485v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=7655206683248590284&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 9: {'ID': 9,\n",
       "  'title': 'The Application of Two-level Attention Models in Deep Convolutional  Neural Network for Fine-grained Image Classification',\n",
       "  'authors': ['Kuiyuan Yang',\n",
       "   'Tianjun Xiao',\n",
       "   'Jiaxing Zhang',\n",
       "   'Zheng Zhang',\n",
       "   'Yichong Xu',\n",
       "   'Yuxin Peng'],\n",
       "  'published': '2014-11-24T13:30:07Z',\n",
       "  'updated': '2014-11-24T13:30:07Z',\n",
       "  'abstract': 'Fine-grained classification is challenging because categories can only bediscriminated by subtle and local differences. Variances in the pose, scale orrotation usually make the problem more difficult. Most fine-grainedclassification systems follow the pipeline of finding foreground object orobject parts (where) to extract discriminative features (what).  In this paper, we propose to apply visual attention to fine-grainedclassification task using deep neural network. Our pipeline integrates threetypes of attention: the bottom-up attention that propose candidate patches, theobject-level top-down attention that selects relevant patches to a certainobject, and the part-level top-down attention that localizes discriminativeparts. We combine these attentions to train domain-specific deep nets, then useit to improve both the what and where aspects. Importantly, we avoid usingexpensive annotations like bounding box or part information from end-to-end.The weak supervision constraint makes our work easier to generalize.  We have verified the effectiveness of the method on the subsets of ILSVRC2012dataset and CUB200_2011 dataset. Our pipeline delivered significantimprovements and achieved the best accuracy under the weakest supervisioncondition. The performance is competitive against other methods that rely onadditional annotations.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '476',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1411.6447v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6192468677545657909&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 10: {'ID': 10,\n",
       "  'title': 'Sampling Matters in Deep Embedding Learning',\n",
       "  'authors': ['Alexander J. Smola',\n",
       "   'Philipp Krähenbühl',\n",
       "   'R. Manmatha',\n",
       "   'Chao-Yuan Wu'],\n",
       "  'published': '2017-06-23T05:14:55Z',\n",
       "  'updated': '2018-01-16T16:54:27Z',\n",
       "  'abstract': 'Deep embeddings answer one simple question: How similar are two images?Learning these embeddings is the bedrock of verification, zero-shot learning,and visual search. The most prominent approaches optimize a deep convolutionalnetwork with a suitable loss function, such as contrastive loss or tripletloss. While a rich line of work focuses solely on the loss functions, we showin this paper that selecting training examples plays an equally important role.We propose distance weighted sampling, which selects more informative andstable examples than traditional approaches. In addition, we show that a simplemargin based loss is sufficient to outperform all other loss functions. Weevaluate our approach on the Stanford Online Products, CAR196, and theCUB200-2011 datasets for image retrieval and clustering, and on the LFW datasetfor face verification. Our method achieves state-of-the-art performance on allof them.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 2840-2848',\n",
       "  'citations': '273',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1706.07567v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=17172154549460467447&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 11: {'ID': 11,\n",
       "  'title': 'Synthesized Classifiers for Zero-Shot Learning',\n",
       "  'authors': ['Fei Sha', 'Boqing Gong', 'Wei-Lun Chao', 'Soravit Changpinyo'],\n",
       "  'published': '2016-03-02T01:59:22Z',\n",
       "  'updated': '2016-05-27T21:48:48Z',\n",
       "  'abstract': 'Given semantic descriptions of object classes, zero-shot learning aims toaccurately recognize objects of the unseen classes, from which no examples areavailable at the training stage, by associating them to the seen classes, fromwhich labeled examples are provided. We propose to tackle this problem from theperspective of manifold learning. Our main idea is to align the semantic spacethat is derived from external information to the model space that concernsitself with recognizing visual features. To this end, we introduce a set of\"phantom\" object classes whose coordinates live in both the semantic space andthe model space. Serving as bases in a dictionary, they can be optimized fromlabeled data such that the synthesized real object classifiers achieve optimaldiscriminative performance. We demonstrate superior accuracy of our approachover the state of the art on four benchmark datasets for zero-shot learning,including the full ImageNet Fall 2011 dataset with more than 20,000 unseenclasses.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '409',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1603.00550v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=9417352588100487015&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 12: {'ID': 12,\n",
       "  'title': 'SphereFace: Deep Hypersphere Embedding for Face Recognition',\n",
       "  'authors': ['Le Song',\n",
       "   'Ming Li',\n",
       "   'Bhiksha Raj',\n",
       "   'Yandong Wen',\n",
       "   'Weiyang Liu',\n",
       "   'Zhiding Yu'],\n",
       "  'published': '2017-04-26T11:37:22Z',\n",
       "  'updated': '2018-01-29T23:24:56Z',\n",
       "  'abstract': 'This paper addresses deep face recognition (FR) problem under open-setprotocol, where ideal face features are expected to have smaller maximalintra-class distance than minimal inter-class distance under a suitably chosenmetric space. However, few existing algorithms can effectively achieve thiscriterion. To this end, we propose the angular softmax (A-Softmax) loss thatenables convolutional neural networks (CNNs) to learn angularly discriminativefeatures. Geometrically, A-Softmax loss can be viewed as imposingdiscriminative constraints on a hypersphere manifold, which intrinsicallymatches the prior that faces also lie on a manifold. Moreover, the size ofangular margin can be quantitatively adjusted by a parameter $m$. We furtherderive specific $m$ to approximate the ideal feature criterion. Extensiveanalysis and experiments on Labeled Face in the Wild (LFW), Youtube Faces (YTF)and MegaFace Challenge show the superiority of A-Softmax loss in FR tasks. Thecode has also been made publicly available.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '908',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1704.08063v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=11996860204567761072&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 13: {'ID': 13,\n",
       "  'title': 'ICNet for Real-Time Semantic Segmentation on High-Resolution Images',\n",
       "  'authors': ['Jianping Shi',\n",
       "   'Xiaojuan Qi',\n",
       "   'Hengshuang Zhao',\n",
       "   'Xiaoyong Shen',\n",
       "   'Jiaya Jia'],\n",
       "  'published': '2017-04-27T13:02:49Z',\n",
       "  'updated': '2018-08-20T03:34:25Z',\n",
       "  'abstract': 'We focus on the challenging task of real-time semantic segmentation in thispaper. It finds many practical applications and yet is with fundamentaldifficulty of reducing a large portion of computation for pixel-wise labelinference. We propose an image cascade network (ICNet) that incorporatesmulti-resolution branches under proper label guidance to address thischallenge. We provide in-depth analysis of our framework and introduce thecascade feature fusion unit to quickly achieve high-quality segmentation. Oursystem yields real-time inference on a single GPU card with decent qualityresults evaluated on challenging datasets like Cityscapes, CamVid andCOCO-Stuff.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 405-420',\n",
       "  'citations': '331',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1704.08545v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=4797791240153082941&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 14: {'ID': 14,\n",
       "  'title': 'A Closer Look at Spatiotemporal Convolutions for Action Recognition',\n",
       "  'authors': ['Lorenzo Torresani',\n",
       "   'Yann LeCun',\n",
       "   'Heng Wang',\n",
       "   'Manohar Paluri',\n",
       "   'Du Tran',\n",
       "   'Jamie Ray'],\n",
       "  'published': '2017-11-30T06:28:20Z',\n",
       "  'updated': '2018-04-12T01:07:30Z',\n",
       "  'abstract': 'In this paper we discuss several forms of spatiotemporal convolutions forvideo analysis and study their effects on action recognition. Our motivationstems from the observation that 2D CNNs applied to individual frames of thevideo have remained solid performers in action recognition. In this work weempirically demonstrate the accuracy advantages of 3D CNNs over 2D CNNs withinthe framework of residual learning. Furthermore, we show that factorizing the3D convolutional filters into separate spatial and temporal components yieldssignificantly advantages in accuracy. Our empirical study leads to the designof a new spatiotemporal convolutional block \"R(2+1)D\" which gives rise to CNNsthat achieve results comparable or superior to the state-of-the-art onSports-1M, Kinetics, UCF101 and HMDB51.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '425',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1711.11248v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=9524036545693727210&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 15: {'ID': 15,\n",
       "  'title': 'Learning Visual Clothing Style with Heterogeneous Dyadic Co-occurrences',\n",
       "  'authors': ['Serge Belongie',\n",
       "   'Balazs Kovacs',\n",
       "   'Kavita Bala',\n",
       "   'Andreas Veit',\n",
       "   'Sean Bell',\n",
       "   'Julian McAuley'],\n",
       "  'published': '2015-09-24T18:59:01Z',\n",
       "  'updated': '2015-09-24T18:59:01Z',\n",
       "  'abstract': \"With the rapid proliferation of smart mobile devices, users now take millionsof photos every day. These include large numbers of clothing and accessoryimages. We would like to answer questions like `What outfit goes well with thispair of shoes?' To answer these types of questions, one has to go beyondlearning visual similarity and learn a visual notion of compatibility acrosscategories. In this paper, we propose a novel learning framework to help answerthese types of questions. The main idea of this framework is to learn a featuretransformation from images of items into a latent space that expressescompatibility. For the feature transformation, we use a Siamese ConvolutionalNeural Network (CNN) architecture, where training examples are pairs of itemsthat are either compatible or incompatible. We model compatibility based onco-occurrence in large-scale user behavior data; in particular co-purchase datafrom Amazon.com. To learn cross-category fit, we introduce a strategic methodto sample training data, where pairs of items are heterogeneous dyads, i.e.,the two elements of a pair belong to different high-level categories. Whilethis approach is applicable to a wide variety of settings, we focus on therepresentative problem of learning compatible clothing style. Our resultsindicate that the proposed framework is capable of learning semanticinformation about visual style and is able to generate outfits of clothes, withitems from different categories, that go well together.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 4642-4650',\n",
       "  'citations': '190',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1509.07473v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=636007317105506418&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 16: {'ID': 16,\n",
       "  'title': 'Single-Shot Refinement Neural Network for Object Detection',\n",
       "  'authors': ['Xiao Bian',\n",
       "   'Stan Z. Li',\n",
       "   'Shifeng Zhang',\n",
       "   'Longyin Wen',\n",
       "   'Zhen Lei'],\n",
       "  'published': '2017-11-18T17:05:34Z',\n",
       "  'updated': '2018-01-03T13:01:54Z',\n",
       "  'abstract': 'For object detection, the two-stage approach (e.g., Faster R-CNN) has beenachieving the highest accuracy, whereas the one-stage approach (e.g., SSD) hasthe advantage of high efficiency. To inherit the merits of both whileovercoming their disadvantages, in this paper, we propose a novel single-shotbased detector, called RefineDet, that achieves better accuracy than two-stagemethods and maintains comparable efficiency of one-stage methods. RefineDetconsists of two inter-connected modules, namely, the anchor refinement moduleand the object detection module. Specifically, the former aims to (1) filterout negative anchors to reduce search space for the classifier, and (2)coarsely adjust the locations and sizes of anchors to provide betterinitialization for the subsequent regressor. The latter module takes therefined anchors as the input from the former to further improve the regressionand predict multi-class label. Meanwhile, we design a transfer connection blockto transfer the features in the anchor refinement module to predict locations,sizes and class labels of objects in the object detection module. Themulti-task loss function enables us to train the whole network in an end-to-endway. Extensive experiments on PASCAL VOC 2007, PASCAL VOC 2012, and MS COCOdemonstrate that RefineDet achieves state-of-the-art detection accuracy withhigh efficiency. Code is available at https://github.com/sfzhang15/RefineDet',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '389',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1711.06897v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2942593974177604568&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 17: {'ID': 17,\n",
       "  'title': 'Information Dropout: Learning Optimal Representations Through Noisy  Computation',\n",
       "  'authors': ['Stefano Soatto', 'Alessandro Achille'],\n",
       "  'published': '2016-11-04T12:46:37Z',\n",
       "  'updated': '2017-02-12T09:26:25Z',\n",
       "  'abstract': 'The cross-entropy loss commonly used in deep learning is closely related tothe defining properties of optimal representations, but does not enforce someof the key properties. We show that this can be solved by adding aregularization term, which is in turn related to injecting multiplicative noisein the activations of a Deep Neural Network, a special case of which is thecommon practice of dropout. We show that our regularized loss function can beefficiently minimized using Information Dropout, a generalization of dropoutrooted in information theoretic principles that automatically adapts to thedata and can better exploit architectures of limited capacity. When the task isthe reconstruction of the input, we show that our loss function yields aVariational Autoencoder as a special case, thus providing a link betweenrepresentation learning, information theory and variational inference. Finally,we prove that we can promote the creation of disentangled representationssimply by enforcing a factorized prior, a fact that has been observedempirically in recent work. Our experiments validate the theoretical intuitionsbehind our method, and we find that information dropout achieves a comparableor better generalization performance than binary dropout, especially on smallermodels, since it can automatically adapt the noise to the structure of thenetwork, as well as to the test sample.',\n",
       "  'categories': ['stat.ML', 'cs.LG', 'stat.CO'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 40 (12), 2897\\xa0…',\n",
       "  'citations': '151',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1611.01353v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=1630853992736406234&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 18: {'ID': 18,\n",
       "  'title': 'Learning Feature Pyramids for Human Pose Estimation',\n",
       "  'authors': ['Wei Yang',\n",
       "   'Wanli Ouyang',\n",
       "   'Shuang Li',\n",
       "   'Xiaogang Wang',\n",
       "   'Hongsheng Li'],\n",
       "  'published': '2017-08-03T11:26:04Z',\n",
       "  'updated': '2017-08-03T11:26:04Z',\n",
       "  'abstract': 'Articulated human pose estimation is a fundamental yet challenging task incomputer vision. The difficulty is particularly pronounced in scale variationsof human body parts when camera view changes or severe foreshortening happens.Although pyramid methods are widely used to handle scale changes at inferencetime, learning feature pyramids in deep convolutional neural networks (DCNNs)is still not well explored. In this work, we design a Pyramid Residual Module(PRMs) to enhance the invariance in scales of DCNNs. Given input features, thePRMs learn convolutional filters on various scales of input features, which areobtained with different subsampling ratios in a multi-branch network. Moreover,we observe that it is inappropriate to adopt existing methods to initialize theweights of multi-branch networks, which achieve superior performance than plainnetworks in many tasks recently. Therefore, we provide theoretic derivation toextend the current weight initialization scheme to multi-branch networkstructures. We investigate our method on two standard benchmarks for human poseestimation. Our approach obtains state-of-the-art results on both benchmarks.Code is available at https://github.com/bearpaw/PyraNet.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 1281-1290',\n",
       "  'citations': '210',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1708.01101v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=9559160894911178847&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 19: {'ID': 19,\n",
       "  'title': 'DeeperCut: A Deeper, Stronger, and Faster Multi-Person Pose Estimation  Model',\n",
       "  'authors': ['Bjoern Andres',\n",
       "   'Bernt Schiele',\n",
       "   'Eldar Insafutdinov',\n",
       "   'Mykhaylo Andriluka',\n",
       "   'Leonid Pishchulin'],\n",
       "  'published': '2016-05-10T19:49:40Z',\n",
       "  'updated': '2016-11-30T19:03:17Z',\n",
       "  'abstract': 'The goal of this paper is to advance the state-of-the-art of articulated poseestimation in scenes with multiple people. To that end we contribute on threefronts. We propose (1) improved body part detectors that generate effectivebottom-up proposals for body parts; (2) novel image-conditioned pairwise termsthat allow to assemble the proposals into a variable number of consistent bodypart configurations; and (3) an incremental optimization strategy that exploresthe search space more efficiently thus leading both to better performance andsignificant speed-up factors. Evaluation is done on two single-person and twomulti-person pose estimation benchmarks. The proposed approach significantlyoutperforms best known multi-person pose estimation results while demonstratingcompetitive performance on the task of single person pose estimation. Modelsand code available at http://pose.mpi-inf.mpg.de',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (6), 34-50',\n",
       "  'citations': '523',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1605.03170v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=16366609760461117541&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 20: {'ID': 20,\n",
       "  'title': 'How Far are We from Solving Pedestrian Detection?',\n",
       "  'authors': ['Bernt Schiele',\n",
       "   'Jan Hosang',\n",
       "   'Mohamed Omran',\n",
       "   'Rodrigo Benenson',\n",
       "   'Shanshan Zhang'],\n",
       "  'published': '2016-02-03T09:45:56Z',\n",
       "  'updated': '2016-06-21T11:33:13Z',\n",
       "  'abstract': 'Encouraged by the recent progress in pedestrian detection, we investigate thegap between current state-of-the-art methods and the \"perfect single framedetector\". We enable our analysis by creating a human baseline for pedestriandetection (over the Caltech dataset), and by manually clustering the recurrenterrors of a top detector. Our results characterize both localization andbackground-versus-foreground errors. To address localization errors we studythe impact of training annotation noise on the detector performance, and showthat we can improve even with a small portion of sanitized training data. Toaddress background/foreground discrimination, we study convnets for pedestriandetection, and discuss which factors affect their performance. Other than ourin-depth analysis, we report top performance on the Caltech dataset, andprovide a new sanitized set of training and test annotations.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '342',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1602.01237v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=8448032675851172312&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 21: {'ID': 21,\n",
       "  'title': 'PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume',\n",
       "  'authors': ['Jan Kautz', 'Deqing Sun', 'Ming-Yu Liu', 'Xiaodong Yang'],\n",
       "  'published': '2017-09-07T17:47:59Z',\n",
       "  'updated': '2018-06-25T20:34:58Z',\n",
       "  'abstract': 'We present a compact but effective CNN model for optical flow, calledPWC-Net. PWC-Net has been designed according to simple and well-establishedprinciples: pyramidal processing, warping, and the use of a cost volume. Castin a learnable feature pyramid, PWC-Net uses the cur- rent optical flowestimate to warp the CNN features of the second image. It then uses the warpedfeatures and features of the first image to construct a cost volume, which isprocessed by a CNN to estimate the optical flow. PWC-Net is 17 times smaller insize and easier to train than the recent FlowNet2 model. Moreover, itoutperforms all published optical flow methods on the MPI Sintel final pass andKITTI 2015 benchmarks, running at about 35 fps on Sintel resolution (1024x436)images. Our models are available on https://github.com/NVlabs/PWC-Net.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '473',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1709.02371v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5637139753418325919&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 22: {'ID': 22,\n",
       "  'title': 'Residual Dense Network for Image Super-Resolution',\n",
       "  'authors': ['Yun Fu',\n",
       "   'Yulun Zhang',\n",
       "   'Bineng Zhong',\n",
       "   'Yu Kong',\n",
       "   'Yapeng Tian'],\n",
       "  'published': '2018-02-24T04:40:06Z',\n",
       "  'updated': '2018-03-27T02:53:58Z',\n",
       "  'abstract': 'A very deep convolutional neural network (CNN) has recently achieved greatsuccess for image super-resolution (SR) and offered hierarchical features aswell. However, most deep CNN based SR models do not make full use of thehierarchical features from the original low-resolution (LR) images, therebyachieving relatively-low performance. In this paper, we propose a novelresidual dense network (RDN) to address this problem in image SR. We fullyexploit the hierarchical features from all the convolutional layers.Specifically, we propose residual dense block (RDB) to extract abundant localfeatures via dense connected convolutional layers. RDB further allows directconnections from the state of preceding RDB to all the layers of current RDB,leading to a contiguous memory (CM) mechanism. Local feature fusion in RDB isthen used to adaptively learn more effective features from preceding andcurrent local features and stabilizes the training of wider network. Afterfully obtaining dense local features, we use global feature fusion to jointlyand adaptively learn global hierarchical features in a holistic way. Extensiveexperiments on benchmark datasets with different degradation models show thatour RDN achieves favorable performance against state-of-the-art methods.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '641',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1802.08797v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=10362327168909609533&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 23: {'ID': 23,\n",
       "  'title': 'Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition',\n",
       "  'authors': ['Gang Wang', 'Amir Shahroudy', 'Jun Liu', 'Dong Xu'],\n",
       "  'published': '2016-07-24T13:39:11Z',\n",
       "  'updated': '2016-07-24T13:39:11Z',\n",
       "  'abstract': '3D action recognition - analysis of human actions based on 3D skeleton data -becomes popular recently due to its succinctness, robustness, andview-invariant representation. Recent attempts on this problem suggested todevelop RNN-based learning methods to model the contextual dependency in thetemporal domain. In this paper, we extend this idea to spatio-temporal domainsto analyze the hidden sources of action-related information within the inputdata over both domains concurrently. Inspired by the graphical structure of thehuman skeleton, we further propose a more powerful tree-structure basedtraversal method. To handle the noise and occlusion in 3D skeleton data, weintroduce new gating mechanism within LSTM to learn the reliability of thesequential input data and accordingly adjust its effect on updating thelong-term context information stored in the memory cell. Our method achievesstate-of-the-art performance on 4 challenging benchmark datasets for 3D humanaction analysis.',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.LG', 'cs.NE'],\n",
       "  'journal': 'ECCV (3), 816-833',\n",
       "  'citations': '574',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1607.07043v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5437659519108245860&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 24: {'ID': 24,\n",
       "  'title': 'Data Fusion by Matrix Factorization',\n",
       "  'authors': ['Blaž Zupan', 'Marinka Žitnik'],\n",
       "  'published': '2013-07-02T19:35:21Z',\n",
       "  'updated': '2015-02-06T16:15:38Z',\n",
       "  'abstract': \"For most problems in science and engineering we can obtain data sets thatdescribe the observed system from various perspectives and record the behaviorof its individual components. Heterogeneous data sets can be collectively minedby data fusion. Fusion can focus on a specific target relation and exploitdirectly associated data together with contextual data and data about system'sconstraints. In the paper we describe a data fusion approach with penalizedmatrix tri-factorization (DFMF) that simultaneously factorizes data matrices toreveal hidden associations. The approach can directly consider any data thatcan be expressed in a matrix, including those from feature-basedrepresentations, ontologies, associations and networks. We demonstrate theutility of DFMF for gene function prediction task with eleven different datasources and for prediction of pharmacologic actions by fusing six data sources.Our data fusion algorithm compares favorably to alternative data integrationapproaches and achieves higher accuracy than can be obtained from any singledata source alone.\",\n",
       "  'categories': ['cs.LG',\n",
       "   'cs.AI',\n",
       "   'cs.DB',\n",
       "   'stat.ML',\n",
       "   '15A83, 15A23, 40C05, 65F30',\n",
       "   'H.2.8; G.1.3; I.2.6; H.3.3'],\n",
       "  'journal': 'IEEE Annals of the History of Computing, 41-53',\n",
       "  'citations': '137',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1307.0803v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=1546682805495024399&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 25: {'ID': 25,\n",
       "  'title': 'ContextLocNet: Context-Aware Deep Network Models for Weakly Supervised  Localization',\n",
       "  'authors': ['Minsu Cho', 'Vadim Kantorov', 'Maxime Oquab', 'Ivan Laptev'],\n",
       "  'published': '2016-09-14T16:17:03Z',\n",
       "  'updated': '2016-09-14T16:17:03Z',\n",
       "  'abstract': 'We aim to localize objects in images using image-level supervision only.Previous approaches to this problem mainly focus on discriminative objectregions and often fail to locate precise object boundaries. We address thisproblem by introducing two types of context-aware guidance models, additive andcontrastive models, that leverage their surrounding context regions to improvelocalization. The additive model encourages the predicted object region to besupported by its surrounding context region. The contrastive model encouragesthe predicted object region to be outstanding from its surrounding contextregion. Our approach benefits from the recent success of convolutional neuralnetworks for object recognition and extends Fast R-CNN to weakly supervisedobject localization. Extensive experimental evaluation on the PASCAL VOC 2007and 2012 benchmarks shows hat our context-aware approach significantly improvesweakly supervised localization and detection.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (5), 350-365',\n",
       "  'citations': '165',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1609.04331v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=7079163902281756229&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 26: {'ID': 26,\n",
       "  'title': 'Learning Multi-Domain Convolutional Neural Networks for Visual Tracking',\n",
       "  'authors': ['Hyeonseob Nam', 'Bohyung Han'],\n",
       "  'published': '2015-10-27T15:53:00Z',\n",
       "  'updated': '2016-01-06T06:14:53Z',\n",
       "  'abstract': 'We propose a novel visual tracking algorithm based on the representationsfrom a discriminatively trained Convolutional Neural Network (CNN). Ouralgorithm pretrains a CNN using a large set of videos with trackingground-truths to obtain a generic target representation. Our network iscomposed of shared layers and multiple branches of domain-specific layers,where domains correspond to individual training sequences and each branch isresponsible for binary classification to identify the target in each domain. Wetrain the network with respect to each domain iteratively to obtain generictarget representations in the shared layers. When tracking a target in a newsequence, we construct a new network by combining the shared layers in thepretrained CNN with a new binary classification layer, which is updated online.Online tracking is performed by evaluating the candidate windows randomlysampled around the previous target state. The proposed algorithm illustratesoutstanding performance compared with state-of-the-art methods in existingtracking benchmarks.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '1425',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1510.07945v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=17336344330227401207&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 27: {'ID': 27,\n",
       "  'title': 'Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks',\n",
       "  'authors': ['Yi Yang', 'Wei Xu', 'Zhiheng Huang', 'Jiang Wang', 'Haonan Yu'],\n",
       "  'published': '2015-10-26T22:47:00Z',\n",
       "  'updated': '2016-04-06T02:24:35Z',\n",
       "  'abstract': 'We present an approach that exploits hierarchical Recurrent Neural Networks(RNNs) to tackle the video captioning problem, i.e., generating one or multiplesentences to describe a realistic video. Our hierarchical framework contains asentence generator and a paragraph generator. The sentence generator producesone simple short sentence that describes a specific short video interval. Itexploits both temporal- and spatial-attention mechanisms to selectively focuson visual elements during generation. The paragraph generator captures theinter-sentence dependency by taking as input the sentential embedding producedby the sentence generator, combining it with the paragraph history, andoutputting the new initial state for the sentence generator. We evaluate ourapproach on two large-scale benchmark datasets: YouTubeClips andTACoS-MultiLevel. The experiments demonstrate that our approach significantlyoutperforms the current state-of-the-art methods with BLEU@4 scores 0.499 and0.305 respectively.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '394',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1510.07712v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=1019806543495834224&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 28: {'ID': 28,\n",
       "  'title': 'Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image  Captioning',\n",
       "  'authors': ['Jiasen Lu', 'Richard Socher', 'Caiming Xiong', 'Devi Parikh'],\n",
       "  'published': '2016-12-06T16:03:50Z',\n",
       "  'updated': '2017-06-06T06:59:15Z',\n",
       "  'abstract': 'Attention-based neural encoder-decoder frameworks have been widely adoptedfor image captioning. Most methods force visual attention to be active forevery generated word. However, the decoder likely requires little to no visualinformation from the image to predict non-visual words such as \"the\" and \"of\".Other words that may seem visual can often be predicted reliably just from thelanguage model e.g., \"sign\" after \"behind a red stop\" or \"phone\" following\"talking on a cell\". In this paper, we propose a novel adaptive attention modelwith a visual sentinel. At each time step, our model decides whether to attendto the image (and if so, to which regions) or to the visual sentinel. The modeldecides whether to attend to the image and where, in order to extractmeaningful information for sequential word generation. We test our method onthe COCO image captioning 2015 challenge dataset and Flickr30K. Our approachsets the new state-of-the-art by a significant margin.',\n",
       "  'categories': ['cs.CV', 'cs.AI'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '582',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1612.01887v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=3919912098681596308&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 29: {'ID': 29,\n",
       "  'title': 'Joint Detection and Identification Feature Learning for Person Search',\n",
       "  'authors': ['Shuang Li',\n",
       "   'Tong Xiao',\n",
       "   'Xiaogang Wang',\n",
       "   'Liang Lin',\n",
       "   'Bochao Wang'],\n",
       "  'published': '2016-04-07T02:16:26Z',\n",
       "  'updated': '2017-04-06T01:31:08Z',\n",
       "  'abstract': 'Existing person re-identification benchmarks and methods mainly focus onmatching cropped pedestrian images between queries and candidates. However, itis different from real-world scenarios where the annotations of pedestrianbounding boxes are unavailable and the target person needs to be searched froma gallery of whole scene images. To close the gap, we propose a new deeplearning framework for person search. Instead of breaking it down into twoseparate tasks---pedestrian detection and person re-identification, we jointlyhandle both aspects in a single convolutional neural network. An OnlineInstance Matching (OIM) loss function is proposed to train the networkeffectively, which is scalable to datasets with numerous identities. Tovalidate our approach, we collect and annotate a large-scale benchmark datasetfor person search. It contains 18,184 images, 8,432 identities, and 96,143pedestrian bounding boxes. Experiments show that our framework outperformsother separate approaches, and the proposed OIM loss function converges muchfaster and better than the conventional Softmax loss.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '326',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1604.01850v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2818455563202720828&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 30: {'ID': 30,\n",
       "  'title': 'Deep Contrast Learning for Salient Object Detection',\n",
       "  'authors': ['Yizhou Yu', 'Guanbin Li'],\n",
       "  'published': '2016-03-07T08:50:33Z',\n",
       "  'updated': '2016-03-07T08:50:33Z',\n",
       "  'abstract': 'Salient object detection has recently witnessed substantial progress due topowerful features extracted using deep convolutional neural networks (CNNs).However, existing CNN-based methods operate at the patch level instead of thepixel level. Resulting saliency maps are typically blurry, especially near theboundary of salient objects. Furthermore, image patches are treated asindependent samples even when they are overlapping, giving rise to significantredundancy in computation and storage. In this CVPR 2016 paper, we propose anend-to-end deep contrast network to overcome the aforementioned limitations.Our deep network consists of two complementary components, a pixel-level fullyconvolutional stream and a segment-wise spatial pooling stream. The firststream directly produces a saliency map with pixel-level accuracy from an inputimage. The second stream extracts segment-wise features very efficiently, andbetter models saliency discontinuities along object boundaries. Finally, afully connected CRF model can be optionally incorporated to improve spatialcoherence and contour localization in the fused result from these two streams.Experimental results demonstrate that our deep model significantly improves thestate of the art.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '486',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1603.01976v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=9364796585299081371&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 31: {'ID': 31,\n",
       "  'title': 'OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity  Fields',\n",
       "  'authors': ['Zhe Cao',\n",
       "   'Gines Hidalgo',\n",
       "   'Yaser Sheikh',\n",
       "   'Tomas Simon',\n",
       "   'Shih-En Wei'],\n",
       "  'published': '2018-12-18T18:50:33Z',\n",
       "  'updated': '2019-05-30T23:46:18Z',\n",
       "  'abstract': 'Realtime multi-person 2D pose estimation is a key component in enablingmachines to have an understanding of people in images and videos. In this work,we present a realtime approach to detect the 2D pose of multiple people in animage. The proposed method uses a nonparametric representation, which we referto as Part Affinity Fields (PAFs), to learn to associate body parts withindividuals in the image. This bottom-up system achieves high accuracy andrealtime performance, regardless of the number of people in the image. Inprevious work, PAFs and body part location estimation were refinedsimultaneously across training stages. We demonstrate that a PAF-onlyrefinement rather than both PAF and body part location refinement results in asubstantial increase in both runtime performance and accuracy. We also presentthe first combined body and foot keypoint detector, based on an internalannotated foot dataset that we have publicly released. We show that thecombined detector not only reduces the inference time compared to running themsequentially, but also maintains the accuracy of each component individually.This work has culminated in the release of OpenPose, the first open-sourcerealtime system for multi-person 2D pose detection, including body, foot, hand,and facial keypoints.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence',\n",
       "  'citations': '485',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1812.08008v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14138178942767913504&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 32: {'ID': 32,\n",
       "  'title': 'Compact Bilinear Pooling',\n",
       "  'authors': ['Trevor Darrell', 'Oscar Beijbom', 'Yang Gao', 'Ning Zhang'],\n",
       "  'published': '2015-11-19T05:34:35Z',\n",
       "  'updated': '2016-04-12T01:59:15Z',\n",
       "  'abstract': 'Bilinear models has been shown to achieve impressive performance on a widerange of visual tasks, such as semantic segmentation, fine grained recognitionand face recognition. However, bilinear features are high dimensional,typically on the order of hundreds of thousands to a few million, which makesthem impractical for subsequent analysis. We propose two compact bilinearrepresentations with the same discriminative power as the full bilinearrepresentation but with only a few thousand dimensions. Our compactrepresentations allow back-propagation of classification errors enabling anend-to-end optimization of the visual recognition system. The compact bilinearrepresentations are derived through a novel kernelized analysis of bilinearpooling which provide insights into the discriminative power of bilinearpooling, and a platform for further research in compact pooling methods.Experimentation illustrate the utility of the proposed representations forimage classification and few-shot learning across several datasets.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '419',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1511.06062v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=9277709843201647782&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 33: {'ID': 33,\n",
       "  'title': 'ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture  Design',\n",
       "  'authors': ['Xiangyu Zhang', 'Hai-Tao Zheng', 'Jian Sun', 'Ningning Ma'],\n",
       "  'published': '2018-07-30T04:18:25Z',\n",
       "  'updated': '2018-07-30T04:18:25Z',\n",
       "  'abstract': 'Currently, the neural network architecture design is mostly guided by the\\\\emph{indirect} metric of computation complexity, i.e., FLOPs. However, the\\\\emph{direct} metric, e.g., speed, also depends on the other factors such asmemory access cost and platform characterics. Thus, this work proposes toevaluate the direct metric on the target platform, beyond only consideringFLOPs. Based on a series of controlled experiments, this work derives severalpractical \\\\emph{guidelines} for efficient network design. Accordingly, a newarchitecture is presented, called \\\\emph{ShuffleNet V2}. Comprehensive ablationexperiments verify that our model is the state-of-the-art in terms of speed andaccuracy tradeoff.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 116-131',\n",
       "  'citations': '593',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1807.11164v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5793416042116133559&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 34: {'ID': 34,\n",
       "  'title': 'SPICE: Semantic Propositional Image Caption Evaluation',\n",
       "  'authors': ['Stephen Gould',\n",
       "   'Basura Fernando',\n",
       "   'Peter Anderson',\n",
       "   'Mark Johnson'],\n",
       "  'published': '2016-07-29T14:26:27Z',\n",
       "  'updated': '2016-07-29T14:26:27Z',\n",
       "  'abstract': \"There is considerable interest in the task of automatically generating imagecaptions. However, evaluation is challenging. Existing automatic evaluationmetrics are primarily sensitive to n-gram overlap, which is neither necessarynor sufficient for the task of simulating human judgment. We hypothesize thatsemantic propositional content is an important component of human captionevaluation, and propose a new automated caption evaluation metric defined overscene graphs coined SPICE. Extensive evaluations across a range of models anddatasets indicate that SPICE captures human judgments over model-generatedcaptions better than other automatic metrics (e.g., system-level correlation of0.88 with human judgments on the MS COCO dataset, versus 0.43 for CIDEr and0.53 for METEOR). Furthermore, SPICE can answer questions such as `whichcaption-generator best understands colors?' and `can caption-generators count?'\",\n",
       "  'categories': ['cs.CV', 'cs.CL'],\n",
       "  'journal': 'ECCV (5), 382-398',\n",
       "  'citations': '402',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1607.08822v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=8345837387819021644&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 35: {'ID': 35,\n",
       "  'title': 'Residual Attention Network for Image Classification',\n",
       "  'authors': ['Fei Wang',\n",
       "   'Honggang Zhang',\n",
       "   'Xiaoou Tang',\n",
       "   'Xiaogang Wang',\n",
       "   'Chen Qian',\n",
       "   'Shuo Yang',\n",
       "   'Cheng Li',\n",
       "   'Mengqing Jiang'],\n",
       "  'published': '2017-04-23T10:03:49Z',\n",
       "  'updated': '2017-04-23T10:03:49Z',\n",
       "  'abstract': 'In this work, we propose \"Residual Attention Network\", a convolutional neuralnetwork using attention mechanism which can incorporate with state-of-art feedforward network architecture in an end-to-end training fashion. Our ResidualAttention Network is built by stacking Attention Modules which generateattention-aware features. The attention-aware features from different moduleschange adaptively as layers going deeper. Inside each Attention Module,bottom-up top-down feedforward structure is used to unfold the feedforward andfeedback attention process into a single feedforward process. Importantly, wepropose attention residual learning to train very deep Residual AttentionNetworks which can be easily scaled up to hundreds of layers. Extensiveanalyses are conducted on CIFAR-10 and CIFAR-100 datasets to verify theeffectiveness of every module mentioned above. Our Residual Attention Networkachieves state-of-the-art object recognition performance on three benchmarkdatasets including CIFAR-10 (3.90% error), CIFAR-100 (20.45% error) andImageNet (4.8% single model and single crop, top-5 error). Note that, ourmethod achieves 0.6% top-1 accuracy improvement with 46% trunk depth and 69%forward FLOPs comparing to ResNet-200. The experiment also demonstrates thatour network is robust against noisy labels.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '925',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1704.06904v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=9108679738301713484&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 36: {'ID': 36,\n",
       "  'title': 'Learning a Convolutional Neural Network for Non-uniform Motion Blur  Removal',\n",
       "  'authors': ['Jian Sun', 'Zongben Xu', 'Jean Ponce', 'Wenfei Cao'],\n",
       "  'published': '2015-03-02T16:22:51Z',\n",
       "  'updated': '2015-04-12T10:05:53Z',\n",
       "  'abstract': 'In this paper, we address the problem of estimating and removing non-uniformmotion blur from a single blurry image. We propose a deep learning approach topredicting the probabilistic distribution of motion blur at the patch levelusing a convolutional neural network (CNN). We further extend the candidate setof motion kernels predicted by the CNN using carefully designed imagerotations. A Markov random field model is then used to infer a densenon-uniform motion blur field enforcing motion smoothness. Finally, motion bluris removed by a non-uniform deblurring model using patch-level image prior.Experimental evaluations show that our approach can effectively estimate andremove complex non-uniform motion blur that is not handled well by previousapproaches.',\n",
       "  'categories': ['cs.CV', 'I.4'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '356',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1503.00593v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=9317930386552557543&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 37: {'ID': 37,\n",
       "  'title': 'Delving Deep into Rectifiers: Surpassing Human-Level Performance on  ImageNet Classification',\n",
       "  'authors': ['Xiangyu Zhang', 'Jian Sun', 'Shaoqing Ren', 'Kaiming He'],\n",
       "  'published': '2015-02-06T10:44:00Z',\n",
       "  'updated': '2015-02-06T10:44:00Z',\n",
       "  'abstract': 'Rectified activation units (rectifiers) are essential for state-of-the-artneural networks. In this work, we study rectifier neural networks for imageclassification from two aspects. First, we propose a Parametric RectifiedLinear Unit (PReLU) that generalizes the traditional rectified unit. PReLUimproves model fitting with nearly zero extra computational cost and littleoverfitting risk. Second, we derive a robust initialization method thatparticularly considers the rectifier nonlinearities. This method enables us totrain extremely deep rectified models directly from scratch and to investigatedeeper or wider network architectures. Based on our PReLU networks(PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012classification dataset. This is a 26% relative improvement over the ILSVRC 2014winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpasshuman-level performance (5.1%, Russakovsky et al.) on this visual recognitionchallenge.',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.LG'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 1026-1034',\n",
       "  'citations': '8392',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1502.01852v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6243061688889140249&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 38: {'ID': 38,\n",
       "  'title': 'R-C3D: Region Convolutional 3D Network for Temporal Activity Detection',\n",
       "  'authors': ['Kate Saenko', 'Huijuan Xu', 'Abir Das'],\n",
       "  'published': '2017-03-22T18:49:05Z',\n",
       "  'updated': '2017-08-04T22:37:54Z',\n",
       "  'abstract': \"We address the problem of activity detection in continuous, untrimmed videostreams. This is a difficult task that requires extracting meaningfulspatio-temporal features to capture activities, accurately localizing the startand end times of each activity. We introduce a new model, Region Convolutional3D Network (R-C3D), which encodes the video streams using a three-dimensionalfully convolutional network, then generates candidate temporal regionscontaining activities, and finally classifies selected regions into specificactivities. Computation is saved due to the sharing of convolutional featuresbetween the proposal and the classification pipelines. The entire model istrained end-to-end with jointly optimized localization and classificationlosses. R-C3D is faster than existing methods (569 frames per second on asingle Titan X Maxwell GPU) and achieves state-of-the-art results on THUMOS'14.We further demonstrate that our model is a general activity detection frameworkthat does not rely on assumptions about particular dataset properties byevaluating our approach on ActivityNet and Charades. Our code is available athttp://ai.bu.edu/r-c3d/.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 5783-5792',\n",
       "  'citations': '282',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1703.07814v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2971122051682764042&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 39: {'ID': 39,\n",
       "  'title': 'Virtual Worlds as Proxy for Multi-Object Tracking Analysis',\n",
       "  'authors': ['Qiao Wang', 'Eleonora Vig', 'Yohann Cabon', 'Adrien Gaidon'],\n",
       "  'published': '2016-05-20T18:03:07Z',\n",
       "  'updated': '2016-05-20T18:03:07Z',\n",
       "  'abstract': 'Modern computer vision algorithms typically require expensive dataacquisition and accurate manual labeling. In this work, we instead leverage therecent progress in computer graphics to generate fully labeled, dynamic, andphoto-realistic proxy virtual worlds. We propose an efficient real-to-virtualworld cloning method, and validate our approach by building and publiclyreleasing a new video dataset, called Virtual KITTI (seehttp://www.xrce.xerox.com/Research-Development/Computer-Vision/Proxy-Virtual-Worlds),automatically labeled with accurate ground truth for object detection,tracking, scene and instance segmentation, depth, and optical flow. We providequantitative experimental evidence suggesting that (i) modern deep learningalgorithms pre-trained on real data behave similarly in real and virtualworlds, and (ii) pre-training on virtual data improves performance. As the gapbetween real and virtual worlds is small, virtual worlds enable measuring theimpact of various weather and imaging conditions on recognition performance,all other things being equal. We show these factors may affect drasticallyotherwise high-performing deep models for tracking.',\n",
       "  'categories': ['cs.CV', 'cs.LG', 'cs.NE', 'stat.ML'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '464',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1605.06457v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=11727455440906017188&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 40: {'ID': 40,\n",
       "  'title': 'Semi-Supervised Deep Learning for Monocular Depth Map Prediction',\n",
       "  'authors': ['Yevhen Kuznietsov', 'Jörg Stückler', 'Bastian Leibe'],\n",
       "  'published': '2017-02-09T05:08:22Z',\n",
       "  'updated': '2017-05-09T21:58:52Z',\n",
       "  'abstract': 'Supervised deep learning often suffers from the lack of sufficient trainingdata. Specifically in the context of monocular depth map prediction, it isbarely possible to determine dense ground truth depth images in realisticdynamic outdoor environments. When using LiDAR sensors, for instance, noise ispresent in the distance measurements, the calibration between sensors cannot beperfect, and the measurements are typically much sparser than the cameraimages. In this paper, we propose a novel approach to depth map prediction frommonocular images that learns in a semi-supervised way. While we use sparseground-truth depth for supervised learning, we also enforce our deep network toproduce photoconsistent dense depth maps in a stereo setup using a direct imagealignment loss. In experiments we demonstrate superior performance in depth mapprediction from single images compared to the state-of-the-art methods.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '300',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1702.02706v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6028523748057348493&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 41: {'ID': 41,\n",
       "  'title': 'Making the V in VQA Matter: Elevating the Role of Image Understanding in  Visual Question Answering',\n",
       "  'authors': ['Tejas Khot',\n",
       "   'Douglas Summers-Stay',\n",
       "   'Dhruv Batra',\n",
       "   'Devi Parikh',\n",
       "   'Yash Goyal'],\n",
       "  'published': '2016-12-02T20:57:07Z',\n",
       "  'updated': '2017-05-15T17:58:49Z',\n",
       "  'abstract': 'Problems at the intersection of vision and language are of significantimportance both as challenging research questions and for the rich set ofapplications they enable. However, inherent structure in our world and bias inour language tend to be a simpler signal for learning than visual modalities,resulting in models that ignore visual information, leading to an inflatedsense of their capability.  We propose to counter these language priors for the task of Visual QuestionAnswering (VQA) and make vision (the V in VQA) matter! Specifically, we balancethe popular VQA dataset by collecting complementary images such that everyquestion in our balanced dataset is associated with not just a single image,but rather a pair of similar images that result in two different answers to thequestion. Our dataset is by construction more balanced than the original VQAdataset and has approximately twice the number of image-question pairs. Ourcomplete balanced dataset is publicly available at www.visualqa.org as part ofthe 2nd iteration of the Visual Question Answering Dataset and Challenge (VQAv2.0).  We further benchmark a number of state-of-art VQA models on our balanceddataset. All models perform significantly worse on our balanced dataset,suggesting that these models have indeed learned to exploit language priors.This finding provides the first concrete empirical evidence for what seems tobe a qualitative sense among practitioners.  Finally, our data collection protocol for identifying complementary imagesenables us to develop a novel interpretable model, which in addition toproviding an answer to the given (image, question) pair, also provides acounter-example based explanation. Specifically, it identifies an image that issimilar to the original image, but it believes has a different answer to thesame question. This can help in building trust for machines among their users.',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '552',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1612.00837v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=18356113909117574420&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 42: {'ID': 42,\n",
       "  'title': 'Non-local Neural Networks',\n",
       "  'authors': ['Abhinav Gupta', 'Ross Girshick', 'Xiaolong Wang', 'Kaiming He'],\n",
       "  'published': '2017-11-21T18:51:16Z',\n",
       "  'updated': '2018-04-13T06:40:44Z',\n",
       "  'abstract': 'Both convolutional and recurrent operations are building blocks that processone local neighborhood at a time. In this paper, we present non-localoperations as a generic family of building blocks for capturing long-rangedependencies. Inspired by the classical non-local means method in computervision, our non-local operation computes the response at a position as aweighted sum of the features at all positions. This building block can beplugged into many computer vision architectures. On the task of videoclassification, even without any bells and whistles, our non-local models cancompete or outperform current competition winners on both Kinetics and Charadesdatasets. In static image recognition, our non-local models improve objectdetection/segmentation and pose estimation on the COCO suite of tasks. Code isavailable at https://github.com/facebookresearch/video-nonlocal-net .',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '1339',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1711.07971v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=10047890866676489078&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 43: {'ID': 43,\n",
       "  'title': 'Training Region-based Object Detectors with Online Hard Example Mining',\n",
       "  'authors': ['Abhinav Gupta', 'Abhinav Shrivastava', 'Ross Girshick'],\n",
       "  'published': '2016-04-12T19:44:13Z',\n",
       "  'updated': '2016-04-12T19:44:13Z',\n",
       "  'abstract': 'The field of object detection has made significant advances riding on thewave of region-based ConvNets, but their training procedure still includes manyheuristics and hyperparameters that are costly to tune. We present a simple yetsurprisingly effective online hard example mining (OHEM) algorithm for trainingregion-based ConvNet detectors. Our motivation is the same as it has alwaysbeen -- detection datasets contain an overwhelming number of easy examples anda small number of hard examples. Automatic selection of these hard examples canmake training more effective and efficient. OHEM is a simple and intuitivealgorithm that eliminates several heuristics and hyperparameters in common use.But more importantly, it yields consistent and significant boosts in detectionperformance on benchmarks like PASCAL VOC 2007 and 2012. Its effectivenessincreases as datasets become larger and more difficult, as demonstrated by theresults on the MS COCO dataset. Moreover, combined with complementary advancesin the field, OHEM leads to state-of-the-art results of 78.9% and 76.3% mAP onPASCAL VOC 2007 and 2012 respectively.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '1002',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1604.03540v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14496871685581355140&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 44: {'ID': 44,\n",
       "  'title': 'iCaRL: Incremental Classifier and Representation Learning',\n",
       "  'authors': ['Sylvestre-Alvise Rebuffi',\n",
       "   'Georg Sperl',\n",
       "   'Christoph H. Lampert',\n",
       "   'Alexander Kolesnikov'],\n",
       "  'published': '2016-11-23T10:24:11Z',\n",
       "  'updated': '2017-04-14T16:41:02Z',\n",
       "  'abstract': 'A major open problem on the road to artificial intelligence is thedevelopment of incrementally learning systems that learn about more and moreconcepts over time from a stream of data. In this work, we introduce a newtraining strategy, iCaRL, that allows learning in such a class-incremental way:only the training data for a small number of classes has to be present at thesame time and new classes can be added progressively. iCaRL learns strongclassifiers and a data representation simultaneously. This distinguishes itfrom earlier works that were fundamentally limited to fixed datarepresentations and therefore incompatible with deep learning architectures. Weshow by experiments on CIFAR-100 and ImageNet ILSVRC 2012 data that iCaRL canlearn many classes incrementally over a long period of time where otherstrategies quickly fail.',\n",
       "  'categories': ['cs.CV', 'cs.LG', 'stat.ML'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '431',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1611.07725v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=13522192663157931909&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 45: {'ID': 45,\n",
       "  'title': 'Identity Mappings in Deep Residual Networks',\n",
       "  'authors': ['Xiangyu Zhang', 'Jian Sun', 'Shaoqing Ren', 'Kaiming He'],\n",
       "  'published': '2016-03-16T10:53:56Z',\n",
       "  'updated': '2016-07-25T15:18:32Z',\n",
       "  'abstract': 'Deep residual networks have emerged as a family of extremely deeparchitectures showing compelling accuracy and nice convergence behaviors. Inthis paper, we analyze the propagation formulations behind the residualbuilding blocks, which suggest that the forward and backward signals can bedirectly propagated from one block to any other block, when using identitymappings as the skip connections and after-addition activation. A series ofablation experiments support the importance of these identity mappings. Thismotivates us to propose a new residual unit, which makes training easier andimproves generalization. We report improved results using a 1001-layer ResNeton CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet.Code is available at: https://github.com/KaimingHe/resnet-1k-layers',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'journal': 'ECCV (4), 630-645',\n",
       "  'citations': '3778',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1603.05027v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14035416619237709781&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 46: {'ID': 46,\n",
       "  'title': 'Seed, Expand and Constrain: Three Principles for Weakly-Supervised Image  Segmentation',\n",
       "  'authors': ['Christoph H. Lampert', 'Alexander Kolesnikov'],\n",
       "  'published': '2016-03-19T14:13:42Z',\n",
       "  'updated': '2016-08-06T18:49:45Z',\n",
       "  'abstract': 'We introduce a new loss function for the weakly-supervised training ofsemantic image segmentation models based on three guiding principles: to seedwith weak localization cues, to expand objects based on the information aboutwhich classes can occur in an image, and to constrain the segmentations tocoincide with object boundaries. We show experimentally that training a deepconvolutional neural network using the proposed loss function leads tosubstantially better segmentations than previous state-of-the-art methods onthe challenging PASCAL VOC 2012 dataset. We furthermore give insight into theworking mechanism of our method by a detailed experimental study thatillustrates how the segmentation quality is affected by each term of theproposed loss function as well as their combinations.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (4), 695-711',\n",
       "  'citations': '267',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1603.06098v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=7814757711470903185&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 47: {'ID': 47,\n",
       "  'title': 'Higher Order Conditional Random Fields in Deep Neural Networks',\n",
       "  'authors': ['Sadeep Jayasumana',\n",
       "   'Shuai Zheng',\n",
       "   'Anurag Arnab',\n",
       "   'Philip Torr'],\n",
       "  'published': '2015-11-25T17:02:31Z',\n",
       "  'updated': '2016-07-29T18:16:18Z',\n",
       "  'abstract': \"We address the problem of semantic segmentation using deep learning. Mostsegmentation systems include a Conditional Random Field (CRF) to produce astructured output that is consistent with the image's visual features. Recentdeep learning approaches have incorporated CRFs into Convolutional NeuralNetworks (CNNs), with some even training the CRF end-to-end with the rest ofthe network. However, these approaches have not employed higher orderpotentials, which have previously been shown to significantly improvesegmentation performance. In this paper, we demonstrate that two types ofhigher order potential, based on object detections and superpixels, can beincluded in a CRF embedded within a deep network. We design these higher orderpotentials to allow inference with the differentiable mean field algorithm. Asa result, all the parameters of our richer CRF model can be learned end-to-endwith our pixelwise CNN classifier. We achieve state-of-the-art segmentationperformance on the PASCAL VOC benchmark with these trainable higher orderpotentials.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (2), 524-540',\n",
       "  'citations': '181',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1511.08119v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=17919025335598312597&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 48: {'ID': 48,\n",
       "  'title': 'Multi-View 3D Object Detection Network for Autonomous Driving',\n",
       "  'authors': ['Bo Li', 'Huimin Ma', 'Xiaozhi Chen', 'Ji Wan', 'Tian Xia'],\n",
       "  'published': '2016-11-23T12:08:38Z',\n",
       "  'updated': '2017-06-22T03:23:51Z',\n",
       "  'abstract': \"This paper aims at high-accuracy 3D object detection in autonomous drivingscenario. We propose Multi-View 3D networks (MV3D), a sensory-fusion frameworkthat takes both LIDAR point cloud and RGB images as input and predicts oriented3D bounding boxes. We encode the sparse 3D point cloud with a compactmulti-view representation. The network is composed of two subnetworks: one for3D object proposal generation and another for multi-view feature fusion. Theproposal network generates 3D candidate boxes efficiently from the bird's eyeview representation of 3D point cloud. We design a deep fusion scheme tocombine region-wise features from multiple views and enable interactionsbetween intermediate layers of different paths. Experiments on the challengingKITTI benchmark show that our approach outperforms the state-of-the-art byaround 25% and 30% AP on the tasks of 3D localization and 3D detection. Inaddition, for 2D detection, our approach obtains 10.3% higher AP than thestate-of-the-art on the hard data among the LIDAR-based methods.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '696',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1611.07759v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=4264986223142283168&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 49: {'ID': 49,\n",
       "  'title': 'Near-Online Multi-target Tracking with Aggregated Local Flow Descriptor',\n",
       "  'authors': ['Wongun Choi'],\n",
       "  'published': '2015-04-09T14:57:32Z',\n",
       "  'updated': '2015-04-09T14:57:32Z',\n",
       "  'abstract': 'In this paper, we focus on the two key aspects of multiple target trackingproblem: 1) designing an accurate affinity measure to associate detections and2) implementing an efficient and accurate (near) online multiple targettracking algorithm. As the first contribution, we introduce a novel AggregatedLocal Flow Descriptor (ALFD) that encodes the relative motion pattern between apair of temporally distant detections using long term interest pointtrajectories (IPTs). Leveraging on the IPTs, the ALFD provides a robustaffinity measure for estimating the likelihood of matching detectionsregardless of the application scenarios. As another contribution, we present aNear-Online Multi-target Tracking (NOMT) algorithm. The tracking problem isformulated as a data-association between targets and detections in a temporalwindow, that is performed repeatedly at every frame. While being efficient,NOMT achieves robustness via integrating multiple cues including ALFD metric,target dynamics, appearance similarity, and long term trajectory regularizationinto the model. Our ablative analysis verifies the superiority of the ALFDmetric over the other conventional affinity metrics. We run a comprehensiveexperimental evaluation on two challenging tracking datasets, KITTI and MOTdatasets. The NOMT method combined with ALFD metric achieves the best accuracyin both datasets with significant margins (about 10% higher MOTA) over thestate-of-the-arts.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 3029-3037',\n",
       "  'citations': '271',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1504.02340v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=3120250776569516262&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 50: {'ID': 50,\n",
       "  'title': 'Performance Measures and a Data Set for Multi-Target, Multi-Camera  Tracking',\n",
       "  'authors': ['Ergys Ristani',\n",
       "   'Roger S. Zou',\n",
       "   'Rita Cucchiara',\n",
       "   'Francesco Solera',\n",
       "   'Carlo Tomasi'],\n",
       "  'published': '2016-09-06T21:58:25Z',\n",
       "  'updated': '2016-09-19T17:27:19Z',\n",
       "  'abstract': 'To help accelerate progress in multi-target, multi-camera tracking systems,we present (i) a new pair of precision-recall measures of performance thattreats errors of all types uniformly and emphasizes correct identification oversources of error; (ii) the largest fully-annotated and calibrated data set todate with more than 2 million frames of 1080p, 60fps video taken by 8 camerasobserving more than 2,700 identities over 85 minutes; and (iii) a referencesoftware system as a comparison baseline. We show that (i) our measuresproperly account for bottom-line identity match performance in the multi-camerasetting; (ii) our data set poses realistic challenges to current trackers; and(iii) the performance of our system is comparable to the state of the art.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV Workshops (2), 17-35',\n",
       "  'citations': '712',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1609.01775v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2017351755480753877&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 51: {'ID': 51,\n",
       "  'title': 'What do different evaluation metrics tell us about saliency models?',\n",
       "  'authors': ['Zoya Bylinskii',\n",
       "   'Antonio Torralba',\n",
       "   'Aude Oliva',\n",
       "   'Frédo Durand',\n",
       "   'Tilke Judd'],\n",
       "  'published': '2016-04-12T22:16:20Z',\n",
       "  'updated': '2017-04-06T23:46:40Z',\n",
       "  'abstract': \"How best to evaluate a saliency model's ability to predict where humans lookin images is an open research question. The choice of evaluation metric dependson how saliency is defined and how the ground truth is represented. Metricsdiffer in how they rank saliency models, and this results from how falsepositives and false negatives are treated, whether viewing biases are accountedfor, whether spatial deviations are factored in, and how the saliency maps arepre-processed. In this paper, we provide an analysis of 8 different evaluationmetrics and their properties. With the help of systematic experiments andvisualizations of metric computations, we add interpretability to saliencyscores and more transparency to the evaluation of saliency models. Building offthe differences in metric properties and behaviors, we make recommendations formetric selections under specific assumptions and for specific applications.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 41 (3), 740-757',\n",
       "  'citations': '303',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1604.03605v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=9868191028129995803&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 52: {'ID': 52,\n",
       "  'title': 'ModDrop: adaptive multi-modal gesture recognition',\n",
       "  'authors': ['Florian Nebout',\n",
       "   'Graham W. Taylor',\n",
       "   'Natalia Neverova',\n",
       "   'Christian Wolf'],\n",
       "  'published': '2014-12-31T09:55:43Z',\n",
       "  'updated': '2015-06-06T14:46:33Z',\n",
       "  'abstract': 'We present a method for gesture detection and localisation based onmulti-scale and multi-modal deep learning. Each visual modality capturesspatial information at a particular spatial scale (such as motion of the upperbody or a hand), and the whole system operates at three temporal scales. Key toour technique is a training strategy which exploits: i) careful initializationof individual modalities; and ii) gradual fusion involving random dropping ofseparate channels (dubbed ModDrop) for learning cross-modality correlationswhile preserving uniqueness of each modality-specific representation. Wepresent experiments on the ChaLearn 2014 Looking at People Challenge gesturerecognition track, in which we placed first out of 17 teams. Fusing multiplemodalities at several spatial and temporal scales leads to a significantincrease in recognition rates, allowing the model to compensate for errors ofthe individual classifiers as well as noise in the separate channels.Futhermore, the proposed ModDrop training technique ensures robustness of theclassifier to missing signals in one or several channels to produce meaningfulpredictions from any number of available modalities. In addition, wedemonstrate the applicability of the proposed fusion scheme to modalities ofarbitrary nature by experiments on the same dataset augmented with audio.',\n",
       "  'categories': ['cs.CV', 'cs.HC', 'cs.LG'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 38 (8), 1692-1706',\n",
       "  'citations': '192',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1501.00102v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2575377636567989889&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 53: {'ID': 53,\n",
       "  'title': 'CNN-SLAM: Real-time dense monocular SLAM with learned depth prediction',\n",
       "  'authors': ['Nassir Navab',\n",
       "   'Federico Tombari',\n",
       "   'Iro Laina',\n",
       "   'Keisuke Tateno'],\n",
       "  'published': '2017-04-11T18:37:11Z',\n",
       "  'updated': '2017-04-11T18:37:11Z',\n",
       "  'abstract': 'Given the recent advances in depth prediction from Convolutional NeuralNetworks (CNNs), this paper investigates how predicted depth maps from a deepneural network can be deployed for accurate and dense monocular reconstruction.We propose a method where CNN-predicted dense depth maps are naturally fusedtogether with depth measurements obtained from direct monocular SLAM. Ourfusion scheme privileges depth prediction in image locations where monocularSLAM approaches tend to fail, e.g. along low-textured regions, and vice-versa.We demonstrate the use of depth prediction for estimating the absolute scale ofthe reconstruction, hence overcoming one of the major limitations of monocularSLAM. Finally, we propose a framework to efficiently fuse semantic labels,obtained from a single frame, with dense SLAM, yielding semantically coherentscene reconstruction from a single view. Evaluation results on two benchmarkdatasets show the robustness and accuracy of our approach.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '319',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1704.03489v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=13958434312553724438&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 54: {'ID': 54,\n",
       "  'title': 'Deep Colorization',\n",
       "  'authors': ['Bin Sheng', 'Qingxiong Yang', 'Zezhou Cheng'],\n",
       "  'published': '2016-04-30T07:58:05Z',\n",
       "  'updated': '2016-04-30T07:58:05Z',\n",
       "  'abstract': 'This paper investigates into the colorization problem which converts agrayscale image to a colorful version. This is a very difficult problem andnormally requires manual adjustment to achieve artifact-free quality. Forinstance, it normally requires human-labelled color scribbles on the grayscaletarget image or a careful selection of colorful reference images (e.g.,capturing the same scene in the grayscale target image). Unlike the previousmethods, this paper aims at a high-quality fully-automatic colorization method.With the assumption of a perfect patch matching technique, the use of anextremely large-scale reference database (that contains sufficient colorimages) is the most reliable solution to the colorization problem. However,patch matching noise will increase with respect to the size of the referencedatabase in practice. Inspired by the recent success in deep learningtechniques which provide amazing modeling of large-scale data, this paperre-formulates the colorization problem so that deep learning techniques can bedirectly employed. To ensure artifact-free quality, a joint bilateral filteringbased post-processing step is proposed. We further develop an adaptive imageclustering technique to incorporate the global image information. Numerousexperiments demonstrate that our method outperforms the state-of-art algorithmsboth in terms of quality and speed.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 415-423',\n",
       "  'citations': '271',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1605.00075v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=12455232730149289375&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 55: {'ID': 55,\n",
       "  'title': 'Flowing ConvNets for Human Pose Estimation in Videos',\n",
       "  'authors': ['Andrew Zisserman', 'Tomas Pfister', 'James Charles'],\n",
       "  'published': '2015-06-09T13:17:33Z',\n",
       "  'updated': '2015-11-08T16:52:59Z',\n",
       "  'abstract': \"The objective of this work is human pose estimation in videos, where multipleframes are available. We investigate a ConvNet architecture that is able tobenefit from temporal context by combining information across the multipleframes using optical flow.  To this end we propose a network architecture with the following novelties:(i) a deeper network than previously investigated for regressing heatmaps; (ii)spatial fusion layers that learn an implicit spatial model; (iii) optical flowis used to align heatmap predictions from neighbouring frames; and (iv) a finalparametric pooling layer which learns to combine the aligned heatmaps into apooled confidence map.  We show that this architecture outperforms a number of others, including onethat uses optical flow solely at the input layers, one that regresses jointcoordinates directly, and one that predicts heatmaps without spatial fusion.  The new architecture outperforms the state of the art by a large margin onthree video pose estimation datasets, including the very challenging Poses inthe Wild dataset, and outperforms other deep methods that don't use a graphicalmodel on the single-image FLIC benchmark (and also Chen &amp; Yuille and Tompson etal. in the high precision region).\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 1913-1921',\n",
       "  'citations': '371',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1506.02897v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=13561683774802539240&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 56: {'ID': 56,\n",
       "  'title': 'Aligning Books and Movies: Towards Story-like Visual Explanations by  Watching Movies and Reading Books',\n",
       "  'authors': ['Raquel Urtasun',\n",
       "   'Sanja Fidler',\n",
       "   'Ryan Kiros',\n",
       "   'Antonio Torralba',\n",
       "   'Yukun Zhu',\n",
       "   'Richard Zemel',\n",
       "   'Ruslan Salakhutdinov'],\n",
       "  'published': '2015-06-22T19:26:56Z',\n",
       "  'updated': '2015-06-22T19:26:56Z',\n",
       "  'abstract': 'Books are a rich source of both fine-grained information, how a character, anobject or a scene looks like, as well as high-level semantics, what someone isthinking, feeling and how these states evolve through a story. This paper aimsto align books to their movie releases in order to provide rich descriptiveexplanations for visual content that go semantically far beyond the captionsavailable in current datasets. To align movies and books we exploit a neuralsentence embedding that is trained in an unsupervised way from a large corpusof books, as well as a video-text neural embedding for computing similaritiesbetween movie clips and sentences in the book. We propose a context-aware CNNto combine information from multiple sources. We demonstrate good quantitativeperformance for movie/book alignment and show several qualitative examples thatshowcase the diversity of tasks our model can be used for.',\n",
       "  'categories': ['cs.CV', 'cs.CL'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 19-27',\n",
       "  'citations': '598',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1506.06724v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=4414474086145090761&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 57: {'ID': 57,\n",
       "  'title': 'FaceNet: A Unified Embedding for Face Recognition and Clustering',\n",
       "  'authors': ['James Philbin', 'Dmitry Kalenichenko', 'Florian Schroff'],\n",
       "  'published': '2015-03-12T18:10:53Z',\n",
       "  'updated': '2015-06-17T23:35:47Z',\n",
       "  'abstract': 'Despite significant recent advances in the field of face recognition,implementing face verification and recognition efficiently at scale presentsserious challenges to current approaches. In this paper we present a system,called FaceNet, that directly learns a mapping from face images to a compactEuclidean space where distances directly correspond to a measure of facesimilarity. Once this space has been produced, tasks such as face recognition,verification and clustering can be easily implemented using standard techniqueswith FaceNet embeddings as feature vectors.  Our method uses a deep convolutional network trained to directly optimize theembedding itself, rather than an intermediate bottleneck layer as in previousdeep learning approaches. To train, we use triplets of roughly aligned matching/ non-matching face patches generated using a novel online triplet miningmethod. The benefit of our approach is much greater representationalefficiency: we achieve state-of-the-art face recognition performance using only128-bytes per face.  On the widely used Labeled Faces in the Wild (LFW) dataset, our systemachieves a new record accuracy of 99.63%. On YouTube Faces DB it achieves95.12%. Our system cuts the error rate in comparison to the best publishedresult by 30% on both datasets.  We also introduce the concept of harmonic embeddings, and a harmonic tripletloss, which describe different versions of face embeddings (produced bydifferent networks) that are compatible to each other and allow for directcomparison between each other.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '5405',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1503.03832v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=11100188708037387301&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 58: {'ID': 58,\n",
       "  'title': 'Zero-Shot Learning via Semantic Similarity Embedding',\n",
       "  'authors': ['Ziming Zhang', 'Venkatesh Saligrama'],\n",
       "  'published': '2015-09-15T23:18:52Z',\n",
       "  'updated': '2015-09-25T20:26:08Z',\n",
       "  'abstract': 'In this paper we consider a version of the zero-shot learning problem whereseen class source and target domain data are provided. The goal duringtest-time is to accurately predict the class label of an unseen target domaininstance based on revealed source domain side information (\\\\eg attributes) forunseen classes. Our method is based on viewing each source or target data as amixture of seen class proportions and we postulate that the mixture patternshave to be similar if the two instances belong to the same unseen class. Thisperspective leads us to learning source/target embedding functions that map anarbitrary source/target domain data into a same semantic space where similaritycan be readily measured. We develop a max-margin framework to learn thesesimilarity functions and jointly optimize parameters by means of crossvalidation. Our test results are compelling, leading to significant improvementin terms of accuracy on most benchmark datasets for zero-shot recognition.',\n",
       "  'categories': ['cs.CV', 'stat.ML'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 4166-4174',\n",
       "  'citations': '376',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1509.04767v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=3847883737916740072&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 59: {'ID': 59,\n",
       "  'title': 'DenseCap: Fully Convolutional Localization Networks for Dense Captioning',\n",
       "  'authors': ['Justin Johnson', 'Li Fei-Fei', 'Andrej Karpathy'],\n",
       "  'published': '2015-11-24T05:13:54Z',\n",
       "  'updated': '2015-11-24T05:13:54Z',\n",
       "  'abstract': 'We introduce the dense captioning task, which requires a computer visionsystem to both localize and describe salient regions in images in naturallanguage. The dense captioning task generalizes object detection when thedescriptions consist of a single word, and Image Captioning when one predictedregion covers the full image. To address the localization and description taskjointly we propose a Fully Convolutional Localization Network (FCLN)architecture that processes an image with a single, efficient forward pass,requires no external regions proposals, and can be trained end-to-end with asingle round of optimization. The architecture is composed of a ConvolutionalNetwork, a novel dense localization layer, and Recurrent Neural Networklanguage model that generates the label sequences. We evaluate our network onthe Visual Genome dataset, which comprises 94,000 images and 4,100,000region-grounded captions. We observe both speed and accuracy improvements overbaselines based on current state of the art approaches in both generation andretrieval settings.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '720',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1511.07571v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=7906290079702357124&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 60: {'ID': 60,\n",
       "  'title': 'Detect to Track and Track to Detect',\n",
       "  'authors': ['Andrew Zisserman', 'Christoph Feichtenhofer', 'Axel Pinz'],\n",
       "  'published': '2017-10-11T08:33:48Z',\n",
       "  'updated': '2018-03-07T10:49:41Z',\n",
       "  'abstract': 'Recent approaches for high accuracy detection and tracking of objectcategories in video consist of complex multistage solutions that become morecumbersome each year. In this paper we propose a ConvNet architecture thatjointly performs detection and tracking, solving the task in a simple andeffective way. Our contributions are threefold: (i) we set up a ConvNetarchitecture for simultaneous detection and tracking, using a multi-taskobjective for frame-based object detection and across-frame track regression;(ii) we introduce correlation features that represent object co-occurrencesacross time to aid the ConvNet during tracking; and (iii) we link the framelevel detections based on our across-frame tracklets to produce high accuracydetections at the video level. Our ConvNet architecture for spatiotemporalobject detection is evaluated on the large-scale ImageNet VID dataset where itachieves state-of-the-art results. Our approach provides better single modelperformance than the winning method of the last ImageNet challenge while beingconceptually much simpler. Finally, we show that by increasing the temporalstride we can dramatically increase the tracker speed.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 3038-3046',\n",
       "  'citations': '181',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1710.03958v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=16427653933463726101&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 61: {'ID': 61,\n",
       "  'title': 'Beyond Correlation Filters: Learning Continuous Convolution Operators  for Visual Tracking',\n",
       "  'authors': ['Martin Danelljan',\n",
       "   'Andreas Robinson',\n",
       "   'Fahad Shahbaz Khan',\n",
       "   'Michael Felsberg'],\n",
       "  'published': '2016-08-12T12:24:11Z',\n",
       "  'updated': '2016-08-29T10:33:17Z',\n",
       "  'abstract': 'Discriminative Correlation Filters (DCF) have demonstrated excellentperformance for visual object tracking. The key to their success is the abilityto efficiently exploit available negative data by including all shiftedversions of a training sample. However, the underlying DCF formulation isrestricted to single-resolution feature maps, significantly limiting itspotential. In this paper, we go beyond the conventional DCF framework andintroduce a novel formulation for training continuous convolution filters. Weemploy an implicit interpolation model to pose the learning problem in thecontinuous spatial domain. Our proposed formulation enables efficientintegration of multi-resolution deep feature maps, leading to superior resultson three object tracking benchmarks: OTB-2015 (+5.1% in mean OP), Temple-Color(+4.6% in mean OP), and VOT2015 (20% relative reduction in failure rate).Additionally, our approach is capable of sub-pixel localization, crucial forthe task of accurate feature point tracking. We also demonstrate theeffectiveness of our learning formulation in extensive feature point trackingexperiments. Code and supplementary material are available athttp://www.cvl.isy.liu.se/research/objrec/visualtracking/conttrack/index.html.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (5), 472-488',\n",
       "  'citations': '914',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1608.03773v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=8202800204845032663&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 62: {'ID': 62,\n",
       "  'title': 'Learning to track for spatio-temporal action localization',\n",
       "  'authors': ['Cordelia Schmid', 'Zaid Harchaoui', 'Philippe Weinzaepfel'],\n",
       "  'published': '2015-06-05T14:48:46Z',\n",
       "  'updated': '2015-09-27T11:21:16Z',\n",
       "  'abstract': 'We propose an effective approach for spatio-temporal action localization inrealistic videos. The approach first detects proposals at the frame-level andscores them with a combination of static and motion CNN features. It thentracks high-scoring proposals throughout the video using atracking-by-detection approach. Our tracker relies simultaneously oninstance-level and class-level detectors. The tracks are scored using aspatio-temporal motion histogram, a descriptor at the track level, incombination with the CNN features. Finally, we perform temporal localization ofthe action using a sliding-window approach at the track level. We presentexperimental results for spatio-temporal localization on the UCF-Sports, J-HMDBand UCF-101 action localization datasets, where our approach outperforms thestate of the art with a margin of 15%, 7% and 12% respectively in mAP.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 3164-3172',\n",
       "  'citations': '230',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1506.01929v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=1893083230530095061&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 63: {'ID': 63,\n",
       "  'title': 'Supervised Learning of Semantics-Preserving Hash via Deep Convolutional  Neural Networks',\n",
       "  'authors': ['Huei-Fang Yang', 'Kevin Lin', 'Chu-Song Chen'],\n",
       "  'published': '2015-07-01T04:40:31Z',\n",
       "  'updated': '2017-02-14T07:31:18Z',\n",
       "  'abstract': 'This paper presents a simple yet effective supervised deep hash approach thatconstructs binary hash codes from labeled data for large-scale image search. Weassume that the semantic labels are governed by several latent attributes witheach attribute on or off, and classification relies on these attributes. Basedon this assumption, our approach, dubbed supervised semantics-preserving deephashing (SSDH), constructs hash functions as a latent layer in a deep networkand the binary codes are learned by minimizing an objective function definedover classification error and other desirable hash codes properties. With thisdesign, SSDH has a nice characteristic that classification and retrieval areunified in a single learning model. Moreover, SSDH performs joint learning ofimage representations, hash codes, and classification in a point-wised manner,and thus is scalable to large-scale datasets. SSDH is simple and can berealized by a slight enhancement of an existing deep architecture forclassification; yet it is effective and outperforms other hashing approaches onseveral benchmarks and large datasets. Compared with state-of-the-artapproaches, SSDH achieves higher retrieval accuracy, while the classificationperformance is not sacrificed.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 40 (2), 437-451',\n",
       "  'citations': '172',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1507.00101v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=914342533906912610&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 64: {'ID': 64,\n",
       "  'title': 'Deep Metric Learning via Lifted Structured Feature Embedding',\n",
       "  'authors': ['Silvio Savarese',\n",
       "   'Stefanie Jegelka',\n",
       "   'Yu Xiang',\n",
       "   'Hyun Oh Song'],\n",
       "  'published': '2015-11-19T23:41:11Z',\n",
       "  'updated': '2015-11-19T23:41:11Z',\n",
       "  'abstract': 'Learning the distance metric between pairs of examples is of great importancefor learning and visual recognition. With the remarkable success from the stateof the art convolutional neural networks, recent works have shown promisingresults on discriminatively training the networks to learn semantic featureembeddings where similar examples are mapped close to each other and dissimilarexamples are mapped farther apart. In this paper, we describe an algorithm fortaking full advantage of the training batches in the neural network training bylifting the vector of pairwise distances within the batch to the matrix ofpairwise distances. This step enables the algorithm to learn the state of theart feature embedding by optimizing a novel structured prediction objective onthe lifted problem. Additionally, we collected Online Products dataset: 120kimages of 23k classes of online products for metric learning. Our experimentson the CUB-200-2011, CARS196, and Online Products datasets demonstratesignificant improvement over existing deep feature embedding methods on allexperimented embedding sizes with the GoogLeNet network.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '661',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1511.06452v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2086461462991139373&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 65: {'ID': 65,\n",
       "  'title': 'DeepLab: Semantic Image Segmentation with Deep Convolutional Nets,  Atrous Convolution, and Fully Connected CRFs',\n",
       "  'authors': ['Iasonas Kokkinos',\n",
       "   'Kevin Murphy',\n",
       "   'Alan L. Yuille',\n",
       "   'George Papandreou',\n",
       "   'Liang-Chieh Chen'],\n",
       "  'published': '2016-06-02T21:52:21Z',\n",
       "  'updated': '2017-05-12T03:25:47Z',\n",
       "  'abstract': 'In this work we address the task of semantic image segmentation with DeepLearning and make three main contributions that are experimentally shown tohave substantial practical merit. First, we highlight convolution withupsampled filters, or \\'atrous convolution\\', as a powerful tool in denseprediction tasks. Atrous convolution allows us to explicitly control theresolution at which feature responses are computed within Deep ConvolutionalNeural Networks. It also allows us to effectively enlarge the field of view offilters to incorporate larger context without increasing the number ofparameters or the amount of computation. Second, we propose atrous spatialpyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPPprobes an incoming convolutional feature layer with filters at multiplesampling rates and effective fields-of-views, thus capturing objects as well asimage context at multiple scales. Third, we improve the localization of objectboundaries by combining methods from DCNNs and probabilistic graphical models.The commonly deployed combination of max-pooling and downsampling in DCNNsachieves invariance but has a toll on localization accuracy. We overcome thisby combining the responses at the final DCNN layer with a fully connectedConditional Random Field (CRF), which is shown both qualitatively andquantitatively to improve localization performance. Our proposed \"DeepLab\"system sets the new state-of-art at the PASCAL VOC-2012 semantic imagesegmentation task, reaching 79.7% mIOU in the test set, and advances theresults on three other datasets: PASCAL-Context, PASCAL-Person-Part, andCityscapes. All of our code is made publicly available online.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 40 (4), 834-848',\n",
       "  'citations': '5113',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1606.00915v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=12855937004888359241&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 66: {'ID': 66,\n",
       "  'title': 'Deep Learning Face Attributes in the Wild',\n",
       "  'authors': ['Ziwei Liu', 'Ping Luo', 'Xiaogang Wang', 'Xiaoou Tang'],\n",
       "  'published': '2014-11-28T07:13:54Z',\n",
       "  'updated': '2015-09-24T13:52:26Z',\n",
       "  'abstract': 'Predicting face attributes in the wild is challenging due to complex facevariations. We propose a novel deep learning framework for attribute predictionin the wild. It cascades two CNNs, LNet and ANet, which are fine-tuned jointlywith attribute tags, but pre-trained differently. LNet is pre-trained bymassive general object categories for face localization, while ANet ispre-trained by massive face identities for attribute prediction. This frameworknot only outperforms the state-of-the-art with a large margin, but also revealsvaluable facts on learning face representation.  (1) It shows how the performances of face localization (LNet) and attributeprediction (ANet) can be improved by different pre-training strategies.  (2) It reveals that although the filters of LNet are fine-tuned only withimage-level attribute tags, their response maps over entire images have strongindication of face locations. This fact enables training LNet for facelocalization with only image-level annotations, but without face bounding boxesor landmarks, which are required by all attribute recognition works.  (3) It also demonstrates that the high-level hidden neurons of ANetautomatically discover semantic concepts after pre-training with massive faceidentities, and such concepts are significantly enriched after fine-tuning withattribute tags. Each attribute can be well explained with a sparse linearcombination of these concepts.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 3730-3738',\n",
       "  'citations': '2586',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1411.7766v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=9137261784815578205&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 67: {'ID': 67,\n",
       "  'title': 'Genetic CNN',\n",
       "  'authors': ['Alan Yuille', 'Lingxi Xie'],\n",
       "  'published': '2017-03-04T19:44:16Z',\n",
       "  'updated': '2017-03-04T19:44:16Z',\n",
       "  'abstract': 'The deep Convolutional Neural Network (CNN) is the state-of-the-art solutionfor large-scale visual recognition. Following basic principles such asincreasing the depth and constructing highway connections, researchers havemanually designed a lot of fixed network structures and verified theireffectiveness.  In this paper, we discuss the possibility of learning deep network structuresautomatically. Note that the number of possible network structures increasesexponentially with the number of layers in the network, which inspires us toadopt the genetic algorithm to efficiently traverse this large search space. Wefirst propose an encoding method to represent each network structure in afixed-length binary string, and initialize the genetic algorithm by generatinga set of randomized individuals. In each generation, we define standard geneticoperations, e.g., selection, mutation and crossover, to eliminate weakindividuals and then generate more competitive ones. The competitiveness ofeach individual is defined as its recognition accuracy, which is obtained viatraining the network from scratch and evaluating it on a validation set. We runthe genetic process on two small datasets, i.e., MNIST and CIFAR10,demonstrating its ability to evolve and find high-quality structures which arelittle studied before. These structures are also transferrable to thelarge-scale ILSVRC2012 dataset.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 1379-1388',\n",
       "  'citations': '287',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1703.01513v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=3972137416848470599&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 68: {'ID': 68,\n",
       "  'title': 'Multimodal Unsupervised Image-to-Image Translation',\n",
       "  'authors': ['Xun Huang', 'Ming-Yu Liu', 'Jan Kautz', 'Serge Belongie'],\n",
       "  'published': '2018-04-12T21:17:54Z',\n",
       "  'updated': '2018-08-14T18:44:12Z',\n",
       "  'abstract': 'Unsupervised image-to-image translation is an important and challengingproblem in computer vision. Given an image in the source domain, the goal is tolearn the conditional distribution of corresponding images in the targetdomain, without seeing any pairs of corresponding images. While thisconditional distribution is inherently multimodal, existing approaches make anoverly simplified assumption, modeling it as a deterministic one-to-onemapping. As a result, they fail to generate diverse outputs from a given sourcedomain image. To address this limitation, we propose a Multimodal UnsupervisedImage-to-image Translation (MUNIT) framework. We assume that the imagerepresentation can be decomposed into a content code that is domain-invariant,and a style code that captures domain-specific properties. To translate animage to another domain, we recombine its content code with a random style codesampled from the style space of the target domain. We analyze the proposedframework and establish several theoretical results. Extensive experiments withcomparisons to the state-of-the-art approaches further demonstrates theadvantage of the proposed framework. Moreover, our framework allows users tocontrol the style of translation outputs by providing an example style image.Code and pretrained models are available at https://github.com/nvlabs/MUNIT',\n",
       "  'categories': ['cs.CV', 'cs.LG', 'stat.ML'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 172-189',\n",
       "  'citations': '553',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1804.04732v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=13317525907573308290&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 69: {'ID': 69,\n",
       "  'title': 'A Point Set Generation Network for 3D Object Reconstruction from a  Single Image',\n",
       "  'authors': ['Hao Su', 'Leonidas Guibas', 'Haoqiang Fan'],\n",
       "  'published': '2016-12-02T09:20:09Z',\n",
       "  'updated': '2016-12-07T01:12:53Z',\n",
       "  'abstract': 'Generation of 3D data by deep neural network has been attracting increasingattention in the research community. The majority of extant works resort toregular representations such as volumetric grids or collection of images;however, these representations obscure the natural invariance of 3D shapesunder geometric transformations and also suffer from a number of other issues.In this paper we address the problem of 3D reconstruction from a single image,generating a straight-forward form of output -- point cloud coordinates. Alongwith this problem arises a unique and interesting issue, that the groundtruthshape for an input image may be ambiguous. Driven by this unorthodox outputform and the inherent ambiguity in groundtruth, we design architecture, lossfunction and learning paradigm that are novel and effective. Our final solutionis a conditional shape sampler, capable of predicting multiple plausible 3Dpoint clouds from an input image. In experiments not only can our systemoutperform state-of-the-art methods on single image based 3d reconstructionbenchmarks; but it also shows a strong performance for 3d shape completion andpromising ability in making multiple plausible predictions.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '529',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1612.00603v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2714970910185586914&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 70: {'ID': 70,\n",
       "  'title': 'DSOD: Learning Deeply Supervised Object Detectors from Scratch',\n",
       "  'authors': ['Jianguo Li',\n",
       "   'Xiangyang Xue',\n",
       "   'Zhiqiang Shen',\n",
       "   'Yurong Chen',\n",
       "   'Zhuang Liu',\n",
       "   'Yu-Gang Jiang'],\n",
       "  'published': '2017-08-03T17:33:05Z',\n",
       "  'updated': '2018-04-30T02:17:30Z',\n",
       "  'abstract': 'We present Deeply Supervised Object Detector (DSOD), a framework that canlearn object detectors from scratch. State-of-the-art object objectors relyheavily on the off-the-shelf networks pre-trained on large-scale classificationdatasets like ImageNet, which incurs learning bias due to the difference onboth the loss functions and the category distributions between classificationand detection tasks. Model fine-tuning for the detection task could alleviatethis bias to some extent but not fundamentally. Besides, transferringpre-trained models from classification to detection between discrepant domainsis even more difficult (e.g. RGB to depth images). A better solution to tacklethese two critical problems is to train object detectors from scratch, whichmotivates our proposed DSOD. Previous efforts in this direction mostly faileddue to much more complicated loss functions and limited training data in objectdetection. In DSOD, we contribute a set of design principles for trainingobject detectors from scratch. One of the key findings is that deepsupervision, enabled by dense layer-wise connections, plays a critical role inlearning a good detector. Combining with several other principles, we developDSOD following the single-shot detection (SSD) framework. Experiments on PASCALVOC 2007, 2012 and MS COCO datasets demonstrate that DSOD can achieve betterresults than the state-of-the-art solutions with much more compact models. Forinstance, DSOD outperforms SSD on all three benchmarks with real-time detectionspeed, while requires only 1/2 parameters to SSD and 1/10 parameters to FasterRCNN. Our code and models are available at: https://github.com/szq0214/DSOD .',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 1919-1927',\n",
       "  'citations': '282',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1708.01241v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=11269322695561322552&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 71: {'ID': 71,\n",
       "  'title': 'Material Recognition in the Wild with the Materials in Context Database',\n",
       "  'authors': ['Kavita Bala', 'Sean Bell', 'Paul Upchurch', 'Noah Snavely'],\n",
       "  'published': '2014-12-01T20:11:44Z',\n",
       "  'updated': '2015-04-14T05:29:32Z',\n",
       "  'abstract': 'Recognizing materials in real-world images is a challenging task. Real-worldmaterials have rich surface texture, geometry, lighting conditions, andclutter, which combine to make the problem particularly difficult. In thispaper, we introduce a new, large-scale, open dataset of materials in the wild,the Materials in Context Database (MINC), and combine this dataset with deeplearning to achieve material recognition and segmentation of images in thewild.  MINC is an order of magnitude larger than previous material databases, whilebeing more diverse and well-sampled across its 23 categories. Using MINC, wetrain convolutional neural networks (CNNs) for two tasks: classifying materialsfrom patches, and simultaneous material recognition and segmentation in fullimages. For patch-based classification on MINC we found that the bestperforming CNN architectures can achieve 85.2% mean class accuracy. We convertthese trained CNN classifiers into an efficient fully convolutional frameworkcombined with a fully connected conditional random field (CRF) to predict thematerial at every pixel in an image, achieving 73.1% mean class accuracy. Ourexperiments demonstrate that having a large, well-sampled dataset such as MINCis crucial for real-world material recognition and segmentation.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '299',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1412.0623v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5759554714077518327&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 72: {'ID': 72,\n",
       "  'title': 'Fully Convolutional Networks for Semantic Segmentation',\n",
       "  'authors': ['Evan Shelhamer', 'Trevor Darrell', 'Jonathan Long'],\n",
       "  'published': '2016-05-20T04:30:16Z',\n",
       "  'updated': '2016-05-20T04:30:16Z',\n",
       "  'abstract': 'Convolutional networks are powerful visual models that yield hierarchies offeatures. We show that convolutional networks by themselves, trainedend-to-end, pixels-to-pixels, improve on the previous best result in semanticsegmentation. Our key insight is to build \"fully convolutional\" networks thattake input of arbitrary size and produce correspondingly-sized output withefficient inference and learning. We define and detail the space of fullyconvolutional networks, explain their application to spatially dense predictiontasks, and draw connections to prior models. We adapt contemporaryclassification networks (AlexNet, the VGG net, and GoogLeNet) into fullyconvolutional networks and transfer their learned representations byfine-tuning to the segmentation task. We then define a skip architecture thatcombines semantic information from a deep, coarse layer with appearanceinformation from a shallow, fine layer to produce accurate and detailedsegmentations. Our fully convolutional network achieves improved segmentationof PASCAL VOC (30% relative improvement to 67.2% mean IU on 2012), NYUDv2, SIFTFlow, and PASCAL-Context, while inference takes one tenth of a second for atypical image.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '16664',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1605.06211v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=16635967164511657165&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 73: {'ID': 73,\n",
       "  'title': 'Learning a Deep Model for Human Action Recognition from Novel Viewpoints',\n",
       "  'authors': ['Ajmal Mian', 'Mubarak Shah', 'Hossein Rahmani'],\n",
       "  'published': '2016-02-02T08:42:44Z',\n",
       "  'updated': '2016-02-02T08:42:44Z',\n",
       "  'abstract': 'Recognizing human actions from unknown and unseen (novel) views is achallenging problem. We propose a Robust Non-Linear Knowledge Transfer Model(R-NKTM) for human action recognition from novel views. The proposed R-NKTM isa deep fully-connected neural network that transfers knowledge of human actionsfrom any unknown view to a shared high-level virtual view by finding anon-linear virtual path that connects the views. The R-NKTM is learned fromdense trajectories of synthetic 3D human models fitted to real motion capturedata and generalizes to real videos of human actions. The strength of ourtechnique is that we learn a single R-NKTM for all actions and all viewpointsfor knowledge transfer of any real human action video without the need forre-training or fine-tuning the model. Thus, R-NKTM can efficiently scale toincorporate new action classes. R-NKTM is learned with dummy labels and doesnot require knowledge of the camera viewpoint at any stage. Experiments onthree benchmark cross-view human action datasets show that our methodoutperforms existing state-of-the-art.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 40 (3), 667-681',\n",
       "  'citations': '150',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1602.00828v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=839820257704895273&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 74: {'ID': 74,\n",
       "  'title': 'CVAE-GAN: Fine-Grained Image Generation through Asymmetric Training',\n",
       "  'authors': ['Jianmin Bao',\n",
       "   'Dong Chen',\n",
       "   'Houqiang Li',\n",
       "   'Fang Wen',\n",
       "   'Gang Hua'],\n",
       "  'published': '2017-03-29T17:49:48Z',\n",
       "  'updated': '2017-10-12T15:19:40Z',\n",
       "  'abstract': 'We present variational generative adversarial networks, a general learningframework that combines a variational auto-encoder with a generativeadversarial network, for synthesizing images in fine-grained categories, suchas faces of a specific person or objects in a category. Our approach models animage as a composition of label and latent attributes in a probabilistic model.By varying the fine-grained category label fed into the resulting generativemodel, we can generate images in a specific category with randomly drawn valueson a latent attribute vector. Our approach has two novel aspects. First, weadopt a cross entropy loss for the discriminative and classifier network, but amean discrepancy objective for the generative network. This kind of asymmetricloss function makes the GAN training more stable. Second, we adopt an encodernetwork to learn the relationship between the latent space and the real imagespace, and use pairwise feature matching to keep the structure of generatedimages. We experiment with natural images of faces, flowers, and birds, anddemonstrate that the proposed models are capable of generating realistic anddiverse samples with fine-grained category labels. We further show that ourmodels can be applied to other tasks, such as image inpainting,super-resolution, and data augmentation for training better face recognitionmodels.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 2745-2754',\n",
       "  'citations': '185',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1703.10155v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2268885274749575018&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 75: {'ID': 75,\n",
       "  'title': 'Learning to Deblur',\n",
       "  'authors': ['Michael Hirsch',\n",
       "   'Christian J. Schuler',\n",
       "   'Bernhard Schölkopf',\n",
       "   'Stefan Harmeling'],\n",
       "  'published': '2014-06-28T21:56:31Z',\n",
       "  'updated': '2014-06-28T21:56:31Z',\n",
       "  'abstract': 'We describe a learning-based approach to blind image deconvolution. It uses adeep layered architecture, parts of which are borrowed from recent work onneural network learning, and parts of which incorporate computations that arespecific to image deconvolution. The system is trained end-to-end on a set ofartificially generated training examples, enabling competitive performance inblind deconvolution, both with respect to quality and runtime.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 38 (7), 1439-1451',\n",
       "  'citations': '272',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1406.7444v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=15689694022022171806&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 76: {'ID': 76,\n",
       "  'title': 'Deep Semantic Ranking Based Hashing for Multi-Label Image Retrieval',\n",
       "  'authors': ['Liang Wang', 'Tieniu Tan', 'Fang Zhao', 'Yongzhen Huang'],\n",
       "  'published': '2015-01-26T07:33:40Z',\n",
       "  'updated': '2015-04-19T04:28:58Z',\n",
       "  'abstract': 'With the rapid growth of web images, hashing has received increasinginterests in large scale image retrieval. Research efforts have been devoted tolearning compact binary codes that preserve semantic similarity based onlabels. However, most of these hashing methods are designed to handle simplebinary similarity. The complex multilevel semantic structure of imagesassociated with multiple labels have not yet been well explored. Here wepropose a deep semantic ranking based method for learning hash functions thatpreserve multilevel semantic similarity between multi-label images. In ourapproach, deep convolutional neural network is incorporated into hash functionsto jointly learn feature representations and mappings from them to hash codes,which avoids the limitation of semantic representation power of hand-craftedfeatures. Meanwhile, a ranking list that encodes the multilevel similarityinformation is employed to guide the learning of such deep hash functions. Aneffective scheme based on surrogate loss is used to solve the intractableoptimization problem of nonsmooth and multivariate ranking measures involved inthe learning procedure. Experimental results show the superiority of ourproposed approach over several state-of-the-art hashing methods in term ofranking evaluation metrics when tested on multi-label image datasets.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '477',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1501.06272v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=11750530144158326644&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 77: {'ID': 77,\n",
       "  'title': 'Multiscale Combinatorial Grouping for Image Segmentation and Object  Proposal Generation',\n",
       "  'authors': ['Ferran Marques',\n",
       "   'Jonathan T. Barron',\n",
       "   'Pablo Arbelaez',\n",
       "   'Jitendra Malik',\n",
       "   'Jordi Pont-Tuset'],\n",
       "  'published': '2015-03-03T07:58:22Z',\n",
       "  'updated': '2016-03-01T09:00:09Z',\n",
       "  'abstract': 'We propose a unified approach for bottom-up hierarchical image segmentationand object proposal generation for recognition, called Multiscale CombinatorialGrouping (MCG). For this purpose, we first develop a fast normalized cutsalgorithm. We then propose a high-performance hierarchical segmenter that makeseffective use of multiscale information. Finally, we propose a groupingstrategy that combines our multiscale regions into highly-accurate objectproposals by exploring efficiently their combinatorial space. We also presentSingle-scale Combinatorial Grouping (SCG), a faster version of MCG thatproduces competitive proposals in under five second per image. We conduct anextensive and comprehensive empirical validation on the BSDS500, SegVOC12, SBD,and COCO datasets, showing that MCG produces state-of-the-art contours,hierarchical regions, and object proposals.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 39 (1), 128-140',\n",
       "  'citations': '332',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1503.00848v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14415692680426662888&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 78: {'ID': 78,\n",
       "  'title': 'VolumeDeform: Real-time Volumetric Non-rigid Reconstruction',\n",
       "  'authors': ['Marc Stamminger',\n",
       "   'Michael Zollhöfer',\n",
       "   'Matthias Innmann',\n",
       "   'Christian Theobalt',\n",
       "   'Matthias Nießner'],\n",
       "  'published': '2016-03-27T02:09:03Z',\n",
       "  'updated': '2016-07-30T06:07:24Z',\n",
       "  'abstract': \"We present a novel approach for the reconstruction of dynamic geometricshapes using a single hand-held consumer-grade RGB-D sensor at real-time rates.Our method does not require a pre-defined shape template to start with andbuilds up the scene model from scratch during the scanning process. Geometryand motion are parameterized in a unified manner by a volumetric representationthat encodes a distance field of the surface geometry as well as the non-rigidspace deformation. Motion tracking is based on a set of extracted sparse colorfeatures in combination with a dense depth-based constraint formulation. Thisenables accurate tracking and drastically reduces drift inherent to standardmodel-to-depth alignment. We cast finding the optimal deformation of space as anon-linear regularized variational optimization problem by enforcing localsmoothness and proximity to the input constraints. The problem is tackled inreal-time at the camera's capture rate using a data-parallel flip-flopoptimization strategy. Our results demonstrate robust tracking even for fastmotion and scenes that lack geometric features.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (8), 362-379',\n",
       "  'citations': '178',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1603.08161v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=13088888152296380135&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 79: {'ID': 79,\n",
       "  'title': 'Deep Learning of Local RGB-D Patches for 3D Object Detection and 6D Pose  Estimation',\n",
       "  'authors': ['Nassir Navab',\n",
       "   'Fausto Milletari',\n",
       "   'Federico Tombari',\n",
       "   'Wadim Kehl',\n",
       "   'Slobodan Ilic'],\n",
       "  'published': '2016-07-20T17:38:15Z',\n",
       "  'updated': '2016-07-20T17:38:15Z',\n",
       "  'abstract': 'We present a 3D object detection method that uses regressed descriptors oflocally-sampled RGB-D patches for 6D vote casting. For regression, we employ aconvolutional auto-encoder that has been trained on a large collection ofrandom local patches. During testing, scene patch descriptors are matchedagainst a database of synthetic model view patches and cast 6D object voteswhich are subsequently filtered to refined hypotheses. We evaluate on threedatasets to show that our method generalizes well to previously unseen inputdata, delivers robust detection results that compete with and surpass thestate-of-the-art while being scalable in the number of objects.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (3), 205-220',\n",
       "  'citations': '160',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1607.06038v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=9758854061155998455&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 80: {'ID': 80,\n",
       "  'title': 'DeMoN: Depth and Motion Network for Learning Monocular Stereo',\n",
       "  'authors': ['Benjamin Ummenhofer',\n",
       "   'Eddy Ilg',\n",
       "   'Jonas Uhrig',\n",
       "   'Nikolaus Mayer',\n",
       "   'Huizhong Zhou',\n",
       "   'Alexey Dosovitskiy',\n",
       "   'Thomas Brox'],\n",
       "  'published': '2016-12-07T20:26:53Z',\n",
       "  'updated': '2017-04-11T09:14:10Z',\n",
       "  'abstract': 'In this paper we formulate structure from motion as a learning problem. Wetrain a convolutional network end-to-end to compute depth and camera motionfrom successive, unconstrained image pairs. The architecture is composed ofmultiple stacked encoder-decoder networks, the core part being an iterativenetwork that is able to improve its own predictions. The network estimates notonly depth and motion, but additionally surface normals, optical flow betweenthe images and confidence of the matching. A crucial component of the approachis a training loss based on spatial relative differences. Compared totraditional two-frame structure from motion methods, results are more accurateand more robust. In contrast to the popular depth-from-single-image networks,DeMoN learns the concept of matching and, thus, better generalizes tostructures not seen during training.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '331',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1612.02401v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=17697059828222209959&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 81: {'ID': 81,\n",
       "  'title': 'Encoder-Decoder with Atrous Separable Convolution for Semantic Image  Segmentation',\n",
       "  'authors': ['Florian Schroff',\n",
       "   'George Papandreou',\n",
       "   'Hartwig Adam',\n",
       "   'Yukun Zhu',\n",
       "   'Liang-Chieh Chen'],\n",
       "  'published': '2018-02-07T19:37:11Z',\n",
       "  'updated': '2018-08-22T20:41:10Z',\n",
       "  'abstract': 'Spatial pyramid pooling module or encode-decoder structure are used in deepneural networks for semantic segmentation task. The former networks are able toencode multi-scale contextual information by probing the incoming features withfilters or pooling operations at multiple rates and multiple effectivefields-of-view, while the latter networks can capture sharper object boundariesby gradually recovering the spatial information. In this work, we propose tocombine the advantages from both methods. Specifically, our proposed model,DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder moduleto refine the segmentation results especially along object boundaries. Wefurther explore the Xception model and apply the depthwise separableconvolution to both Atrous Spatial Pyramid Pooling and decoder modules,resulting in a faster and stronger encoder-decoder network. We demonstrate theeffectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets,achieving the test set performance of 89.0\\\\% and 82.1\\\\% without anypost-processing. Our paper is accompanied with a publicly available referenceimplementation of the proposed models in Tensorflow at\\\\url{https://github.com/tensorflow/models/tree/master/research/deeplab}.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 801-818',\n",
       "  'citations': '1782',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1802.02611v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5370440400011509498&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 82: {'ID': 82,\n",
       "  'title': 'Generative Visual Manipulation on the Natural Image Manifold',\n",
       "  'authors': ['Alexei A. Efros',\n",
       "   'Eli Shechtman',\n",
       "   'Philipp Krähenbühl',\n",
       "   'Jun-Yan Zhu'],\n",
       "  'published': '2016-09-12T19:46:08Z',\n",
       "  'updated': '2018-12-16T22:00:59Z',\n",
       "  'abstract': 'Realistic image manipulation is challenging because it requires modifying theimage appearance in a user-controlled way, while preserving the realism of theresult. Unless the user has considerable artistic skill, it is easy to \"falloff\" the manifold of natural images while editing. In this paper, we propose tolearn the natural image manifold directly from data using a generativeadversarial neural network. We then define a class of image editing operations,and constrain their output to lie on that learned manifold at all times. Themodel automatically adjusts the output keeping all edits as realistic aspossible. All our manipulations are expressed in terms of constrainedoptimization and are applied in near-real time. We evaluate our algorithm onthe task of realistic photo manipulation of shape and color. The presentedmethod can further be used for changing one image to look like the other, aswell as generating novel imagery from scratch based on user\\'s scribbles.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (5), 597-613',\n",
       "  'citations': '684',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1609.03552v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=7355130399477157824&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 83: {'ID': 83,\n",
       "  'title': 'A Neural Approach to Blind Motion Deblurring',\n",
       "  'authors': ['Ayan Chakrabarti'],\n",
       "  'published': '2016-03-15T17:35:26Z',\n",
       "  'updated': '2016-08-01T18:53:00Z',\n",
       "  'abstract': 'We present a new method for blind motion deblurring that uses a neuralnetwork trained to compute estimates of sharp image patches from observationsthat are blurred by an unknown motion kernel. Instead of regressing directly topatch intensities, this network learns to predict the complex Fouriercoefficients of a deconvolution filter to be applied to the input patch forrestoration. For inference, we apply the network independently to alloverlapping patches in the observed image, and average its outputs to form aninitial estimate of the sharp image. We then explicitly estimate a singleglobal blur kernel by relating this estimate to the observed image, and finallyperform non-blind deconvolution with this kernel. Our method exhibits accuracyand robustness close to state-of-the-art iterative methods, while being muchfaster when parallelized on GPU hardware.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (3), 221-235',\n",
       "  'citations': '181',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1603.04771v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=1674040471379731158&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 84: {'ID': 84,\n",
       "  'title': 'Hollywood in Homes: Crowdsourcing Data Collection for Activity  Understanding',\n",
       "  'authors': ['Gunnar A. Sigurdsson',\n",
       "   'Xiaolong Wang',\n",
       "   'Abhinav Gupta',\n",
       "   'Ali Farhadi',\n",
       "   'Ivan Laptev',\n",
       "   'Gül Varol'],\n",
       "  'published': '2016-04-06T19:56:04Z',\n",
       "  'updated': '2016-07-26T22:49:22Z',\n",
       "  'abstract': 'Computer vision has a great potential to help our daily lives by searchingfor lost keys, watering flowers or reminding us to take a pill. To succeed withsuch tasks, computer vision methods need to be trained from real and diverseexamples of our daily dynamic scenes. While most of such scenes are notparticularly exciting, they typically do not appear on YouTube, in movies or TVbroadcasts. So how do we collect sufficiently many diverse but boring samplesrepresenting our lives? We propose a novel Hollywood in Homes approach tocollect such data. Instead of shooting videos in the lab, we ensure diversityby distributing and crowdsourcing the whole process of video creation fromscript writing to video recording and annotation. Following this procedure wecollect a new dataset, Charades, with hundreds of people recording videos intheir own homes, acting out casual everyday activities. The dataset is composedof 9,848 annotated videos with an average length of 30 seconds, showingactivities of 267 people from three continents. Each video is annotated bymultiple free-text descriptions, action labels, action intervals and classes ofinteracted objects. In total, Charades provides 27,847 video descriptions,66,500 temporally localized intervals for 157 action classes and 41,104 labelsfor 46 object classes. Using this rich data, we evaluate and provide baselineresults for several tasks including action recognition and automaticdescription generation. We believe that the realism, diversity, and casualnature of this dataset will present unique challenges and new opportunities forcomputer vision community.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (1), 510-526',\n",
       "  'citations': '329',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1604.01753v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=10565861317738901762&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 85: {'ID': 85,\n",
       "  'title': 'Efficient Object Localization Using Convolutional Networks',\n",
       "  'authors': ['Yann LeCun',\n",
       "   'Christopher Bregler',\n",
       "   'Jonathan Tompson',\n",
       "   'Arjun Jain',\n",
       "   'Ross Goroshin'],\n",
       "  'published': '2014-11-16T17:23:02Z',\n",
       "  'updated': '2015-06-09T12:29:21Z',\n",
       "  'abstract': \"Recent state-of-the-art performance on human-body pose estimation has beenachieved with Deep Convolutional Networks (ConvNets). Traditional ConvNetarchitectures include pooling and sub-sampling layers which reducecomputational requirements, introduce invariance and prevent over-training.These benefits of pooling come at the cost of reduced localization accuracy. Weintroduce a novel architecture which includes an efficient `positionrefinement' model that is trained to estimate the joint offset location withina small region of the image. This refinement model is jointly trained incascade with a state-of-the-art ConvNet model to achieve improved accuracy inhuman joint location estimation. We show that the variance of our detectorapproaches the variance of human annotations on the FLIC dataset andoutperforms all existing approaches on the MPII-human-pose dataset.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '668',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1411.4280v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5801105860480194411&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 86: {'ID': 86,\n",
       "  'title': 'WIDER FACE: A Face Detection Benchmark',\n",
       "  'authors': ['Xiaoou Tang', 'Ping Luo', 'Shuo Yang', 'Chen Change Loy'],\n",
       "  'published': '2015-11-20T08:33:57Z',\n",
       "  'updated': '2015-11-20T08:33:57Z',\n",
       "  'abstract': 'Face detection is one of the most studied topics in the computer visioncommunity. Much of the progresses have been made by the availability of facedetection benchmark datasets. We show that there is a gap between current facedetection performance and the real world requirements. To facilitate futureface detection research, we introduce the WIDER FACE dataset, which is 10 timeslarger than existing datasets. The dataset contains rich annotations, includingocclusions, poses, event categories, and face bounding boxes. Faces in theproposed dataset are extremely challenging due to large variations in scale,pose and occlusion, as shown in Fig. 1. Furthermore, we show that WIDER FACEdataset is an effective training source for face detection. We benchmarkseveral representative detection systems, providing an overview ofstate-of-the-art performance and propose a solution to deal with large scalevariation. Finally, we discuss common failure cases that worth to be furtherinvestigated. Dataset can be downloaded at:mmlab.ie.cuhk.edu.hk/projects/WIDERFace',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '582',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1511.06523v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=8866787776930184069&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 87: {'ID': 87,\n",
       "  'title': 'Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for  Visual Question Answering',\n",
       "  'authors': ['Kate Saenko', 'Huijuan Xu'],\n",
       "  'published': '2015-11-17T01:00:04Z',\n",
       "  'updated': '2016-03-19T03:06:58Z',\n",
       "  'abstract': 'We address the problem of Visual Question Answering (VQA), which requiresjoint image and language understanding to answer a question about a givenphotograph. Recent approaches have applied deep image captioning methods basedon convolutional-recurrent networks to this problem, but have failed to modelspatial inference. To remedy this, we propose a model we call the SpatialMemory Network and apply it to the VQA task. Memory networks are recurrentneural networks with an explicit attention mechanism that selects certain partsof the information stored in memory. Our Spatial Memory Network stores neuronactivations from different spatial regions of the image in its memory, and usesthe question to choose relevant regions for computing the answer, a process ofwhich constitutes a single \"hop\" in the network. We propose a novel spatialattention architecture that aligns words with image patches in the first hop,and obtain improved results by adding a second attention hop which considersthe whole question to choose visual evidence based on the results of the firsthop. To better understand the inference process learned by the network, wedesign synthetic questions that specifically require spatial inference andvisualize the attention weights. We evaluate our model on two published visualquestion answering datasets, DAQUAR [1] and VQA [2], and obtain improvedresults compared to a strong deep baseline model (iBOWIMG) which concatenatesimage and question features to predict the answer [3].',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.CL', 'cs.NE'],\n",
       "  'journal': 'ECCV (7), 451-466',\n",
       "  'citations': '508',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1511.05234v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=4891918962540678955&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 88: {'ID': 88,\n",
       "  'title': 'Hand Keypoint Detection in Single Images using Multiview Bootstrapping',\n",
       "  'authors': ['Yaser Sheikh', 'Tomas Simon', 'Hanbyul Joo', 'Iain Matthews'],\n",
       "  'published': '2017-04-25T17:37:48Z',\n",
       "  'updated': '2017-04-25T17:37:48Z',\n",
       "  'abstract': 'We present an approach that uses a multi-camera system to train fine-graineddetectors for keypoints that are prone to occlusion, such as the joints of ahand. We call this procedure multiview bootstrapping: first, an initialkeypoint detector is used to produce noisy labels in multiple views of thehand. The noisy detections are then triangulated in 3D using multiview geometryor marked as outliers. Finally, the reprojected triangulations are used as newlabeled training data to improve the detector. We repeat this process,generating more labeled data in each iteration. We derive a result analyticallyrelating the minimum number of views to achieve target true and false positiverates for a given detector. The method is used to train a hand keypointdetector for single images. The resulting keypoint detector runs in realtime onRGB images and has accuracy comparable to methods that use depth sensors. Thesingle view detector, triangulated over multiple views, enables 3D markerlesshand motion capture with complex object interactions.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '434',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1704.07809v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=841979590126160273&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 89: {'ID': 89,\n",
       "  'title': 'Learning Deep Representation for Face Alignment with Auxiliary  Attributes',\n",
       "  'authors': ['Xiaoou Tang', 'Zhanpeng Zhang', 'Ping Luo', 'Chen Change Loy'],\n",
       "  'published': '2014-08-18T10:34:29Z',\n",
       "  'updated': '2015-08-11T10:08:40Z',\n",
       "  'abstract': 'In this study, we show that landmark detection or face alignment task is nota single and independent problem. Instead, its robustness can be greatlyimproved with auxiliary information. Specifically, we jointly optimize landmarkdetection together with the recognition of heterogeneous but subtly correlatedfacial attributes, such as gender, expression, and appearance attributes. Thisis non-trivial since different attribute inference tasks have differentlearning difficulties and convergence rates. To address this problem, weformulate a novel tasks-constrained deep model, which not only learns theinter-task correlation but also employs dynamic task coefficients to facilitatethe optimization convergence when learning multiple complex tasks. Extensiveevaluations show that the proposed task-constrained learning (i) outperformsexisting face alignment methods, especially in dealing with faces with severeocclusion and pose variation, and (ii) reduces model complexity drasticallycompared to the state-of-the-art methods based on cascaded deep model.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 38 (5), 918-930',\n",
       "  'citations': '294',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1408.3967v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=15284991516002520932&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 90: {'ID': 90,\n",
       "  'title': 'Deep3D: Fully Automatic 2D-to-3D Video Conversion with Deep  Convolutional Neural Networks',\n",
       "  'authors': ['Ali Farhadi', 'Ross Girshick', 'Junyuan Xie'],\n",
       "  'published': '2016-04-13T04:35:07Z',\n",
       "  'updated': '2016-04-13T04:35:07Z',\n",
       "  'abstract': 'As 3D movie viewing becomes mainstream and Virtual Reality (VR) marketemerges, the demand for 3D contents is growing rapidly. Producing 3D videos,however, remains challenging. In this paper we propose to use deep neuralnetworks for automatically converting 2D videos and images to stereoscopic 3Dformat. In contrast to previous automatic 2D-to-3D conversion algorithms, whichhave separate stages and need ground truth depth map as supervision, ourapproach is trained end-to-end directly on stereo pairs extracted from 3Dmovies. This novel training scheme makes it possible to exploit orders ofmagnitude more data and significantly increases performance. Indeed, Deep3Doutperforms baselines in both quantitative and human subject evaluations.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (4), 842-857',\n",
       "  'citations': '224',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1604.03650v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=3527188941129179194&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 91: {'ID': 91,\n",
       "  'title': 'Learning to Compare Image Patches via Convolutional Neural Networks',\n",
       "  'authors': ['Nikos Komodakis', 'Sergey Zagoruyko'],\n",
       "  'published': '2015-04-14T17:53:51Z',\n",
       "  'updated': '2015-04-14T17:53:51Z',\n",
       "  'abstract': 'In this paper we show how to learn directly from image data (i.e., withoutresorting to manually-designed features) a general similarity function forcomparing image patches, which is a task of fundamental importance for manycomputer vision problems. To encode such a function, we opt for a CNN-basedmodel that is trained to account for a wide variety of changes in imageappearance. To that end, we explore and study multiple neural networkarchitectures, which are specifically adapted to this task. We show that suchan approach can significantly outperform the state-of-the-art on severalproblems and benchmark datasets.',\n",
       "  'categories': ['cs.CV', 'cs.LG', 'cs.NE'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '930',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1504.03641v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=12180108640713420621&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 92: {'ID': 92,\n",
       "  'title': 'An Uncertain Future: Forecasting from Static Images using Variational  Autoencoders',\n",
       "  'authors': ['Abhinav Gupta',\n",
       "   'Martial Hebert',\n",
       "   'Carl Doersch',\n",
       "   'Jacob Walker'],\n",
       "  'published': '2016-06-25T05:56:46Z',\n",
       "  'updated': '2016-06-25T05:56:46Z',\n",
       "  'abstract': 'In a given scene, humans can often easily predict a set of immediate futureevents that might happen. However, generalized pixel-level anticipation incomputer vision systems is difficult because machine learning struggles withthe ambiguity inherent in predicting the future. In this paper, we focus onpredicting the dense trajectory of pixels in a scene, specifically what willmove in the scene, where it will travel, and how it will deform over the courseof one second. We propose a conditional variational autoencoder as a solutionto this problem. In this framework, direct inference from the image shapes thedistribution of possible trajectories, while latent variables encode anynecessary information that is not available in the image. We show that ourmethod is able to successfully predict events in a wide variety of scenes andcan produce multiple different predictions when the future is ambiguous. Ouralgorithm is trained on thousands of diverse, realistic videos and requiresabsolutely no human labeling. In addition to non-semantic action prediction, wefind that our method learns a representation that is applicable to semanticvision tasks.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (7), 835-851',\n",
       "  'citations': '329',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1606.07873v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=15027433924867386035&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 93: {'ID': 93,\n",
       "  'title': 'Multi-Context Attention for Human Pose Estimation',\n",
       "  'authors': ['Wei Yang',\n",
       "   'Alan L. Yuille',\n",
       "   'Wanli Ouyang',\n",
       "   'Xiao Chu',\n",
       "   'Xiaogang Wang',\n",
       "   'Cheng Ma'],\n",
       "  'published': '2017-02-24T01:10:53Z',\n",
       "  'updated': '2017-02-24T01:10:53Z',\n",
       "  'abstract': 'In this paper, we propose to incorporate convolutional neural networks with amulti-context attention mechanism into an end-to-end framework for human poseestimation. We adopt stacked hourglass networks to generate attention maps fromfeatures at multiple resolutions with various semantics. The Conditional RandomField (CRF) is utilized to model the correlations among neighboring regions inthe attention map. We further combine the holistic attention model, whichfocuses on the global consistency of the full human body, and the body partattention model, which focuses on the detailed description for different bodyparts. Hence our model has the ability to focus on different granularity fromlocal salient regions to global semantic-consistent spaces. Additionally, wedesign novel Hourglass Residual Units (HRUs) to increase the receptive field ofthe network. These units are extensions of residual units with a side branchincorporating filters with larger receptive fields, hence features with variousscales are learned and combined within the HRUs. The effectiveness of theproposed multi-context attention mechanism and the hourglass residual units isevaluated on two widely used human pose estimation benchmarks. Our approachoutperforms all existing methods on both benchmarks over all the body parts.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '308',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1702.07432v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=8990777784067552312&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 94: {'ID': 94,\n",
       "  'title': 'Learning Deep Representations of Fine-grained Visual Descriptions',\n",
       "  'authors': ['Honglak Lee', 'Scott Reed', 'Bernt Schiele', 'Zeynep Akata'],\n",
       "  'published': '2016-05-17T23:08:46Z',\n",
       "  'updated': '2016-05-17T23:08:46Z',\n",
       "  'abstract': 'State-of-the-art methods for zero-shot visual recognition formulate learningas a joint embedding problem of images and side information. In theseformulations the current best complement to visual features are attributes:manually encoded vectors describing shared characteristics among categories.Despite good performance, attributes have limitations: (1) finer-grainedrecognition requires commensurately more attributes, and (2) attributes do notprovide a natural language interface. We propose to overcome these limitationsby training neural language models from scratch; i.e. without pre-training andonly consuming words and characters. Our proposed models train end-to-end toalign with the fine-grained and category-specific content of images. Naturallanguage provides a flexible and compact way of encoding only the salientvisual aspects for distinguishing categories. By training on raw text, ourmodel can do inference on raw text as well, providing humans a familiar modeboth for annotation and retrieval. Our model achieves strong performance onzero-shot text-based image retrieval and significantly outperforms theattribute-based state-of-the-art for zero-shot classification on the CaltechUCSD Birds 200-2011 dataset.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '386',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1605.05395v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=9792549731155959756&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 95: {'ID': 95,\n",
       "  'title': 'Deep Reconstruction-Classification Networks for Unsupervised Domain  Adaptation',\n",
       "  'authors': ['David Balduzzi',\n",
       "   'Mengjie Zhang',\n",
       "   'Wen Li',\n",
       "   'Muhammad Ghifary',\n",
       "   'W. Bastiaan Kleijn'],\n",
       "  'published': '2016-07-12T20:48:58Z',\n",
       "  'updated': '2016-08-01T09:58:13Z',\n",
       "  'abstract': \"In this paper, we propose a novel unsupervised domain adaptation algorithmbased on deep learning for visual object recognition. Specifically, we design anew model called Deep Reconstruction-Classification Network (DRCN), whichjointly learns a shared encoding representation for two tasks: i) supervisedclassification of labeled source data, and ii) unsupervised reconstruction ofunlabeled target data.In this way, the learnt representation not only preservesdiscriminability, but also encodes useful information from the target domain.Our new DRCN model can be optimized by using backpropagation similarly as thestandard neural networks.  We evaluate the performance of DRCN on a series of cross-domain objectrecognition tasks, where DRCN provides a considerable improvement (up to ~8% inaccuracy) over the prior state-of-the-art algorithms. Interestingly, we alsoobserve that the reconstruction pipeline of DRCN transforms images from thesource domain into images whose appearance resembles the target dataset. Thissuggests that DRCN's performance is due to constructing a single compositerepresentation that encodes information about both the structure of targetimages and the classification of source images. Finally, we provide a formalanalysis to justify the algorithm's objective in domain adaptation context.\",\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.LG', 'stat.ML'],\n",
       "  'journal': 'ECCV (4), 597-613',\n",
       "  'citations': '317',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1607.03516v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=9597055017342547307&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 96: {'ID': 96,\n",
       "  'title': 'Deeply-Recursive Convolutional Network for Image Super-Resolution',\n",
       "  'authors': ['Jiwon Kim', 'Kyoung Mu Lee', 'Jung Kwon Lee'],\n",
       "  'published': '2015-11-14T02:21:50Z',\n",
       "  'updated': '2016-11-11T08:40:53Z',\n",
       "  'abstract': 'We propose an image super-resolution method (SR) using a deeply-recursiveconvolutional network (DRCN). Our network has a very deep recursive layer (upto 16 recursions). Increasing recursion depth can improve performance withoutintroducing new parameters for additional convolutions. Albeit advantages,learning a DRCN is very hard with a standard gradient descent method due toexploding/vanishing gradients. To ease the difficulty of training, we proposetwo extensions: recursive-supervision and skip-connection. Our methodoutperforms previous methods by a large margin.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '1064',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1511.04491v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=60804554760851082&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 97: {'ID': 97,\n",
       "  'title': 'Is Faster R-CNN Doing Well for Pedestrian Detection?',\n",
       "  'authors': ['Liang Lin', 'Xiaodan Liang', 'Liliang Zhang', 'Kaiming He'],\n",
       "  'published': '2016-07-24T11:56:13Z',\n",
       "  'updated': '2016-07-27T03:35:19Z',\n",
       "  'abstract': 'Detecting pedestrian has been arguably addressed as a special topic beyondgeneral object detection. Although recent deep learning object detectors suchas Fast/Faster R-CNN [1, 2] have shown excellent performance for general objectdetection, they have limited success for detecting pedestrian, and previousleading pedestrian detectors were in general hybrid methods combininghand-crafted and deep convolutional features. In this paper, we investigateissues involving Faster R-CNN [2] for pedestrian detection. We discover thatthe Region Proposal Network (RPN) in Faster R-CNN indeed performs well as astand-alone pedestrian detector, but surprisingly, the downstream classifierdegrades the results. We argue that two reasons account for the unsatisfactoryaccuracy: (i) insufficient resolution of feature maps for handling smallinstances, and (ii) lack of any bootstrapping strategy for mining hard negativeexamples. Driven by these observations, we propose a very simple but effectivebaseline for pedestrian detection, using an RPN followed by boosted forests onshared, high-resolution convolutional feature maps. We comprehensively evaluatethis method on several benchmarks (Caltech, INRIA, ETH, and KITTI), presentingcompetitive accuracy and good speed. Code will be made publicly available.',\n",
       "  'categories': ['cs.CV', '68U01'],\n",
       "  'journal': 'ECCV (2), 443-457',\n",
       "  'citations': '518',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1607.07032v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=12199992810190996739&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 98: {'ID': 98,\n",
       "  'title': 'SegFlow: Joint Learning for Video Object Segmentation and Optical Flow',\n",
       "  'authors': ['Yi-Hsuan Tsai',\n",
       "   'Jingchun Cheng',\n",
       "   'Ming-Hsuan Yang',\n",
       "   'Shengjin Wang'],\n",
       "  'published': '2017-09-20T07:38:20Z',\n",
       "  'updated': '2017-09-20T07:38:20Z',\n",
       "  'abstract': 'This paper proposes an end-to-end trainable network, SegFlow, forsimultaneously predicting pixel-wise object segmentation and optical flow invideos. The proposed SegFlow has two branches where useful information ofobject segmentation and optical flow is propagated bidirectionally in a unifiedframework. The segmentation branch is based on a fully convolutional network,which has been proved effective in image segmentation task, and the opticalflow branch takes advantage of the FlowNet model. The unified framework istrained iteratively offline to learn a generic notion, and fine-tuned onlinefor specific objects. Extensive experiments on both the video objectsegmentation and optical flow datasets demonstrate that introducing opticalflow improves the performance of segmentation and vice versa, against thestate-of-the-art algorithms.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 686-695',\n",
       "  'citations': '185',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1709.06750v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=9665756598096257394&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 99: {'ID': 99,\n",
       "  'title': 'Learning Efficient Convolutional Networks through Network Slimming',\n",
       "  'authors': ['Jianguo Li',\n",
       "   'Zhiqiang Shen',\n",
       "   'Shoumeng Yan',\n",
       "   'Changshui Zhang',\n",
       "   'Zhuang Liu',\n",
       "   'Gao Huang'],\n",
       "  'published': '2017-08-22T07:35:26Z',\n",
       "  'updated': '2017-08-22T07:35:26Z',\n",
       "  'abstract': 'The deployment of deep convolutional neural networks (CNNs) in many realworld applications is largely hindered by their high computational cost. Inthis paper, we propose a novel learning scheme for CNNs to simultaneously 1)reduce the model size; 2) decrease the run-time memory footprint; and 3) lowerthe number of computing operations, without compromising accuracy. This isachieved by enforcing channel-level sparsity in the network in a simple buteffective way. Different from many existing approaches, the proposed methoddirectly applies to modern CNN architectures, introduces minimum overhead tothe training process, and requires no special software/hardware acceleratorsfor the resulting models. We call our approach network slimming, which takeswide and large networks as input models, but during training insignificantchannels are automatically identified and pruned afterwards, yielding thin andcompact models with comparable accuracy. We empirically demonstrate theeffectiveness of our approach with several state-of-the-art CNN models,including VGGNet, ResNet and DenseNet, on various image classificationdatasets. For VGGNet, a multi-pass version of network slimming gives a 20xreduction in model size and a 5x reduction in computing operations.',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.LG'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 2736-2744',\n",
       "  'citations': '519',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1708.06519v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6535994235718327488&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 100: {'ID': 100,\n",
       "  'title': 'Learning Spatially Regularized Correlation Filters for Visual Tracking',\n",
       "  'authors': ['Gustav Häger',\n",
       "   'Martin Danelljan',\n",
       "   'Fahad Shahbaz Khan',\n",
       "   'Michael Felsberg'],\n",
       "  'published': '2016-08-19T11:11:49Z',\n",
       "  'updated': '2016-08-19T11:11:49Z',\n",
       "  'abstract': 'Robust and accurate visual tracking is one of the most challenging computervision problems. Due to the inherent lack of training data, a robust approachfor constructing a target appearance model is crucial. Recently,discriminatively learned correlation filters (DCF) have been successfullyapplied to address this problem for tracking. These methods utilize a periodicassumption of the training samples to efficiently learn a classifier on allpatches in the target neighborhood. However, the periodic assumption alsointroduces unwanted boundary effects, which severely degrade the quality of thetracking model.  We propose Spatially Regularized Discriminative Correlation Filters (SRDCF)for tracking. A spatial regularization component is introduced in the learningto penalize correlation filter coefficients depending on their spatiallocation. Our SRDCF formulation allows the correlation filters to be learned ona significantly larger set of negative training samples, without corrupting thepositive samples. We further propose an optimization strategy, based on theiterative Gauss-Seidel method, for efficient online learning of our SRDCF.Experiments are performed on four benchmark datasets: OTB-2013, ALOV++,OTB-2015, and VOT2014. Our approach achieves state-of-the-art results on allfour datasets. On OTB-2013 and OTB-2015, we obtain an absolute gain of 8.0% and8.2% respectively, in mean overlap precision, compared to the best existingtrackers.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 4310-4318',\n",
       "  'citations': '1106',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1608.05571v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=16700478325580262692&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 101: {'ID': 101,\n",
       "  'title': 'Person Re-identification by Local Maximal Occurrence Representation and  Metric Learning',\n",
       "  'authors': ['Shengcai Liao', 'Stan Z. Li', 'Xiangyu Zhu', 'Yang Hu'],\n",
       "  'published': '2014-06-17T01:53:37Z',\n",
       "  'updated': '2015-05-06T14:01:28Z',\n",
       "  'abstract': \"Person re-identification is an important technique towards automatic searchof a person's presence in a surveillance video. Two fundamental problems arecritical for person re-identification, feature representation and metriclearning. An effective feature representation should be robust to illuminationand viewpoint changes, and a discriminant metric should be learned to matchvarious person images. In this paper, we propose an effective featurerepresentation called Local Maximal Occurrence (LOMO), and a subspace andmetric learning method called Cross-view Quadratic Discriminant Analysis(XQDA). The LOMO feature analyzes the horizontal occurrence of local features,and maximizes the occurrence to make a stable representation against viewpointchanges. Besides, to handle illumination variations, we apply the Retinextransform and a scale invariant texture operator. To learn a discriminantmetric, we propose to learn a discriminant low dimensional subspace bycross-view quadratic discriminant analysis, and simultaneously, a QDA metric islearned on the derived subspace. We also present a practical computation methodfor XQDA, as well as its regularization. Experiments on four challenging personre-identification databases, VIPeR, QMUL GRID, CUHK Campus, and CUHK03, showthat the proposed method improves the state-of-the-art rank-1 identificationrates by 2.2%, 4.88%, 28.91%, and 31.55% on the four databases, respectively.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '1418',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1406.4216v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14253377065401900395&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 102: {'ID': 102,\n",
       "  'title': 'Cross-dimensional Weighting for Aggregated Deep Convolutional Features',\n",
       "  'authors': ['Simon Osindero', 'Yannis Kalantidis', 'Clayton Mellina'],\n",
       "  'published': '2015-12-13T15:16:02Z',\n",
       "  'updated': '2016-07-30T02:14:18Z',\n",
       "  'abstract': 'We propose a simple and straightforward way of creating powerful imagerepresentations via cross-dimensional weighting and aggregation of deepconvolutional neural network layer outputs. We first present a generalizedframework that encompasses a broad family of approaches and includescross-dimensional pooling and weighting steps. We then propose specificnon-parametric schemes for both spatial- and channel-wise weighting that boostthe effect of highly active spatial responses and at the same time regulateburstiness effects. We experiment on different public datasets for image searchand show that our approach outperforms the current state-of-the-art forapproaches based on pre-trained networks. We also provide an easy-to-use, opensource implementation that reproduces our results.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV Workshops (1), 685-701',\n",
       "  'citations': '262',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1512.04065v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=16745533217620641151&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 103: {'ID': 103,\n",
       "  'title': 'Octree Generating Networks: Efficient Convolutional Architectures for  High-resolution 3D Outputs',\n",
       "  'authors': ['Alexey Dosovitskiy', 'Maxim Tatarchenko', 'Thomas Brox'],\n",
       "  'published': '2017-03-28T07:45:59Z',\n",
       "  'updated': '2017-08-07T18:07:33Z',\n",
       "  'abstract': 'We present a deep convolutional decoder architecture that can generatevolumetric 3D outputs in a compute- and memory-efficient manner by using anoctree representation. The network learns to predict both the structure of theoctree, and the occupancy values of individual cells. This makes it aparticularly valuable technique for generating 3D shapes. In contrast tostandard decoders acting on regular voxel grids, the architecture does not havecubic complexity. This allows representing much higher resolution outputs witha limited memory budget. We demonstrate this in several application domains,including 3D convolutional autoencoders, generation of objects and whole scenesfrom high-level representations, and shape from a single image.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 2088-2096',\n",
       "  'citations': '280',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1703.09438v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6908032390020517809&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 104: {'ID': 104,\n",
       "  'title': 'Learning without Forgetting',\n",
       "  'authors': ['Zhizhong Li', 'Derek Hoiem'],\n",
       "  'published': '2016-06-29T20:54:04Z',\n",
       "  'updated': '2017-02-14T22:32:30Z',\n",
       "  'abstract': 'When building a unified vision system or gradually adding new capabilities toa system, the usual assumption is that training data for all tasks is alwaysavailable. However, as the number of tasks grows, storing and retraining onsuch data becomes infeasible. A new problem arises where we add newcapabilities to a Convolutional Neural Network (CNN), but the training data forits existing capabilities are unavailable. We propose our Learning withoutForgetting method, which uses only new task data to train the network whilepreserving the original capabilities. Our method performs favorably compared tocommonly used feature extraction and fine-tuning adaption techniques andperforms similarly to multitask learning that uses original task data we assumeunavailable. A more surprising observation is that Learning without Forgettingmay be able to replace fine-tuning with similar old and new task datasets forimproved new task performance.',\n",
       "  'categories': ['cs.CV', 'cs.LG', 'stat.ML'],\n",
       "  'journal': 'ECCV (4), 614-629',\n",
       "  'citations': '649',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1606.09282v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=11736167988707774176&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 105: {'ID': 105,\n",
       "  'title': 'Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?',\n",
       "  'authors': ['Kensho Hara', 'Yutaka Satoh', 'Hirokatsu Kataoka'],\n",
       "  'published': '2017-11-27T08:29:06Z',\n",
       "  'updated': '2018-04-02T01:49:37Z',\n",
       "  'abstract': 'The purpose of this study is to determine whether current video datasets havesufficient data for training very deep convolutional neural networks (CNNs)with spatio-temporal three-dimensional (3D) kernels. Recently, the performancelevels of 3D CNNs in the field of action recognition have improvedsignificantly. However, to date, conventional research has only exploredrelatively shallow 3D architectures. We examine the architectures of various 3DCNNs from relatively shallow to very deep ones on current video datasets. Basedon the results of those experiments, the following conclusions could beobtained: (i) ResNet-18 training resulted in significant overfitting forUCF-101, HMDB-51, and ActivityNet but not for Kinetics. (ii) The Kineticsdataset has sufficient data for training of deep 3D CNNs, and enables trainingof up to 152 ResNets layers, interestingly similar to 2D ResNets on ImageNet.ResNeXt-101 achieved 78.4% average accuracy on the Kinetics test set. (iii)Kinetics pretrained simple 3D architectures outperforms complex 2Darchitectures, and the pretrained ResNeXt-101 achieved 94.5% and 70.2% onUCF-101 and HMDB-51, respectively. The use of 2D CNNs trained on ImageNet hasproduced significant progress in various tasks in image. We believe that usingdeep 3D CNNs together with Kinetics will retrace the successful history of 2DCNNs and ImageNet, and stimulate advances in computer vision for videos. Thecodes and pretrained models used in this study are publicly available.https://github.com/kenshohara/3D-ResNets-PyTorch',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '428',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1711.09577v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=4579944187863414163&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 106: {'ID': 106,\n",
       "  'title': 'Scaling Egocentric Vision: The EPIC-KITCHENS Dataset',\n",
       "  'authors': ['Dima Damen',\n",
       "   'Davide Moltisanti',\n",
       "   'Michael Wray',\n",
       "   'Hazel Doughty',\n",
       "   'Sanja Fidler',\n",
       "   'Toby Perrett',\n",
       "   'Antonino Furnari',\n",
       "   'Will Price',\n",
       "   'Jonathan Munro',\n",
       "   'Giovanni Maria Farinella',\n",
       "   'Evangelos Kazakos'],\n",
       "  'published': '2018-04-08T20:07:13Z',\n",
       "  'updated': '2018-07-31T09:05:07Z',\n",
       "  'abstract': \"First-person vision is gaining interest as it offers a unique viewpoint onpeople's interaction with objects, their attention, and even intention.However, progress in this challenging domain has been relatively slow due tothe lack of sufficiently large datasets. In this paper, we introduceEPIC-KITCHENS, a large-scale egocentric video benchmark recorded by 32participants in their native kitchen environments. Our videos depictnonscripted daily activities: we simply asked each participant to startrecording every time they entered their kitchen. Recording took place in 4cities (in North America and Europe) by participants belonging to 10 differentnationalities, resulting in highly diverse cooking styles. Our dataset features55 hours of video consisting of 11.5M frames, which we densely labeled for atotal of 39.6K action segments and 454.3K object bounding boxes. Our annotationis unique in that we had the participants narrate their own videos (afterrecording), thus reflecting true intention, and we crowd-sourced ground-truthsbased on these. We describe our object, action and anticipation challenges, andevaluate several baselines over two test splits, seen and unseen kitchens.Dataset and Project page: http://epic-kitchens.github.io\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 720-736',\n",
       "  'citations': '186',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1804.02748v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6002787991695147105&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 107: {'ID': 107,\n",
       "  'title': 'Densely Connected Convolutional Networks',\n",
       "  'authors': ['Kilian Q. Weinberger',\n",
       "   'Laurens van der Maaten',\n",
       "   'Gao Huang',\n",
       "   'Zhuang Liu'],\n",
       "  'published': '2016-08-25T00:44:55Z',\n",
       "  'updated': '2018-01-28T17:12:02Z',\n",
       "  'abstract': 'Recent work has shown that convolutional networks can be substantiallydeeper, more accurate, and efficient to train if they contain shorterconnections between layers close to the input and those close to the output. Inthis paper, we embrace this observation and introduce the Dense ConvolutionalNetwork (DenseNet), which connects each layer to every other layer in afeed-forward fashion. Whereas traditional convolutional networks with L layershave L connections - one between each layer and its subsequent layer - ournetwork has L(L+1)/2 direct connections. For each layer, the feature-maps ofall preceding layers are used as inputs, and its own feature-maps are used asinputs into all subsequent layers. DenseNets have several compellingadvantages: they alleviate the vanishing-gradient problem, strengthen featurepropagation, encourage feature reuse, and substantially reduce the number ofparameters. We evaluate our proposed architecture on four highly competitiveobject recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet).DenseNets obtain significant improvements over the state-of-the-art on most ofthem, whilst requiring less computation to achieve high performance. Code andpre-trained models are available at https://github.com/liuzhuang13/DenseNet .',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '9733',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1608.06993v5',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=4205512852566836101&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 108: {'ID': 108,\n",
       "  'title': 'Webly Supervised Learning of Convolutional Networks',\n",
       "  'authors': ['Abhinav Gupta', 'Xinlei Chen'],\n",
       "  'published': '2015-05-07T00:56:15Z',\n",
       "  'updated': '2015-10-07T21:53:16Z',\n",
       "  'abstract': 'We present an approach to utilize large amounts of web data for learningCNNs. Specifically inspired by curriculum learning, we present a two-stepapproach for CNN training. First, we use easy images to train an initial visualrepresentation. We then use this initial CNN and adapt it to harder, morerealistic images by leveraging the structure of data and categories. Wedemonstrate that our two-stage CNN outperforms a fine-tuned CNN trained onImageNet on Pascal VOC 2012. We also demonstrate the strength of weblysupervised learning by localizing objects in web images and training a R-CNNstyle detector. It achieves the best performance on VOC 2007 where no VOCtraining data is used. Finally, we show our approach is quite robust to noiseand performs comparably even when we use image search results from March 2013(pre-CNN image search era).',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 1431-1439',\n",
       "  'citations': '194',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1505.01554v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=3840358374055397013&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 109: {'ID': 109,\n",
       "  'title': 'Dual Attention Network for Scene Segmentation',\n",
       "  'authors': ['Jing Liu',\n",
       "   'Jun Fu',\n",
       "   'Yongjun Bao',\n",
       "   'Haijie Tian',\n",
       "   'Hanqing Lu',\n",
       "   'Yong Li',\n",
       "   'Zhiwei Fang'],\n",
       "  'published': '2018-09-09T14:48:22Z',\n",
       "  'updated': '2019-04-21T08:10:54Z',\n",
       "  'abstract': 'In this paper, we address the scene segmentation task by capturing richcontextual dependencies based on the selfattention mechanism. Unlike previousworks that capture contexts by multi-scale features fusion, we propose a DualAttention Networks (DANet) to adaptively integrate local features with theirglobal dependencies. Specifically, we append two types of attention modules ontop of traditional dilated FCN, which model the semantic interdependencies inspatial and channel dimensions respectively. The position attention moduleselectively aggregates the features at each position by a weighted sum of thefeatures at all positions. Similar features would be related to each otherregardless of their distances. Meanwhile, the channel attention moduleselectively emphasizes interdependent channel maps by integrating associatedfeatures among all channel maps. We sum the outputs of the two attentionmodules to further improve feature representation which contributes to moreprecise segmentation results. We achieve new state-of-the-art segmentationperformance on three challenging scene segmentation datasets, i.e., Cityscapes,PASCAL Context and COCO Stuff dataset. In particular, a Mean IoU score of 81.5%on Cityscapes test set is achieved without using coarse data. We make the codeand trained model publicly available at https://github.com/junfu1115/DANet',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '372',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1809.02983v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=7464044873387137971&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 110: {'ID': 110,\n",
       "  'title': 'Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks',\n",
       "  'authors': ['Zhaofan Qiu', 'Ting Yao', 'Tao Mei'],\n",
       "  'published': '2017-11-28T14:24:18Z',\n",
       "  'updated': '2017-11-28T14:24:18Z',\n",
       "  'abstract': 'Convolutional Neural Networks (CNN) have been regarded as a powerful class ofmodels for image recognition problems. Nevertheless, it is not trivial whenutilizing a CNN for learning spatio-temporal video representation. A fewstudies have shown that performing 3D convolutions is a rewarding approach tocapture both spatial and temporal dimensions in videos. However, thedevelopment of a very deep 3D CNN from scratch results in expensivecomputational cost and memory demand. A valid question is why not recycleoff-the-shelf 2D networks for a 3D CNN. In this paper, we devise multiplevariants of bottleneck building blocks in a residual learning framework bysimulating $3\\\\times3\\\\times3$ convolutions with $1\\\\times3\\\\times3$ convolutionalfilters on spatial domain (equivalent to 2D CNN) plus $3\\\\times1\\\\times1$convolutions to construct temporal connections on adjacent feature maps intime. Furthermore, we propose a new architecture, named Pseudo-3D Residual Net(P3D ResNet), that exploits all the variants of blocks but composes each indifferent placement of ResNet, following the philosophy that enhancingstructural diversity with going deep could improve the power of neuralnetworks. Our P3D ResNet achieves clear improvements on Sports-1M videoclassification dataset against 3D CNN and frame-based 2D CNN by 5.3% and 1.8%,respectively. We further examine the generalization performance of videorepresentation produced by our pre-trained P3D ResNet on five differentbenchmarks and three different tasks, demonstrating superior performances overseveral state-of-the-art techniques.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 5533-5541',\n",
       "  'citations': '444',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1711.10305v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2842480107619837805&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 111: {'ID': 111,\n",
       "  'title': 'Xception: Deep Learning with Depthwise Separable Convolutions',\n",
       "  'authors': ['François Chollet'],\n",
       "  'published': '2016-10-07T17:51:51Z',\n",
       "  'updated': '2017-04-04T18:40:27Z',\n",
       "  'abstract': 'We present an interpretation of Inception modules in convolutional neuralnetworks as being an intermediate step in-between regular convolution and thedepthwise separable convolution operation (a depthwise convolution followed bya pointwise convolution). In this light, a depthwise separable convolution canbe understood as an Inception module with a maximally large number of towers.This observation leads us to propose a novel deep convolutional neural networkarchitecture inspired by Inception, where Inception modules have been replacedwith depthwise separable convolutions. We show that this architecture, dubbedXception, slightly outperforms Inception V3 on the ImageNet dataset (whichInception V3 was designed for), and significantly outperforms Inception V3 on alarger image classification dataset comprising 350 million images and 17,000classes. Since the Xception architecture has the same number of parameters asInception V3, the performance gains are not due to increased capacity butrather to a more efficient use of model parameters.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '2746',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1610.02357v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=3110565860331647079&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 112: {'ID': 112,\n",
       "  'title': 'Sequence to Sequence -- Video to Text',\n",
       "  'authors': ['Subhashini Venugopalan',\n",
       "   'Raymond Mooney',\n",
       "   'Jeff Donahue',\n",
       "   'Kate Saenko',\n",
       "   'Marcus Rohrbach',\n",
       "   'Trevor Darrell'],\n",
       "  'published': '2015-05-03T22:32:00Z',\n",
       "  'updated': '2015-10-19T18:01:06Z',\n",
       "  'abstract': 'Real-world videos often have complex dynamics; and methods for generatingopen-domain video descriptions should be sensitive to temporal structure andallow both input (sequence of frames) and output (sequence of words) ofvariable length. To approach this problem, we propose a novel end-to-endsequence-to-sequence model to generate captions for videos. For this we exploitrecurrent neural networks, specifically LSTMs, which have demonstratedstate-of-the-art performance in image caption generation. Our LSTM model istrained on video-sentence pairs and learns to associate a sequence of videoframes to a sequence of words in order to generate a description of the eventin the video clip. Our model naturally is able to learn the temporal structureof the sequence of frames as well as the sequence model of the generatedsentences, i.e. a language model. We evaluate several variants of our modelthat exploit different visual features on a standard set of YouTube videos andtwo movie description datasets (M-VAD and MPII-MD).',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 4534-4542',\n",
       "  'citations': '856',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1505.00487v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2341786742021115670&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 113: {'ID': 113,\n",
       "  'title': 'NetVLAD: CNN architecture for weakly supervised place recognition',\n",
       "  'authors': ['Relja Arandjelović',\n",
       "   'Josef Sivic',\n",
       "   'Akihiko Torii',\n",
       "   'Tomas Pajdla',\n",
       "   'Petr Gronat'],\n",
       "  'published': '2015-11-23T14:51:51Z',\n",
       "  'updated': '2016-05-02T15:42:41Z',\n",
       "  'abstract': 'We tackle the problem of large scale visual place recognition, where the taskis to quickly and accurately recognize the location of a given queryphotograph. We present the following three principal contributions. First, wedevelop a convolutional neural network (CNN) architecture that is trainable inan end-to-end manner directly for the place recognition task. The maincomponent of this architecture, NetVLAD, is a new generalized VLAD layer,inspired by the \"Vector of Locally Aggregated Descriptors\" image representationcommonly used in image retrieval. The layer is readily pluggable into any CNNarchitecture and amenable to training via backpropagation. Second, we develop atraining procedure, based on a new weakly supervised ranking loss, to learnparameters of the architecture in an end-to-end manner from images depictingthe same places over time downloaded from Google Street View Time Machine.Finally, we show that the proposed architecture significantly outperformsnon-learnt image representations and off-the-shelf CNN descriptors on twochallenging place recognition benchmarks, and improves over currentstate-of-the-art compact image representations on standard image retrievalbenchmarks.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '945',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1511.07247v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6182379016997971259&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 114: {'ID': 114,\n",
       "  'title': 'RMPE: Regional Multi-person Pose Estimation',\n",
       "  'authors': ['Shuqin Xie', 'Yu-Wing Tai', 'Hao-Shu Fang', 'Cewu Lu'],\n",
       "  'published': '2016-12-01T04:36:52Z',\n",
       "  'updated': '2018-02-04T04:27:56Z',\n",
       "  'abstract': 'Multi-person pose estimation in the wild is challenging. Althoughstate-of-the-art human detectors have demonstrated good performance, smallerrors in localization and recognition are inevitable. These errors can causefailures for a single-person pose estimator (SPPE), especially for methods thatsolely depend on human detection results. In this paper, we propose a novelregional multi-person pose estimation (RMPE) framework to facilitate poseestimation in the presence of inaccurate human bounding boxes. Our frameworkconsists of three components: Symmetric Spatial Transformer Network (SSTN),Parametric Pose Non-Maximum-Suppression (NMS), and Pose-Guided ProposalsGenerator (PGPG). Our method is able to handle inaccurate bounding boxes andredundant detections, allowing it to achieve a 17% increase in mAP over thestate-of-the-art methods on the MPII (multi person) dataset.Our model andsource codes are publicly available.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 2334-2343',\n",
       "  'citations': '329',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1612.00137v5',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=1149163492102972816&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 115: {'ID': 115,\n",
       "  'title': 'Inside-Outside Net: Detecting Objects in Context with Skip Pooling and  Recurrent Neural Networks',\n",
       "  'authors': ['Kavita Bala',\n",
       "   'Sean Bell',\n",
       "   'Ross Girshick',\n",
       "   'C. Lawrence Zitnick'],\n",
       "  'published': '2015-12-14T00:37:31Z',\n",
       "  'updated': '2015-12-14T00:37:31Z',\n",
       "  'abstract': 'It is well known that contextual and multi-scale representations areimportant for accurate visual recognition. In this paper we present theInside-Outside Net (ION), an object detector that exploits information bothinside and outside the region of interest. Contextual information outside theregion of interest is integrated using spatial recurrent neural networks.Inside, we use skip pooling to extract information at multiple scales andlevels of abstraction. Through extensive experiments we evaluate the designspace and provide readers with an overview of what tricks of the trade areimportant. ION improves state-of-the-art on PASCAL VOC 2012 object detectionfrom 73.9% to 76.4% mAP. On the new and more challenging MS COCO dataset, weimprove state-of-art-the from 19.7% to 33.1% mAP. In the 2015 MS COCO DetectionChallenge, our ION model won the Best Student Entry and finished 3rd placeoverall. As intuition suggests, our detection results provide strong evidencethat context and multi-scale representations improve small object detection.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '723',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1512.04143v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=17824096759436033262&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 116: {'ID': 116,\n",
       "  'title': 'Interpretable Explanations of Black Boxes by Meaningful Perturbation',\n",
       "  'authors': ['Andrea Vedaldi', 'Ruth Fong'],\n",
       "  'published': '2017-04-11T14:15:20Z',\n",
       "  'updated': '2018-01-10T16:03:33Z',\n",
       "  'abstract': 'As machine learning algorithms are increasingly applied to high impact yethigh risk tasks, such as medical diagnosis or autonomous driving, it iscritical that researchers can explain how such algorithms arrived at theirpredictions. In recent years, a number of image saliency methods have beendeveloped to summarize where highly complex neural networks \"look\" in an imagefor evidence for their predictions. However, these techniques are limited bytheir heuristic nature and architectural constraints. In this paper, we maketwo main contributions: First, we propose a general framework for learningdifferent kinds of explanations for any black box algorithm. Second, wespecialise the framework to find the part of an image most responsible for aclassifier decision. Unlike previous works, our method is model-agnostic andtestable because it is grounded in explicit and interpretable imageperturbations.',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.LG', 'stat.ML'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 3429-3437',\n",
       "  'citations': '394',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1704.03296v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=13770569670221791603&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 117: {'ID': 117,\n",
       "  'title': 'Curriculum Domain Adaptation for Semantic Segmentation of Urban Scenes',\n",
       "  'authors': ['Boqing Gong', 'Philip David', 'Yang Zhang'],\n",
       "  'published': '2017-07-29T05:47:43Z',\n",
       "  'updated': '2018-11-14T01:03:47Z',\n",
       "  'abstract': \"During the last half decade, convolutional neural networks (CNNs) havetriumphed over semantic segmentation, which is one of the core tasks in manyapplications such as autonomous driving. However, to train CNNs requires aconsiderable amount of data, which is difficult to collect and laborious toannotate. Recent advances in computer graphics make it possible to train CNNson photo-realistic synthetic imagery with computer-generated annotations.Despite this, the domain mismatch between the real images and the syntheticdata cripples the models' performance. Hence, we propose a curriculum-stylelearning approach to minimize the domain gap in urban scenery semanticsegmentation. The curriculum domain adaptation solves easy tasks first to infernecessary properties about the target domain; in particular, the first task isto learn global label distributions over images and local distributions overlandmark superpixels. These are easy to estimate because images of urban sceneshave strong idiosyncrasies (e.g., the size and spatial relations of buildings,streets, cars, etc.). We then train a segmentation network while regularizingits predictions in the target domain to follow those inferred properties. Inexperiments, our method outperforms the baselines on two datasets and twobackbone networks. We also report extensive ablation studies about ourapproach.\",\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 2020-2030',\n",
       "  'citations': '191',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1707.09465v5',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=17741487139917312877&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 118: {'ID': 118,\n",
       "  'title': 'Learning Complexity-Aware Cascades for Deep Pedestrian Detection',\n",
       "  'authors': ['Mohammad Saberian', 'Zhaowei Cai', 'Nuno Vasconcelos'],\n",
       "  'published': '2015-07-19T22:31:01Z',\n",
       "  'updated': '2015-07-19T22:31:01Z',\n",
       "  'abstract': 'The design of complexity-aware cascaded detectors, combining features of verydifferent complexities, is considered. A new cascade design procedure isintroduced, by formulating cascade learning as the Lagrangian optimization of arisk that accounts for both accuracy and complexity. A boosting algorithm,denoted as complexity aware cascade training (CompACT), is then derived tosolve this optimization. CompACT cascades are shown to seek an optimaltrade-off between accuracy and complexity by pushing features of highercomplexity to the later cascade stages, where only a few difficult candidatepatches remain to be classified. This enables the use of features of vastlydifferent complexities in a single detector. In result, the feature pool can beexpanded to features previously impractical for cascade design, such as theresponses of a deep convolutional neural network (CNN). This is demonstratedthrough the design of a pedestrian detector with a pool of features whosecomplexities span orders of magnitude. The resulting cascade generalizes thecombination of a CNN with an object proposal mechanism: rather than apre-processing stage, CompACT cascades seamlessly integrate CNNs in theirstages. This enables state of the art performance on the Caltech and KITTIdatasets, at fairly fast speeds.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 3361-3369',\n",
       "  'citations': '258',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1507.05348v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=11752335122783750672&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 119: {'ID': 119,\n",
       "  'title': 'Neural Module Networks',\n",
       "  'authors': ['Marcus Rohrbach',\n",
       "   'Jacob Andreas',\n",
       "   'Dan Klein',\n",
       "   'Trevor Darrell'],\n",
       "  'published': '2015-11-09T18:48:39Z',\n",
       "  'updated': '2017-07-24T17:15:06Z',\n",
       "  'abstract': 'Visual question answering is fundamentally compositional in nature---aquestion like \"where is the dog?\" shares substructure with questions like \"whatcolor is the dog?\" and \"where is the cat?\" This paper seeks to simultaneouslyexploit the representational capacity of deep networks and the compositionallinguistic structure of questions. We describe a procedure for constructing andlearning *neural module networks*, which compose collections of jointly-trainedneural \"modules\" into deep networks for question answering. Our approachdecomposes questions into their linguistic substructures, and uses thesestructures to dynamically instantiate modular networks (with reusablecomponents for recognizing dogs, classifying colors, etc.). The resultingcompound networks are jointly trained. We evaluate our approach on twochallenging datasets for visual question answering, achieving state-of-the-artresults on both the VQA natural image dataset and a new dataset of complexquestions about abstract shapes.',\n",
       "  'categories': ['cs.CV', 'cs.CL', 'cs.LG', 'cs.NE'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '570',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1511.02799v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=8968885959793783275&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 120: {'ID': 120,\n",
       "  'title': 'One Network to Solve Them All --- Solving Linear Inverse Problems using  Deep Projection Models',\n",
       "  'authors': ['Chun-Liang Li',\n",
       "   'J. H. Rick Chang',\n",
       "   'B. V. K. Vijaya Kumar',\n",
       "   'Barnabas Poczos',\n",
       "   'Aswin C. Sankaranarayanan'],\n",
       "  'published': '2017-03-29T07:20:10Z',\n",
       "  'updated': '2017-03-29T07:20:10Z',\n",
       "  'abstract': 'While deep learning methods have achieved state-of-the-art performance inmany challenging inverse problems like image inpainting and super-resolution,they invariably involve problem-specific training of the networks. Under thisapproach, different problems require different networks. In scenarios where weneed to solve a wide variety of problems, e.g., on a mobile camera, it isinefficient and costly to use these specially-trained networks. On the otherhand, traditional methods using signal priors can be used in all linear inverseproblems but often have worse performance on challenging tasks. In this work,we provide a middle ground between the two kinds of methods --- we propose ageneral framework to train a single deep neural network that solves arbitrarylinear inverse problems. The proposed network acts as a proximal operator foran optimization algorithm and projects non-image signals onto the set ofnatural images defined by the decision boundary of a classifier. In ourexperiments, the proposed framework demonstrates superior performance overtraditional methods using a wavelet sparsity prior and achieves comparableperformance of specially-trained networks on tasks including compressivesensing and pixel-wise inpainting.',\n",
       "  'categories': ['cs.CV', 'I.4.5'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 5888-5897',\n",
       "  'citations': '177',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1703.09912v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=9525664324429931297&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 121: {'ID': 121,\n",
       "  'title': 'Text Flow: A Unified Text Detection System in Natural Scene Images',\n",
       "  'authors': ['Chang Huang',\n",
       "   'Kai Yu',\n",
       "   'Shijian Lu',\n",
       "   'Chew Lim Tan',\n",
       "   'Shangxuan Tian',\n",
       "   'Yifeng Pan'],\n",
       "  'published': '2016-04-23T08:11:17Z',\n",
       "  'updated': '2016-04-23T08:11:17Z',\n",
       "  'abstract': 'The prevalent scene text detection approach follows four sequential stepscomprising character candidate detection, false character candidate removal,text line extraction, and text line verification. However, errors occur andaccumulate throughout each of these sequential steps which often lead to lowdetection performance. To address these issues, we propose a unified scene textdetection system, namely Text Flow, by utilizing the minimum cost (min-cost)flow network model. With character candidates detected by cascade boosting, themin-cost flow network model integrates the last three sequential steps into asingle process which solves the error accumulation problem at both characterlevel and text line level effectively. The proposed technique has been testedon three public datasets, i.e, ICDAR2011 dataset, ICDAR2013 dataset and amultilingual dataset and it outperforms the state-of-the-art methods on allthree datasets with much higher recall and F-score. The good performance on themultilingual dataset shows that the proposed technique can be used for thedetection of texts in different languages.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 4651-4659',\n",
       "  'citations': '192',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1604.06877v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=10949109774637130903&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 122: {'ID': 122,\n",
       "  'title': 'Optimal Transport for Domain Adaptation',\n",
       "  'authors': ['Rémi Flamary',\n",
       "   'Alain Rakotomamonjy',\n",
       "   'Nicolas Courty',\n",
       "   'Devis Tuia'],\n",
       "  'published': '2015-07-02T10:15:11Z',\n",
       "  'updated': '2016-06-22T13:24:01Z',\n",
       "  'abstract': 'Domain adaptation from one data space (or domain) to another is one of themost challenging tasks of modern data analytics. If the adaptation is donecorrectly, models built on a specific data space become more robust whenconfronted to data depicting the same semantic concepts (the classes), butobserved by another observation system with its own specificities. Among themany strategies proposed to adapt a domain to another, finding a commonrepresentation has shown excellent properties: by finding a commonrepresentation for both domains, a single classifier can be effective in bothand use labelled samples from the source domain to predict the unlabelledsamples of the target domain. In this paper, we propose a regularizedunsupervised optimal transportation model to perform the alignment of therepresentations in the source and target domains. We learn a transportationplan matching both PDFs, which constrains labelled samples in the source domainto remain close during transport. This way, we exploit at the same time the fewlabeled information in the source and the unlabelled distributions observed inboth domains. Experiments in toy and challenging real visual adaptationexamples show the interest of the method, that consistently outperforms stateof the art approaches.',\n",
       "  'categories': ['cs.LG'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 39 (9), 1853-1865',\n",
       "  'citations': '367',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1507.00504v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=3838459407382849555&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 123: {'ID': 123,\n",
       "  'title': 'Face Alignment Across Large Poses: A 3D Solution',\n",
       "  'authors': ['Xiangyu Zhu',\n",
       "   'Xiaoming Liu',\n",
       "   'Stan Z. Li',\n",
       "   'Hailin Shi',\n",
       "   'Zhen Lei'],\n",
       "  'published': '2015-11-23T13:23:19Z',\n",
       "  'updated': '2015-11-23T13:23:19Z',\n",
       "  'abstract': 'Face alignment, which fits a face model to an image and extracts the semanticmeanings of facial pixels, has been an important topic in CV community.However, most algorithms are designed for faces in small to medium poses (below45 degree), lacking the ability to align faces in large poses up to 90 degree.The challenges are three-fold: Firstly, the commonly used landmark-based facemodel assumes that all the landmarks are visible and is therefore not suitablefor profile views. Secondly, the face appearance varies more dramaticallyacross large poses, ranging from frontal view to profile view. Thirdly,labelling landmarks in large poses is extremely challenging since the invisiblelandmarks have to be guessed. In this paper, we propose a solution to the threeproblems in an new alignment framework, called 3D Dense Face Alignment (3DDFA),in which a dense 3D face model is fitted to the image via convolutional neutralnetwork (CNN). We also propose a method to synthesize large-scale trainingsamples in profile views to solve the third problem of data labelling.Experiments on the challenging AFLW database show that our approach achievessignificant improvements over state-of-the-art methods.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '514',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1511.07212v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14925470721697460715&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 124: {'ID': 124,\n",
       "  'title': 'Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in  Video Classification',\n",
       "  'authors': ['Zhuowen Tu',\n",
       "   'Kevin Murphy',\n",
       "   'Chen Sun',\n",
       "   'Jonathan Huang',\n",
       "   'Saining Xie'],\n",
       "  'published': '2017-12-13T16:40:55Z',\n",
       "  'updated': '2018-07-27T03:20:56Z',\n",
       "  'abstract': 'Despite the steady progress in video analysis led by the adoption ofconvolutional neural networks (CNNs), the relative improvement has been lessdrastic as that in 2D static image classification. Three main challenges existincluding spatial (image) feature representation, temporal informationrepresentation, and model/computation complexity. It was recently shown byCarreira and Zisserman that 3D CNNs, inflated from 2D networks and pretrainedon ImageNet, could be a promising way for spatial and temporal representationlearning. However, as for model/computation complexity, 3D CNNs are much moreexpensive than 2D CNNs and prone to overfit. We seek a balance between speedand accuracy by building an effective and efficient video classification systemthrough systematic exploration of critical network design choices. Inparticular, we show that it is possible to replace many of the 3D convolutionsby low-cost 2D convolutions. Rather surprisingly, best result (in both speedand accuracy) is achieved when replacing the 3D convolutions at the bottom ofthe network, suggesting that temporal representation learning on high-levelsemantic features is more useful. Our conclusion generalizes to datasets withvery different properties. When combined with several other cost-effectivedesigns including separable spatial/temporal convolution and feature gating,our system results in an effective video classification system that thatproduces very competitive results on several action classification benchmarks(Kinetics, Something-something, UCF101 and HMDB), as well as two actiondetection (localization) benchmarks (JHMDB and UCF101-24).',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 305-321',\n",
       "  'citations': '197',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1712.04851v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=8810752769877948145&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 125: {'ID': 125,\n",
       "  'title': 'Playing for Data: Ground Truth from Computer Games',\n",
       "  'authors': ['Stephan R. Richter',\n",
       "   'Stefan Roth',\n",
       "   'Vladlen Koltun',\n",
       "   'Vibhav Vineet'],\n",
       "  'published': '2016-08-07T08:20:14Z',\n",
       "  'updated': '2016-08-07T08:20:14Z',\n",
       "  'abstract': 'Recent progress in computer vision has been driven by high-capacity modelstrained on large datasets. Unfortunately, creating large datasets withpixel-level labels has been extremely costly due to the amount of human effortrequired. In this paper, we present an approach to rapidly creatingpixel-accurate semantic label maps for images extracted from modern computergames. Although the source code and the internal operation of commercial gamesare inaccessible, we show that associations between image patches can bereconstructed from the communication between the game and the graphicshardware. This enables rapid propagation of semantic labels within and acrossimages synthesized by the game, with no access to the source code or thecontent. We validate the presented approach by producing dense pixel-levelsemantic annotations for 25 thousand images synthesized by a photorealisticopen-world computer game. Experiments on semantic segmentation datasets showthat using the acquired data to supplement real-world images significantlyincreases accuracy and that the acquired data enables reducing the amount ofhand-labeled real-world data: models trained with game data and just 1/3 of theCamVid training set outperform models trained on the complete CamVid trainingset.',\n",
       "  'categories': ['cs.CV', 'I.4.8'],\n",
       "  'journal': 'ECCV (2), 102-118',\n",
       "  'citations': '668',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1608.02192v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=12822958035144353200&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 126: {'ID': 126,\n",
       "  'title': 'Stacked Cross Attention for Image-Text Matching',\n",
       "  'authors': ['Kuang-Huei Lee',\n",
       "   'Xi Chen',\n",
       "   'Xiaodong He',\n",
       "   'Houdong Hu',\n",
       "   'Gang Hua'],\n",
       "  'published': '2018-03-21T17:22:27Z',\n",
       "  'updated': '2018-07-23T04:41:57Z',\n",
       "  'abstract': 'In this paper, we study the problem of image-text matching. Inferring thelatent semantic alignment between objects or other salient stuff (e.g. snow,sky, lawn) and the corresponding words in sentences allows to capturefine-grained interplay between vision and language, and makes image-textmatching more interpretable. Prior work either simply aggregates the similarityof all possible pairs of regions and words without attending differentially tomore and less important words or regions, or uses a multi-step attentionalprocess to capture limited number of semantic alignments which is lessinterpretable. In this paper, we present Stacked Cross Attention to discoverthe full latent alignments using both image regions and words in a sentence ascontext and infer image-text similarity. Our approach achieves thestate-of-the-art results on the MS-COCO and Flickr30K datasets. On Flickr30K,our approach outperforms the current best methods by 22.1% relatively in textretrieval from image query, and 18.2% relatively in image retrieval with textquery (based on Recall@1). On MS-COCO, our approach improves sentence retrievalby 17.8% relatively and image retrieval by 16.6% relatively (based on Recall@1using the 5K test set). Code has been made available at:https://github.com/kuanghuei/SCAN.',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.LG'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 201-216',\n",
       "  'citations': '158',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1803.08024v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=16231211588572724532&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 127: {'ID': 127,\n",
       "  'title': 'Skeleton-Based Action Recognition Using Spatio-Temporal LSTM Network  with Trust Gates',\n",
       "  'authors': ['Gang Wang',\n",
       "   'Alex C. Kot',\n",
       "   'Jun Liu',\n",
       "   'Amir Shahroudy',\n",
       "   'Dong Xu'],\n",
       "  'published': '2017-06-26T08:35:45Z',\n",
       "  'updated': '2017-06-26T08:35:45Z',\n",
       "  'abstract': \"Skeleton-based human action recognition has attracted a lot of researchattention during the past few years. Recent works attempted to utilizerecurrent neural networks to model the temporal dependencies between the 3Dpositional configurations of human body joints for better analysis of humanactivities in the skeletal data. The proposed work extends this idea to spatialdomain as well as temporal domain to better analyze the hidden sources ofaction-related information within the human skeleton sequences in both of thesedomains simultaneously. Based on the pictorial structure of Kinect's skeletaldata, an effective tree-structure based traversal framework is also proposed.In order to deal with the noise in the skeletal data, a new gating mechanismwithin LSTM module is introduced, with which the network can learn thereliability of the sequential data and accordingly adjust the effect of theinput data on the updating procedure of the long-term context representationstored in the unit's memory cell. Moreover, we introduce a novel multi-modalfeature fusion strategy within the LSTM unit in this paper. The comprehensiveexperimental results on seven challenging benchmark datasets for human actionrecognition demonstrate the effectiveness of the proposed method.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 40 (12), 3007\\xa0…',\n",
       "  'citations': '144',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1706.08276v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=16533928330474340697&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 128: {'ID': 128,\n",
       "  'title': 'The MegaFace Benchmark: 1 Million Faces for Recognition at Scale',\n",
       "  'authors': ['Daniel Miller',\n",
       "   'Evan Brossard',\n",
       "   'Steve Seitz',\n",
       "   'Ira Kemelmacher-Shlizerman'],\n",
       "  'published': '2015-12-02T07:17:54Z',\n",
       "  'updated': '2015-12-02T07:17:54Z',\n",
       "  'abstract': \"Recent face recognition experiments on a major benchmark LFW show stunningperformance--a number of algorithms achieve near to perfect score, surpassinghuman recognition rates. In this paper, we advocate evaluations at the millionscale (LFW includes only 13K photos of 5K people). To this end, we haveassembled the MegaFace dataset and created the first MegaFace challenge. Ourdataset includes One Million photos that capture more than 690K differentindividuals. The challenge evaluates performance of algorithms with increasingnumbers of distractors (going from 10 to 1M) in the gallery set. We presentboth identification and verification performance, evaluate performance withrespect to pose and a person's age, and compare as a function of training datasize (number of photos and people). We report results of state of the art andbaseline algorithms. Our key observations are that testing at the million scalereveals big performance differences (of algorithms that perform similarly wellon smaller scale) and that age invariant recognition as well as pose are stillchallenging for most. The MegaFace dataset, baseline code, and evaluationscripts, are all publicly released for further experimentations at:megaface.cs.washington.edu.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '429',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1512.00596v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6051410257476935491&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 129: {'ID': 129,\n",
       "  'title': 'Going Deeper with Convolutions',\n",
       "  'authors': ['Pierre Sermanet',\n",
       "   'Christian Szegedy',\n",
       "   'Dumitru Erhan',\n",
       "   'Dragomir Anguelov',\n",
       "   'Wei Liu',\n",
       "   'Scott Reed',\n",
       "   'Yangqing Jia',\n",
       "   'Vincent Vanhoucke',\n",
       "   'Andrew Rabinovich'],\n",
       "  'published': '2014-09-17T01:03:11Z',\n",
       "  'updated': '2014-09-17T01:03:11Z',\n",
       "  'abstract': 'We propose a deep convolutional neural network architecture codenamed\"Inception\", which was responsible for setting the new state of the art forclassification and detection in the ImageNet Large-Scale Visual RecognitionChallenge 2014 (ILSVRC 2014). The main hallmark of this architecture is theimproved utilization of the computing resources inside the network. This wasachieved by a carefully crafted design that allows for increasing the depth andwidth of the network while keeping the computational budget constant. Tooptimize quality, the architectural decisions were based on the Hebbianprinciple and the intuition of multi-scale processing. One particularincarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22layers deep network, the quality of which is assessed in the context ofclassification and detection.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '22434',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1409.4842v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=17799971764477278135&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 130: {'ID': 130,\n",
       "  'title': 'Learning Deep Structure-Preserving Image-Text Embeddings',\n",
       "  'authors': ['Yin Li', 'Liwei Wang', 'Svetlana Lazebnik'],\n",
       "  'published': '2015-11-19T07:17:49Z',\n",
       "  'updated': '2016-04-14T03:10:04Z',\n",
       "  'abstract': 'This paper proposes a method for learning joint embeddings of images and textusing a two-branch neural network with multiple layers of linear projectionsfollowed by nonlinearities. The network is trained using a large marginobjective that combines cross-view ranking constraints with within-viewneighborhood structure preservation constraints inspired by metric learningliterature. Extensive experiments show that our approach gains significantimprovements in accuracy for image-to-text and text-to-image retrieval. Ourmethod achieves new state-of-the-art results on the Flickr30K and MSCOCOimage-sentence datasets and shows promise on the new task of phraselocalization on the Flickr30K Entities dataset.',\n",
       "  'categories': ['cs.CV', 'cs.CL', 'cs.LG'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '425',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1511.06078v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=7266256581917835642&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 131: {'ID': 131,\n",
       "  'title': 'Jointly Modeling Embedding and Translation to Bridge Video and Language',\n",
       "  'authors': ['Yingwei Pan', 'Tao Mei', 'Ting Yao', 'Houqiang Li', 'Yong Rui'],\n",
       "  'published': '2015-05-07T20:13:33Z',\n",
       "  'updated': '2015-06-04T07:17:06Z',\n",
       "  'abstract': 'Automatically describing video content with natural language is a fundamentalchallenge of multimedia. Recurrent Neural Networks (RNN), which models sequencedynamics, has attracted increasing attention on visual interpretation. However,most existing approaches generate a word locally with given previous words andthe visual content, while the relationship between sentence semantics andvisual content is not holistically exploited. As a result, the generatedsentences may be contextually correct but the semantics (e.g., subjects, verbsor objects) are not true.  This paper presents a novel unified framework, named Long Short-Term Memorywith visual-semantic Embedding (LSTM-E), which can simultaneously explore thelearning of LSTM and visual-semantic embedding. The former aims to locallymaximize the probability of generating the next word given previous words andvisual content, while the latter is to create a visual-semantic embedding spacefor enforcing the relationship between the semantics of the entire sentence andvisual content. Our proposed LSTM-E consists of three components: a 2-D and/or3-D deep convolutional neural networks for learning powerful videorepresentation, a deep RNN for generating sentences, and a joint embeddingmodel for exploring the relationships between visual content and sentencesemantics. The experiments on YouTube2Text dataset show that our proposedLSTM-E achieves to-date the best reported performance in generating naturalsentences: 45.3% and 31.0% in terms of BLEU@4 and METEOR, respectively. We alsodemonstrate that LSTM-E is superior in predicting Subject-Verb-Object (SVO)triplets to several state-of-the-art techniques.',\n",
       "  'categories': ['cs.CV', 'cs.MM'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '340',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1505.01861v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6864224843988769888&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 132: {'ID': 132,\n",
       "  'title': 'Cross-view Asymmetric Metric Learning for Unsupervised Person  Re-identification',\n",
       "  'authors': ['Hong-Xing Yu', 'Wei-Shi Zheng', 'Ancong Wu'],\n",
       "  'published': '2017-08-27T07:59:29Z',\n",
       "  'updated': '2017-10-18T14:21:46Z',\n",
       "  'abstract': 'While metric learning is important for Person re-identification (RE-ID), asignificant problem in visual surveillance for cross-view pedestrian matching,existing metric models for RE-ID are mostly based on supervised learning thatrequires quantities of labeled samples in all pairs of camera views fortraining. However, this limits their scalabilities to realistic applications,in which a large amount of data over multiple disjoint camera views isavailable but not labelled. To overcome the problem, we propose unsupervisedasymmetric metric learning for unsupervised RE-ID. Our model aims to learn anasymmetric metric, i.e., specific projection for each view, based on asymmetricclustering on cross-view person images. Our model finds a shared space whereview-specific bias is alleviated and thus better matching performance can beachieved. Extensive experiments have been conducted on a baseline and fivelarge-scale RE-ID datasets to demonstrate the effectiveness of the proposedmodel. Through the comparison, we show that our model works much more suitablefor unsupervised RE-ID compared to classical unsupervised metric learningmodels. We also compare with existing unsupervised RE-ID methods, and our modeloutperforms them with notable margins. Specifically, we report the results onlarge-scale unlabelled RE-ID dataset, which is important but unfortunately lessconcerned in literatures.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 994-1002',\n",
       "  'citations': '181',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1708.08062v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=3713641048626785572&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 133: {'ID': 133,\n",
       "  'title': 'Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields',\n",
       "  'authors': ['Zhe Cao', 'Yaser Sheikh', 'Tomas Simon', 'Shih-En Wei'],\n",
       "  'published': '2016-11-24T01:58:16Z',\n",
       "  'updated': '2017-04-14T00:19:18Z',\n",
       "  'abstract': 'We present an approach to efficiently detect the 2D pose of multiple peoplein an image. The approach uses a nonparametric representation, which we referto as Part Affinity Fields (PAFs), to learn to associate body parts withindividuals in the image. The architecture encodes global context, allowing agreedy bottom-up parsing step that maintains high accuracy while achievingrealtime performance, irrespective of the number of people in the image. Thearchitecture is designed to jointly learn part locations and their associationvia two branches of the same sequential prediction process. Our method placedfirst in the inaugural COCO 2016 keypoints challenge, and significantly exceedsthe previous state-of-the-art result on the MPII Multi-Person benchmark, bothin performance and efficiency.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '2394',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1611.08050v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14150631212008688559&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 134: {'ID': 134,\n",
       "  'title': 'OctNet: Learning Deep 3D Representations at High Resolutions',\n",
       "  'authors': ['Gernot Riegler', 'Andreas Geiger', 'Ali Osman Ulusoy'],\n",
       "  'published': '2016-11-15T20:05:45Z',\n",
       "  'updated': '2017-04-10T08:46:56Z',\n",
       "  'abstract': 'We present OctNet, a representation for deep learning with sparse 3D data. Incontrast to existing models, our representation enables 3D convolutionalnetworks which are both deep and high resolution. Towards this goal, we exploitthe sparsity in the input data to hierarchically partition the space using aset of unbalanced octrees where each leaf node stores a pooled featurerepresentation. This allows to focus memory allocation and computation to therelevant dense regions and enables deeper networks without compromisingresolution. We demonstrate the utility of our OctNet representation byanalyzing the impact of resolution on several 3D tasks including 3D objectclassification, orientation estimation and point cloud labeling.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '527',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1611.05009v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5787747825600741306&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 135: {'ID': 135,\n",
       "  'title': 'FlowNet: Learning Optical Flow with Convolutional Networks',\n",
       "  'authors': ['Caner Hazırbaş',\n",
       "   'Philipp Fischer',\n",
       "   'Eddy Ilg',\n",
       "   'Patrick van der Smagt',\n",
       "   'Vladimir Golkov',\n",
       "   'Daniel Cremers',\n",
       "   'Philip Häusser',\n",
       "   'Alexey Dosovitskiy',\n",
       "   'Thomas Brox'],\n",
       "  'published': '2015-04-26T17:30:32Z',\n",
       "  'updated': '2015-05-04T08:50:57Z',\n",
       "  'abstract': 'Convolutional neural networks (CNNs) have recently been very successful in avariety of computer vision tasks, especially on those linked to recognition.Optical flow estimation has not been among the tasks where CNNs weresuccessful. In this paper we construct appropriate CNNs which are capable ofsolving the optical flow estimation problem as a supervised learning task. Wepropose and compare two architectures: a generic architecture and another oneincluding a layer that correlates feature vectors at different image locations.  Since existing ground truth data sets are not sufficiently large to train aCNN, we generate a synthetic Flying Chairs dataset. We show that networkstrained on this unrealistic data still generalize very well to existingdatasets such as Sintel and KITTI, achieving competitive accuracy at framerates of 5 to 10 fps.',\n",
       "  'categories': ['cs.CV', 'cs.LG', 'I.2.6; I.4.8'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 2758-2766',\n",
       "  'citations': '1440',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1504.06852v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14306873502916215398&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 136: {'ID': 136,\n",
       "  'title': 'Semantic Autoencoder for Zero-Shot Learning',\n",
       "  'authors': ['Shaogang Gong', 'Tao Xiang', 'Elyor Kodirov'],\n",
       "  'published': '2017-04-26T20:45:53Z',\n",
       "  'updated': '2017-04-26T20:45:53Z',\n",
       "  'abstract': 'Existing zero-shot learning (ZSL) models typically learn a projectionfunction from a feature space to a semantic embedding space (e.g.~attributespace). However, such a projection function is only concerned with predictingthe training seen class semantic representation (e.g.~attribute prediction) orclassification. When applied to test data, which in the context of ZSL containsdifferent (unseen) classes without training data, a ZSL model typically suffersfrom the project domain shift problem. In this work, we present a novelsolution to ZSL based on learning a Semantic AutoEncoder (SAE). Taking theencoder-decoder paradigm, an encoder aims to project a visual feature vectorinto the semantic space as in the existing ZSL models. However, the decoderexerts an additional constraint, that is, the projection/code must be able toreconstruct the original visual feature. We show that with this additionalreconstruction constraint, the learned projection function from the seenclasses is able to generalise better to the new unseen classes. Importantly,the encoder and decoder are linear and symmetric which enable us to develop anextremely efficient learning algorithm. Extensive experiments on six benchmarkdatasets demonstrate that the proposed SAE outperforms significantly theexisting ZSL models with the additional benefit of lower computational cost.Furthermore, when the SAE is applied to supervised clustering problem, it alsobeats the state-of-the-art.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '343',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1704.08345v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14392129101848795987&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 137: {'ID': 137,\n",
       "  'title': 'Deep Back-Projection Networks For Super-Resolution',\n",
       "  'authors': ['Muhammad Haris', 'Greg Shakhnarovich', 'Norimichi Ukita'],\n",
       "  'published': '2018-03-07T16:05:35Z',\n",
       "  'updated': '2018-03-07T16:05:35Z',\n",
       "  'abstract': 'The feed-forward architectures of recently proposed deep super-resolutionnetworks learn representations of low-resolution inputs, and the non-linearmapping from those to high-resolution output. However, this approach does notfully address the mutual dependencies of low- and high-resolution images. Wepropose Deep Back-Projection Networks (DBPN), that exploit iterative up- anddown-sampling layers, providing an error feedback mechanism for projectionerrors at each stage. We construct mutually-connected up- and down-samplingstages each of which represents different types of image degradation andhigh-resolution components. We show that extending this idea to allowconcatenation of features across up- and down-sampling stages (Dense DBPN)allows us to reconstruct further improve super-resolution, yielding superiorresults and in particular establishing new state of the art results for largescaling factors such as 8x across multiple data sets.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '351',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1803.02735v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=17432708049254932747&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 138: {'ID': 138,\n",
       "  'title': 'Diverse Image-to-Image Translation via Disentangled Representations',\n",
       "  'authors': ['Jia-Bin Huang',\n",
       "   'Maneesh Kumar Singh',\n",
       "   'Hsin-Ying Lee',\n",
       "   'Ming-Hsuan Yang',\n",
       "   'Hung-Yu Tseng'],\n",
       "  'published': '2018-08-02T17:54:27Z',\n",
       "  'updated': '2018-08-02T17:54:27Z',\n",
       "  'abstract': 'Image-to-image translation aims to learn the mapping between two visualdomains. There are two main challenges for many applications: 1) the lack ofaligned training pairs and 2) multiple possible outputs from a single inputimage. In this work, we present an approach based on disentangledrepresentation for producing diverse outputs without paired training images. Toachieve diversity, we propose to embed images onto two spaces: adomain-invariant content space capturing shared information across domains anda domain-specific attribute space. Our model takes the encoded content featuresextracted from a given input and the attribute vectors sampled from theattribute space to produce diverse outputs at test time. To handle unpairedtraining data, we introduce a novel cross-cycle consistency loss based ondisentangled representations. Qualitative results show that our model cangenerate diverse and realistic images on a wide range of tasks without pairedtraining data. For quantitative comparisons, we measure realism with user studyand diversity with a perceptual distance metric. We apply the proposed model todomain adaptation and show competitive performance when compared to thestate-of-the-art on the MNIST-M and the LineMod datasets.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 35-51',\n",
       "  'citations': '309',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1808.00948v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2272463241175511122&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 139: {'ID': 139,\n",
       "  'title': 'Deep Clustering for Unsupervised Learning of Visual Features',\n",
       "  'authors': ['Matthijs Douze',\n",
       "   'Piotr Bojanowski',\n",
       "   'Armand Joulin',\n",
       "   'Mathilde Caron'],\n",
       "  'published': '2018-07-15T09:41:39Z',\n",
       "  'updated': '2019-03-18T16:20:30Z',\n",
       "  'abstract': 'Clustering is a class of unsupervised learning methods that has beenextensively applied and studied in computer vision. Little work has been doneto adapt it to the end-to-end training of visual features on large scaledatasets. In this work, we present DeepCluster, a clustering method thatjointly learns the parameters of a neural network and the cluster assignmentsof the resulting features. DeepCluster iteratively groups the features with astandard clustering algorithm, k-means, and uses the subsequent assignments assupervision to update the weights of the network. We apply DeepCluster to theunsupervised training of convolutional neural networks on large datasets likeImageNet and YFCC100M. The resulting model outperforms the current state of theart by a significant margin on all the standard benchmarks.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 132-149',\n",
       "  'citations': '328',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1807.05520v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=9776210521429980111&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 140: {'ID': 140,\n",
       "  'title': 'Perceptual Losses for Real-Time Style Transfer and Super-Resolution',\n",
       "  'authors': ['Justin Johnson', 'Li Fei-Fei', 'Alexandre Alahi'],\n",
       "  'published': '2016-03-27T01:04:27Z',\n",
       "  'updated': '2016-03-27T01:04:27Z',\n",
       "  'abstract': 'We consider image transformation problems, where an input image istransformed into an output image. Recent methods for such problems typicallytrain feed-forward convolutional neural networks using a \\\\emph{per-pixel} lossbetween the output and ground-truth images. Parallel work has shown thathigh-quality images can be generated by defining and optimizing\\\\emph{perceptual} loss functions based on high-level features extracted frompretrained networks. We combine the benefits of both approaches, and proposethe use of perceptual loss functions for training feed-forward networks forimage transformation tasks. We show results on image style transfer, where afeed-forward network is trained to solve the optimization problem proposed byGatys et al in real-time. Compared to the optimization-based method, ournetwork gives similar qualitative results but is three orders of magnitudefaster. We also experiment with single-image super-resolution, where replacinga per-pixel loss with a perceptual loss gives visually pleasing results.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'journal': 'ECCV (2), 694-711',\n",
       "  'citations': '3317',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1603.08155v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5132755018694140583&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 141: {'ID': 141,\n",
       "  'title': 'Fast Optical Flow using Dense Inverse Search',\n",
       "  'authors': ['Luc Van Gool', 'Radu Timofte', 'Dengxin Dai', 'Till Kroeger'],\n",
       "  'published': '2016-03-11T10:55:07Z',\n",
       "  'updated': '2016-03-11T10:55:07Z',\n",
       "  'abstract': \"Most recent works in optical flow extraction focus on the accuracy andneglect the time complexity. However, in real-life visual applications, such astracking, activity detection and recognition, the time complexity is critical.  We propose a solution with very low time complexity and competitive accuracyfor the computation of dense optical flow. It consists of three parts: 1)inverse search for patch correspondences; 2) dense displacement field creationthrough patch aggregation along multiple scales; 3) variational refinement. Atthe core of our Dense Inverse Search-based method (DIS) is the efficient searchof correspondences inspired by the inverse compositional image alignmentproposed by Baker and Matthews in 2001.  DIS is competitive on standard optical flow benchmarks with largedisplacements. DIS runs at 300Hz up to 600Hz on a single CPU core, reaching thetemporal resolution of human's biological vision system. It is order(s) ofmagnitude faster than state-of-the-art methods in the same range of accuracy,making DIS ideal for visual applications.\",\n",
       "  'categories': ['cs.CV', 'cs.RO'],\n",
       "  'journal': 'ECCV (4), 471-488',\n",
       "  'citations': '150',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1603.03590v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=13749416618552316353&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 142: {'ID': 142,\n",
       "  'title': 'Image Captioning with Semantic Attention',\n",
       "  'authors': ['Chen Fang',\n",
       "   'Hailin Jin',\n",
       "   'Zhaowen Wang',\n",
       "   'Quanzeng You',\n",
       "   'Jiebo Luo'],\n",
       "  'published': '2016-03-12T15:11:43Z',\n",
       "  'updated': '2016-03-12T15:11:43Z',\n",
       "  'abstract': 'Automatically generating a natural language description of an image hasattracted interests recently both because of its importance in practicalapplications and because it connects two major artificial intelligence fields:computer vision and natural language processing. Existing approaches are eithertop-down, which start from a gist of an image and convert it into words, orbottom-up, which come up with words describing various aspects of an image andthen combine them. In this paper, we propose a new algorithm that combines bothapproaches through a model of semantic attention. Our algorithm learns toselectively attend to semantic concept proposals and fuse them into hiddenstates and outputs of recurrent neural networks. The selection and fusion forma feedback connecting the top-down and bottom-up computation. We evaluate ouralgorithm on two public benchmarks: Microsoft COCO and Flickr30K. Experimentalresults show that our algorithm significantly outperforms the state-of-the-artapproaches consistently across different evaluation metrics.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '868',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1603.03925v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=1207835255393155860&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 143: {'ID': 143,\n",
       "  'title': 'Long-term Temporal Convolutions for Action Recognition',\n",
       "  'authors': ['Cordelia Schmid', 'Ivan Laptev', 'Gül Varol'],\n",
       "  'published': '2016-04-15T13:33:13Z',\n",
       "  'updated': '2017-06-02T12:08:57Z',\n",
       "  'abstract': 'Typical human actions last several seconds and exhibit characteristicspatio-temporal structure. Recent methods attempt to capture this structure andlearn action representations with convolutional neural networks. Suchrepresentations, however, are typically learned at the level of a few videoframes failing to model actions at their full temporal extent. In this work welearn video representations using neural networks with long-term temporalconvolutions (LTC). We demonstrate that LTC-CNN models with increased temporalextents improve the accuracy of action recognition. We also study the impact ofdifferent low-level representations, such as raw values of video pixels andoptical flow vector fields and demonstrate the importance of high-qualityoptical flow estimation for learning accurate action models. We reportstate-of-the-art results on two challenging benchmarks for human actionrecognition UCF101 (92.7%) and HMDB51 (67.2%).',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 40 (6), 1510-1517',\n",
       "  'citations': '515',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1604.04494v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=10586603333655451919&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 144: {'ID': 144,\n",
       "  'title': 'Compact Convolutional Neural Network Cascade for Face Detection',\n",
       "  'authors': ['Ilya Kalinovskii', 'Vladimir Spitsyn'],\n",
       "  'published': '2015-08-06T07:01:55Z',\n",
       "  'updated': '2015-11-23T20:01:06Z',\n",
       "  'abstract': 'The problem of faces detection in images or video streams is a classicalproblem of computer vision. The multiple solutions of this problem have beenproposed, but the question of their optimality is still open. Many algorithmsachieve a high quality face detection, but at the cost of high computationalcomplexity. This restricts their application in the real-time systems. Thispaper presents a new solution of the frontal face detection problem based oncompact convolutional neural networks cascade. The test results on FDDB datasetshow that it is competitive with state-of-the-art algorithms. This proposeddetector is implemented using three technologies: SSE/AVX/AVX2 instruction setsfor Intel CPUs, Nvidia CUDA, OpenCL. The detection speed of our approachconsiderably exceeds all the existing CPU-based and GPU-based algorithms.Because of high computational efficiency, our detector can processing 4K UltraHD video stream in real time (up to 27 fps) on mobile platforms (Intel IvyBridge CPUs and Nvidia Kepler GPUs) in searching objects with the dimension60x60 pixels or higher. At the same time its performance weakly dependent onthe background and number of objects in scene. This is achieved by theasynchronous computation of stages in the cascade.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '929',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1508.01292v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=15964276235033945095&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 145: {'ID': 145,\n",
       "  'title': 'Stacked Attention Networks for Image Question Answering',\n",
       "  'authors': ['Jianfeng Gao',\n",
       "   'Li Deng',\n",
       "   'Zichao Yang',\n",
       "   'Xiaodong He',\n",
       "   'Alex Smola'],\n",
       "  'published': '2015-11-07T00:43:32Z',\n",
       "  'updated': '2016-01-26T20:37:49Z',\n",
       "  'abstract': 'This paper presents stacked attention networks (SANs) that learn to answernatural language questions from images. SANs use semantic representation of aquestion as query to search for the regions in an image that are related to theanswer. We argue that image question answering (QA) often requires multiplesteps of reasoning. Thus, we develop a multiple-layer SAN in which we query animage multiple times to infer the answer progressively. Experiments conductedon four image QA data sets demonstrate that the proposed SANs significantlyoutperform previous state-of-the-art approaches. The visualization of theattention layers illustrates the progress that the SAN locates the relevantvisual clues that lead to the answer of the question layer-by-layer.',\n",
       "  'categories': ['cs.LG', 'cs.CL', 'cs.CV', 'cs.NE'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '1034',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1511.02274v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14027637653932093716&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 146: {'ID': 146,\n",
       "  'title': 'Simple Baselines for Human Pose Estimation and Tracking',\n",
       "  'authors': ['Yichen Wei', 'Haiping Wu', 'Bin Xiao'],\n",
       "  'published': '2018-04-17T12:55:23Z',\n",
       "  'updated': '2018-08-21T06:54:53Z',\n",
       "  'abstract': 'There has been significant progress on pose estimation and increasinginterests on pose tracking in recent years. At the same time, the overallalgorithm and system complexity increases as well, making the algorithmanalysis and comparison more difficult. This work provides simple and effectivebaseline methods. They are helpful for inspiring and evaluating new ideas forthe field. State-of-the-art results are achieved on challenging benchmarks. Thecode will be available at https://github.com/leoxiaobin/pose.pytorch.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 466-481',\n",
       "  'citations': '275',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1804.06208v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=16372249111646534211&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 147: {'ID': 147,\n",
       "  'title': 'Learning from Noisy Labels with Distillation',\n",
       "  'authors': ['Yale Song',\n",
       "   'Liangliang Cao',\n",
       "   'Jianchao Yang',\n",
       "   'Li-Jia Li',\n",
       "   'Yuncheng Li',\n",
       "   'Jiebo Luo'],\n",
       "  'published': '2017-03-07T14:15:14Z',\n",
       "  'updated': '2017-04-07T07:21:56Z',\n",
       "  'abstract': 'The ability of learning from noisy labels is very useful in many visualrecognition tasks, as a vast amount of data with noisy labels are relativelyeasy to obtain. Traditionally, the label noises have been treated asstatistical outliers, and approaches such as importance re-weighting andbootstrap have been proposed to alleviate the problem. According to ourobservation, the real-world noisy labels exhibit multi-mode characteristics asthe true labels, rather than behaving like independent random outliers. In thiswork, we propose a unified distillation framework to use side information,including a small clean dataset and label relations in knowledge graph, to\"hedge the risk\" of learning from noisy labels. Furthermore, unlike thetraditional approaches evaluated based on simulated label noises, we propose asuite of new benchmark datasets, in Sports, Species and Artifacts domains, toevaluate the task of learning from noisy labels in the practical setting. Theempirical study demonstrates the effectiveness of our proposed method in allthe domains.',\n",
       "  'categories': ['cs.CV', 'cs.LG', 'stat.ML'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 1910-1918',\n",
       "  'citations': '183',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1703.02391v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=538809569064858449&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 148: {'ID': 148,\n",
       "  'title': 'Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution',\n",
       "  'authors': ['Ming-Hsuan Yang',\n",
       "   'Jia-Bin Huang',\n",
       "   'Wei-Sheng Lai',\n",
       "   'Narendra Ahuja'],\n",
       "  'published': '2017-04-12T20:04:06Z',\n",
       "  'updated': '2017-10-09T22:47:12Z',\n",
       "  'abstract': 'Convolutional neural networks have recently demonstrated high-qualityreconstruction for single-image super-resolution. In this paper, we propose theLaplacian Pyramid Super-Resolution Network (LapSRN) to progressivelyreconstruct the sub-band residuals of high-resolution images. At each pyramidlevel, our model takes coarse-resolution feature maps as input, predicts thehigh-frequency residuals, and uses transposed convolutions for upsampling tothe finer level. Our method does not require the bicubic interpolation as thepre-processing step and thus dramatically reduces the computational complexity.We train the proposed LapSRN with deep supervision using a robust Charbonnierloss function and achieve high-quality reconstruction. Furthermore, our networkgenerates multi-scale predictions in one feed-forward pass through theprogressive reconstruction, thereby facilitates resource-aware applications.Extensive quantitative and qualitative evaluations on benchmark datasets showthat the proposed algorithm performs favorably against the state-of-the-artmethods in terms of speed and accuracy.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '793',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1704.03915v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=10927478870884296040&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 149: {'ID': 149,\n",
       "  'title': 'Discriminative Correlation Filter with Channel and Spatial Reliability',\n",
       "  'authors': ['Alan Lukežič',\n",
       "   'Tomáš Vojíř',\n",
       "   'Luka Čehovin',\n",
       "   'Matej Kristan',\n",
       "   'Jiří Matas'],\n",
       "  'published': '2016-11-25T14:18:52Z',\n",
       "  'updated': '2019-01-14T08:05:50Z',\n",
       "  'abstract': 'Short-term tracking is an open and challenging problem for whichdiscriminative correlation filters (DCF) have shown excellent performance. Weintroduce the channel and spatial reliability concepts to DCF tracking andprovide a novel learning algorithm for its efficient and seamless integrationin the filter update and the tracking process. The spatial reliability mapadjusts the filter support to the part of the object suitable for tracking.This both allows to enlarge the search region and improves tracking ofnon-rectangular objects. Reliability scores reflect channel-wise quality of thelearned filters and are used as feature weighting coefficients in localization.Experimentally, with only two simple standard features, HoGs and Colornames,the novel CSR-DCF method -- DCF with Channel and Spatial Reliability --achieves state-of-the-art results on VOT 2016, VOT 2015 and OTB100. The CSR-DCFruns in real-time on a CPU.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '486',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1611.08461v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=10607552131352769008&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 150: {'ID': 150,\n",
       "  'title': 'Multi-modal Factorized Bilinear Pooling with Co-Attention Learning for  Visual Question Answering',\n",
       "  'authors': ['Zhou Yu', 'Jianping Fan', 'Dacheng Tao', 'Jun Yu'],\n",
       "  'published': '2017-08-04T12:17:49Z',\n",
       "  'updated': '2017-08-04T12:17:49Z',\n",
       "  'abstract': 'Visual question answering (VQA) is challenging because it requires asimultaneous understanding of both the visual content of images and the textualcontent of questions. The approaches used to represent the images and questionsin a fine-grained manner and questions and to fuse these multi-modal featuresplay key roles in performance. Bilinear pooling based models have been shown tooutperform traditional linear models for VQA, but their high-dimensionalrepresentations and high computational complexity may seriously limit theirapplicability in practice. For multi-modal feature fusion, here we develop aMulti-modal Factorized Bilinear (MFB) pooling approach to efficiently andeffectively combine multi-modal features, which results in superior performancefor VQA compared with other bilinear pooling approaches. For fine-grained imageand question representation, we develop a co-attention mechanism using anend-to-end deep network architecture to jointly learn both the image andquestion attentions. Combining the proposed MFB approach with co-attentionlearning in a new network architecture provides a unified model for VQA. Ourexperimental results demonstrate that the single MFB with co-attention modelachieves new state-of-the-art performance on the real-world VQA dataset. Codeavailable at https://github.com/yuzcccc/mfb.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 1821-1830',\n",
       "  'citations': '196',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1708.01471v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=12567765357636779208&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 151: {'ID': 151,\n",
       "  'title': 'Deep Continuous Fusion for Multi-Sensor 3D Object Detection',\n",
       "  'authors': ['Raquel Urtasun', 'Ming Liang', 'Shenlong Wang', 'Bin Yang'],\n",
       "  'published': '2020-12-20T18:43:41Z',\n",
       "  'updated': '2020-12-20T18:43:41Z',\n",
       "  'abstract': 'In this paper, we propose a novel 3D object detector that can exploit bothLIDAR as well as cameras to perform very accurate localization. Towards thisgoal, we design an end-to-end learnable architecture that exploits continuousconvolutions to fuse image and LIDAR feature maps at different levels ofresolution. Our proposed continuous fusion layer encode both discrete-stateimage features as well as continuous geometric information. This enables us todesign a novel, reliable and efficient end-to-end learnable 3D object detectorbased on multiple sensors. Our experimental evaluation on both KITTI as well asa large scale 3D object detection benchmark shows significant improvements overthe state of the art.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 641-656',\n",
       "  'citations': '162',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2012.10992v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=15523011476685408326&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 152: {'ID': 152,\n",
       "  'title': 'An Empirical Study and Analysis of Generalized Zero-Shot Learning for  Object Recognition in the Wild',\n",
       "  'authors': ['Fei Sha', 'Boqing Gong', 'Wei-Lun Chao', 'Soravit Changpinyo'],\n",
       "  'published': '2016-05-13T17:05:01Z',\n",
       "  'updated': '2017-01-11T08:33:25Z',\n",
       "  'abstract': \"Zero-shot learning (ZSL) methods have been studied in the unrealistic settingwhere test data are assumed to come from unseen classes only. In this paper, weadvocate studying the problem of generalized zero-shot learning (GZSL) wherethe test data's class memberships are unconstrained. We show empirically thatnaively using the classifiers constructed by ZSL approaches does not performwell in the generalized setting. Motivated by this, we propose a simple buteffective calibration method that can be used to balance two conflictingforces: recognizing data from seen classes versus those from unseen ones. Wedevelop a performance metric to characterize such a trade-off and examine theutility of this metric in evaluating various ZSL approaches. Our analysisfurther shows that there is a large gap between the performance of existingapproaches and an upper bound established via idealized semantic embeddings,suggesting that improving class semantic embeddings is vital to GZSL.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (2), 52-68',\n",
       "  'citations': '218',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1605.04253v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14821524106914898406&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 153: {'ID': 153,\n",
       "  'title': 'P-CNN: Pose-based CNN Features for Action Recognition',\n",
       "  'authors': ['Guilhem Chéron', 'Cordelia Schmid', 'Ivan Laptev'],\n",
       "  'published': '2015-06-11T10:02:03Z',\n",
       "  'updated': '2015-09-23T10:48:29Z',\n",
       "  'abstract': 'This work targets human action recognition in video. While recent methodstypically represent actions by statistics of local video features, here weargue for the importance of a representation derived from human pose. To thisend we propose a new Pose-based Convolutional Neural Network descriptor (P-CNN)for action recognition. The descriptor aggregates motion and appearanceinformation along tracks of human body parts. We investigate different schemesof temporal aggregation and experiment with P-CNN features obtained both forautomatically estimated and manually annotated human poses. We evaluate ourmethod on the recent and challenging JHMDB and MPII Cooking datasets. For bothdatasets our method shows consistent improvement over the state of the art.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 3218-3226',\n",
       "  'citations': '406',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1506.03607v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=10794121515679536826&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 154: {'ID': 154,\n",
       "  'title': 'Receptive Field Block Net for Accurate and Fast Object Detection',\n",
       "  'authors': ['Yunhong Wang', 'Songtao Liu', 'Di Huang'],\n",
       "  'published': '2017-11-21T13:18:26Z',\n",
       "  'updated': '2018-07-26T08:37:02Z',\n",
       "  'abstract': 'Current top-performing object detectors depend on deep CNN backbones, such asResNet-101 and Inception, benefiting from their powerful featurerepresentations but suffering from high computational costs. Conversely, somelightweight model based detectors fulfil real time processing, while theiraccuracies are often criticized. In this paper, we explore an alternative tobuild a fast and accurate detector by strengthening lightweight features usinga hand-crafted mechanism. Inspired by the structure of Receptive Fields (RFs)in human visual systems, we propose a novel RF Block (RFB) module, which takesthe relationship between the size and eccentricity of RFs into account, toenhance the feature discriminability and robustness. We further assemble RFB tothe top of SSD, constructing the RFB Net detector. To evaluate itseffectiveness, experiments are conducted on two major benchmarks and theresults show that RFB Net is able to reach the performance of advanced verydeep detectors while keeping the real-time speed. Code is available athttps://github.com/ruinmessi/RFBNet.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 385-400',\n",
       "  'citations': '212',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1711.07767v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=17707966079671814827&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 155: {'ID': 155,\n",
       "  'title': 'Boosting Adversarial Attacks with Momentum',\n",
       "  'authors': ['Jun Zhu',\n",
       "   'Jianguo Li',\n",
       "   'Hang Su',\n",
       "   'Fangzhou Liao',\n",
       "   'Tianyu Pang',\n",
       "   'Yinpeng Dong',\n",
       "   'Xiaolin Hu'],\n",
       "  'published': '2017-10-17T04:03:04Z',\n",
       "  'updated': '2018-03-22T12:46:44Z',\n",
       "  'abstract': 'Deep neural networks are vulnerable to adversarial examples, which posessecurity concerns on these algorithms due to the potentially severeconsequences. Adversarial attacks serve as an important surrogate to evaluatethe robustness of deep learning models before they are deployed. However, mostof existing adversarial attacks can only fool a black-box model with a lowsuccess rate. To address this issue, we propose a broad class of momentum-basediterative algorithms to boost adversarial attacks. By integrating the momentumterm into the iterative process for attacks, our methods can stabilize updatedirections and escape from poor local maxima during the iterations, resultingin more transferable adversarial examples. To further improve the success ratesfor black-box attacks, we apply momentum iterative algorithms to an ensemble ofmodels, and show that the adversarially trained models with a strong defenseability are also vulnerable to our black-box attacks. We hope that the proposedmethods will serve as a benchmark for evaluating the robustness of various deepmodels and defense methods. With this method, we won the first places in NIPS2017 Non-targeted Adversarial Attack and Targeted Adversarial Attackcompetitions.',\n",
       "  'categories': ['cs.LG', 'stat.ML'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '373',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1710.06081v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=7632768926296642400&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 156: {'ID': 156,\n",
       "  'title': 'Learning Visual Features from Large Weakly Supervised Data',\n",
       "  'authors': ['Nicolas Vasilache',\n",
       "   'Laurens van der Maaten',\n",
       "   'Armand Joulin',\n",
       "   'Allan Jabri'],\n",
       "  'published': '2015-11-06T22:08:37Z',\n",
       "  'updated': '2015-11-06T22:08:37Z',\n",
       "  'abstract': 'Convolutional networks trained on large supervised dataset produce visualfeatures which form the basis for the state-of-the-art in many computer-visionproblems. Further improvements of these visual features will likely requireeven larger manually labeled data sets, which severely limits the pace at whichprogress can be made. In this paper, we explore the potential of leveragingmassive, weakly-labeled image collections for learning good visual features. Wetrain convolutional networks on a dataset of 100 million Flickr photos andcaptions, and show that these networks produce features that perform well in arange of vision problems. We also show that the networks appropriately captureword similarity, and learn correspondences between different languages.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (7), 67-84',\n",
       "  'citations': '156',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1511.02251v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=15654421007486751596&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 157: {'ID': 157,\n",
       "  'title': 'Shuffle and Learn: Unsupervised Learning using Temporal Order  Verification',\n",
       "  'authors': ['Martial Hebert', 'Ishan Misra', 'C. Lawrence Zitnick'],\n",
       "  'published': '2016-03-28T21:00:43Z',\n",
       "  'updated': '2016-07-26T17:26:01Z',\n",
       "  'abstract': 'In this paper, we present an approach for learning a visual representationfrom the raw spatiotemporal signals in videos. Our representation is learnedwithout supervision from semantic labels. We formulate our method as anunsupervised sequential verification task, i.e., we determine whether asequence of frames from a video is in the correct temporal order. With thissimple task and no semantic labels, we learn a powerful visual representationusing a Convolutional Neural Network (CNN). The representation containscomplementary information to that learned from supervised image datasets likeImageNet. Qualitative results show that our method captures information that istemporally varying, such as human pose. When used as pre-training for actionrecognition, our method gives significant gains over learning without externaldata on benchmark datasets like UCF101 and HMDB51. To demonstrate itssensitivity to human pose, we show results for pose estimation on the FLIC andMPII datasets that are competitive, or better than approaches usingsignificantly more supervision. Our method can be combined with supervisedrepresentations to provide an additional boost in accuracy.',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.LG'],\n",
       "  'journal': 'ECCV (1), 527-544',\n",
       "  'citations': '264',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1603.08561v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6423607982997301091&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 158: {'ID': 158,\n",
       "  'title': 'Deeply supervised salient object detection with short connections',\n",
       "  'authors': ['Qibin Hou',\n",
       "   'Zhuowen Tu',\n",
       "   'Ali Borji',\n",
       "   'Ming-Ming Cheng',\n",
       "   'Philip Torr',\n",
       "   'Xiao-Wei Hu'],\n",
       "  'published': '2016-11-15T14:19:06Z',\n",
       "  'updated': '2018-03-16T01:46:40Z',\n",
       "  'abstract': 'Recent progress on saliency detection is substantial, benefiting mostly fromthe explosive development of Convolutional Neural Networks (CNNs). Semanticsegmentation and saliency detection algorithms developed lately have beenmostly based on Fully Convolutional Neural Networks (FCNs). There is still alarge room for improvement over the generic FCN models that do not explicitlydeal with the scale-space problem. Holistically-Nested Edge Detector (HED)provides a skip-layer structure with deep supervision for edge and boundarydetection, but the performance gain of HED on salience detection is notobvious. In this paper, we propose a new method for saliency detection byintroducing short connections to the skip-layer structures within the HEDarchitecture. Our framework provides rich multi-scale feature maps at eachlayer, a property that is critically needed to perform segment detection. Ourmethod produces state-of-the-art results on 5 widely tested salient objectdetection benchmarks, with advantages in terms of efficiency (0.15 seconds perimage), effectiveness, and simplicity over the existing algorithms.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 41 (4), 815-828',\n",
       "  'citations': '534',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1611.04849v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=16939402952199717847&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 159: {'ID': 159,\n",
       "  'title': 'ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes',\n",
       "  'authors': ['Angela Dai',\n",
       "   'Manolis Savva',\n",
       "   'Maciej Halber',\n",
       "   'Thomas Funkhouser',\n",
       "   'Matthias Nießner',\n",
       "   'Angel X. Chang'],\n",
       "  'published': '2017-02-14T22:08:03Z',\n",
       "  'updated': '2017-04-11T08:09:33Z',\n",
       "  'abstract': 'A key requirement for leveraging supervised deep learning methods is theavailability of large, labeled datasets. Unfortunately, in the context of RGB-Dscene understanding, very little data is available -- current datasets cover asmall range of scene views and have limited semantic annotations. To addressthis issue, we introduce ScanNet, an RGB-D video dataset containing 2.5M viewsin 1513 scenes annotated with 3D camera poses, surface reconstructions, andsemantic segmentations. To collect this data, we designed an easy-to-use andscalable RGB-D capture system that includes automated surface reconstructionand crowdsourced semantic annotation. We show that using this data helpsachieve state-of-the-art performance on several 3D scene understanding tasks,including 3D object classification, semantic voxel labeling, and CAD modelretrieval. The dataset is freely available at http://www.scan-net.org.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '544',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1702.04405v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=11711174654424692449&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 160: {'ID': 160,\n",
       "  'title': 'Compositional Human Pose Regression',\n",
       "  'authors': ['Xiao Sun', 'Yichen Wei', 'Shuang Liang', 'Jiaxiang Shang'],\n",
       "  'published': '2017-04-01T11:59:41Z',\n",
       "  'updated': '2017-08-02T03:31:39Z',\n",
       "  'abstract': 'Regression based methods are not performing as well as detection basedmethods for human pose estimation. A central problem is that the structuralinformation in the pose is not well exploited in the previous regressionmethods. In this work, we propose a structure-aware regression approach. Itadopts a reparameterized pose representation using bones instead of joints. Itexploits the joint connection structure to define a compositional loss functionthat encodes the long range interactions in the pose. It is simple, effective,and general for both 2D and 3D pose estimation in a unified setting.Comprehensive evaluation validates the effectiveness of our approach. Itsignificantly advances the state-of-the-art on Human3.6M and is competitivewith state-of-the-art results on MPII.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 2602-2611',\n",
       "  'citations': '177',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1704.00159v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=7900105936909746966&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 161: {'ID': 161,\n",
       "  'title': 'Amulet: Aggregating Multi-level Convolutional Features for Salient  Object Detection',\n",
       "  'authors': ['Pingping Zhang',\n",
       "   'Huchuan Lu',\n",
       "   'Hongyu Wang',\n",
       "   'Dong Wang',\n",
       "   'Xiang Ruan'],\n",
       "  'published': '2017-08-07T06:29:38Z',\n",
       "  'updated': '2017-08-07T06:29:38Z',\n",
       "  'abstract': 'Fully convolutional neural networks (FCNs) have shown outstanding performancein many dense labeling problems. One key pillar of these successes is miningrelevant information from features in convolutional layers. However, how tobetter aggregate multi-level convolutional feature maps for salient objectdetection is underexplored. In this work, we present Amulet, a genericaggregating multi-level convolutional feature framework for salient objectdetection. Our framework first integrates multi-level feature maps intomultiple resolutions, which simultaneously incorporate coarse semantics andfine details. Then it adaptively learns to combine these feature maps at eachresolution and predict saliency maps with the combined features. Finally, thepredicted results are efficiently fused to generate the final saliency map. Inaddition, to achieve accurate boundary inference and semantic enhancement,edge-aware feature maps in low-level layers and the predicted results of lowresolution features are recursively embedded into the learning framework. Byaggregating multi-level convolutional features in this efficient and flexiblemanner, the proposed saliency model provides accurate salient object labeling.Comprehensive experiments demonstrate that our method performs favorablyagainst state-of-the art approaches in terms of near all compared evaluationmetrics.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 202-211',\n",
       "  'citations': '290',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1708.02001v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=13232938366237462902&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 162: {'ID': 162,\n",
       "  'title': 'Learning Background-Aware Correlation Filters for Visual Tracking',\n",
       "  'authors': ['Hamed Kiani Galoogahi', 'Simon Lucey', 'Ashton Fagg'],\n",
       "  'published': '2017-03-14T17:16:23Z',\n",
       "  'updated': '2017-03-21T22:48:46Z',\n",
       "  'abstract': 'Correlation Filters (CFs) have recently demonstrated excellent performance interms of rapidly tracking objects under challenging photometric and geometricvariations. The strength of the approach comes from its ability to efficientlylearn - \"on the fly\" - how the object is changing over time. A fundamentaldrawback to CFs, however, is that the background of the object is not bemodelled over time which can result in suboptimal results. In this paper wepropose a Background-Aware CF that can model how both the foreground andbackground of the object varies over time. Our approach, like conventional CFs,is extremely computationally efficient - and extensive experiments overmultiple tracking benchmarks demonstrate the superior accuracy and real-timeperformance of our method compared to the state-of-the-art trackers includingthose based on a deep learning paradigm.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 1135-1143',\n",
       "  'citations': '419',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1703.04590v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5112849340686869429&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 163: {'ID': 163,\n",
       "  'title': 'Aggregated Residual Transformations for Deep Neural Networks',\n",
       "  'authors': ['Ross Girshick',\n",
       "   'Zhuowen Tu',\n",
       "   'Piotr Dollár',\n",
       "   'Kaiming He',\n",
       "   'Saining Xie'],\n",
       "  'published': '2016-11-16T20:34:42Z',\n",
       "  'updated': '2017-04-11T01:53:41Z',\n",
       "  'abstract': 'We present a simple, highly modularized network architecture for imageclassification. Our network is constructed by repeating a building block thataggregates a set of transformations with the same topology. Our simple designresults in a homogeneous, multi-branch architecture that has only a fewhyper-parameters to set. This strategy exposes a new dimension, which we call\"cardinality\" (the size of the set of transformations), as an essential factorin addition to the dimensions of depth and width. On the ImageNet-1K dataset,we empirically show that even under the restricted condition of maintainingcomplexity, increasing cardinality is able to improve classification accuracy.Moreover, increasing cardinality is more effective than going deeper or widerwhen we increase the capacity. Our models, named ResNeXt, are the foundationsof our entry to the ILSVRC 2016 classification task in which we secured 2ndplace. We further investigate ResNeXt on an ImageNet-5K set and the COCOdetection set, also showing better results than its ResNet counterpart. Thecode and models are publicly available online.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '2396',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1611.05431v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=11890020786646058356&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 164: {'ID': 164,\n",
       "  'title': 'Multi-view Supervision for Single-view Reconstruction via Differentiable  Ray Consistency',\n",
       "  'authors': ['Alexei A. Efros',\n",
       "   'Tinghui Zhou',\n",
       "   'Shubham Tulsiani',\n",
       "   'Jitendra Malik'],\n",
       "  'published': '2017-04-20T17:56:53Z',\n",
       "  'updated': '2017-04-20T17:56:53Z',\n",
       "  'abstract': 'We study the notion of consistency between a 3D shape and a 2D observationand propose a differentiable formulation which allows computing gradients ofthe 3D shape given an observation from an arbitrary view. We do so byreformulating view consistency using a differentiable ray consistency (DRC)term. We show that this formulation can be incorporated in a learning frameworkto leverage different types of multi-view observations e.g. foreground masks,depth, color images, semantics etc. as supervision for learning single-view 3Dprediction. We present empirical analysis of our technique in a controlledsetting. We also show that this approach allows us to improve over existingtechniques for single-view reconstruction of objects from the PASCAL VOCdataset.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence',\n",
       "  'citations': '236',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1704.06254v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14665868878676865845&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 165: {'ID': 165,\n",
       "  'title': 'Learning Representations for Automatic Colorization',\n",
       "  'authors': ['Gregory Shakhnarovich', 'Michael Maire', 'Gustav Larsson'],\n",
       "  'published': '2016-03-22T04:08:01Z',\n",
       "  'updated': '2017-08-13T17:50:50Z',\n",
       "  'abstract': 'We develop a fully automatic image colorization system. Our approachleverages recent advances in deep networks, exploiting both low-level andsemantic representations. As many scene elements naturally appear according tomultimodal color distributions, we train our model to predict per-pixel colorhistograms. This intermediate output can be used to automatically generate acolor image, or further manipulated prior to image formation. On both fully andpartially automatic colorization tasks, we outperform existing methods. We alsoexplore colorization as a vehicle for self-supervised visual representationlearning.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (4), 577-593',\n",
       "  'citations': '375',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1603.06668v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=17700316019242317574&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 166: {'ID': 166,\n",
       "  'title': 'Generative Image Modeling using Style and Structure Adversarial Networks',\n",
       "  'authors': ['Abhinav Gupta', 'Xiaolong Wang'],\n",
       "  'published': '2016-03-17T19:33:20Z',\n",
       "  'updated': '2016-07-26T03:54:23Z',\n",
       "  'abstract': 'Current generative frameworks use end-to-end learning and generate images bysampling from uniform noise distribution. However, these approaches ignore themost basic principle of image formation: images are product of: (a) Structure:the underlying 3D model; (b) Style: the texture mapped onto structure. In thispaper, we factorize the image generation process and propose Style andStructure Generative Adversarial Network (S^2-GAN). Our S^2-GAN has twocomponents: the Structure-GAN generates a surface normal map; the Style-GANtakes the surface normal map as input and generates the 2D image. Apart from areal vs. generated loss function, we use an additional loss with computedsurface normals from generated images. The two GANs are first trainedindependently, and then merged together via joint learning. We show our S^2-GANmodel is interpretable, generates more realistic images and can be used tolearn unsupervised RGBD representations.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (4), 318-335',\n",
       "  'citations': '381',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1603.05631v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6379869390147689561&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 167: {'ID': 167,\n",
       "  'title': 'Visual7W: Grounded Question Answering in Images',\n",
       "  'authors': ['Li Fei-Fei', 'Michael Bernstein', 'Yuke Zhu', 'Oliver Groth'],\n",
       "  'published': '2015-11-11T08:29:14Z',\n",
       "  'updated': '2016-04-09T07:18:10Z',\n",
       "  'abstract': \"We have seen great progress in basic perceptual tasks such as objectrecognition and detection. However, AI models still fail to match humans inhigh-level vision tasks due to the lack of capacities for deeper reasoning.Recently the new task of visual question answering (QA) has been proposed toevaluate a model's capacity for deep image understanding. Previous works haveestablished a loose, global association between QA sentences and images.However, many questions and answers, in practice, relate to local regions inthe images. We establish a semantic link between textual descriptions and imageregions by object-level grounding. It enables a new type of QA with visualanswers, in addition to textual answers used in previous work. We study thevisual QA tasks in a grounded setting with a large collection of 7Wmultiple-choice QA pairs. Furthermore, we evaluate human performance andseveral baseline models on the QA tasks. Finally, we propose a novel LSTM modelwith spatial attention to tackle the 7W QA tasks.\",\n",
       "  'categories': ['cs.CV', 'cs.LG', 'cs.NE'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '442',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1511.03416v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=1446328645386125300&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 168: {'ID': 168,\n",
       "  'title': 'CBAM: Convolutional Block Attention Module',\n",
       "  'authors': ['Joon-Young Lee',\n",
       "   'Jongchan Park',\n",
       "   'In So Kweon',\n",
       "   'Sanghyun Woo'],\n",
       "  'published': '2018-07-17T16:05:59Z',\n",
       "  'updated': '2018-07-18T11:20:08Z',\n",
       "  'abstract': 'We propose Convolutional Block Attention Module (CBAM), a simple yeteffective attention module for feed-forward convolutional neural networks.Given an intermediate feature map, our module sequentially infers attentionmaps along two separate dimensions, channel and spatial, then the attentionmaps are multiplied to the input feature map for adaptive feature refinement.Because CBAM is a lightweight and general module, it can be integrated into anyCNN architectures seamlessly with negligible overheads and is end-to-endtrainable along with base CNNs. We validate our CBAM through extensiveexperiments on ImageNet-1K, MS~COCO detection, and VOC~2007 detection datasets.Our experiments show consistent improvements in classification and detectionperformances with various models, demonstrating the wide applicability of CBAM.The code and models will be publicly available.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 3-19',\n",
       "  'citations': '678',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1807.06521v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=57030359015830554&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 169: {'ID': 169,\n",
       "  'title': 'Structural-RNN: Deep Learning on Spatio-Temporal Graphs',\n",
       "  'authors': ['Ashesh Jain',\n",
       "   'Silvio Savarese',\n",
       "   'Amir R. Zamir',\n",
       "   'Ashutosh Saxena'],\n",
       "  'published': '2015-11-17T07:49:58Z',\n",
       "  'updated': '2016-04-11T19:00:24Z',\n",
       "  'abstract': 'Deep Recurrent Neural Network architectures, though remarkably capable atmodeling sequences, lack an intuitive high-level spatio-temporal structure.That is while many problems in computer vision inherently have an underlyinghigh-level structure and can benefit from it. Spatio-temporal graphs are apopular tool for imposing such high-level intuitions in the formulation of realworld problems. In this paper, we propose an approach for combining the powerof high-level spatio-temporal graphs and sequence learning success of RecurrentNeural Networks~(RNNs). We develop a scalable method for casting an arbitraryspatio-temporal graph as a rich RNN mixture that is feedforward, fullydifferentiable, and jointly trainable. The proposed method is generic andprincipled as it can be used for transforming any spatio-temporal graph throughemploying a certain set of well defined steps. The evaluations of the proposedapproach on a diverse set of problems, ranging from modeling human motion toobject interactions, shows improvement over the state-of-the-art with a largemargin. We expect this method to empower new approaches to problem formulationthrough high-level spatio-temporal graphs and Recurrent Neural Networks.',\n",
       "  'categories': ['cs.CV', 'cs.LG', 'cs.NE', 'cs.RO'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '433',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1511.05298v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=18108454464973674206&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 170: {'ID': 170,\n",
       "  'title': 'Flickr30k Entities: Collecting Region-to-Phrase Correspondences for  Richer Image-to-Sentence Models',\n",
       "  'authors': ['Bryan A. Plummer',\n",
       "   'Chris M. Cervantes',\n",
       "   'Julia Hockenmaier',\n",
       "   'Svetlana Lazebnik',\n",
       "   'Liwei Wang',\n",
       "   'Juan C. Caicedo'],\n",
       "  'published': '2015-05-19T04:46:03Z',\n",
       "  'updated': '2016-09-19T20:20:42Z',\n",
       "  'abstract': 'The Flickr30k dataset has become a standard benchmark for sentence-basedimage description. This paper presents Flickr30k Entities, which augments the158k captions from Flickr30k with 244k coreference chains, linking mentions ofthe same entities across different captions for the same image, and associatingthem with 276k manually annotated bounding boxes. Such annotations areessential for continued progress in automatic image description and groundedlanguage understanding. They enable us to define a new benchmark forlocalization of textual entity mentions in an image. We present a strongbaseline for this task that combines an image-text embedding, detectors forcommon objects, a color classifier, and a bias towards selecting largerobjects. While our baseline rivals in accuracy more complex state-of-the-artmodels, we show that its gains cannot be easily parlayed into improvements onsuch tasks as image-sentence retrieval, thus underlining the limitations ofcurrent methods and the need for further research.',\n",
       "  'categories': ['cs.CV', 'cs.CL'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 2641-2649',\n",
       "  'citations': '445',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1505.04870v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=7850289731690963804&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 171: {'ID': 171,\n",
       "  'title': 'Differential Recurrent Neural Networks for Action Recognition',\n",
       "  'authors': ['Vivek Veeriah', 'Guo-Jun Qi', 'Naifan Zhuang'],\n",
       "  'published': '2015-04-25T03:59:14Z',\n",
       "  'updated': '2015-04-25T03:59:14Z',\n",
       "  'abstract': 'The long short-term memory (LSTM) neural network is capable of processingcomplex sequential information since it utilizes special gating schemes forlearning representations from long input sequences. It has the potential tomodel any sequential time-series data, where the current hidden state has to beconsidered in the context of the past hidden states. This property makes LSTMan ideal choice to learn the complex dynamics of various actions.Unfortunately, the conventional LSTMs do not consider the impact ofspatio-temporal dynamics corresponding to the given salient motion patterns,when they gate the information that ought to be memorized through time. Toaddress this problem, we propose a differential gating scheme for the LSTMneural network, which emphasizes on the change in information gain caused bythe salient motions between the successive frames. This change in informationgain is quantified by Derivative of States (DoS), and thus the proposed LSTMmodel is termed as differential Recurrent Neural Network (dRNN). We demonstratethe effectiveness of the proposed model by automatically recognizing actionsfrom the real-world 2D and 3D human action datasets. Our study is one of thefirst works towards demonstrating the potential of learning complex time-seriesrepresentations via high-order derivatives of states.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 4041-4049',\n",
       "  'citations': '333',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1504.06678v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=11699479310351332027&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 172: {'ID': 172,\n",
       "  'title': 'Path Aggregation Network for Instance Segmentation',\n",
       "  'authors': ['Shu Liu', 'Jianping Shi', 'Haifang Qin', 'Lu Qi', 'Jiaya Jia'],\n",
       "  'published': '2018-03-05T07:46:36Z',\n",
       "  'updated': '2018-09-18T04:26:54Z',\n",
       "  'abstract': 'The way that information propagates in neural networks is of greatimportance. In this paper, we propose Path Aggregation Network (PANet) aimingat boosting information flow in proposal-based instance segmentation framework.Specifically, we enhance the entire feature hierarchy with accuratelocalization signals in lower layers by bottom-up path augmentation, whichshortens the information path between lower layers and topmost feature. Wepresent adaptive feature pooling, which links feature grid and all featurelevels to make useful information in each feature level propagate directly tofollowing proposal subnetworks. A complementary branch capturing differentviews for each proposal is created to further improve mask prediction. Theseimprovements are simple to implement, with subtle extra computational overhead.Our PANet reaches the 1st place in the COCO 2017 Challenge InstanceSegmentation task and the 2nd place in Object Detection task withoutlarge-batch training. It is also state-of-the-art on MVD and Cityscapes. Codeis available at https://github.com/ShuLiu1993/PANet',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '373',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1803.01534v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=8407059638791168431&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 173: {'ID': 173,\n",
       "  'title': 'Cross-domain Image Retrieval with a Dual Attribute-aware Ranking Network',\n",
       "  'authors': ['Shuicheng Yan',\n",
       "   'Junshi Huang',\n",
       "   'Qiang Chen',\n",
       "   'Rogerio S. Feris'],\n",
       "  'published': '2015-05-29T04:46:37Z',\n",
       "  'updated': '2015-05-29T04:46:37Z',\n",
       "  'abstract': 'We address the problem of cross-domain image retrieval, considering thefollowing practical application: given a user photo depicting a clothing image,our goal is to retrieve the same or attribute-similar clothing items fromonline shopping stores. This is a challenging problem due to the largediscrepancy between online shopping images, usually taken in ideallighting/pose/background conditions, and user photos captured in uncontrolledconditions. To address this problem, we propose a Dual Attribute-aware RankingNetwork (DARN) for retrieval feature learning. More specifically, DARN consistsof two sub-networks, one for each domain, whose retrieval featurerepresentations are driven by semantic attribute learning. We show that thisattribute-guided learning is a key factor for retrieval accuracy improvement.In addition, to further align with the nature of the retrieval problem, weimpose a triplet visual similarity constraint for learning to rank across thetwo sub-networks. Another contribution of our work is a large-scale datasetwhich makes the network learning feasible. We exploit customer review websitesto crawl a large set of online shopping images and corresponding offline userphotos with fine-grained clothing attributes, i.e., around 450,000 onlineshopping images and about 90,000 exact offline counterpart images of thoseonline ones. All these images are collected from real-world consumer websitesreflecting the diversity of the data modality, which makes this dataset uniqueand rare in the academic community. We extensively evaluate the retrievalperformance of networks in different configurations. The top-20 retrievalaccuracy is doubled when using the proposed DARN other than the current popularsolution using pre-trained CNN features only (0.570 vs. 0.268).',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 1062-1070',\n",
       "  'citations': '259',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1505.07922v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=16812097667139377249&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 174: {'ID': 174,\n",
       "  'title': 'One-Shot Video Object Segmentation',\n",
       "  'authors': ['Luc Van Gool',\n",
       "   'Daniel Cremers',\n",
       "   'Laura Leal-Taixé',\n",
       "   'Sergi Caelles',\n",
       "   'Jordi Pont-Tuset',\n",
       "   'Kevis-Kokitsi Maninis'],\n",
       "  'published': '2016-11-16T09:58:37Z',\n",
       "  'updated': '2017-04-13T08:08:55Z',\n",
       "  'abstract': 'This paper tackles the task of semi-supervised video object segmentation,i.e., the separation of an object from the background in a video, given themask of the first frame. We present One-Shot Video Object Segmentation (OSVOS),based on a fully-convolutional neural network architecture that is able tosuccessively transfer generic semantic information, learned on ImageNet, to thetask of foreground segmentation, and finally to learning the appearance of asingle annotated object of the test sequence (hence one-shot). Although allframes are processed independently, the results are temporally coherent andstable. We perform experiments on two annotated video segmentation databases,which show that OSVOS is fast and improves the state of the art by asignificant margin (79.8% vs 68.0%).',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '414',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1611.05198v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=1190653439359688733&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 175: {'ID': 175,\n",
       "  'title': 'Least Squares Generative Adversarial Networks',\n",
       "  'authors': ['Stephen Paul Smolley',\n",
       "   'Haoran Xie',\n",
       "   'Xudong Mao',\n",
       "   'Raymond Y. K. Lau',\n",
       "   'Qing Li',\n",
       "   'Zhen Wang'],\n",
       "  'published': '2016-11-13T03:38:28Z',\n",
       "  'updated': '2017-04-05T05:44:47Z',\n",
       "  'abstract': 'Unsupervised learning with generative adversarial networks (GANs) has provenhugely successful. Regular GANs hypothesize the discriminator as a classifierwith the sigmoid cross entropy loss function. However, we found that this lossfunction may lead to the vanishing gradients problem during the learningprocess. To overcome such a problem, we propose in this paper the Least SquaresGenerative Adversarial Networks (LSGANs) which adopt the least squares lossfunction for the discriminator. We show that minimizing the objective functionof LSGAN yields minimizing the Pearson $\\\\chi^2$ divergence. There are twobenefits of LSGANs over regular GANs. First, LSGANs are able to generate higherquality images than regular GANs. Second, LSGANs perform more stable during thelearning process. We evaluate LSGANs on five scene datasets and theexperimental results show that the images generated by LSGANs are of betterquality than the ones generated by regular GANs. We also conduct two comparisonexperiments between LSGANs and regular GANs to illustrate the stability ofLSGANs.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 2794-2802',\n",
       "  'citations': '1405',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1611.04076v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=10000224537675260362&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 176: {'ID': 176,\n",
       "  'title': 'Learning to Generate Chairs, Tables and Cars with Convolutional Networks',\n",
       "  'authors': ['Alexey Dosovitskiy',\n",
       "   'Thomas Brox',\n",
       "   'Maxim Tatarchenko',\n",
       "   'Jost Tobias Springenberg'],\n",
       "  'published': '2014-11-21T16:01:04Z',\n",
       "  'updated': '2017-08-02T20:53:43Z',\n",
       "  'abstract': \"We train generative 'up-convolutional' neural networks which are able togenerate images of objects given object style, viewpoint, and color. We trainthe networks on rendered 3D models of chairs, tables, and cars. Our experimentsshow that the networks do not merely learn all images by heart, but rather finda meaningful representation of 3D models allowing them to assess the similarityof different models, interpolate between given views to generate the missingones, extrapolate views, and invent new objects not present in the training setby recombining training instances, or even two different object classes.Moreover, we show that such generative networks can be used to findcorrespondences between different objects from the dataset, outperformingexisting approaches on this task.\",\n",
       "  'categories': ['cs.CV', 'cs.LG', 'cs.NE'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 39 (4), 692-705',\n",
       "  'citations': '148',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1411.5928v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=17979767425510316336&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 177: {'ID': 177,\n",
       "  'title': 'VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection',\n",
       "  'authors': ['Yin Zhou', 'Oncel Tuzel'],\n",
       "  'published': '2017-11-17T04:25:24Z',\n",
       "  'updated': '2017-11-17T04:25:24Z',\n",
       "  'abstract': \"Accurate detection of objects in 3D point clouds is a central problem in manyapplications, such as autonomous navigation, housekeeping robots, andaugmented/virtual reality. To interface a highly sparse LiDAR point cloud witha region proposal network (RPN), most existing efforts have focused onhand-crafted feature representations, for example, a bird's eye viewprojection. In this work, we remove the need of manual feature engineering for3D point clouds and propose VoxelNet, a generic 3D detection network thatunifies feature extraction and bounding box prediction into a single stage,end-to-end trainable deep network. Specifically, VoxelNet divides a point cloudinto equally spaced 3D voxels and transforms a group of points within eachvoxel into a unified feature representation through the newly introduced voxelfeature encoding (VFE) layer. In this way, the point cloud is encoded as adescriptive volumetric representation, which is then connected to a RPN togenerate detections. Experiments on the KITTI car detection benchmark show thatVoxelNet outperforms the state-of-the-art LiDAR based 3D detection methods by alarge margin. Furthermore, our network learns an effective discriminativerepresentation of objects with various geometries, leading to encouragingresults in 3D detection of pedestrians and cyclists, based on only LiDAR.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '605',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1711.06396v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=8219640496202186405&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 178: {'ID': 178,\n",
       "  'title': 'End-to-end representation learning for Correlation Filter based tracking',\n",
       "  'authors': ['Jack Valmadre',\n",
       "   'Philip H. S. Torr',\n",
       "   'Luca Bertinetto',\n",
       "   'João F. Henriques',\n",
       "   'Andrea Vedaldi'],\n",
       "  'published': '2017-04-20T07:51:27Z',\n",
       "  'updated': '2017-04-20T07:51:27Z',\n",
       "  'abstract': 'The Correlation Filter is an algorithm that trains a linear template todiscriminate between images and their translations. It is well suited to objecttracking because its formulation in the Fourier domain provides a fastsolution, enabling the detector to be re-trained once per frame. Previous worksthat use the Correlation Filter, however, have adopted features that wereeither manually designed or trained for a different task. This work is thefirst to overcome this limitation by interpreting the Correlation Filterlearner, which has a closed-form solution, as a differentiable layer in a deepneural network. This enables learning deep features that are tightly coupled tothe Correlation Filter. Experiments illustrate that our method has theimportant practical benefit of allowing lightweight architectures to achievestate-of-the-art performance at high framerates.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '675',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1704.06036v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=13208202265319570145&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 179: {'ID': 179,\n",
       "  'title': '3D-R2N2: A Unified Approach for Single and Multi-view 3D Object  Reconstruction',\n",
       "  'authors': ['JunYoung Gwak',\n",
       "   'Kevin Chen',\n",
       "   'Silvio Savarese',\n",
       "   'Christopher B. Choy',\n",
       "   'Danfei Xu'],\n",
       "  'published': '2016-04-02T01:28:27Z',\n",
       "  'updated': '2016-04-02T01:28:27Z',\n",
       "  'abstract': 'Inspired by the recent success of methods that employ shape priors to achieverobust 3D reconstructions, we propose a novel recurrent neural networkarchitecture that we call the 3D Recurrent Reconstruction Neural Network(3D-R2N2). The network learns a mapping from images of objects to theirunderlying 3D shapes from a large collection of synthetic data. Our networktakes in one or more images of an object instance from arbitrary viewpoints andoutputs a reconstruction of the object in the form of a 3D occupancy grid.Unlike most of the previous works, our network does not require any imageannotations or object class labels for training or testing. Our extensiveexperimental analysis shows that our reconstruction framework i) outperformsthe state-of-the-art methods for single view reconstruction, and ii) enablesthe 3D reconstruction of objects in situations when traditional SFM/SLAMmethods fail (because of lack of texture and/or wide baseline).',\n",
       "  'categories': ['cs.CV', 'cs.AI'],\n",
       "  'journal': 'ECCV (8), 628-644',\n",
       "  'citations': '591',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1604.00449v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=12676093382957955088&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 180: {'ID': 180,\n",
       "  'title': 'Large Kernel Matters -- Improve Semantic Segmentation by Global  Convolutional Network',\n",
       "  'authors': ['Xiangyu Zhang',\n",
       "   'Gang Yu',\n",
       "   'Guiming Luo',\n",
       "   'Chao Peng',\n",
       "   'Jian Sun'],\n",
       "  'published': '2017-03-08T06:14:55Z',\n",
       "  'updated': '2017-03-08T06:14:55Z',\n",
       "  'abstract': 'One of recent trends [30, 31, 14] in network architec- ture design isstacking small filters (e.g., 1x1 or 3x3) in the entire network because thestacked small filters is more ef- ficient than a large kernel, given the samecomputational complexity. However, in the field of semantic segmenta- tion,where we need to perform dense per-pixel prediction, we find that the largekernel (and effective receptive field) plays an important role when we have toperform the clas- sification and localization tasks simultaneously. Followingour design principle, we propose a Global Convolutional Network to address boththe classification and localization issues for the semantic segmentation. Wealso suggest a residual-based boundary refinement to further refine the ob-ject boundaries. Our approach achieves state-of-art perfor- mance on two publicbenchmarks and significantly outper- forms previous results, 82.2% (vs 80.2%)on PASCAL VOC 2012 dataset and 76.9% (vs 71.8%) on Cityscapes dataset.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '555',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1703.02719v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=17980729144178402832&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 181: {'ID': 181,\n",
       "  'title': 'Distractor-aware Siamese Networks for Visual Object Tracking',\n",
       "  'authors': ['Junjie Yan',\n",
       "   'Bo Li',\n",
       "   'Zheng Zhu',\n",
       "   'Qiang Wang',\n",
       "   'Weiming Hu',\n",
       "   'Wei Wu'],\n",
       "  'published': '2018-08-18T06:38:10Z',\n",
       "  'updated': '2018-08-18T06:38:10Z',\n",
       "  'abstract': 'Recently, Siamese networks have drawn great attention in visual trackingcommunity because of their balanced accuracy and speed. However, features usedin most Siamese tracking approaches can only discriminate foreground from thenon-semantic backgrounds. The semantic backgrounds are always considered asdistractors, which hinders the robustness of Siamese trackers. In this paper,we focus on learning distractor-aware Siamese networks for accurate andlong-term tracking. To this end, features used in traditional Siamese trackersare analyzed at first. We observe that the imbalanced distribution of trainingdata makes the learned features less discriminative. During the off-linetraining phase, an effective sampling strategy is introduced to control thisdistribution and make the model focus on the semantic distractors. Duringinference, a novel distractor-aware module is designed to perform incrementallearning, which can effectively transfer the general embedding to the currentvideo domain. In addition, we extend the proposed approach for long-termtracking by introducing a simple yet effective local-to-global search regionstrategy. Extensive experiments on benchmarks show that our approachsignificantly outperforms the state-of-the-arts, yielding 9.6% relative gain inVOT2016 dataset and 35.9% relative gain in UAV20L dataset. The proposed trackercan perform at 160 FPS on short-term benchmarks and 110 FPS on long-termbenchmarks.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 101-117',\n",
       "  'citations': '244',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1808.06048v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5659298886572595606&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 182: {'ID': 182,\n",
       "  'title': 'ShuffleNet: An Extremely Efficient Convolutional Neural Network for  Mobile Devices',\n",
       "  'authors': ['Mengxiao Lin', 'Xiangyu Zhang', 'Jian Sun', 'Xinyu Zhou'],\n",
       "  'published': '2017-07-04T17:42:58Z',\n",
       "  'updated': '2017-12-07T18:06:34Z',\n",
       "  'abstract': 'We introduce an extremely computation-efficient CNN architecture namedShuffleNet, which is designed specially for mobile devices with very limitedcomputing power (e.g., 10-150 MFLOPs). The new architecture utilizes two newoperations, pointwise group convolution and channel shuffle, to greatly reducecomputation cost while maintaining accuracy. Experiments on ImageNetclassification and MS COCO object detection demonstrate the superiorperformance of ShuffleNet over other structures, e.g. lower top-1 error(absolute 7.8%) than recent MobileNet on ImageNet classification task, underthe computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNetachieves ~13x actual speedup over AlexNet while maintaining comparableaccuracy.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '1392',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1707.01083v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6469340744827368429&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 183: {'ID': 183,\n",
       "  'title': 'Modeling Context in Referring Expressions',\n",
       "  'authors': ['Alexander C. Berg',\n",
       "   'Patrick Poirson',\n",
       "   'Licheng Yu',\n",
       "   'Tamara L. Berg',\n",
       "   'Shan Yang'],\n",
       "  'published': '2016-07-31T22:21:42Z',\n",
       "  'updated': '2016-08-10T19:01:37Z',\n",
       "  'abstract': 'Humans refer to objects in their environments all the time, especially indialogue with other people. We explore generating and comprehending naturallanguage referring expressions for objects in images. In particular, we focuson incorporating better measures of visual context into referring expressionmodels and find that visual comparison to other objects within an image helpsimprove performance significantly. We also develop methods to tie the languagegeneration process together, so that we generate expressions for all objects ofa particular category jointly. Evaluation on three recent datasets - RefCOCO,RefCOCO+, and RefCOCOg, shows the advantages of our methods for both referringexpression generation and comprehension.',\n",
       "  'categories': ['cs.CV', 'cs.CL'],\n",
       "  'journal': 'ECCV (2), 69-85',\n",
       "  'citations': '178',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1608.00272v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=15846549089633334863&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 184: {'ID': 184,\n",
       "  'title': 'Learning Deep Feature Representations with Domain Guided Dropout for  Person Re-identification',\n",
       "  'authors': ['Wanli Ouyang', 'Hongsheng Li', 'Xiaogang Wang', 'Tong Xiao'],\n",
       "  'published': '2016-04-26T05:39:53Z',\n",
       "  'updated': '2016-04-26T05:39:53Z',\n",
       "  'abstract': 'Learning generic and robust feature representations with data from multipledomains for the same problem is of great value, especially for the problemsthat have multiple datasets but none of them are large enough to provideabundant data variations. In this work, we present a pipeline for learning deepfeature representations from multiple domains with Convolutional NeuralNetworks (CNNs). When training a CNN with data from all the domains, someneurons learn representations shared across several domains, while some othersare effective only for a specific one. Based on this important observation, wepropose a Domain Guided Dropout algorithm to improve the feature learningprocedure. Experiments show the effectiveness of our pipeline and the proposedalgorithm. Our methods on the person re-identification problem outperformstate-of-the-art methods on multiple datasets by large margins.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '707',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1604.07528v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14693788551611879825&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 185: {'ID': 185,\n",
       "  'title': 'Survey on RGB, 3D, Thermal, and Multimodal Approaches for Facial  Expression Recognition: History, Trends, and Affect-related Applications',\n",
       "  'authors': ['Ciprian Corneanu',\n",
       "   'Marc Oliu',\n",
       "   'Jeffrey F. Cohn',\n",
       "   'Sergio Escalera'],\n",
       "  'published': '2016-06-10T09:12:05Z',\n",
       "  'updated': '2016-06-10T09:12:05Z',\n",
       "  'abstract': 'Facial expressions are an important way through which humans interactsocially. Building a system capable of automatically recognizing facialexpressions from images and video has been an intense field of study in recentyears. Interpreting such expressions remains challenging and much research isneeded about the way they relate to human affect. This paper presents a generaloverview of automatic RGB, 3D, thermal and multimodal facial expressionanalysis. We define a new taxonomy for the field, encompassing all steps fromface detection to facial expression recognition, and describe and classify thestate of the art methods accordingly. We also present the important datasetsand the bench-marking of most influential methods. We conclude with a generaldiscussion about trends, important questions and future lines of research.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 38 (8), 1548-1568',\n",
       "  'citations': '300',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1606.03237v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2032446979764600962&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 186: {'ID': 186,\n",
       "  'title': 'Beyond Face Rotation: Global and Local Perception GAN for Photorealistic  and Identity Preserving Frontal View Synthesis',\n",
       "  'authors': ['Rui Huang', 'Ran He', 'Tianyu Li', 'Shu Zhang'],\n",
       "  'published': '2017-04-13T12:18:13Z',\n",
       "  'updated': '2017-08-04T03:44:37Z',\n",
       "  'abstract': 'Photorealistic frontal view synthesis from a single face image has a widerange of applications in the field of face recognition. Although data-drivendeep learning methods have been proposed to address this problem by seekingsolutions from ample face data, this problem is still challenging because it isintrinsically ill-posed. This paper proposes a Two-Pathway GenerativeAdversarial Network (TP-GAN) for photorealistic frontal view synthesis bysimultaneously perceiving global structures and local details. Four landmarklocated patch networks are proposed to attend to local textures in addition tothe commonly used global encoder-decoder network. Except for the novelarchitecture, we make this ill-posed problem well constrained by introducing acombination of adversarial loss, symmetry loss and identity preserving loss.The combined loss function leverages both frontal face distribution andpre-trained discriminative deep face models to guide an identity preservinginference of frontal views from profiles. Different from previous deep learningmethods that mainly rely on intermediate features for recognition, our methoddirectly leverages the synthesized identity preserving image for downstreamtasks like face recognition and attribution estimation. Experimental resultsdemonstrate that our method not only presents compelling perceptual results butalso outperforms state-of-the-art results on large pose face recognition.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 2439-2448',\n",
       "  'citations': '322',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1704.04086v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=15204586527488059616&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 187: {'ID': 187,\n",
       "  'title': 'Understanding and Diagnosing Visual Tracking Systems',\n",
       "  'authors': ['Jiaya Jia', 'Dit-Yan Yeung', 'Jianping Shi', 'Naiyan Wang'],\n",
       "  'published': '2015-04-23T06:37:29Z',\n",
       "  'updated': '2015-04-23T06:37:29Z',\n",
       "  'abstract': 'Several benchmark datasets for visual tracking research have been proposed inrecent years. Despite their usefulness, whether they are sufficient forunderstanding and diagnosing the strengths and weaknesses of different trackersremains questionable. To address this issue, we propose a framework by breakinga tracker down into five constituent parts, namely, motion model, featureextractor, observation model, model updater, and ensemble post-processor. Wethen conduct ablative experiments on each component to study how it affects theoverall result. Surprisingly, our findings are discrepant with some commonbeliefs in the visual tracking research community. We find that the featureextractor plays the most important role in a tracker. On the other hand,although the observation model is the focus of many studies, we find that itoften brings no significant improvement. Moreover, the motion model and modelupdater contain many details that could affect the result. Also, the ensemblepost-processor can improve the result substantially when the constituenttrackers have high diversity. Based on our findings, we put together some veryelementary building blocks to give a basic tracker which is competitive inperformance to the state-of-the-art trackers. We believe our framework canprovide a solid baseline when conducting controlled experiments for visualtracking research.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 3101-3109',\n",
       "  'citations': '291',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1504.06055v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=368796726239543774&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 188: {'ID': 188,\n",
       "  'title': 'MoCoGAN: Decomposing Motion and Content for Video Generation',\n",
       "  'authors': ['Jan Kautz', 'Sergey Tulyakov', 'Ming-Yu Liu', 'Xiaodong Yang'],\n",
       "  'published': '2017-07-17T03:42:17Z',\n",
       "  'updated': '2017-12-14T00:04:02Z',\n",
       "  'abstract': 'Visual signals in a video can be divided into content and motion. Whilecontent specifies which objects are in the video, motion describes theirdynamics. Based on this prior, we propose the Motion and Content decomposedGenerative Adversarial Network (MoCoGAN) framework for video generation. Theproposed framework generates a video by mapping a sequence of random vectors toa sequence of video frames. Each random vector consists of a content part and amotion part. While the content part is kept fixed, the motion part is realizedas a stochastic process. To learn motion and content decomposition in anunsupervised manner, we introduce a novel adversarial learning scheme utilizingboth image and video discriminators. Extensive experimental results on severalchallenging datasets with qualitative and quantitative comparison to thestate-of-the-art approaches, verify effectiveness of the proposed framework. Inaddition, we show that MoCoGAN allows one to generate videos with same contentbut different motion as well as videos with different content and same motion.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '314',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1707.04993v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5549182152929176563&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 189: {'ID': 189,\n",
       "  'title': 'Learning Two-Branch Neural Networks for Image-Text Matching Tasks',\n",
       "  'authors': ['Yin Li', 'Liwei Wang', 'Svetlana Lazebnik', 'Jing Huang'],\n",
       "  'published': '2017-04-11T18:00:25Z',\n",
       "  'updated': '2018-05-01T18:21:59Z',\n",
       "  'abstract': 'Image-language matching tasks have recently attracted a lot of attention inthe computer vision field. These tasks include image-sentence matching, i.e.,given an image query, retrieving relevant sentences and vice versa, andregion-phrase matching or visual grounding, i.e., matching a phrase to relevantregions. This paper investigates two-branch neural networks for learning thesimilarity between these two data modalities. We propose two network structuresthat produce different output representations. The first one, referred to as anembedding network, learns an explicit shared latent embedding space with amaximum-margin ranking loss and novel neighborhood constraints. Compared tostandard triplet sampling, we perform improved neighborhood sampling that takesneighborhood information into consideration while constructing mini-batches.The second network structure, referred to as a similarity network, fuses thetwo branches via element-wise product and is trained with regression loss todirectly predict a similarity score. Extensive experiments show that ournetworks achieve high accuracies for phrase localization on the Flickr30KEntities dataset and for bi-directional image-sentence retrieval on Flickr30Kand MSCOCO datasets.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 41 (2), 394-407',\n",
       "  'citations': '170',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1704.03470v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6105371047266812619&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 190: {'ID': 190,\n",
       "  'title': 'Gated Siamese Convolutional Neural Network Architecture for Human  Re-Identification',\n",
       "  'authors': ['Gang Wang', 'Rahul Rama Varior', 'Mrinal Haloi'],\n",
       "  'published': '2016-07-28T09:40:18Z',\n",
       "  'updated': '2016-09-26T16:28:58Z',\n",
       "  'abstract': 'Matching pedestrians across multiple camera views, known as humanre-identification, is a challenging research problem that has numerousapplications in visual surveillance. With the resurgence of ConvolutionalNeural Networks (CNNs), several end-to-end deep Siamese CNN architectures havebeen proposed for human re-identification with the objective of projecting theimages of similar pairs (i.e. same identity) to be closer to each other andthose of dissimilar pairs to be distant from each other. However, currentnetworks extract fixed representations for each image regardless of otherimages which are paired with it and the comparison with other images is doneonly at the final level. In this setting, the network is at risk of failing toextract finer local patterns that may be essential to distinguish positivepairs from hard negative pairs. In this paper, we propose a gating function toselectively emphasize such fine common local patterns by comparing themid-level features across pairs of images. This produces flexiblerepresentations for the same image according to the images they are pairedwith. We conduct experiments on the CUHK03, Market-1501 and VIPeR datasets anddemonstrate improved performance compared to a baseline Siamese CNNarchitecture.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (8), 791-808',\n",
       "  'citations': '546',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1607.08378v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=7406142390965719882&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 191: {'ID': 191,\n",
       "  'title': 'StackGAN++: Realistic Image Synthesis with Stacked Generative  Adversarial Networks',\n",
       "  'authors': ['Dimitris Metaxas',\n",
       "   'Shaoting Zhang',\n",
       "   'Xiaogang Wang',\n",
       "   'Hongsheng Li',\n",
       "   'Tao Xu',\n",
       "   'Han Zhang',\n",
       "   'Xiaolei Huang'],\n",
       "  'published': '2017-10-19T18:45:59Z',\n",
       "  'updated': '2018-06-28T00:49:19Z',\n",
       "  'abstract': 'Although Generative Adversarial Networks (GANs) have shown remarkable successin various tasks, they still face challenges in generating high quality images.In this paper, we propose Stacked Generative Adversarial Networks (StackGAN)aiming at generating high-resolution photo-realistic images. First, we proposea two-stage generative adversarial network architecture, StackGAN-v1, fortext-to-image synthesis. The Stage-I GAN sketches the primitive shape andcolors of the object based on given text description, yielding low-resolutionimages. The Stage-II GAN takes Stage-I results and text descriptions as inputs,and generates high-resolution images with photo-realistic details. Second, anadvanced multi-stage generative adversarial network architecture, StackGAN-v2,is proposed for both conditional and unconditional generative tasks. OurStackGAN-v2 consists of multiple generators and discriminators in a tree-likestructure; images at multiple scales corresponding to the same scene aregenerated from different branches of the tree. StackGAN-v2 shows more stabletraining behavior than StackGAN-v1 by jointly approximating multipledistributions. Extensive experiments demonstrate that the proposed stackedgenerative adversarial networks significantly outperform other state-of-the-artmethods in generating photo-realistic images.',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'stat.ML'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 41 (8), 1947-1962',\n",
       "  'citations': '255',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1710.10916v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5251688290326540457&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 192: {'ID': 192,\n",
       "  'title': 'Contextual Action Recognition with R*CNN',\n",
       "  'authors': ['Georgia Gkioxari', 'Jitendra Malik', 'Ross Girshick'],\n",
       "  'published': '2015-05-05T21:56:10Z',\n",
       "  'updated': '2016-03-25T01:06:01Z',\n",
       "  'abstract': 'There are multiple cues in an image which reveal what action a person isperforming. For example, a jogger has a pose that is characteristic forjogging, but the scene (e.g. road, trail) and the presence of other joggers canbe an additional source of information. In this work, we exploit the simpleobservation that actions are accompanied by contextual cues to build a strongaction recognition system. We adapt RCNN to use more than one region forclassification while still maintaining the ability to localize the action. Wecall our system R*CNN. The action-specific models and the feature maps aretrained jointly, allowing for action specific representations to emerge. R*CNNachieves 90.2% mean AP on the PASAL VOC Action dataset, outperforming all otherapproaches in the field by a significant margin. Last, we show that R*CNN isnot limited to action recognition. In particular, R*CNN can also be used totackle fine-grained tasks such as attribute classification. We validate thisclaim by reporting state-of-the-art performance on the Berkeley Attributes ofPeople dataset.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 1080-1088',\n",
       "  'citations': '279',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1505.01197v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2293931322701475482&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 193: {'ID': 193,\n",
       "  'title': 'From Image-level to Pixel-level Labeling with Convolutional Networks',\n",
       "  'authors': ['Pedro O. Pinheiro', 'Ronan Collobert'],\n",
       "  'published': '2014-11-23T12:06:36Z',\n",
       "  'updated': '2015-04-24T07:26:01Z',\n",
       "  'abstract': 'We are interested in inferring object segmentation by leveraging only objectclass information, and by considering only minimal priors on the objectsegmentation task. This problem could be viewed as a kind of weakly supervisedsegmentation task, and naturally fits the Multiple Instance Learning (MIL)framework: every training image is known to have (or not) at least one pixelcorresponding to the image class label, and the segmentation task can berewritten as inferring the pixels belonging to the class of the object (givenone image, and its object class). We propose a Convolutional NeuralNetwork-based model, which is constrained during training to put more weight onpixels which are important for classifying the image. We show that at testtime, the model has learned to discriminate the right pixels well enough, suchthat it performs very well on an existing segmentation benchmark, by addingonly few smoothing priors. Our system is trained using a subset of the Imagenetdataset and the segmentation experiments are performed on the challengingPascal VOC dataset (with no fine-tuning of the model on Pascal VOC). Our modelbeats the state of the art results in weakly supervised object segmentationtask by a large margin. We also compare the performance of our model with stateof the art fully-supervised segmentation approaches.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '393',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1411.6228v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=3976557999693641396&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 194: {'ID': 194,\n",
       "  'title': 'Multi-view 3D Models from Single Images with a Convolutional Network',\n",
       "  'authors': ['Alexey Dosovitskiy', 'Maxim Tatarchenko', 'Thomas Brox'],\n",
       "  'published': '2015-11-20T17:34:17Z',\n",
       "  'updated': '2016-08-02T10:14:07Z',\n",
       "  'abstract': 'We present a convolutional network capable of inferring a 3D representationof a previously unseen object given a single image of this object. Concretely,the network can predict an RGB image and a depth map of the object as seen froman arbitrary view. Several of these depth maps fused together give a full pointcloud of the object. The point cloud can in turn be transformed into a surfacemesh. The network is trained on renderings of synthetic 3D models of cars andchairs. It successfully deals with objects on cluttered background andgenerates reasonable predictions for real images of cars.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (7), 322-337',\n",
       "  'citations': '225',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1511.06702v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=10769239376973109797&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 195: {'ID': 195,\n",
       "  'title': 'Learning to Estimate 3D Hand Pose from Single RGB Images',\n",
       "  'authors': ['Christian Zimmermann', 'Thomas Brox'],\n",
       "  'published': '2017-05-03T12:50:18Z',\n",
       "  'updated': '2017-10-15T15:52:51Z',\n",
       "  'abstract': 'Low-cost consumer depth cameras and deep learning have enabled reasonable 3Dhand pose estimation from single depth images. In this paper, we present anapproach that estimates 3D hand pose from regular RGB images. This task has farmore ambiguities due to the missing depth information. To this end, we proposea deep network that learns a network-implicit 3D articulation prior. Togetherwith detected keypoints in the images, this network yields good estimates ofthe 3D pose. We introduce a large scale 3D hand pose dataset based on synthetichand models for training the involved networks. Experiments on a variety oftest sets, including one on sign language recognition, demonstrate thefeasibility of 3D hand pose estimation on single color images.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 4903-4911',\n",
       "  'citations': '222',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1705.01389v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=8391612928924567341&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 196: {'ID': 196,\n",
       "  'title': 'Multi-Oriented Text Detection with Fully Convolutional Networks',\n",
       "  'authors': ['Wei Shen',\n",
       "   'Wenyu Liu',\n",
       "   'Zheng Zhang',\n",
       "   'Cong Yao',\n",
       "   'Xiang Bai',\n",
       "   'Chengquan Zhang'],\n",
       "  'published': '2016-04-14T02:37:05Z',\n",
       "  'updated': '2016-04-16T13:22:03Z',\n",
       "  'abstract': 'In this paper, we propose a novel approach for text detec- tion in naturalimages. Both local and global cues are taken into account for localizing textlines in a coarse-to-fine pro- cedure. First, a Fully Convolutional Network(FCN) model is trained to predict the salient map of text regions in a holisticmanner. Then, text line hypotheses are estimated by combining the salient mapand character components. Fi- nally, another FCN classifier is used to predictthe centroid of each character, in order to remove the false hypotheses. Theframework is general for handling text in multiple ori- entations, languagesand fonts. The proposed method con- sistently achieves the state-of-the-artperformance on three text detection benchmarks: MSRA-TD500, ICDAR2015 andICDAR2013.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '347',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1604.04018v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=4660690942997100394&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 197: {'ID': 197,\n",
       "  'title': 'EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical  Flow',\n",
       "  'authors': ['Cordelia Schmid',\n",
       "   'Zaid Harchaoui',\n",
       "   'Jerome Revaud',\n",
       "   'Philippe Weinzaepfel'],\n",
       "  'published': '2015-01-12T08:19:09Z',\n",
       "  'updated': '2015-05-19T14:46:16Z',\n",
       "  'abstract': 'We propose a novel approach for optical flow estimation , targeted at largedisplacements with significant oc-clusions. It consists of two steps: i) densematching by edge-preserving interpolation from a sparse set of matches; ii)variational energy minimization initialized with the dense matches. Thesparse-to-dense interpolation relies on an appropriate choice of the distance,namely an edge-aware geodesic distance. This distance is tailored to handleocclusions and motion boundaries -- two common and difficult issues for opticalflow computation. We also propose an approximation scheme for the geodesicdistance to allow fast computation without loss of performance. Subsequent tothe dense interpolation step, standard one-level variational energyminimization is carried out on the dense matches to obtain the final flowestimation. The proposed approach, called Edge-Preserving Interpolation ofCorrespondences (EpicFlow) is fast and robust to large displacements. Itsignificantly outperforms the state of the art on MPI-Sintel and performs onpar on Kitti and Middlebury.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '604',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1501.02565v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5600914496244269397&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 198: {'ID': 198,\n",
       "  'title': 'Deep Human Parsing with Active Template Regression',\n",
       "  'authors': ['Xiaodan Liang',\n",
       "   'Luoqi Liu',\n",
       "   'Xiaohui Shen',\n",
       "   'Jian Dong',\n",
       "   'Jianchao Yang',\n",
       "   'Liang Lin',\n",
       "   'Si Liu',\n",
       "   'Shuicheng Yan'],\n",
       "  'published': '2015-03-09T08:14:12Z',\n",
       "  'updated': '2015-03-09T08:14:12Z',\n",
       "  'abstract': 'In this work, the human parsing task, namely decomposing a human image intosemantic fashion/body regions, is formulated as an Active Template Regression(ATR) problem, where the normalized mask of each fashion/body item is expressedas the linear combination of the learned mask templates, and then morphed to amore precise mask with the active shape parameters, including position, scaleand visibility of each semantic region. The mask template coefficients and theactive shape parameters together can generate the human parsing results, andare thus called the structure outputs for human parsing. The deep ConvolutionalNeural Network (CNN) is utilized to build the end-to-end relation between theinput human image and the structure outputs for human parsing. Morespecifically, the structure outputs are predicted by two separate networks. Thefirst CNN network is with max-pooling, and designed to predict the templatecoefficients for each label mask, while the second CNN network is withoutmax-pooling to preserve sensitivity to label mask position and accuratelypredict the active shape parameters. For a new image, the structure outputs ofthe two networks are fused to generate the probability of each label for eachpixel, and super-pixel smoothing is finally used to refine the human parsingresult. Comprehensive evaluations on a large dataset well demonstrate thesignificant superiority of the ATR framework over other state-of-the-arts forhuman parsing. In particular, the F1-score reaches $64.38\\\\%$ by our ATRframework, significantly higher than $44.76\\\\%$ based on the state-of-the-artalgorithm.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 37 (12), 2402\\xa0…',\n",
       "  'citations': '162',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1503.02391v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=7979281404335784312&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 199: {'ID': 199,\n",
       "  'title': 'Compression Artifacts Reduction by a Deep Convolutional Network',\n",
       "  'authors': ['Xiaoou Tang', 'Chao Dong', 'Yubin Deng', 'Chen Change Loy'],\n",
       "  'published': '2015-04-27T09:30:30Z',\n",
       "  'updated': '2015-04-27T09:30:30Z',\n",
       "  'abstract': 'Lossy compression introduces complex compression artifacts, particularly theblocking artifacts, ringing effects and blurring. Existing algorithms eitherfocus on removing blocking artifacts and produce blurred output, or restoressharpened images that are accompanied with ringing effects. Inspired by thedeep convolutional networks (DCN) on super-resolution, we formulate a compactand efficient network for seamless attenuation of different compressionartifacts. We also demonstrate that a deeper model can be effectively trainedwith the features learned in a shallow network. Following a similar \"easy tohard\" idea, we systematically investigate several practical transfer settingsand show the effectiveness of transfer learning in low-level vision problems.Our method shows superior performance than the state-of-the-arts both on thebenchmark datasets and the real-world use case (i.e. Twitter). In addition, weshow that our method can be applied as pre-processing to facilitate otherlow-level vision routines when they take compressed images as input.',\n",
       "  'categories': ['cs.CV', 'I.4.5; I.2.6'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 576-584',\n",
       "  'citations': '382',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1504.06993v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2364220051429841371&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 200: {'ID': 200,\n",
       "  'title': 'Semantic Image Inpainting with Deep Generative Models',\n",
       "  'authors': ['Alexander G. Schwing',\n",
       "   'Minh N. Do',\n",
       "   'Chen Chen',\n",
       "   'Teck Yian Lim',\n",
       "   'Mark Hasegawa-Johnson',\n",
       "   'Raymond A. Yeh'],\n",
       "  'published': '2016-07-26T04:52:48Z',\n",
       "  'updated': '2017-07-13T16:29:21Z',\n",
       "  'abstract': 'Semantic image inpainting is a challenging task where large missing regionshave to be filled based on the available visual data. Existing methods whichextract information from only a single image generally produce unsatisfactoryresults due to the lack of high level context. In this paper, we propose anovel method for semantic image inpainting, which generates the missing contentby conditioning on the available data. Given a trained generative model, wesearch for the closest encoding of the corrupted image in the latent imagemanifold using our context and prior losses. This encoding is then passedthrough the generative model to infer the missing content. In our method,inference is possible irrespective of how the missing content is structured,while the state-of-the-art learning based method requires specific informationabout the holes in the training phase. Experiments on three datasets show thatour method successfully predicts information in large missing regions andachieves pixel-level photorealism, significantly outperforming thestate-of-the-art methods.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '524',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1607.07539v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=16696847548454034293&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 201: {'ID': 201,\n",
       "  'title': 'Pose-Normalized Image Generation for Person Re-identification',\n",
       "  'authors': ['Yang Wu',\n",
       "   'Xiangyang Xue',\n",
       "   'Tao Xiang',\n",
       "   'Xuelin Qian',\n",
       "   'Jie Qiu',\n",
       "   'Wenxuan Wang',\n",
       "   'Yanwei Fu',\n",
       "   'Yu-Gang Jiang'],\n",
       "  'published': '2017-12-06T15:18:53Z',\n",
       "  'updated': '2018-04-25T05:57:05Z',\n",
       "  'abstract': 'Person Re-identification (re-id) faces two major challenges: the lack ofcross-view paired training data and learning discriminative identity-sensitiveand view-invariant features in the presence of large pose variations. In thiswork, we address both problems by proposing a novel deep person imagegeneration model for synthesizing realistic person images conditional on thepose. The model is based on a generative adversarial network (GAN) designedspecifically for pose normalization in re-id, thus termed pose-normalizationGAN (PN-GAN). With the synthesized images, we can learn a new type of deepre-id feature free of the influence of pose variations. We show that thisfeature is strong on its own and complementary to features learned with theoriginal images. Importantly, under the transfer learning setting, we show thatour model generalizes well to any new re-id dataset without the need forcollecting any training data for model fine-tuning. The model thus has thepotential to make re-id model truly scalable.',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.MM', 'stat.ML'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 650-667',\n",
       "  'citations': '148',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1712.02225v6',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=15077486839262294602&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 202: {'ID': 202,\n",
       "  'title': 'Drawing and Recognizing Chinese Characters with Recurrent Neural Network',\n",
       "  'authors': ['Cheng-Lin Liu',\n",
       "   'Yoshua Bengio',\n",
       "   'Xu-Yao Zhang',\n",
       "   'Fei Yin',\n",
       "   'Yan-Ming Zhang'],\n",
       "  'published': '2016-06-21T12:35:31Z',\n",
       "  'updated': '2016-06-21T12:35:31Z',\n",
       "  'abstract': 'Recent deep learning based approaches have achieved great success onhandwriting recognition. Chinese characters are among the most widely adoptedwriting systems in the world. Previous research has mainly focused onrecognizing handwritten Chinese characters. However, recognition is only oneaspect for understanding a language, another challenging and interesting taskis to teach a machine to automatically write (pictographic) Chinese characters.In this paper, we propose a framework by using the recurrent neural network(RNN) as both a discriminative model for recognizing Chinese characters and agenerative model for drawing (generating) Chinese characters. To recognizeChinese characters, previous methods usually adopt the convolutional neuralnetwork (CNN) models which require transforming the online handwritingtrajectory into image-like representations. Instead, our RNN based approach isan end-to-end system which directly deals with the sequential structure anddoes not require any domain-specific knowledge. With the RNN system (combiningan LSTM and GRU), state-of-the-art performance can be achieved on theICDAR-2013 competition database. Furthermore, under the RNN framework, aconditional generative model with character embedding is proposed forautomatically drawing recognizable Chinese characters. The generated characters(in vector format) are human-readable and also can be recognized by thediscriminative RNN model with high accuracy. Experimental results verify theeffectiveness of using RNNs as both generative and discriminative models forthe tasks of drawing and recognizing Chinese characters.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 40 (4), 849-862',\n",
       "  'citations': '157',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1606.06539v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=3206241657136519344&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 203: {'ID': 203,\n",
       "  'title': 'Learning efficient sparse and low rank models',\n",
       "  'authors': ['Pablo Sprechmann', 'Guillermo Sapiro', 'Alex M. Bronstein'],\n",
       "  'published': '2012-12-14T22:50:44Z',\n",
       "  'updated': '2012-12-14T22:50:44Z',\n",
       "  'abstract': 'Parsimony, including sparsity and low rank, has been shown to successfullymodel data in numerous machine learning and signal processing tasks.Traditionally, such modeling approaches rely on an iterative algorithm thatminimizes an objective function with parsimony-promoting terms. The inherentlysequential structure and data-dependent complexity and latency of iterativeoptimization constitute a major limitation in many applications requiringreal-time performance or involving large-scale data. Another limitationencountered by these modeling techniques is the difficulty of their inclusionin discriminative learning scenarios. In this work, we propose to move theemphasis from the model to the pursuit algorithm, and develop a process-centricview of parsimonious modeling, in which a learned deterministicfixed-complexity pursuit process is used in lieu of iterative optimization. Weshow a principled way to construct learnable pursuit process architectures forstructured sparse and robust low rank models, derived from the iteration ofproximal descent algorithms. These architectures learn to approximate the exactparsimonious representation at a fraction of the complexity of the standardoptimization methods. We also show that appropriate training regimes allow tonaturally extend parsimonious models to discriminative settings.State-of-the-art results are demonstrated on several challenging problems inimage and audio processing with several orders of magnitude speedup compared tothe exact optimization algorithms.',\n",
       "  'categories': ['cs.LG'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 37 (9), 1821-1833',\n",
       "  'citations': '142',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1212.3631v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=1323632366206716921&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 204: {'ID': 204,\n",
       "  'title': 'RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic  Segmentation',\n",
       "  'authors': ['Chunhua Shen', 'Guosheng Lin', 'Anton Milan', 'Ian Reid'],\n",
       "  'published': '2016-11-20T23:39:52Z',\n",
       "  'updated': '2016-11-25T02:01:05Z',\n",
       "  'abstract': 'Recently, very deep convolutional neural networks (CNNs) have shownoutstanding performance in object recognition and have also been the firstchoice for dense classification problems such as semantic segmentation.However, repeated subsampling operations like pooling or convolution stridingin deep CNNs lead to a significant decrease in the initial image resolution.Here, we present RefineNet, a generic multi-path refinement network thatexplicitly exploits all the information available along the down-samplingprocess to enable high-resolution prediction using long-range residualconnections. In this way, the deeper layers that capture high-level semanticfeatures can be directly refined using fine-grained features from earlierconvolutions. The individual components of RefineNet employ residualconnections following the identity mapping mindset, which allows for effectiveend-to-end training. Further, we introduce chained residual pooling, whichcaptures rich background context in an efficient manner. We carry outcomprehensive experiments and set new state-of-the-art results on seven publicdatasets. In particular, we achieve an intersection-over-union score of 83.4 onthe challenging PASCAL VOC 2012 dataset, which is the best reported result todate.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '1135',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1611.06612v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=9682999547844662749&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 205: {'ID': 205,\n",
       "  'title': 'Learning Spatiotemporal Features with 3D Convolutional Networks',\n",
       "  'authors': ['Lorenzo Torresani',\n",
       "   'Manohar Paluri',\n",
       "   'Lubomir Bourdev',\n",
       "   'Rob Fergus',\n",
       "   'Du Tran'],\n",
       "  'published': '2014-12-02T03:05:54Z',\n",
       "  'updated': '2015-10-07T01:29:12Z',\n",
       "  'abstract': 'We propose a simple, yet effective approach for spatiotemporal featurelearning using deep 3-dimensional convolutional networks (3D ConvNets) trainedon a large scale supervised video dataset. Our findings are three-fold: 1) 3DConvNets are more suitable for spatiotemporal feature learning compared to 2DConvNets; 2) A homogeneous architecture with small 3x3x3 convolution kernels inall layers is among the best performing architectures for 3D ConvNets; and 3)Our learned features, namely C3D (Convolutional 3D), with a simple linearclassifier outperform state-of-the-art methods on 4 different benchmarks andare comparable with current best methods on the other 2 benchmarks. Inaddition, the features are compact: achieving 52.8% accuracy on UCF101 datasetwith only 10 dimensions and also very efficient to compute due to the fastinference of ConvNets. Finally, they are conceptually very simple and easy totrain and use.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 4489-4497',\n",
       "  'citations': '3448',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1412.0767v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6546377120299846276&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 206: {'ID': 206,\n",
       "  'title': 'Learning Uncertain Convolutional Features for Accurate Saliency  Detection',\n",
       "  'authors': ['Pingping Zhang',\n",
       "   'Baocai Yin',\n",
       "   'Huchuan Lu',\n",
       "   'Hongyu Wang',\n",
       "   'Dong Wang'],\n",
       "  'published': '2017-08-07T08:18:04Z',\n",
       "  'updated': '2017-08-07T08:18:04Z',\n",
       "  'abstract': 'Deep convolutional neural networks (CNNs) have delivered superior performancein many computer vision tasks. In this paper, we propose a novel deep fullyconvolutional network model for accurate salient object detection. The keycontribution of this work is to learn deep uncertain convolutional features(UCF), which encourage the robustness and accuracy of saliency detection. Weachieve this via introducing a reformulated dropout (R-dropout) after specificconvolutional layers to construct an uncertain ensemble of internal featureunits. In addition, we propose an effective hybrid upsampling method to reducethe checkerboard artifacts of deconvolution operators in our decoder network.The proposed methods can also be applied to other deep convolutional networks.Compared with existing saliency detection methods, the proposed UCF model isable to incorporate uncertainties for more accurate object boundary inference.Extensive experiments demonstrate that our proposed saliency model performsfavorably against state-of-the-art approaches. The uncertain feature learningmechanism as well as the upsampling method can significantly improveperformance on other pixel-wise vision tasks.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 212-221',\n",
       "  'citations': '233',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1708.02031v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=10483308076131838384&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 207: {'ID': 207,\n",
       "  'title': 'Context Encoders: Feature Learning by Inpainting',\n",
       "  'authors': ['Alexei A. Efros',\n",
       "   'Deepak Pathak',\n",
       "   'Philipp Krahenbuhl',\n",
       "   'Jeff Donahue',\n",
       "   'Trevor Darrell'],\n",
       "  'published': '2016-04-25T19:42:46Z',\n",
       "  'updated': '2016-11-21T20:56:42Z',\n",
       "  'abstract': 'We present an unsupervised visual feature learning algorithm driven bycontext-based pixel prediction. By analogy with auto-encoders, we proposeContext Encoders -- a convolutional neural network trained to generate thecontents of an arbitrary image region conditioned on its surroundings. In orderto succeed at this task, context encoders need to both understand the contentof the entire image, as well as produce a plausible hypothesis for the missingpart(s). When training context encoders, we have experimented with both astandard pixel-wise reconstruction loss, as well as a reconstruction plus anadversarial loss. The latter produces much sharper results because it canbetter handle multiple modes in the output. We found that a context encoderlearns a representation that captures not just appearance but also thesemantics of visual structures. We quantitatively demonstrate the effectivenessof our learned features for CNN pre-training on classification, detection, andsegmentation tasks. Furthermore, context encoders can be used for semanticinpainting tasks, either stand-alone or as initialization for non-parametricmethods.',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.GR', 'cs.LG'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '1832',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1604.07379v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=11404163095581754770&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 208: {'ID': 208,\n",
       "  'title': 'Classification with Noisy Labels by Importance Reweighting',\n",
       "  'authors': ['Tongliang Liu', 'Dacheng Tao'],\n",
       "  'published': '2014-11-27T23:18:51Z',\n",
       "  'updated': '2015-07-18T04:03:44Z',\n",
       "  'abstract': 'In this paper, we study a classification problem in which sample labels arerandomly corrupted. In this scenario, there is an unobservable sample withnoise-free labels. However, before being observed, the true labels areindependently flipped with a probability $\\\\rho\\\\in[0,0.5)$, and the random labelnoise can be class-conditional. Here, we address two fundamental problemsraised by this scenario. The first is how to best use the abundant surrogateloss functions designed for the traditional classification problem when thereis label noise. We prove that any surrogate loss function can be used forclassification with noisy labels by using importance reweighting, withconsistency assurance that the label noise does not ultimately hinder thesearch for the optimal classifier of the noise-free sample. The other is theopen problem of how to obtain the noise rate $\\\\rho$. We show that the rate isupper bounded by the conditional probability $P(y|x)$ of the noisy sample.Consequently, the rate can be estimated, because the upper bound can be easilyreached in classification problems. Experimental results on synthetic and realdatasets confirm the efficiency of our methods.',\n",
       "  'categories': ['stat.ML', 'cs.LG'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 38 (3), 447-461',\n",
       "  'citations': '352',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1411.7718v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=3136715945596210840&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 209: {'ID': 209,\n",
       "  'title': 'Evaluation of Output Embeddings for Fine-Grained Image Classification',\n",
       "  'authors': ['Bernt Schiele',\n",
       "   'Scott Reed',\n",
       "   'Zeynep Akata',\n",
       "   'Daniel Walter',\n",
       "   'Honglak Lee'],\n",
       "  'published': '2014-09-30T06:49:53Z',\n",
       "  'updated': '2015-08-28T09:00:48Z',\n",
       "  'abstract': 'Image classification has advanced significantly in recent years with theavailability of large-scale image sets. However, fine-grained classificationremains a major challenge due to the annotation cost of large numbers offine-grained categories. This project shows that compelling classificationperformance can be achieved on such categories even without labeled trainingdata. Given image and class embeddings, we learn a compatibility function suchthat matching embeddings are assigned a higher score than mismatching ones;zero-shot classification of an image proceeds by finding the label yielding thehighest joint compatibility score. We use state-of-the-art image features andfocus on different supervised attributes and unsupervised output embeddingseither derived from hierarchies or learned from unlabeled text corpora. Weestablish a substantially improved state-of-the-art on the Animals withAttributes and Caltech-UCSD Birds datasets. Most encouragingly, we demonstratethat purely unsupervised output embeddings (learned from Wikipedia and improvedwith fine-grained text) achieve compelling results, even outperforming theprevious supervised state-of-the-art. By combining different output embeddings,we further improve results.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '552',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1409.8403v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=10459619424134737970&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 210: {'ID': 210,\n",
       "  'title': 'Cascade R-CNN: Delving into High Quality Object Detection',\n",
       "  'authors': ['Zhaowei Cai', 'Nuno Vasconcelos'],\n",
       "  'published': '2017-12-03T07:24:45Z',\n",
       "  'updated': '2017-12-03T07:24:45Z',\n",
       "  'abstract': 'In object detection, an intersection over union (IoU) threshold is requiredto define positives and negatives. An object detector, trained with low IoUthreshold, e.g. 0.5, usually produces noisy detections. However, detectionperformance tends to degrade with increasing the IoU thresholds. Two mainfactors are responsible for this: 1) overfitting during training, due toexponentially vanishing positive samples, and 2) inference-time mismatchbetween the IoUs for which the detector is optimal and those of the inputhypotheses. A multi-stage object detection architecture, the Cascade R-CNN, isproposed to address these problems. It consists of a sequence of detectorstrained with increasing IoU thresholds, to be sequentially more selectiveagainst close false positives. The detectors are trained stage by stage,leveraging the observation that the output of a detector is a good distributionfor training the next higher quality detector. The resampling of progressivelyimproved hypotheses guarantees that all detectors have a positive set ofexamples of equivalent size, reducing the overfitting problem. The same cascadeprocedure is applied at inference, enabling a closer match between thehypotheses and the detector quality of each stage. A simple implementation ofthe Cascade R-CNN is shown to surpass all single-model object detectors on thechallenging COCO dataset. Experiments also show that the Cascade R-CNN iswidely applicable across detector architectures, achieving consistent gainsindependently of the baseline detector strength. The code will be madeavailable at https://github.com/zhaoweicai/cascade-rcnn.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '533',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1712.00726v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=90386599069203250&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 211: {'ID': 211,\n",
       "  'title': 'Fast Direct Methods for Gaussian Processes',\n",
       "  'authors': [\"Michael O'Neil\",\n",
       "   'Sivaram Ambikasaran',\n",
       "   'Leslie Greengard',\n",
       "   'David W. Hogg',\n",
       "   'Daniel Foreman-Mackey'],\n",
       "  'published': '2014-03-24T16:02:01Z',\n",
       "  'updated': '2015-04-04T16:39:27Z',\n",
       "  'abstract': 'A number of problems in probability and statistics can be addressed using themultivariate normal (Gaussian) distribution. In the one-dimensional case,computing the probability for a given mean and variance simply requires theevaluation of the corresponding Gaussian density. In the $n$-dimensionalsetting, however, it requires the inversion of an $n \\\\times n$ covariancematrix, $C$, as well as the evaluation of its determinant, $\\\\det(C)$. In manycases, such as regression using Gaussian processes, the covariance matrix is ofthe form $C = \\\\sigma^2 I + K$, where $K$ is computed using a specifiedcovariance kernel which depends on the data and additional parameters(hyperparameters). The matrix $C$ is typically dense, causing standard directmethods for inversion and determinant evaluation to require $\\\\mathcal O(n^3)$work. This cost is prohibitive for large-scale modeling. Here, we show that forthe most commonly used covariance functions, the matrix $C$ can behierarchically factored into a product of block low-rank updates of theidentity matrix, yielding an $\\\\mathcal O (n\\\\log^2 n) $ algorithm for inversion.More importantly, we show that this factorization enables the evaluation of thedeterminant $\\\\det(C)$, permitting the direct calculation of probabilities inhigh dimensions under fairly broad assumptions on the kernel defining $K$. Ourfast algorithm brings many problems in marginalization and the adaptation ofhyperparameters within practical reach using a single CPU core. The combinationof nearly optimal scaling in terms of problem size with high-performancecomputing resources will permit the modeling of previously intractableproblems. We illustrate the performance of the scheme on standard covariancekernels.',\n",
       "  'categories': ['math.NA', 'astro-ph.IM', 'math.ST', 'stat.TH'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 38 (2), 252-265',\n",
       "  'citations': '234',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1403.6015v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=7122560326210979193&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 212: {'ID': 212,\n",
       "  'title': 'SCA-CNN: Spatial and Channel-wise Attention in Convolutional Networks  for Image Captioning',\n",
       "  'authors': ['Long Chen',\n",
       "   'Hanwang Zhang',\n",
       "   'Jian Shao',\n",
       "   'Wei Liu',\n",
       "   'Tat-Seng Chua',\n",
       "   'Liqiang Nie',\n",
       "   'Jun Xiao'],\n",
       "  'published': '2016-11-17T07:39:46Z',\n",
       "  'updated': '2017-04-12T05:48:44Z',\n",
       "  'abstract': 'Visual attention has been successfully applied in structural prediction taskssuch as visual captioning and question answering. Existing visual attentionmodels are generally spatial, i.e., the attention is modeled as spatialprobabilities that re-weight the last conv-layer feature map of a CNN encodingan input image. However, we argue that such spatial attention does notnecessarily conform to the attention mechanism --- a dynamic feature extractorthat combines contextual fixations over time, as CNN features are naturallyspatial, channel-wise and multi-layer. In this paper, we introduce a novelconvolutional neural network dubbed SCA-CNN that incorporates Spatial andChannel-wise Attentions in a CNN. In the task of image captioning, SCA-CNNdynamically modulates the sentence generation context in multi-layer featuremaps, encoding where (i.e., attentive spatial locations at multiple layers) andwhat (i.e., attentive channels) the visual attention is. We evaluate theproposed SCA-CNN architecture on three benchmark image captioning datasets:Flickr8K, Flickr30K, and MSCOCO. It is consistently observed that SCA-CNNsignificantly outperforms state-of-the-art visual attention-based imagecaptioning methods.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '541',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1611.05594v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6724303383476915310&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 213: {'ID': 213,\n",
       "  'title': 'Unlabeled Samples Generated by GAN Improve the Person Re-identification  Baseline in vitro',\n",
       "  'authors': ['Zhedong Zheng', 'Yi Yang', 'Liang Zheng'],\n",
       "  'published': '2017-01-26T14:30:40Z',\n",
       "  'updated': '2017-08-22T01:21:10Z',\n",
       "  'abstract': 'The main contribution of this paper is a simple semi-supervised pipeline thatonly uses the original training set without collecting extra data. It ischallenging in 1) how to obtain more training data only from the training setand 2) how to use the newly generated data. In this work, the generativeadversarial network (GAN) is used to generate unlabeled samples. We propose thelabel smoothing regularization for outliers (LSRO). This method assigns auniform label distribution to the unlabeled images, which regularizes thesupervised model and improves the baseline. We verify the proposed method on apractical problem: person re-identification (re-ID). This task aims to retrievea query person from other cameras. We adopt the deep convolutional generativeadversarial network (DCGAN) for sample generation, and a baseline convolutionalneural network (CNN) for representation learning. Experiments show that addingthe GAN-generated data effectively improves the discriminative ability oflearned CNN embeddings. On three large-scale datasets, Market-1501, CUHK03 andDukeMTMC-reID, we obtain +4.37%, +1.6% and +2.46% improvement in rank-1precision over the baseline CNN, respectively. We additionally apply theproposed method to fine-grained bird recognition and achieve a +0.6%improvement over a strong baseline. The code is available athttps://github.com/layumi/Person-reID_GAN.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 3754-3762',\n",
       "  'citations': '783',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1701.07717v5',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=270746001988088124&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 214: {'ID': 214,\n",
       "  'title': 'Pyramid Stereo Matching Network',\n",
       "  'authors': ['Jia-Ren Chang', 'Yong-Sheng Chen'],\n",
       "  'published': '2018-03-23T06:40:09Z',\n",
       "  'updated': '2018-03-23T06:40:09Z',\n",
       "  'abstract': 'Recent work has shown that depth estimation from a stereo pair of images canbe formulated as a supervised learning task to be resolved with convolutionalneural networks (CNNs). However, current architectures rely on patch-basedSiamese networks, lacking the means to exploit context information for findingcorrespondence in illposed regions. To tackle this problem, we propose PSMNet,a pyramid stereo matching network consisting of two main modules: spatialpyramid pooling and 3D CNN. The spatial pyramid pooling module takes advantageof the capacity of global context information by aggregating context indifferent scales and locations to form a cost volume. The 3D CNN learns toregularize cost volume using stacked multiple hourglass networks in conjunctionwith intermediate supervision. The proposed approach was evaluated on severalbenchmark datasets. Our method ranked first in the KITTI 2012 and 2015leaderboards before March 18, 2018. The codes of PSMNet are available at:https://github.com/JiaRenChang/PSMNet.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '313',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1803.08669v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6542126250928371536&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 215: {'ID': 215,\n",
       "  'title': 'Learning to Track at 100 FPS with Deep Regression Networks',\n",
       "  'authors': ['Sebastian Thrun', 'David Held', 'Silvio Savarese'],\n",
       "  'published': '2016-04-06T20:39:34Z',\n",
       "  'updated': '2016-08-16T02:34:48Z',\n",
       "  'abstract': \"Machine learning techniques are often used in computer vision due to theirability to leverage large amounts of training data to improve performance.Unfortunately, most generic object trackers are still trained from scratchonline and do not benefit from the large number of videos that are readilyavailable for offline training. We propose a method for offline training ofneural networks that can track novel objects at test-time at 100 fps. Ourtracker is significantly faster than previous methods that use neural networksfor tracking, which are typically very slow to run and not practical forreal-time applications. Our tracker uses a simple feed-forward network with noonline training required. The tracker learns a generic relationship betweenobject motion and appearance and can be used to track novel objects that do notappear in the training set. We test our network on a standard trackingbenchmark to demonstrate our tracker's state-of-the-art performance. Further,our performance improves as we add more videos to our offline training set. Tothe best of our knowledge, our tracker is the first neural-network tracker thatlearns to track generic objects at 100 fps.\",\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.LG', 'cs.RO'],\n",
       "  'journal': 'ECCV (1), 749-765',\n",
       "  'citations': '705',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1604.01802v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2517440561757741723&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 216: {'ID': 216,\n",
       "  'title': 'Object Detection Networks on Convolutional Feature Maps',\n",
       "  'authors': ['Xiangyu Zhang',\n",
       "   'Ross Girshick',\n",
       "   'Kaiming He',\n",
       "   'Jian Sun',\n",
       "   'Shaoqing Ren'],\n",
       "  'published': '2015-04-23T07:15:10Z',\n",
       "  'updated': '2016-08-17T15:51:13Z',\n",
       "  'abstract': 'Most object detectors contain two important components: a feature extractorand an object classifier. The feature extractor has rapidly evolved withsignificant research efforts leading to better deep convolutionalarchitectures. The object classifier, however, has not received much attentionand many recent systems (like SPPnet and Fast/Faster R-CNN) use simplemulti-layer perceptrons. This paper demonstrates that carefully designing deepnetworks for object classification is just as important. We experiment withregion-wise classifier networks that use shared, region-independentconvolutional features. We call them \"Networks on Convolutional feature maps\"(NoCs). We discover that aside from deep feature maps, a deep and convolutionalper-region classifier is of particular importance for object detection, whereaslatest superior image classification models (such as ResNets and GoogLeNets) donot directly lead to good detection accuracy without using such a per-regionclassifier. We show by experiments that despite the effective ResNets andFaster R-CNN systems, the design of NoCs is an essential element for the1st-place winning entries in ImageNet and MS COCO challenges 2015.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 39 (7), 1476-1481',\n",
       "  'citations': '240',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1504.06066v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6385320480924360974&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 217: {'ID': 217,\n",
       "  'title': 'STC: A Simple to Complex Framework for Weakly-supervised Semantic  Segmentation',\n",
       "  'authors': ['Yunpeng Chen',\n",
       "   'Xiaodan Liang',\n",
       "   'Yao Zhao',\n",
       "   'Yunchao Wei',\n",
       "   'Xiaohui Shen',\n",
       "   'Ming-Ming Cheng',\n",
       "   'Shuicheng Yan',\n",
       "   'Jiashi Feng'],\n",
       "  'published': '2015-09-10T13:45:01Z',\n",
       "  'updated': '2016-12-07T10:59:12Z',\n",
       "  'abstract': 'Recently, significant improvement has been made on semantic objectsegmentation due to the development of deep convolutional neural networks(DCNNs). Training such a DCNN usually relies on a large number of images withpixel-level segmentation masks, and annotating these images is very costly interms of both finance and human effort. In this paper, we propose a simple tocomplex (STC) framework in which only image-level annotations are utilized tolearn DCNNs for semantic segmentation. Specifically, we first train an initialsegmentation network called Initial-DCNN with the saliency maps of simpleimages (i.e., those with a single category of major object(s) and cleanbackground). These saliency maps can be automatically obtained by existingbottom-up salient object detection techniques, where no supervision informationis needed. Then, a better network called Enhanced-DCNN is learned withsupervision from the predicted segmentation masks of simple images based on theInitial-DCNN as well as the image-level annotations. Finally, more pixel-levelsegmentation masks of complex images (two or more categories of objects withcluttered background), which are inferred by using Enhanced-DCNN andimage-level annotations, are utilized as the supervision information to learnthe Powerful-DCNN for semantic segmentation. Our method utilizes $40$K simpleimages from Flickr.com and 10K complex images from PASCAL VOC for step-wiselyboosting the segmentation network. Extensive experimental results on PASCAL VOC2012 segmentation benchmark well demonstrate the superiority of the proposedSTC framework compared with other state-of-the-arts.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 39 (11), 2314\\xa0…',\n",
       "  'citations': '275',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1509.03150v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=16335215949291653626&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 218: {'ID': 218,\n",
       "  'title': 'High-Speed Tracking with Kernelized Correlation Filters',\n",
       "  'authors': ['João F. Henriques',\n",
       "   'Rui Caseiro',\n",
       "   'Jorge Batista',\n",
       "   'Pedro Martins'],\n",
       "  'published': '2014-04-30T04:16:38Z',\n",
       "  'updated': '2014-11-05T01:32:56Z',\n",
       "  'abstract': 'The core component of most modern trackers is a discriminative classifier,tasked with distinguishing between the target and the surrounding environment.To cope with natural image changes, this classifier is typically trained withtranslated and scaled sample patches. Such sets of samples are riddled withredundancies -- any overlapping pixels are constrained to be the same. Based onthis simple observation, we propose an analytic model for datasets of thousandsof translated patches. By showing that the resulting data matrix is circulant,we can diagonalize it with the Discrete Fourier Transform, reducing bothstorage and computation by several orders of magnitude. Interestingly, forlinear regression our formulation is equivalent to a correlation filter, usedby some of the fastest competitive trackers. For kernel regression, however, wederive a new Kernelized Correlation Filter (KCF), that unlike other kernelalgorithms has the exact same complexity as its linear counterpart. Building onit, we also propose a fast multi-channel extension of linear correlationfilters, via a linear kernel, which we call Dual Correlation Filter (DCF). BothKCF and DCF outperform top-ranking trackers such as Struck or TLD on a 50videos benchmark, despite running at hundreds of frames-per-second, and beingimplemented in a few lines of code (Algorithm 1). To encourage furtherdevelopments, our tracking framework was made open-source.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis & Machine Intelligence, 583-596',\n",
       "  'citations': '3286',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1404.7584v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2356790868259107933&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 219: {'ID': 219,\n",
       "  'title': 'DensePose: Dense Human Pose Estimation In The Wild',\n",
       "  'authors': ['Iasonas Kokkinos', 'Rıza Alp Güler', 'Natalia Neverova'],\n",
       "  'published': '2018-02-01T18:53:26Z',\n",
       "  'updated': '2018-02-01T18:53:26Z',\n",
       "  'abstract': \"In this work, we establish dense correspondences between RGB image and asurface-based representation of the human body, a task we refer to as densehuman pose estimation. We first gather dense correspondences for 50K personsappearing in the COCO dataset by introducing an efficient annotation pipeline.We then use our dataset to train CNN-based systems that deliver densecorrespondence 'in the wild', namely in the presence of background, occlusionsand scale variations. We improve our training set's effectiveness by trainingan 'inpainting' network that can fill in missing groundtruth values and reportclear improvements with respect to the best results that would be achievable inthe past. We experiment with fully-convolutional networks and region-basedmodels and observe a superiority of the latter; we further improve accuracythrough cascading, obtaining a system that delivers highly0accurate results inreal time. Supplementary materials and videos are provided on the project pagehttp://densepose.org\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '317',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1802.00434v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=18442139278667180530&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 220: {'ID': 220,\n",
       "  'title': 'Deep Multi-scale Convolutional Neural Network for Dynamic Scene  Deblurring',\n",
       "  'authors': ['Tae Hyun Kim', 'Seungjun Nah', 'Kyoung Mu Lee'],\n",
       "  'published': '2016-12-07T10:08:33Z',\n",
       "  'updated': '2018-05-07T06:15:57Z',\n",
       "  'abstract': 'Non-uniform blind deblurring for general dynamic scenes is a challengingcomputer vision problem as blurs arise not only from multiple object motionsbut also from camera shake, scene depth variation. To remove these complicatedmotion blurs, conventional energy optimization based methods rely on simpleassumptions such that blur kernel is partially uniform or locally linear.Moreover, recent machine learning based methods also depend on synthetic blurdatasets generated under these assumptions. This makes conventional deblurringmethods fail to remove blurs where blur kernel is difficult to approximate orparameterize (e.g. object motion boundaries). In this work, we propose amulti-scale convolutional neural network that restores sharp images in anend-to-end manner where blur is caused by various sources. Together, we presentmulti-scale loss function that mimics conventional coarse-to-fine approaches.Furthermore, we propose a new large-scale dataset that provides pairs ofrealistic blurry image and the corresponding ground truth sharp image that areobtained by a high-speed camera. With the proposed model trained on thisdataset, we demonstrate empirically that our method achieves thestate-of-the-art performance in dynamic scene deblurring not onlyqualitatively, but also quantitatively.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '453',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1612.02177v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5306020987846368461&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 221: {'ID': 221,\n",
       "  'title': 'Re-ranking Person Re-identification with k-reciprocal Encoding',\n",
       "  'authors': ['Shaozi Li', 'Donglin Cao', 'Liang Zheng', 'Zhun Zhong'],\n",
       "  'published': '2017-01-29T16:31:51Z',\n",
       "  'updated': '2017-05-05T02:46:47Z',\n",
       "  'abstract': 'When considering person re-identification (re-ID) as a retrieval process,re-ranking is a critical step to improve its accuracy. Yet in the re-IDcommunity, limited effort has been devoted to re-ranking, especially thosefully automatic, unsupervised solutions. In this paper, we propose ak-reciprocal encoding method to re-rank the re-ID results. Our hypothesis isthat if a gallery image is similar to the probe in the k-reciprocal nearestneighbors, it is more likely to be a true match. Specifically, given an image,a k-reciprocal feature is calculated by encoding its k-reciprocal nearestneighbors into a single vector, which is used for re-ranking under the Jaccarddistance. The final distance is computed as the combination of the originaldistance and the Jaccard distance. Our re-ranking method does not require anyhuman interaction or any labeled data, so it is applicable to large-scaledatasets. Experiments on the large-scale Market-1501, CUHK03, MARS, and PRWdatasets confirm the effectiveness of our method.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '592',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1701.08398v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5132863931120457431&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 222: {'ID': 222,\n",
       "  'title': 'Person Transfer GAN to Bridge Domain Gap for Person Re-Identification',\n",
       "  'authors': ['Wen Gao', 'Qi Tian', 'Shiliang Zhang', 'Longhui Wei'],\n",
       "  'published': '2017-11-23T03:23:21Z',\n",
       "  'updated': '2018-06-25T12:50:58Z',\n",
       "  'abstract': 'Although the performance of person Re-Identification (ReID) has beensignificantly boosted, many challenging issues in real scenarios have not beenfully investigated, e.g., the complex scenes and lighting variations, viewpointand pose changes, and the large number of identities in a camera network. Tofacilitate the research towards conquering those issues, this paper contributesa new dataset called MSMT17 with many important features, e.g., 1) the rawvideos are taken by an 15-camera network deployed in both indoor and outdoorscenes, 2) the videos cover a long period of time and present complex lightingvariations, and 3) it contains currently the largest number of annotatedidentities, i.e., 4,101 identities and 126,441 bounding boxes. We also observethat, domain gap commonly exists between datasets, which essentially causessevere performance drop when training and testing on different datasets. Thisresults in that available training data cannot be effectively leveraged for newtesting domains. To relieve the expensive costs of annotating new trainingsamples, we propose a Person Transfer Generative Adversarial Network (PTGAN) tobridge the domain gap. Comprehensive experiments show that the domain gap couldbe substantially narrowed-down by the PTGAN.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '353',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1711.08565v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=13540004844953944715&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 223: {'ID': 223,\n",
       "  'title': 'MovieQA: Understanding Stories in Movies through Question-Answering',\n",
       "  'authors': ['Raquel Urtasun',\n",
       "   'Sanja Fidler',\n",
       "   'Antonio Torralba',\n",
       "   'Yukun Zhu',\n",
       "   'Makarand Tapaswi',\n",
       "   'Rainer Stiefelhagen'],\n",
       "  'published': '2015-12-09T15:34:31Z',\n",
       "  'updated': '2016-09-21T04:52:35Z',\n",
       "  'abstract': 'We introduce the MovieQA dataset which aims to evaluate automatic storycomprehension from both video and text. The dataset consists of 14,944questions about 408 movies with high semantic diversity. The questions rangefrom simpler \"Who\" did \"What\" to \"Whom\", to \"Why\" and \"How\" certain eventsoccurred. Each question comes with a set of five possible answers; a correctone and four deceiving answers provided by human annotators. Our dataset isunique in that it contains multiple sources of information -- video clips,plots, subtitles, scripts, and DVS. We analyze our data through variousstatistics and methods. We further extend existing QA techniques to show thatquestion-answering with such open-ended semantics is hard. We make this dataset public along with an evaluation benchmark to encourage inspiring work inthis challenging domain.',\n",
       "  'categories': ['cs.CV', 'cs.CL'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '300',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1512.02902v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=9733167792031628523&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 224: {'ID': 224,\n",
       "  'title': 'Boosting Image Captioning with Attributes',\n",
       "  'authors': ['Yingwei Pan', 'Tao Mei', 'Zhaofan Qiu', 'Yehao Li', 'Ting Yao'],\n",
       "  'published': '2016-11-05T13:12:29Z',\n",
       "  'updated': '2016-11-05T13:12:29Z',\n",
       "  'abstract': 'Automatically describing an image with a natural language has been anemerging challenge in both fields of computer vision and natural languageprocessing. In this paper, we present Long Short-Term Memory with Attributes(LSTM-A) - a novel architecture that integrates attributes into the successfulConvolutional Neural Networks (CNNs) plus Recurrent Neural Networks (RNNs)image captioning framework, by training them in an end-to-end manner. Toincorporate attributes, we construct variants of architectures by feeding imagerepresentations and attributes into RNNs in different ways to explore themutual but also fuzzy relationship between them. Extensive experiments areconducted on COCO image captioning dataset and our framework achieves superiorresults when compared to state-of-the-art deep models. Most remarkably, weobtain METEOR/CIDEr-D of 25.2%/98.6% on testing data of widely used andpublicly available splits in (Karpathy &amp; Fei-Fei, 2015) when extracting imagerepresentations by GoogleNet and achieve to date top-1 performance on COCOcaptioning Leaderboard.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 4894-4902',\n",
       "  'citations': '314',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1611.01646v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=18010599519948968721&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 225: {'ID': 225,\n",
       "  'title': 'MnasNet: Platform-Aware Neural Architecture Search for Mobile',\n",
       "  'authors': ['Vijay Vasudevan',\n",
       "   'Mark Sandler',\n",
       "   'Ruoming Pang',\n",
       "   'Mingxing Tan',\n",
       "   'Bo Chen',\n",
       "   'Quoc V. Le',\n",
       "   'Andrew Howard'],\n",
       "  'published': '2018-07-31T01:34:21Z',\n",
       "  'updated': '2019-05-29T01:30:05Z',\n",
       "  'abstract': 'Designing convolutional neural networks (CNN) for mobile devices ischallenging because mobile models need to be small and fast, yet stillaccurate. Although significant efforts have been dedicated to design andimprove mobile CNNs on all dimensions, it is very difficult to manually balancethese trade-offs when there are so many architectural possibilities toconsider. In this paper, we propose an automated mobile neural architecturesearch (MNAS) approach, which explicitly incorporate model latency into themain objective so that the search can identify a model that achieves a goodtrade-off between accuracy and latency. Unlike previous work, where latency isconsidered via another, often inaccurate proxy (e.g., FLOPS), our approachdirectly measures real-world inference latency by executing the model on mobilephones. To further strike the right balance between flexibility and searchspace size, we propose a novel factorized hierarchical search space thatencourages layer diversity throughout the network. Experimental results showthat our approach consistently outperforms state-of-the-art mobile CNN modelsacross multiple vision tasks. On the ImageNet classification task, our MnasNetachieves 75.2% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8xfaster than MobileNetV2 [29] with 0.5% higher accuracy and 2.3x faster thanNASNet [36] with 1.2% higher accuracy. Our MnasNet also achieves better mAPquality than MobileNets for COCO object detection. Code is athttps://github.com/tensorflow/tpu/tree/master/models/official/mnasnet',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '446',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1807.11626v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=1725364759924402840&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 226: {'ID': 226,\n",
       "  'title': 'Dense-Captioning Events in Videos',\n",
       "  'authors': ['Frederic Ren',\n",
       "   'Li Fei-Fei',\n",
       "   'Juan Carlos Niebles',\n",
       "   'Ranjay Krishna',\n",
       "   'Kenji Hata'],\n",
       "  'published': '2017-05-02T01:21:58Z',\n",
       "  'updated': '2017-05-02T01:21:58Z',\n",
       "  'abstract': 'Most natural videos contain numerous events. For example, in a video of a\"man playing a piano\", the video might also contain \"another man dancing\" or \"acrowd clapping\". We introduce the task of dense-captioning events, whichinvolves both detecting and describing events in a video. We propose a newmodel that is able to identify all events in a single pass of the video whilesimultaneously describing the detected events with natural language. Our modelintroduces a variant of an existing proposal module that is designed to captureboth short as well as long events that span minutes. To capture thedependencies between the events in a video, our model introduces a newcaptioning module that uses contextual information from past and future eventsto jointly describe all events. We also introduce ActivityNet Captions, alarge-scale benchmark for dense-captioning events. ActivityNet Captionscontains 20k videos amounting to 849 video hours with 100k total descriptions,each with it\\'s unique start and end time. Finally, we report performances ofour model for dense-captioning events, video retrieval and localization.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 706-715',\n",
       "  'citations': '223',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1705.00754v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2567058154422245749&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 227: {'ID': 227,\n",
       "  'title': 'MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition',\n",
       "  'authors': ['Jianfeng Gao',\n",
       "   'Lei Zhang',\n",
       "   'Yuxiao Hu',\n",
       "   'Xiaodong He',\n",
       "   'Yandong Guo'],\n",
       "  'published': '2016-07-27T19:18:16Z',\n",
       "  'updated': '2016-07-27T19:18:16Z',\n",
       "  'abstract': 'In this paper, we design a benchmark task and provide the associated datasetsfor recognizing face images and link them to corresponding entity keys in aknowledge base. More specifically, we propose a benchmark task to recognize onemillion celebrities from their face images, by using all the possibly collectedface images of this individual on the web as training data. The richinformation provided by the knowledge base helps to conduct disambiguation andimprove the recognition accuracy, and contributes to various real-worldapplications, such as image captioning and news video analysis. Associated withthis task, we design and provide concrete measurement set, evaluation protocol,as well as training data. We also present in details our experiment setup andreport promising baseline results. Our benchmark task could lead to one of thelargest classification problems in computer vision. To the best of ourknowledge, our training dataset, which contains 10M images in version 1, is thelargest publicly available one in the world.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (3), 87-102',\n",
       "  'citations': '639',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1607.08221v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=7096719334274798105&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 228: {'ID': 228,\n",
       "  'title': 'Accelerating Very Deep Convolutional Networks for Classification and  Detection',\n",
       "  'authors': ['Kaiming He', 'Xiangyu Zhang', 'Jian Sun', 'Jianhua Zou'],\n",
       "  'published': '2015-05-26T03:30:59Z',\n",
       "  'updated': '2015-11-18T06:16:59Z',\n",
       "  'abstract': 'This paper aims to accelerate the test-time computation of convolutionalneural networks (CNNs), especially very deep CNNs that have substantiallyimpacted the computer vision community. Unlike previous methods that aredesigned for approximating linear filters or linear responses, our method takesthe nonlinear units into account. We develop an effective solution to theresulting nonlinear optimization problem without the need of stochasticgradient descent (SGD). More importantly, while previous methods mainly focuson optimizing one or two layers, our nonlinear method enables an asymmetricreconstruction that reduces the rapidly accumulated error when multiple (e.g.,&gt;=10) layers are approximated. For the widely used very deep VGG-16 model, ourmethod achieves a whole-model speedup of 4x with merely a 0.3% increase oftop-5 error in ImageNet classification. Our 4x accelerated VGG-16 model alsoshows a graceful accuracy degradation for object detection when plugged intothe Fast R-CNN detector.',\n",
       "  'categories': ['cs.CV', 'cs.LG', 'cs.NE'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 38 (10), 1943\\xa0…',\n",
       "  'citations': '355',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1505.06798v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=11183077033015235296&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 229: {'ID': 229,\n",
       "  'title': 'StackGAN: Text to Photo-realistic Image Synthesis with Stacked  Generative Adversarial Networks',\n",
       "  'authors': ['Dimitris Metaxas',\n",
       "   'Shaoting Zhang',\n",
       "   'Xiaogang Wang',\n",
       "   'Hongsheng Li',\n",
       "   'Tao Xu',\n",
       "   'Han Zhang',\n",
       "   'Xiaolei Huang'],\n",
       "  'published': '2016-12-10T03:11:37Z',\n",
       "  'updated': '2017-08-05T02:18:21Z',\n",
       "  'abstract': 'Synthesizing high-quality images from text descriptions is a challengingproblem in computer vision and has many practical applications. Samplesgenerated by existing text-to-image approaches can roughly reflect the meaningof the given descriptions, but they fail to contain necessary details and vividobject parts. In this paper, we propose Stacked Generative Adversarial Networks(StackGAN) to generate 256x256 photo-realistic images conditioned on textdescriptions. We decompose the hard problem into more manageable sub-problemsthrough a sketch-refinement process. The Stage-I GAN sketches the primitiveshape and colors of the object based on the given text description, yieldingStage-I low-resolution images. The Stage-II GAN takes Stage-I results and textdescriptions as inputs, and generates high-resolution images withphoto-realistic details. It is able to rectify defects in Stage-I results andadd compelling details with the refinement process. To improve the diversity ofthe synthesized images and stabilize the training of the conditional-GAN, weintroduce a novel Conditioning Augmentation technique that encouragessmoothness in the latent conditioning manifold. Extensive experiments andcomparisons with state-of-the-arts on benchmark datasets demonstrate that theproposed method achieves significant improvements on generating photo-realisticimages conditioned on text descriptions.',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'stat.ML'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 5907-5915',\n",
       "  'citations': '1139',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1612.03242v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=17276767268002585863&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 230: {'ID': 230,\n",
       "  'title': 'Learning to Refine Object Segments',\n",
       "  'authors': ['Piotr Dollàr',\n",
       "   'Pedro O. Pinheiro',\n",
       "   'Ronan Collobert',\n",
       "   'Tsung-Yi Lin'],\n",
       "  'published': '2016-03-29T09:33:44Z',\n",
       "  'updated': '2016-07-26T20:40:51Z',\n",
       "  'abstract': \"Object segmentation requires both object-level information and low-levelpixel data. This presents a challenge for feedforward networks: lower layers inconvolutional nets capture rich spatial information, while upper layers encodeobject-level knowledge but are invariant to factors such as pose andappearance. In this work we propose to augment feedforward nets for objectsegmentation with a novel top-down refinement approach. The resultingbottom-up/top-down architecture is capable of efficiently generatinghigh-fidelity object masks. Similarly to skip connections, our approachleverages features at all layers of the net. Unlike skip connections, ourapproach does not attempt to output independent predictions at each layer.Instead, we first output a coarse `mask encoding' in a feedforward pass, thenrefine this mask encoding in a top-down pass utilizing features at successivelylower layers. The approach is simple, fast, and effective. Building on therecent DeepMask network for generating object proposals, we show accuracyimprovements of 10-20% in average recall for various setups. Additionally, byoptimizing the overall network architecture, our approach, which we callSharpMask, is 50% faster than the original DeepMask network (under .8s perimage).\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (1), 75-91',\n",
       "  'citations': '554',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1603.08695v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=3803871981164739399&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 231: {'ID': 231,\n",
       "  'title': 'HashNet: Deep Learning to Hash by Continuation',\n",
       "  'authors': ['Zhangjie Cao',\n",
       "   'Mingsheng Long',\n",
       "   'Philip S. Yu',\n",
       "   'Jianmin Wang'],\n",
       "  'published': '2017-02-02T17:29:24Z',\n",
       "  'updated': '2017-07-29T17:55:50Z',\n",
       "  'abstract': 'Learning to hash has been widely applied to approximate nearest neighborsearch for large-scale multimedia retrieval, due to its computation efficiencyand retrieval quality. Deep learning to hash, which improves retrieval qualityby end-to-end representation learning and hash encoding, has receivedincreasing attention recently. Subject to the ill-posed gradient difficulty inthe optimization with sign activations, existing deep learning to hash methodsneed to first learn continuous representations and then generate binary hashcodes in a separated binarization step, which suffer from substantial loss ofretrieval quality. This work presents HashNet, a novel deep architecture fordeep learning to hash by continuation method with convergence guarantees, whichlearns exactly binary hash codes from imbalanced similarity data. The key ideais to attack the ill-posed gradient problem in optimizing deep networks withnon-smooth binary activations by continuation method, in which we begin fromlearning an easier network with smoothed activation function and let it evolveduring the training, until it eventually goes back to being the original,difficult to optimize, deep network with the sign activation function.Comprehensive empirical evidence shows that HashNet can generate exactly binaryhash codes and yield state-of-the-art multimedia retrieval performance onstandard benchmarks.',\n",
       "  'categories': ['cs.LG', 'cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 5608-5617',\n",
       "  'citations': '207',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1702.00758v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=9571949737669699009&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 232: {'ID': 232,\n",
       "  'title': 'Multimodal Convolutional Neural Networks for Matching Image and Sentence',\n",
       "  'authors': ['Zhengdong Lu', 'Hang Li', 'Lin Ma', 'Lifeng Shang'],\n",
       "  'published': '2015-04-23T07:10:13Z',\n",
       "  'updated': '2015-08-29T09:35:09Z',\n",
       "  'abstract': 'In this paper, we propose multimodal convolutional neural networks (m-CNNs)for matching image and sentence. Our m-CNN provides an end-to-end frameworkwith convolutional architectures to exploit image representation, wordcomposition, and the matching relations between the two modalities. Morespecifically, it consists of one image CNN encoding the image content, and onematching CNN learning the joint representation of image and sentence. Thematching CNN composes words to different semantic fragments and learns theinter-modal relations between image and the composed fragments at differentlevels, thus fully exploit the matching relations between image and sentence.Experimental results on benchmark databases of bidirectional image and sentenceretrieval demonstrate that the proposed m-CNNs can effectively capture theinformation necessary for image and sentence matching. Specifically, ourproposed m-CNNs for bidirectional image and sentence retrieval on Flickr30K andMicrosoft COCO databases achieve the state-of-the-art performances.',\n",
       "  'categories': ['cs.CV', 'cs.CL', 'cs.NE'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 2623-2631',\n",
       "  'citations': '242',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1504.06063v5',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=8242855245788745193&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 233: {'ID': 233,\n",
       "  'title': 'Predicting Depth, Surface Normals and Semantic Labels with a Common  Multi-Scale Convolutional Architecture',\n",
       "  'authors': ['David Eigen', 'Rob Fergus'],\n",
       "  'published': '2014-11-18T04:49:08Z',\n",
       "  'updated': '2015-12-17T03:19:36Z',\n",
       "  'abstract': 'In this paper we address three different computer vision tasks using a singlebasic architecture: depth prediction, surface normal estimation, and semanticlabeling. We use a multiscale convolutional network that is able to adapteasily to each task using only small modifications, regressing from the inputimage to the output map directly. Our method progressively refines predictionsusing a sequence of scales, and captures many image details without anysuperpixels or low-level segmentation. We achieve state-of-the-art performanceon benchmarks for all three tasks.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 2650-2658',\n",
       "  'citations': '1452',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1411.4734v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=12162254258128213973&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 234: {'ID': 234,\n",
       "  'title': 'Taskonomy: Disentangling Task Transfer Learning',\n",
       "  'authors': ['Amir Zamir',\n",
       "   'Leonidas Guibas',\n",
       "   'Silvio Savarese',\n",
       "   'Jitendra Malik',\n",
       "   'William Shen',\n",
       "   'Alexander Sax'],\n",
       "  'published': '2018-04-23T10:46:28Z',\n",
       "  'updated': '2018-04-23T10:46:28Z',\n",
       "  'abstract': 'Do visual tasks have a relationship, or are they unrelated? For instance,could having surface normals simplify estimating the depth of an image?Intuition answers these questions positively, implying existence of a structureamong visual tasks. Knowing this structure has notable values; it is theconcept underlying transfer learning and provides a principled way foridentifying redundancies across tasks, e.g., to seamlessly reuse supervisionamong related tasks or solve many tasks in one system without piling up thecomplexity.  We proposes a fully computational approach for modeling the structure ofspace of visual tasks. This is done via finding (first and higher-order)transfer learning dependencies across a dictionary of twenty six 2D, 2.5D, 3D,and semantic tasks in a latent space. The product is a computational taxonomicmap for task transfer learning. We study the consequences of this structure,e.g. nontrivial emerged relationships, and exploit them to reduce the demandfor labeled data. For example, we show that the total number of labeleddatapoints needed for solving a set of 10 tasks can be reduced by roughly 2/3(compared to training independently) while keeping the performance nearly thesame. We provide a set of tools for computing and probing this taxonomicalstructure including a solver that users can employ to devise efficientsupervision policies for their use cases.',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.LG', 'cs.NE', 'cs.RO'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '300',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1804.08328v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14005457704431675372&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 235: {'ID': 235,\n",
       "  'title': 'Computing the Stereo Matching Cost with a Convolutional Neural Network',\n",
       "  'authors': ['Yann LeCun', 'Jure Žbontar'],\n",
       "  'published': '2014-09-15T16:54:42Z',\n",
       "  'updated': '2015-10-20T15:08:48Z',\n",
       "  'abstract': 'We present a method for extracting depth information from a rectified imagepair. We train a convolutional neural network to predict how well two imagepatches match and use it to compute the stereo matching cost. The cost isrefined by cross-based cost aggregation and semiglobal matching, followed by aleft-right consistency check to eliminate errors in the occluded regions. Ourstereo method achieves an error rate of 2.61 % on the KITTI stereo dataset andis currently (August 2014) the top performing method on this dataset.',\n",
       "  'categories': ['cs.CV', 'cs.LG', 'cs.NE'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '503',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1409.4326v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=8341548540017327145&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 236: {'ID': 236,\n",
       "  'title': 'PlaNet - Photo Geolocation with Convolutional Neural Networks',\n",
       "  'authors': ['Ilya Kostrikov', 'Tobias Weyand', 'James Philbin'],\n",
       "  'published': '2016-02-17T06:27:55Z',\n",
       "  'updated': '2016-02-17T06:27:55Z',\n",
       "  'abstract': 'Is it possible to build a system to determine the location where a photo wastaken using just its pixels? In general, the problem seems exceptionallydifficult: it is trivial to construct situations where no location can beinferred. Yet images often contain informative cues such as landmarks, weatherpatterns, vegetation, road markings, and architectural details, which incombination may allow one to determine an approximate location and occasionallyan exact location. Websites such as GeoGuessr and View from your Window suggestthat humans are relatively good at integrating these cues to geolocate images,especially en-masse. In computer vision, the photo geolocation problem isusually approached using image retrieval methods. In contrast, we pose theproblem as one of classification by subdividing the surface of the earth intothousands of multi-scale geographic cells, and train a deep network usingmillions of geotagged images. While previous approaches only recognizelandmarks or perform approximate matching using global image descriptors, ourmodel is able to use and integrate multiple visible cues. We show that theresulting model, called PlaNet, outperforms previous approaches and evenattains superhuman levels of accuracy in some cases. Moreover, we extend ourmodel to photo albums by combining it with a long short-term memory (LSTM)architecture. By learning to exploit temporal coherence to geolocate uncertainphotos, we demonstrate that this model achieves a 50% performance improvementover the single-image model.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (8), 37-55',\n",
       "  'citations': '261',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1602.05314v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6973177724812221842&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 237: {'ID': 237,\n",
       "  'title': 'What makes for effective detection proposals?',\n",
       "  'authors': ['Rodrigo Benenson',\n",
       "   'Bernt Schiele',\n",
       "   'Piotr Dollár',\n",
       "   'Jan Hosang'],\n",
       "  'published': '2015-02-17T22:45:14Z',\n",
       "  'updated': '2015-08-01T16:33:25Z',\n",
       "  'abstract': 'Current top performing object detectors employ detection proposals to guidethe search for objects, thereby avoiding exhaustive sliding window searchacross images. Despite the popularity and widespread use of detectionproposals, it is unclear which trade-offs are made when using them duringobject detection. We provide an in-depth analysis of twelve proposal methodsalong with four baselines regarding proposal repeatability, ground truthannotation recall on PASCAL, ImageNet, and MS COCO, and their impact on DPM,R-CNN, and Fast R-CNN detection performance. Our analysis shows that for objectdetection improving proposal localisation accuracy is as important as improvingrecall. We introduce a novel metric, the average recall (AR), which rewardsboth high recall and good localisation and correlates surprisingly well withdetection performance. Our findings show common strengths and weaknesses ofexisting methods, and provide insights and metrics for selecting and tuningproposal methods.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 38 (4), 814-830',\n",
       "  'citations': '590',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1502.05082v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=17186097982497728066&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 238: {'ID': 238,\n",
       "  'title': 'Look, Listen and Learn',\n",
       "  'authors': ['Andrew Zisserman', 'Relja Arandjelović'],\n",
       "  'published': '2017-05-23T10:37:54Z',\n",
       "  'updated': '2017-08-01T12:04:50Z',\n",
       "  'abstract': 'We consider the question: what can be learnt by looking at and listening to alarge number of unlabelled videos? There is a valuable, but so far untapped,source of information contained in the video itself -- the correspondencebetween the visual and the audio streams, and we introduce a novel\"Audio-Visual Correspondence\" learning task that makes use of this. Trainingvisual and audio networks from scratch, without any additional supervisionother than the raw unconstrained videos themselves, is shown to successfullysolve this task, and, more interestingly, result in good visual and audiorepresentations. These features set the new state-of-the-art on two soundclassification benchmarks, and perform on par with the state-of-the-artself-supervised approaches on ImageNet classification. We also demonstrate thatthe network is able to localize objects in both modalities, as well as performfine-grained recognition tasks.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 609-617',\n",
       "  'citations': '225',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1705.08168v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=11013860824511413609&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 239: {'ID': 239,\n",
       "  'title': 'The Unreasonable Effectiveness of Noisy Data for Fine-Grained  Recognition',\n",
       "  'authors': ['Howard Zhou',\n",
       "   'Benjamin Sapp',\n",
       "   'Li Fei-Fei',\n",
       "   'Alexander Toshev',\n",
       "   'Jonathan Krause',\n",
       "   'James Philbin',\n",
       "   'Tom Duerig',\n",
       "   'Andrew Howard'],\n",
       "  'published': '2015-11-20T22:40:30Z',\n",
       "  'updated': '2016-10-18T18:35:31Z',\n",
       "  'abstract': 'Current approaches for fine-grained recognition do the following: First,recruit experts to annotate a dataset of images, optionally also collectingmore structured data in the form of part annotations and bounding boxes.Second, train a model utilizing this data. Toward the goal of solvingfine-grained recognition, we introduce an alternative approach, leveragingfree, noisy data from the web and simple, generic methods of recognition. Thisapproach has benefits in both performance and scalability. We demonstrate itsefficacy on four fine-grained datasets, greatly exceeding existing state of theart without the manual collection of even a single label, and furthermore showfirst results at scaling to more than 10,000 fine-grained categories.Quantitatively, we achieve top-1 accuracies of 92.3% on CUB-200-2011, 85.4% onBirdsnap, 93.4% on FGVC-Aircraft, and 80.8% on Stanford Dogs without usingtheir annotated training sets. We compare our approach to an active learningapproach for expanding fine-grained datasets.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (3), 301-320',\n",
       "  'citations': '230',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1511.06789v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=8931264715802059719&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 240: {'ID': 240,\n",
       "  'title': 'Learning Deep Context-aware Features over Body and Latent Parts for  Person Re-identification',\n",
       "  'authors': ['Xiaotang Chen', 'Zhang Zhang', 'Dangwei Li', 'Kaiqi Huang'],\n",
       "  'published': '2017-10-18T01:48:02Z',\n",
       "  'updated': '2017-10-18T01:48:02Z',\n",
       "  'abstract': 'Person Re-identification (ReID) is to identify the same person acrossdifferent cameras. It is a challenging task due to the large variations inperson pose, occlusion, background clutter, etc How to extract powerfulfeatures is a fundamental problem in ReID and is still an open problem today.In this paper, we design a Multi-Scale Context-Aware Network (MSCAN) to learnpowerful features over full body and body parts, which can well capture thelocal context knowledge by stacking multi-scale convolutions in each layer.Moreover, instead of using predefined rigid parts, we propose to learn andlocalize deformable pedestrian parts using Spatial Transformer Networks (STN)with novel spatial constraints. The learned body parts can release somedifficulties, eg pose variations and background clutters, in part-basedrepresentation. Finally, we integrate the representation learning processes offull body and body parts into a unified framework for person ReID throughmulti-class person identification tasks. Extensive evaluations on currentchallenging large-scale person ReID datasets, including the image-basedMarket1501, CUHK03 and sequence-based MARS datasets, show that the proposedmethod achieves the state-of-the-art results.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '407',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1710.06555v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=11787115398937943065&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 241: {'ID': 241,\n",
       "  'title': \"DeepStereo: Learning to Predict New Views from the World's Imagery\",\n",
       "  'authors': ['James Philbin', 'John Flynn', 'Ivan Neulander', 'Noah Snavely'],\n",
       "  'published': '2015-06-22T23:48:21Z',\n",
       "  'updated': '2015-06-22T23:48:21Z',\n",
       "  'abstract': 'Deep networks have recently enjoyed enormous success when applied torecognition and classification problems in computer vision, but their use ingraphics problems has been limited. In this work, we present a novel deeparchitecture that performs new view synthesis directly from pixels, trainedfrom a large number of posed image sets. In contrast to traditional approacheswhich consist of multiple complex stages of processing, each of which requirecareful tuning and can fail in unexpected ways, our system is trainedend-to-end. The pixels from neighboring views of a scene are presented to thenetwork which then directly produces the pixels of the unseen view. Thebenefits of our approach include generality (we only require posed image setsand can easily apply our method to different domains), and high quality resultson traditionally difficult scenes. We believe this is due to the end-to-endnature of our system which is able to plausibly generate pixels according tocolor, depth, and texture priors learnt automatically from the training data.To verify our method we show that it can convincingly reproduce known testviews from nearby imagery. Additionally we show images rendered from novelviewpoints. To our knowledge, our work is the first to apply deep learning tothe problem of new view synthesis from sets of real-world, natural imagery.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '306',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1506.06825v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=17671464158325028599&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 242: {'ID': 242,\n",
       "  'title': 'Designing Energy-Efficient Convolutional Neural Networks using  Energy-Aware Pruning',\n",
       "  'authors': ['Tien-Ju Yang', 'Yu-Hsin Chen', 'Vivienne Sze'],\n",
       "  'published': '2016-11-16T03:00:40Z',\n",
       "  'updated': '2017-04-18T19:49:29Z',\n",
       "  'abstract': 'Deep convolutional neural networks (CNNs) are indispensable tostate-of-the-art computer vision algorithms. However, they are still rarelydeployed on battery-powered mobile devices, such as smartphones and wearablegadgets, where vision algorithms can enable many revolutionary real-worldapplications. The key limiting factor is the high energy consumption of CNNprocessing due to its high computational complexity. While there are manyprevious efforts that try to reduce the CNN model size or amount ofcomputation, we find that they do not necessarily result in lower energyconsumption, and therefore do not serve as a good metric for energy costestimation.  To close the gap between CNN design and energy consumption optimization, wepropose an energy-aware pruning algorithm for CNNs that directly uses energyconsumption estimation of a CNN to guide the pruning process. The energyestimation methodology uses parameters extrapolated from actual hardwaremeasurements that target realistic battery-powered system setups. The proposedlayer-by-layer pruning algorithm also prunes more aggressively than previouslyproposed pruning methods by minimizing the error in output feature maps insteadof filter weights. For each layer, the weights are first pruned and thenlocally fine-tuned with a closed-form least-square solution to quickly restorethe accuracy. After all layers are pruned, the entire network is furtherglobally fine-tuned using back-propagation. With the proposed pruning method,the energy consumption of AlexNet and GoogLeNet are reduced by 3.7x and 1.6x,respectively, with less than 1% top-5 accuracy loss. Finally, we show thatpruning the AlexNet with a reduced number of target classes can greatlydecrease the number of weights but the energy reduction is limited.  Energy modeling tool and energy-aware pruned models available athttp://eyeriss.mit.edu/energy.html',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '323',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1611.05128v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=12390538587338513101&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 243: {'ID': 243,\n",
       "  'title': 'Direct Sparse Odometry',\n",
       "  'authors': ['Daniel Cremers', 'Jakob Engel', 'Vladlen Koltun'],\n",
       "  'published': '2016-07-09T04:02:31Z',\n",
       "  'updated': '2016-10-07T04:31:21Z',\n",
       "  'abstract': 'We propose a novel direct sparse visual odometry formulation. It combines afully direct probabilistic model (minimizing a photometric error) withconsistent, joint optimization of all model parameters, including geometry --represented as inverse depth in a reference frame -- and camera motion. This isachieved in real time by omitting the smoothness prior used in other directmethods and instead sampling pixels evenly throughout the images. Since ourmethod does not depend on keypoint detectors or descriptors, it can naturallysample pixels from across all image regions that have intensity gradient,including edges or smooth intensity variations on mostly white walls. Theproposed model integrates a full photometric calibration, accounting forexposure time, lens vignetting, and non-linear response functions. Wethoroughly evaluate our method on three different datasets comprising severalhours of video. The experiments show that the presented approach significantlyoutperforms state-of-the-art direct and indirect methods in a variety ofreal-world settings, both in terms of tracking accuracy and robustness.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 40 (3), 611-625',\n",
       "  'citations': '970',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1607.02565v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6310083354345062495&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 244: {'ID': 244,\n",
       "  'title': 'AMC: AutoML for Model Compression and Acceleration on Mobile Devices',\n",
       "  'authors': ['Song Han',\n",
       "   'Yihui He',\n",
       "   'Hanrui Wang',\n",
       "   'Ji Lin',\n",
       "   'Li-Jia Li',\n",
       "   'Zhijian Liu'],\n",
       "  'published': '2018-02-10T01:32:44Z',\n",
       "  'updated': '2019-01-16T03:25:50Z',\n",
       "  'abstract': 'Model compression is a critical technique to efficiently deploy neuralnetwork models on mobile devices which have limited computation resources andtight power budgets. Conventional model compression techniques rely onhand-crafted heuristics and rule-based policies that require domain experts toexplore the large design space trading off among model size, speed, andaccuracy, which is usually sub-optimal and time-consuming. In this paper, wepropose AutoML for Model Compression (AMC) which leverage reinforcementlearning to provide the model compression policy. This learning-basedcompression policy outperforms conventional rule-based compression policy byhaving higher compression ratio, better preserving the accuracy and freeinghuman labor. Under 4x FLOPs reduction, we achieved 2.7% better accuracy thanthe handcrafted model compression policy for VGG-16 on ImageNet. We appliedthis automated, push-the-button compression pipeline to MobileNet and achieved1.81x speedup of measured inference latency on an Android phone and 1.43xspeedup on the Titan XP GPU, with only 0.1% loss of ImageNet Top-1 accuracy.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 784-800',\n",
       "  'citations': '320',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1802.03494v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2282234460810497997&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 245: {'ID': 245,\n",
       "  'title': 'DeepID-Net: Deformable Deep Convolutional Neural Networks for Object  Detection',\n",
       "  'authors': ['Xingyu Zeng',\n",
       "   'Wanli Ouyang',\n",
       "   'Xiaoou Tang',\n",
       "   'Ping Luo',\n",
       "   'Xiaogang Wang',\n",
       "   'Hongsheng Li',\n",
       "   'Zhe Wang',\n",
       "   'Shi Qiu',\n",
       "   'Chen-Change Loy',\n",
       "   'Yonglong Tian',\n",
       "   'Shuo Yang'],\n",
       "  'published': '2014-12-17T22:41:35Z',\n",
       "  'updated': '2015-06-02T03:24:08Z',\n",
       "  'abstract': 'In this paper, we propose deformable deep convolutional neural networks forgeneric object detection. This new deep learning object detection framework hasinnovations in multiple aspects. In the proposed new deep architecture, a newdeformation constrained pooling (def-pooling) layer models the deformation ofobject parts with geometric constraint and penalty. A new pre-training strategyis proposed to learn feature representations more suitable for the objectdetection task and with good generalization capability. By changing the netstructures, training strategies, adding and removing some key components in thedetection pipeline, a set of models with large diversity are obtained, whichsignificantly improves the effectiveness of model averaging. The proposedapproach improves the mean averaged precision obtained by RCNN\\\\cite{girshick2014rich}, which was the state-of-the-art, from 31\\\\% to 50.3\\\\% onthe ILSVRC2014 detection test set. It also outperforms the winner ofILSVRC2014, GoogLeNet, by 6.1\\\\%. Detailed component-wise analysis is alsoprovided through extensive experimental evaluation, which provide a global viewfor people to understand the deep learning object detection pipeline.',\n",
       "  'categories': ['cs.CV', 'cs.NE'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '329',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1412.5661v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=8741182892774090657&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 246: {'ID': 246,\n",
       "  'title': 'Human Pose Estimation with Iterative Error Feedback',\n",
       "  'authors': ['Katerina Fragkiadaki',\n",
       "   'Jitendra Malik',\n",
       "   'Joao Carreira',\n",
       "   'Pulkit Agrawal'],\n",
       "  'published': '2015-07-23T16:20:57Z',\n",
       "  'updated': '2016-06-12T19:10:55Z',\n",
       "  'abstract': 'Hierarchical feature extractors such as Convolutional Networks (ConvNets)have achieved impressive performance on a variety of classification tasks usingpurely feedforward processing. Feedforward architectures can learn richrepresentations of the input space but do not explicitly model dependencies inthe output spaces, that are quite structured for tasks such as articulatedhuman pose estimation or object segmentation. Here we propose a framework thatexpands the expressive power of hierarchical feature extractors to encompassboth input and output spaces, by introducing top-down feedback. Instead ofdirectly predicting the outputs in one go, we use a self-correcting model thatprogressively changes an initial solution by feeding back error predictions, ina process we call Iterative Error Feedback (IEF). IEF shows excellentperformance on the task of articulated pose estimation in the challenging MPIIand LSP benchmarks, matching the state-of-the-art without requiring groundtruth scale annotation.',\n",
       "  'categories': ['cs.CV', 'cs.LG', 'cs.NE'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '417',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1507.06550v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=8678128637314457598&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 247: {'ID': 247,\n",
       "  'title': 'SSH: Single Stage Headless Face Detector',\n",
       "  'authors': ['Mahyar Najibi',\n",
       "   'Pouya Samangouei',\n",
       "   'Larry Davis',\n",
       "   'Rama Chellappa'],\n",
       "  'published': '2017-08-14T01:12:24Z',\n",
       "  'updated': '2017-10-18T00:07:03Z',\n",
       "  'abstract': 'We introduce the Single Stage Headless (SSH) face detector. Unlike two stageproposal-classification detectors, SSH detects faces in a single stage directlyfrom the early convolutional layers in a classification network. SSH isheadless. That is, it is able to achieve state-of-the-art results whileremoving the \"head\" of its underlying classification network -- i.e. all fullyconnected layers in the VGG-16 which contains a large number of parameters.Additionally, instead of relying on an image pyramid to detect faces withvarious scales, SSH is scale-invariant by design. We simultaneously detectfaces with different scales in a single forward pass of the network, but fromdifferent layers. These properties make SSH fast and light-weight.Surprisingly, with a headless VGG-16, SSH beats the ResNet-101-basedstate-of-the-art on the WIDER dataset. Even though, unlike the currentstate-of-the-art, SSH does not use an image pyramid and is 5X faster. Moreover,if an image pyramid is deployed, our light-weight network achievesstate-of-the-art on all subsets of the WIDER dataset, improving the AP by 2.5%.SSH also reaches state-of-the-art results on the FDDB and Pascal-Faces datasetswhile using a small input size, leading to a runtime of 50 ms/image on a GPU.The code is available at https://github.com/mahyarnajibi/SSH.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 4875-4884',\n",
       "  'citations': '190',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1708.03979v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=1379264465068802050&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 248: {'ID': 248,\n",
       "  'title': 'Lip Reading Sentences in the Wild',\n",
       "  'authors': ['Joon Son Chung',\n",
       "   'Andrew Zisserman',\n",
       "   'Oriol Vinyals',\n",
       "   'Andrew Senior'],\n",
       "  'published': '2016-11-16T16:53:46Z',\n",
       "  'updated': '2017-01-30T22:46:20Z',\n",
       "  'abstract': \"The goal of this work is to recognise phrases and sentences being spoken by atalking face, with or without the audio. Unlike previous works that havefocussed on recognising a limited number of words or phrases, we tackle lipreading as an open-world problem - unconstrained natural language sentences,and in the wild videos.  Our key contributions are: (1) a 'Watch, Listen, Attend and Spell' (WLAS)network that learns to transcribe videos of mouth motion to characters; (2) acurriculum learning strategy to accelerate training and to reduce overfitting;(3) a 'Lip Reading Sentences' (LRS) dataset for visual speech recognition,consisting of over 100,000 natural sentences from British television.  The WLAS model trained on the LRS dataset surpasses the performance of allprevious work on standard lip reading benchmark datasets, often by asignificant margin. This lip reading performance beats a professional lipreader on videos from BBC television, and we also demonstrate that visualinformation helps to improve speech recognition performance even when the audiois available.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'CVPR, 3444-3453',\n",
       "  'citations': '303',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1611.05358v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=3875356599930052575&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 249: {'ID': 249,\n",
       "  'title': 'Photographic Image Synthesis with Cascaded Refinement Networks',\n",
       "  'authors': ['Vladlen Koltun', 'Qifeng Chen'],\n",
       "  'published': '2017-07-28T20:24:44Z',\n",
       "  'updated': '2017-07-28T20:24:44Z',\n",
       "  'abstract': 'We present an approach to synthesizing photographic images conditioned onsemantic layouts. Given a semantic label map, our approach produces an imagewith photographic appearance that conforms to the input layout. The approachthus functions as a rendering engine that takes a two-dimensional semanticspecification of the scene and produces a corresponding photographic image.Unlike recent and contemporaneous work, our approach does not rely onadversarial training. We show that photographic images can be synthesized fromsemantic layouts by a single feedforward network with appropriate structure,trained end-to-end with a direct regression objective. The presented approachscales seamlessly to high resolutions; we demonstrate this by synthesizingphotographic images at 2-megapixel resolution, the full resolution of ourtraining data. Extensive perceptual experiments on datasets of outdoor andindoor scenes demonstrate that images synthesized by the presented approach areconsiderably more realistic than alternative approaches. The results are shownin the supplementary video at https://youtu.be/0fhUJT21-bs',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.GR', 'cs.LG'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 1511-1520',\n",
       "  'citations': '447',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1707.09405v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=1610098828132013923&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 250: {'ID': 250,\n",
       "  'title': 'Image-to-Image Translation with Conditional Adversarial Networks',\n",
       "  'authors': ['Alexei A. Efros',\n",
       "   'Tinghui Zhou',\n",
       "   'Phillip Isola',\n",
       "   'Jun-Yan Zhu'],\n",
       "  'published': '2016-11-21T20:48:16Z',\n",
       "  'updated': '2018-11-26T13:54:12Z',\n",
       "  'abstract': 'We investigate conditional adversarial networks as a general-purpose solutionto image-to-image translation problems. These networks not only learn themapping from input image to output image, but also learn a loss function totrain this mapping. This makes it possible to apply the same generic approachto problems that traditionally would require very different loss formulations.We demonstrate that this approach is effective at synthesizing photos fromlabel maps, reconstructing objects from edge maps, and colorizing images, amongother tasks. Indeed, since the release of the pix2pix software associated withthis paper, a large number of internet users (many of them artists) have postedtheir own experiments with our system, further demonstrating its wideapplicability and ease of adoption without the need for parameter tweaking. Asa community, we no longer hand-engineer our mapping functions, and this worksuggests we can achieve reasonable results without hand-engineering our lossfunctions either.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '5624',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1611.07004v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=16757839449706651543&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 251: {'ID': 251,\n",
       "  'title': 'ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic  Segmentation',\n",
       "  'authors': ['Anat Caspi',\n",
       "   'Sachin Mehta',\n",
       "   'Linda Shapiro',\n",
       "   'Mohammad Rastegari',\n",
       "   'Hannaneh Hajishirzi'],\n",
       "  'published': '2018-03-19T06:42:47Z',\n",
       "  'updated': '2018-07-25T00:45:02Z',\n",
       "  'abstract': 'We introduce a fast and efficient convolutional neural network, ESPNet, forsemantic segmentation of high resolution images under resource constraints.ESPNet is based on a new convolutional module, efficient spatial pyramid (ESP),which is efficient in terms of computation, memory, and power. ESPNet is 22times faster (on a standard GPU) and 180 times smaller than thestate-of-the-art semantic segmentation network PSPNet, while its category-wiseaccuracy is only 8% less. We evaluated ESPNet on a variety of semanticsegmentation datasets including Cityscapes, PASCAL VOC, and a breast biopsywhole slide image dataset. Under the same constraints on memory andcomputation, ESPNet outperforms all the current efficient CNN networks such asMobileNet, ShuffleNet, and ENet on both standard metrics and our newlyintroduced performance metrics that measure efficiency on edge devices. Ournetwork can process high resolution images at a rate of 112 and 9 frames persecond on a standard GPU and edge device, respectively.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 552-568',\n",
       "  'citations': '156',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1803.06815v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=11517977191335408434&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 252: {'ID': 252,\n",
       "  'title': 'Fast and Accurate Image Super-Resolution with Deep Laplacian Pyramid  Networks',\n",
       "  'authors': ['Ming-Hsuan Yang',\n",
       "   'Jia-Bin Huang',\n",
       "   'Wei-Sheng Lai',\n",
       "   'Narendra Ahuja'],\n",
       "  'published': '2017-10-04T17:58:55Z',\n",
       "  'updated': '2018-08-09T19:31:38Z',\n",
       "  'abstract': 'Convolutional neural networks have recently demonstrated high-qualityreconstruction for single image super-resolution. However, existing methodsoften require a large number of network parameters and entail heavycomputational loads at runtime for generating high-accuracy super-resolutionresults. In this paper, we propose the deep Laplacian Pyramid Super-ResolutionNetwork for fast and accurate image super-resolution. The proposed networkprogressively reconstructs the sub-band residuals of high-resolution images atmultiple pyramid levels. In contrast to existing methods that involve thebicubic interpolation for pre-processing (which results in large feature maps),the proposed method directly extracts features from the low-resolution inputspace and thereby entails low computational loads. We train the proposednetwork with deep supervision using the robust Charbonnier loss functions andachieve high-quality image reconstruction. Furthermore, we utilize therecursive layers to share parameters across as well as within pyramid levels,and thus drastically reduce the number of parameters. Extensive quantitativeand qualitative evaluations on benchmark datasets show that the proposedalgorithm performs favorably against the state-of-the-art methods in terms ofrun-time and image quality.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 41 (11), 2599\\xa0…',\n",
       "  'citations': '158',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1710.01992v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5617644766164625142&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 253: {'ID': 253,\n",
       "  'title': 'Transductive Multi-view Zero-Shot Learning',\n",
       "  'authors': ['Shaogang Gong',\n",
       "   'Yanwei Fu',\n",
       "   'Tao Xiang',\n",
       "   'Timothy M. Hospedales'],\n",
       "  'published': '2015-01-19T17:04:11Z',\n",
       "  'updated': '2015-03-03T04:43:44Z',\n",
       "  'abstract': 'Most existing zero-shot learning approaches exploit transfer learning via anintermediate-level semantic representation shared between an annotatedauxiliary dataset and a target dataset with different classes and noannotation. A projection from a low-level feature space to the semanticrepresentation space is learned from the auxiliary dataset and is appliedwithout adaptation to the target dataset. In this paper we identify twoinherent limitations with these approaches. First, due to having disjoint andpotentially unrelated classes, the projection functions learned from theauxiliary dataset/domain are biased when applied directly to the targetdataset/domain. We call this problem the projection domain shift problem andpropose a novel framework, transductive multi-view embedding, to solve it. Thesecond limitation is the prototype sparsity problem which refers to the factthat for each target class, only a single prototype is available for zero-shotlearning given a semantic representation. To overcome this problem, a novelheterogeneous multi-view hypergraph label propagation method is formulated forzero-shot learning in the transductive embedding space. It effectively exploitsthe complementary information offered by different semantic representations andtakes advantage of the manifold structures of multiple representation spaces ina coherent manner. We demonstrate through extensive experiments that theproposed approach (1) rectifies the projection shift between the auxiliary andtarget domains, (2) exploits the complementarity of multiple semanticrepresentations, (3) significantly outperforms existing methods for bothzero-shot and N-shot recognition on three image and video benchmark datasets,and (4) enables novel cross-view annotation tasks.',\n",
       "  'categories': ['cs.CV', 'cs.DS', 'cs.MM'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 37 (11), 2332\\xa0…',\n",
       "  'citations': '261',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1501.04560v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2010923217576224597&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 254: {'ID': 254,\n",
       "  'title': 'High-Resolution Image Synthesis and Semantic Manipulation with  Conditional GANs',\n",
       "  'authors': ['Bryan Catanzaro',\n",
       "   'Ting-Chun Wang',\n",
       "   'Jun-Yan Zhu',\n",
       "   'Andrew Tao',\n",
       "   'Ming-Yu Liu',\n",
       "   'Jan Kautz'],\n",
       "  'published': '2017-11-30T18:57:21Z',\n",
       "  'updated': '2018-08-20T17:55:56Z',\n",
       "  'abstract': 'We present a new method for synthesizing high-resolution photo-realisticimages from semantic label maps using conditional generative adversarialnetworks (conditional GANs). Conditional GANs have enabled a variety ofapplications, but the results are often limited to low-resolution and still farfrom realistic. In this work, we generate 2048x1024 visually appealing resultswith a novel adversarial loss, as well as new multi-scale generator anddiscriminator architectures. Furthermore, we extend our framework tointeractive visual manipulation with two additional features. First, weincorporate object instance segmentation information, which enables objectmanipulations such as removing/adding objects and changing the object category.Second, we propose a method to generate diverse results given the same input,allowing users to edit the object appearance interactively. Human opinionstudies demonstrate that our method significantly outperforms existing methods,advancing both the quality and the resolution of deep image synthesis andediting.',\n",
       "  'categories': ['cs.CV', 'cs.GR', 'cs.LG'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '941',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1711.11585v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=8637738140607437341&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 255: {'ID': 255,\n",
       "  'title': 'Semantic Scene Completion from a Single Depth Image',\n",
       "  'authors': ['Manolis Savva',\n",
       "   'Shuran Song',\n",
       "   'Fisher Yu',\n",
       "   'Thomas Funkhouser',\n",
       "   'Andy Zeng',\n",
       "   'Angel X. Chang'],\n",
       "  'published': '2016-11-28T03:38:42Z',\n",
       "  'updated': '2016-11-28T03:38:42Z',\n",
       "  'abstract': 'This paper focuses on semantic scene completion, a task for producing acomplete 3D voxel representation of volumetric occupancy and semantic labelsfor a scene from a single-view depth map observation. Previous work hasconsidered scene completion and semantic labeling of depth maps separately.However, we observe that these two problems are tightly intertwined. Toleverage the coupled nature of these two tasks, we introduce the semantic scenecompletion network (SSCNet), an end-to-end 3D convolutional network that takesa single depth image as input and simultaneously outputs occupancy and semanticlabels for all voxels in the camera view frustum. Our network uses adilation-based 3D context module to efficiently expand the receptive field andenable 3D context learning. To train our network, we construct SUNCG - amanually created large-scale dataset of synthetic 3D scenes with densevolumetric annotations. Our experiments demonstrate that the joint modeloutperforms methods addressing each task in isolation and outperformsalternative approaches on the semantic scene completion task.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '459',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1611.08974v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=3370839631979937893&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 256: {'ID': 256,\n",
       "  'title': 'CREST: Convolutional Residual Learning for Visual Tracking',\n",
       "  'authors': ['Ming-Hsuan Yang',\n",
       "   'Jiawei Zhang',\n",
       "   'Rynson Lau',\n",
       "   'Lijun Gong',\n",
       "   'Yibing Song',\n",
       "   'Chao Ma'],\n",
       "  'published': '2017-08-01T09:47:20Z',\n",
       "  'updated': '2017-08-01T09:47:20Z',\n",
       "  'abstract': 'Discriminative correlation filters (DCFs) have been shown to performsuperiorly in visual tracking. They only need a small set of training samplesfrom the initial frame to generate an appearance model. However, existing DCFslearn the filters separately from feature extraction, and update these filtersusing a moving average operation with an empirical weight. These DCF trackershardly benefit from the end-to-end training. In this paper, we propose theCREST algorithm to reformulate DCFs as a one-layer convolutional neuralnetwork. Our method integrates feature extraction, response map generation aswell as model update into the neural networks for an end-to-end training. Toreduce model degradation during online update, we apply residual learning totake appearance changes into account. Extensive experiments on the benchmarkdatasets demonstrate that our CREST tracker performs favorably againststate-of-the-art trackers.',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.MM'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 2555-2564',\n",
       "  'citations': '293',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1708.00225v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=3453971945427589773&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 257: {'ID': 257,\n",
       "  'title': 'Image Inpainting for Irregular Holes Using Partial Convolutions',\n",
       "  'authors': ['Kevin J. Shih',\n",
       "   'Bryan Catanzaro',\n",
       "   'Ting-Chun Wang',\n",
       "   'Guilin Liu',\n",
       "   'Fitsum A. Reda',\n",
       "   'Andrew Tao'],\n",
       "  'published': '2018-04-20T17:00:14Z',\n",
       "  'updated': '2018-12-15T22:22:34Z',\n",
       "  'abstract': 'Existing deep learning based image inpainting methods use a standardconvolutional network over the corrupted image, using convolutional filterresponses conditioned on both valid pixels as well as the substitute values inthe masked holes (typically the mean value). This often leads to artifacts suchas color discrepancy and blurriness. Post-processing is usually used to reducesuch artifacts, but are expensive and may fail. We propose the use of partialconvolutions, where the convolution is masked and renormalized to beconditioned on only valid pixels. We further include a mechanism toautomatically generate an updated mask for the next layer as part of theforward pass. Our model outperforms other methods for irregular masks. We showqualitative and quantitative comparisons with other methods to validate ourapproach.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 85-100',\n",
       "  'citations': '383',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1804.07723v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=13570032890679637624&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 258: {'ID': 258,\n",
       "  'title': 'Pedestrian Detection aided by Deep Learning Semantic Tasks',\n",
       "  'authors': ['Xiaogang Wang', 'Ping Luo', 'Yonglong Tian', 'Xiaoou Tang'],\n",
       "  'published': '2014-11-29T04:34:23Z',\n",
       "  'updated': '2014-11-29T04:34:23Z',\n",
       "  'abstract': \"Deep learning methods have achieved great success in pedestrian detection,owing to its ability to learn features from raw pixels. However, they mainlycapture middle-level representations, such as pose of pedestrian, but confusepositive with hard negative samples, which have large ambiguity, e.g. the shapeand appearance of `tree trunk' or `wire pole' are similar to pedestrian incertain viewpoint. This ambiguity can be distinguished by high-levelrepresentation. To this end, this work jointly optimizes pedestrian detectionwith semantic tasks, including pedestrian attributes (e.g. `carrying backpack')and scene attributes (e.g. `road', `tree', and `horizontal'). Rather thanexpensively annotating scene attributes, we transfer attributes informationfrom existing scene segmentation datasets to the pedestrian dataset, byproposing a novel deep model to learn high-level features from multiple tasksand multiple data sources. Since distinct tasks have distinct convergence ratesand data from different datasets have different distributions, a multi-taskobjective function is carefully designed to coordinate tasks and reducediscrepancies among datasets. The importance coefficients of tasks and networkparameters in this objective function can be iteratively estimated. Extensiveevaluations show that the proposed approach outperforms the state-of-the-art onthe challenging Caltech and ETH datasets, where it reduces the miss rates ofprevious deep models by 17 and 5.5 percent, respectively.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '339',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1412.0069v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=138052881719749555&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 259: {'ID': 259,\n",
       "  'title': 'Grounding of Textual Phrases in Images by Reconstruction',\n",
       "  'authors': ['Bernt Schiele',\n",
       "   'Ronghang Hu',\n",
       "   'Marcus Rohrbach',\n",
       "   'Trevor Darrell',\n",
       "   'Anna Rohrbach'],\n",
       "  'published': '2015-11-12T01:13:47Z',\n",
       "  'updated': '2017-02-17T21:02:05Z',\n",
       "  'abstract': 'Grounding (i.e. localizing) arbitrary, free-form textual phrases in visualcontent is a challenging problem with many applications for human-computerinteraction and image-text reference resolution. Few datasets provide theground truth spatial localization of phrases, thus it is desirable to learnfrom data with no or little grounding supervision. We propose a novel approachwhich learns grounding by reconstructing a given phrase using an attentionmechanism, which can be either latent or optimized directly. During trainingour approach encodes the phrase using a recurrent network language model andthen learns to attend to the relevant image region in order to reconstruct theinput phrase. At test time, the correct attention, i.e., the grounding, isevaluated. If grounding supervision is available it can be directly applied viaa loss over the attention mechanism. We demonstrate the effectiveness of ourapproach on the Flickr 30k Entities and ReferItGame datasets with differentlevels of supervision, ranging from no supervision over partial supervision tofull supervision. Our supervised variant improves by a large margin over thestate-of-the-art on both datasets.',\n",
       "  'categories': ['cs.CV', 'cs.CL', 'cs.LG'],\n",
       "  'journal': 'ECCV (1), 817-834',\n",
       "  'citations': '260',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1511.03745v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=15011608142174468380&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 260: {'ID': 260,\n",
       "  'title': 'Real-Time Single Image and Video Super-Resolution Using an Efficient  Sub-Pixel Convolutional Neural Network',\n",
       "  'authors': ['Zehan Wang',\n",
       "   'Andrew P. Aitken',\n",
       "   'Johannes Totz',\n",
       "   'Ferenc Huszár',\n",
       "   'Rob Bishop',\n",
       "   'Wenzhe Shi',\n",
       "   'Daniel Rueckert',\n",
       "   'Jose Caballero'],\n",
       "  'published': '2016-09-16T17:58:14Z',\n",
       "  'updated': '2016-09-23T17:16:37Z',\n",
       "  'abstract': 'Recently, several models based on deep neural networks have achieved greatsuccess in terms of both reconstruction accuracy and computational performancefor single image super-resolution. In these methods, the low resolution (LR)input image is upscaled to the high resolution (HR) space using a singlefilter, commonly bicubic interpolation, before reconstruction. This means thatthe super-resolution (SR) operation is performed in HR space. We demonstratethat this is sub-optimal and adds computational complexity. In this paper, wepresent the first convolutional neural network (CNN) capable of real-time SR of1080p videos on a single K2 GPU. To achieve this, we propose a novel CNNarchitecture where the feature maps are extracted in the LR space. In addition,we introduce an efficient sub-pixel convolution layer which learns an array ofupscaling filters to upscale the final LR feature maps into the HR output. Bydoing so, we effectively replace the handcrafted bicubic filter in the SRpipeline with more complex upscaling filters specifically trained for eachfeature map, whilst also reducing the computational complexity of the overallSR operation. We evaluate the proposed approach using images and videos frompublicly available datasets and show that it performs significantly better(+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude fasterthan previous CNN-based methods.',\n",
       "  'categories': ['cs.CV', 'stat.ML'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '1633',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1609.05158v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2184221316251559363&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 261: {'ID': 261,\n",
       "  'title': 'An exploration of parameter redundancy in deep networks with circulant  projections',\n",
       "  'authors': ['Alok Choudhary',\n",
       "   'Rogerio S. Feris',\n",
       "   'Felix X. Yu',\n",
       "   'Shih-Fu Chang',\n",
       "   'Yu Cheng',\n",
       "   'Sanjiv Kumar'],\n",
       "  'published': '2015-02-11T20:56:02Z',\n",
       "  'updated': '2015-10-27T06:45:51Z',\n",
       "  'abstract': 'We explore the redundancy of parameters in deep neural networks by replacingthe conventional linear projection in fully-connected layers with the circulantprojection. The circulant structure substantially reduces memory footprint andenables the use of the Fast Fourier Transform to speed up the computation.Considering a fully-connected neural network layer with d input nodes, and doutput nodes, this method improves the time complexity from O(d^2) to O(dlogd)and space complexity from O(d^2) to O(d). The space savings are particularlyimportant for modern deep convolutional neural network architectures, wherefully-connected layers typically contain more than 90% of the networkparameters. We further show that the gradient computation and optimization ofthe circulant projections can be performed very efficiently. Our experiments onthree standard datasets show that the proposed approach achieves thissignificant gain in storage and efficiency with minimal increase in error ratecompared to neural networks with unstructured projections.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 2857-2865',\n",
       "  'citations': '203',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1502.03436v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=700401025169084360&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 262: {'ID': 262,\n",
       "  'title': 'Quantization and Training of Neural Networks for Efficient  Integer-Arithmetic-Only Inference',\n",
       "  'authors': ['Andrew Howard',\n",
       "   'Matthew Tang',\n",
       "   'Benoit Jacob',\n",
       "   'Dmitry Kalenichenko',\n",
       "   'Hartwig Adam',\n",
       "   'Menglong Zhu',\n",
       "   'Bo Chen',\n",
       "   'Skirmantas Kligys'],\n",
       "  'published': '2017-12-15T23:56:52Z',\n",
       "  'updated': '2017-12-15T23:56:52Z',\n",
       "  'abstract': 'The rising popularity of intelligent mobile devices and the dauntingcomputational cost of deep learning-based models call for efficient andaccurate on-device inference schemes. We propose a quantization scheme thatallows inference to be carried out using integer-only arithmetic, which can beimplemented more efficiently than floating point inference on commonlyavailable integer-only hardware. We also co-design a training procedure topreserve end-to-end model accuracy post quantization. As a result, the proposedquantization scheme improves the tradeoff between accuracy and on-devicelatency. The improvements are significant even on MobileNets, a model familyknown for run-time efficiency, and are demonstrated in ImageNet classificationand COCO detection on popular CPUs.',\n",
       "  'categories': ['cs.LG', 'stat.ML'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '435',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1712.05877v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=17191795448759529953&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 263: {'ID': 263,\n",
       "  'title': 'Simultaneous Deep Transfer Across Domains and Tasks',\n",
       "  'authors': ['Eric Tzeng', 'Kate Saenko', 'Trevor Darrell', 'Judy Hoffman'],\n",
       "  'published': '2015-10-08T03:42:45Z',\n",
       "  'updated': '2015-10-08T03:42:45Z',\n",
       "  'abstract': 'Recent reports suggest that a generic supervised deep CNN model trained on alarge-scale dataset reduces, but does not remove, dataset bias. Fine-tuningdeep models in a new domain can require a significant amount of labeled data,which for many applications is simply not available. We propose a new CNNarchitecture to exploit unlabeled and sparsely labeled target domain data. Ourapproach simultaneously optimizes for domain invariance to facilitate domaintransfer and uses a soft label distribution matching loss to transferinformation between tasks. Our proposed adaptation method offers empiricalperformance which exceeds previously published results on two standardbenchmark visual domain adaptation tasks, evaluated across supervised andsemi-supervised adaptation settings.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 4068-4076',\n",
       "  'citations': '731',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1510.02192v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2973992380342580480&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 264: {'ID': 264,\n",
       "  'title': 'Searching for MobileNetV3',\n",
       "  'authors': ['Vijay Vasudevan',\n",
       "   'Mark Sandler',\n",
       "   'Bo Chen',\n",
       "   'Ruoming Pang',\n",
       "   'Weijun Wang',\n",
       "   'Hartwig Adam',\n",
       "   'Yukun Zhu',\n",
       "   'Mingxing Tan',\n",
       "   'Grace Chu',\n",
       "   'Liang-Chieh Chen',\n",
       "   'Quoc V. Le',\n",
       "   'Andrew Howard'],\n",
       "  'published': '2019-05-06T19:38:31Z',\n",
       "  'updated': '2019-11-20T17:26:40Z',\n",
       "  'abstract': 'We present the next generation of MobileNets based on a combination ofcomplementary search techniques as well as a novel architecture design.MobileNetV3 is tuned to mobile phone CPUs through a combination ofhardware-aware network architecture search (NAS) complemented by the NetAdaptalgorithm and then subsequently improved through novel architecture advances.This paper starts the exploration of how automated search algorithms andnetwork design can work together to harness complementary approaches improvingthe overall state of the art. Through this process we create two new MobileNetmodels for release: MobileNetV3-Large and MobileNetV3-Small which are targetedfor high and low resource use cases. These models are then adapted and appliedto the tasks of object detection and semantic segmentation. For the task ofsemantic segmentation (or any dense pixel prediction), we propose a newefficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling(LR-ASPP). We achieve new state of the art results for mobile classification,detection and segmentation. MobileNetV3-Large is 3.2\\\\% more accurate onImageNet classification while reducing latency by 15\\\\% compared to MobileNetV2.MobileNetV3-Small is 4.6\\\\% more accurate while reducing latency by 5\\\\% comparedto MobileNetV2. MobileNetV3-Large detection is 25\\\\% faster at roughly the sameaccuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30\\\\%faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 1314-1324',\n",
       "  'citations': '214',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1905.02244v5',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=10660854575390248964&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 265: {'ID': 265,\n",
       "  'title': 'ThiNet: A Filter Level Pruning Method for Deep Neural Network  Compression',\n",
       "  'authors': ['Weiyao Lin', 'Jian-Hao Luo', 'Jianxin Wu'],\n",
       "  'published': '2017-07-20T02:16:16Z',\n",
       "  'updated': '2017-07-20T02:16:16Z',\n",
       "  'abstract': 'We propose an efficient and unified framework, namely ThiNet, tosimultaneously accelerate and compress CNN models in both training andinference stages. We focus on the filter level pruning, i.e., the whole filterwould be discarded if it is less important. Our method does not change theoriginal network structure, thus it can be perfectly supported by anyoff-the-shelf deep learning libraries. We formally establish filter pruning asan optimization problem, and reveal that we need to prune filters based onstatistics information computed from its next layer, not the current layer,which differentiates ThiNet from existing methods. Experimental resultsdemonstrate the effectiveness of this strategy, which has advanced thestate-of-the-art. We also show the performance of ThiNet on ILSVRC-12benchmark. ThiNet achieves 3.31$\\\\times$ FLOPs reduction and 16.63$\\\\times$compression on VGG-16, with only 0.52$\\\\%$ top-5 accuracy drop. Similarexperiments with ResNet-50 reveal that even for a compact network, ThiNet canalso reduce more than half of the parameters and FLOPs, at the cost of roughly1$\\\\%$ top-5 accuracy drop. Moreover, the original VGG-16 model can be furtherpruned into a very small model with only 5.05MB model size, preserving AlexNetlevel accuracy but showing much stronger generalization ability.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 5058-5066',\n",
       "  'citations': '539',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1707.06342v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6995656357579329531&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 266: {'ID': 266,\n",
       "  'title': 'Audio-Visual Scene Analysis with Self-Supervised Multisensory Features',\n",
       "  'authors': ['Alexei A. Efros', 'Andrew Owens'],\n",
       "  'published': '2018-04-10T17:36:50Z',\n",
       "  'updated': '2018-10-09T07:15:29Z',\n",
       "  'abstract': \"The thud of a bouncing ball, the onset of speech as lips open -- when visualand audio events occur together, it suggests that there might be a common,underlying event that produced both signals. In this paper, we argue that thevisual and audio components of a video signal should be modeled jointly using afused multisensory representation. We propose to learn such a representation ina self-supervised way, by training a neural network to predict whether videoframes and audio are temporally aligned. We use this learned representation forthree applications: (a) sound source localization, i.e. visualizing the sourceof sound in a video; (b) audio-visual action recognition; and (c) on/off-screenaudio source separation, e.g. removing the off-screen translator's voice from aforeign official's speech. Code, models, and video results are available on ourwebpage: http://andrewowens.com/multisensory\",\n",
       "  'categories': ['cs.CV', 'cs.SD', 'eess.AS'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 631-648',\n",
       "  'citations': '168',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1804.03641v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2861344245084232221&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 267: {'ID': 267,\n",
       "  'title': 'Arbitrary Style Transfer in Real-time with Adaptive Instance  Normalization',\n",
       "  'authors': ['Xun Huang', 'Serge Belongie'],\n",
       "  'published': '2017-03-20T17:51:31Z',\n",
       "  'updated': '2017-07-30T09:32:17Z',\n",
       "  'abstract': 'Gatys et al. recently introduced a neural algorithm that renders a contentimage in the style of another image, achieving so-called style transfer.However, their framework requires a slow iterative optimization process, whichlimits its practical application. Fast approximations with feed-forward neuralnetworks have been proposed to speed up neural style transfer. Unfortunately,the speed improvement comes at a cost: the network is usually tied to a fixedset of styles and cannot adapt to arbitrary new styles. In this paper, wepresent a simple yet effective approach that for the first time enablesarbitrary style transfer in real-time. At the heart of our method is a noveladaptive instance normalization (AdaIN) layer that aligns the mean and varianceof the content features with those of the style features. Our method achievesspeed comparable to the fastest existing approach, without the restriction to apre-defined set of styles. In addition, our approach allows flexible usercontrols such as content-style trade-off, style interpolation, color &amp; spatialcontrols, all using a single feed-forward neural network.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 1501-1510',\n",
       "  'citations': '672',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1703.06868v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6462913724934880335&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 268: {'ID': 268,\n",
       "  'title': 'Ambient Sound Provides Supervision for Visual Learning',\n",
       "  'authors': ['Josh H. McDermott',\n",
       "   'Jiajun Wu',\n",
       "   'Antonio Torralba',\n",
       "   'Andrew Owens',\n",
       "   'William T. Freeman'],\n",
       "  'published': '2016-08-25T04:50:16Z',\n",
       "  'updated': '2016-12-05T19:14:26Z',\n",
       "  'abstract': 'The sound of crashing waves, the roar of fast-moving cars -- sound conveysimportant information about the objects in our surroundings. In this work, weshow that ambient sounds can be used as a supervisory signal for learningvisual models. To demonstrate this, we train a convolutional neural network topredict a statistical summary of the sound associated with a video frame. Weshow that, through this process, the network learns a representation thatconveys information about objects and scenes. We evaluate this representationon several recognition tasks, finding that its performance is comparable tothat of other state-of-the-art unsupervised learning methods. Finally, we showthrough visualizations that the network learns units that are selective toobjects that are often associated with characteristic sounds.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (1), 801-816',\n",
       "  'citations': '189',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1608.07017v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=8165699140474849967&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 269: {'ID': 269,\n",
       "  'title': 'The Unreasonable Effectiveness of Deep Features as a Perceptual Metric',\n",
       "  'authors': ['Alexei A. Efros',\n",
       "   'Oliver Wang',\n",
       "   'Phillip Isola',\n",
       "   'Eli Shechtman',\n",
       "   'Richard Zhang'],\n",
       "  'published': '2018-01-11T18:54:17Z',\n",
       "  'updated': '2018-04-10T19:25:07Z',\n",
       "  'abstract': 'While it is nearly effortless for humans to quickly assess the perceptualsimilarity between two images, the underlying processes are thought to be quitecomplex. Despite this, the most widely used perceptual metrics today, such asPSNR and SSIM, are simple, shallow functions, and fail to account for manynuances of human perception. Recently, the deep learning community has foundthat features of the VGG network trained on ImageNet classification has beenremarkably useful as a training loss for image synthesis. But how perceptualare these so-called \"perceptual losses\"? What elements are critical for theirsuccess? To answer these questions, we introduce a new dataset of humanperceptual similarity judgments. We systematically evaluate deep featuresacross different architectures and tasks and compare them with classic metrics.We find that deep features outperform all previous metrics by large margins onour dataset. More surprisingly, this result is not restricted toImageNet-trained VGG features, but holds across different deep architecturesand levels of supervision (supervised, self-supervised, or even unsupervised).Our results suggest that perceptual similarity is an emergent property sharedacross deep visual representations.',\n",
       "  'categories': ['cs.CV', 'cs.GR'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '659',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1801.03924v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14149575231067904672&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 270: {'ID': 270,\n",
       "  'title': 'Implicit 3D Orientation Learning for 6D Object Detection from RGB Images',\n",
       "  'authors': ['Manuel Brucker',\n",
       "   'Maximilian Durner',\n",
       "   'Martin Sundermeyer',\n",
       "   'Zoltan-Csaba Marton',\n",
       "   'Rudolph Triebel'],\n",
       "  'published': '2019-02-04T16:03:57Z',\n",
       "  'updated': '2019-07-17T14:12:26Z',\n",
       "  'abstract': 'We propose a real-time RGB-based pipeline for object detection and 6D poseestimation. Our novel 3D orientation estimation is based on a variant of theDenoising Autoencoder that is trained on simulated views of a 3D model usingDomain Randomization. This so-called Augmented Autoencoder has severaladvantages over existing methods: It does not require real, pose-annotatedtraining data, generalizes to various test sensors and inherently handlesobject and view symmetries. Instead of learning an explicit mapping from inputimages to object poses, it provides an implicit representation of objectorientations defined by samples in a latent space. Our pipeline achievesstate-of-the-art performance on the T-LESS dataset both in the RGB and RGB-Ddomain. We also evaluate on the LineMOD dataset where we can compete with othersynthetically trained approaches. We further increase performance by correcting3D orientation estimates to account for perspective errors when the objectdeviates from the image center and show extended results.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 699-715',\n",
       "  'citations': '145',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1902.01275v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=7729841025223671845&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 271: {'ID': 271,\n",
       "  'title': 'Speed/accuracy trade-offs for modern convolutional object detectors',\n",
       "  'authors': ['Anoop Korattikara',\n",
       "   'Kevin Murphy',\n",
       "   'Chen Sun',\n",
       "   'Jonathan Huang',\n",
       "   'Menglong Zhu',\n",
       "   'Yang Song',\n",
       "   'Vivek Rathod',\n",
       "   'Sergio Guadarrama',\n",
       "   'Alireza Fathi',\n",
       "   'Zbigniew Wojna',\n",
       "   'Ian Fischer'],\n",
       "  'published': '2016-11-30T06:06:15Z',\n",
       "  'updated': '2017-04-25T03:42:55Z',\n",
       "  'abstract': 'The goal of this paper is to serve as a guide for selecting a detectionarchitecture that achieves the right speed/memory/accuracy balance for a givenapplication and platform. To this end, we investigate various ways to tradeaccuracy for speed and memory usage in modern convolutional object detectionsystems. A number of successful systems have been proposed in recent years, butapples-to-apples comparisons are difficult due to different base featureextractors (e.g., VGG, Residual Networks), different default image resolutions,as well as different hardware and software platforms. We present a unifiedimplementation of the Faster R-CNN [Ren et al., 2015], R-FCN [Dai et al., 2016]and SSD [Liu et al., 2015] systems, which we view as \"meta-architectures\" andtrace out the speed/accuracy trade-off curve created by using alternativefeature extractors and varying other critical parameters such as image sizewithin each of these meta-architectures. On one extreme end of this spectrumwhere speed and memory are critical, we present a detector that achieves realtime speeds and can be deployed on a mobile device. On the opposite end inwhich accuracy is critical, we present a detector that achievesstate-of-the-art performance measured on the COCO detection task.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '1381',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1611.10012v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=8642196741698958427&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 272: {'ID': 272,\n",
       "  'title': 'Multimodal Machine Learning: A Survey and Taxonomy',\n",
       "  'authors': ['Chaitanya Ahuja',\n",
       "   'Louis-Philippe Morency',\n",
       "   'Tadas Baltrušaitis'],\n",
       "  'published': '2017-05-26T01:35:31Z',\n",
       "  'updated': '2017-08-01T17:39:39Z',\n",
       "  'abstract': 'Our experience of the world is multimodal - we see objects, hear sounds, feeltexture, smell odors, and taste flavors. Modality refers to the way in whichsomething happens or is experienced and a research problem is characterized asmultimodal when it includes multiple such modalities. In order for ArtificialIntelligence to make progress in understanding the world around us, it needs tobe able to interpret such multimodal signals together. Multimodal machinelearning aims to build models that can process and relate information frommultiple modalities. It is a vibrant multi-disciplinary field of increasingimportance and with extraordinary potential. Instead of focusing on specificmultimodal applications, this paper surveys the recent advances in multimodalmachine learning itself and presents them in a common taxonomy. We go beyondthe typical early and late fusion categorization and identify broaderchallenges that are faced by multimodal machine learning, namely:representation, translation, alignment, fusion, and co-learning. This newtaxonomy will enable researchers to better understand the state of the fieldand identify directions for future research.',\n",
       "  'categories': ['cs.LG'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 41 (2), 423-443',\n",
       "  'citations': '443',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1705.09406v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=1491293889113334971&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 273: {'ID': 273,\n",
       "  'title': '3D Object Proposals using Stereo Imagery for Accurate Object Class  Detection',\n",
       "  'authors': ['Kaustav Kundu',\n",
       "   'Raquel Urtasun',\n",
       "   'Sanja Fidler',\n",
       "   'Huimin Ma',\n",
       "   'Yukun Zhu',\n",
       "   'Xiaozhi Chen'],\n",
       "  'published': '2016-08-27T13:39:04Z',\n",
       "  'updated': '2017-04-25T07:58:07Z',\n",
       "  'abstract': 'The goal of this paper is to perform 3D object detection in the context ofautonomous driving. Our method first aims at generating a set of high-quality3D object proposals by exploiting stereo imagery. We formulate the problem asminimizing an energy function that encodes object size priors, placement ofobjects on the ground plane as well as several depth informed features thatreason about free space, point cloud densities and distance to the ground. Wethen exploit a CNN on top of these proposals to perform object detection. Inparticular, we employ a convolutional neural net (CNN) that exploits contextand depth information to jointly regress to 3D bounding box coordinates andobject pose. Our experiments show significant performance gains over existingRGB and RGB-D object proposal methods on the challenging KITTI benchmark. Whencombined with the CNN, our approach outperforms all existing results in objectdetection and orientation estimation tasks for all three KITTI object classes.Furthermore, we experiment also with the setting where LIDAR information isavailable, and show that using both LIDAR and stereo leads to the best result.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 40 (5), 1259-1272',\n",
       "  'citations': '132',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1608.07711v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=13281551936113812657&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 274: {'ID': 274,\n",
       "  'title': 'Efficient piecewise training of deep structured models for semantic  segmentation',\n",
       "  'authors': ['Guosheng Lin',\n",
       "   'Anton van dan Hengel',\n",
       "   'Chunhua Shen',\n",
       "   'Ian Reid'],\n",
       "  'published': '2015-04-04T14:26:23Z',\n",
       "  'updated': '2016-06-06T00:26:44Z',\n",
       "  'abstract': \"Recent advances in semantic image segmentation have mostly been achieved bytraining deep convolutional neural networks (CNNs). We show how to improvesemantic segmentation through the use of contextual information; specifically,we explore `patch-patch' context between image regions, and `patch-background'context. For learning from the patch-patch context, we formulate ConditionalRandom Fields (CRFs) with CNN-based pairwise potential functions to capturesemantic correlations between neighboring patches. Efficient piecewise trainingof the proposed deep structured model is then applied to avoid repeatedexpensive CRF inference for back propagation. For capturing thepatch-background context, we show that a network design with traditionalmulti-scale image input and sliding pyramid pooling is effective for improvingperformance. Our experimental results set new state-of-the-art performance on anumber of popular semantic segmentation datasets, including NYUDv2, PASCAL VOC2012, PASCAL-Context, and SIFT-flow. In particular, we achieve anintersection-over-union score of 78.0 on the challenging PASCAL VOC 2012dataset.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '665',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1504.01013v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14079239548452791512&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 275: {'ID': 275,\n",
       "  'title': 'Latent Embeddings for Zero-shot Classification',\n",
       "  'authors': ['Gaurav Sharma',\n",
       "   'Bernt Schiele',\n",
       "   'Quynh Nguyen',\n",
       "   'Matthias Hein',\n",
       "   'Zeynep Akata',\n",
       "   'Yongqin Xian'],\n",
       "  'published': '2016-03-29T19:24:38Z',\n",
       "  'updated': '2016-04-10T10:33:02Z',\n",
       "  'abstract': 'We present a novel latent embedding model for learning a compatibilityfunction between image and class embeddings, in the context of zero-shotclassification. The proposed method augments the state-of-the-art bilinearcompatibility model by incorporating latent variables. Instead of learning asingle bilinear map, it learns a collection of maps with the selection, ofwhich map to use, being a latent variable for the current image-class pair. Wetrain the model with a ranking based objective function which penalizesincorrect rankings of the true class for a given image. We empiricallydemonstrate that our model improves the state-of-the-art for various classembeddings consistently on three challenging publicly available datasets forthe zero-shot setting. Moreover, our method leads to visually highlyinterpretable results with clear clusters of different fine-grained objectproperties that correspond to different latent variable maps.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '353',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1603.08895v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2270096778718786860&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 276: {'ID': 276,\n",
       "  'title': 'Large Pose 3D Face Reconstruction from a Single Image via Direct  Volumetric CNN Regression',\n",
       "  'authors': ['Vasileios Argyriou',\n",
       "   'Georgios Tzimiropoulos',\n",
       "   'Adrian Bulat',\n",
       "   'Aaron S. Jackson'],\n",
       "  'published': '2017-03-22T20:00:15Z',\n",
       "  'updated': '2017-09-08T09:10:08Z',\n",
       "  'abstract': '3D face reconstruction is a fundamental Computer Vision problem ofextraordinary difficulty. Current systems often assume the availability ofmultiple facial images (sometimes from the same subject) as input, and mustaddress a number of methodological challenges such as establishing densecorrespondences across large facial poses, expressions, and non-uniformillumination. In general these methods require complex and inefficientpipelines for model building and fitting. In this work, we propose to addressmany of these limitations by training a Convolutional Neural Network (CNN) onan appropriate dataset consisting of 2D images and 3D facial models or scans.Our CNN works with just a single 2D facial image, does not require accuratealignment nor establishes dense correspondence between images, works forarbitrary facial poses and expressions, and can be used to reconstruct thewhole 3D facial geometry (including the non-visible parts of the face)bypassing the construction (during training) and fitting (during testing) of a3D Morphable Model. We achieve this via a simple CNN architecture that performsdirect regression of a volumetric representation of the 3D facial geometry froma single 2D image. We also demonstrate how the related task of facial landmarklocalization can be incorporated into the proposed framework and help improvereconstruction quality, especially for the cases of large poses and facialexpressions. Testing code will be made available online, along with pre-trainedmodels http://aaronsplace.co.uk/papers/jackson2017recon',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 1031-1039',\n",
       "  'citations': '206',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1703.07834v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=15834263933659566679&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 277: {'ID': 277,\n",
       "  'title': 'Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a  Single Image',\n",
       "  'authors': ['Angjoo Kanazawa',\n",
       "   'Federica Bogo',\n",
       "   'Michael J. Black',\n",
       "   'Javier Romero',\n",
       "   'Christoph Lassner',\n",
       "   'Peter Gehler'],\n",
       "  'published': '2016-07-27T14:46:36Z',\n",
       "  'updated': '2016-07-27T14:46:36Z',\n",
       "  'abstract': 'We describe the first method to automatically estimate the 3D pose of thehuman body as well as its 3D shape from a single unconstrained image. Weestimate a full 3D mesh and show that 2D joints alone carry a surprising amountof information about body shape. The problem is challenging because of thecomplexity of the human body, articulation, occlusion, clothing, lighting, andthe inherent ambiguity in inferring 3D from 2D. To solve this, we first use arecently published CNN-based method, DeepCut, to predict (bottom-up) the 2Dbody joint locations. We then fit (top-down) a recently published statisticalbody shape model, called SMPL, to the 2D joints. We do so by minimizing anobjective function that penalizes the error between the projected 3D modeljoints and detected 2D joints. Because SMPL captures correlations in humanshape across the population, we are able to robustly fit it to very littledata. We further leverage the 3D model to prevent solutions that causeinterpenetration. We evaluate our method, SMPLify, on the Leeds Sports,HumanEva, and Human3.6M datasets, showing superior pose accuracy with respectto the state of the art.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (5), 561-578',\n",
       "  'citations': '493',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1607.08128v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=8170741491731797357&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 278: {'ID': 278,\n",
       "  'title': 'Recurrent Instance Segmentation',\n",
       "  'authors': ['Bernardino Romera-Paredes', 'Philip H. S. Torr'],\n",
       "  'published': '2015-11-25T23:28:14Z',\n",
       "  'updated': '2016-10-24T23:57:19Z',\n",
       "  'abstract': 'Instance segmentation is the problem of detecting and delineating eachdistinct object of interest appearing in an image. Current instancesegmentation approaches consist of ensembles of modules that are trainedindependently of each other, thus missing opportunities for joint learning.Here we propose a new instance segmentation paradigm consisting in anend-to-end method that learns how to segment instances sequentially. The modelis based on a recurrent neural network that sequentially finds objects andtheir segmentations one at a time. This net is provided with a spatial memorythat keeps track of what pixels have been explained and allows occlusionhandling. In order to train the model we designed a principled loss functionthat accurately represents the properties of the instance segmentation problem.In the experiments carried out, we found that our method outperforms recentapproaches on multiple person segmentation, and all state of the art approacheson the Plant Phenotyping dataset for leaf counting.',\n",
       "  'categories': ['cs.CV', 'cs.AI'],\n",
       "  'journal': 'ECCV (6), 312-329',\n",
       "  'citations': '228',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1511.08250v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14727618954232036196&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 279: {'ID': 279,\n",
       "  'title': '3D Bounding Box Estimation Using Deep Learning and Geometry',\n",
       "  'authors': ['Dragomir Anguelov',\n",
       "   'John Flynn',\n",
       "   'Arsalan Mousavian',\n",
       "   'Jana Kosecka'],\n",
       "  'published': '2016-12-01T22:16:48Z',\n",
       "  'updated': '2017-04-10T19:05:46Z',\n",
       "  'abstract': 'We present a method for 3D object detection and pose estimation from a singleimage. In contrast to current techniques that only regress the 3D orientationof an object, our method first regresses relatively stable 3D object propertiesusing a deep convolutional neural network and then combines these estimateswith geometric constraints provided by a 2D object bounding box to produce acomplete 3D bounding box. The first network output estimates the 3D objectorientation using a novel hybrid discrete-continuous loss, which significantlyoutperforms the L2 loss. The second output regresses the 3D object dimensions,which have relatively little variance compared to alternatives and can often bepredicted for many object types. These estimates, combined with the geometricconstraints on translation imposed by the 2D bounding box, enable us to recovera stable and accurate 3D object pose. We evaluate our method on the challengingKITTI object detection benchmark both on the official metric of 3D orientationestimation and also on the accuracy of the obtained 3D bounding boxes. Althoughconceptually simple, our method outperforms more complex and computationallyexpensive approaches that leverage semantic segmentation, instance levelsegmentation and flat ground priors and sub-category detection. Ourdiscrete-continuous loss also produces state of the art results for 3Dviewpoint estimation on the Pascal 3D+ dataset.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '325',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1612.00496v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=17791401340833451444&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 280: {'ID': 280,\n",
       "  'title': 'Single Image 3D Interpreter Network',\n",
       "  'authors': ['Joseph J. Lim',\n",
       "   'Jiajun Wu',\n",
       "   'Joshua B. Tenenbaum',\n",
       "   'Antonio Torralba',\n",
       "   'Tianfan Xue',\n",
       "   'Yuandong Tian',\n",
       "   'William T. Freeman'],\n",
       "  'published': '2016-04-29T04:52:46Z',\n",
       "  'updated': '2016-10-04T19:35:54Z',\n",
       "  'abstract': 'Understanding 3D object structure from a single image is an important butdifficult task in computer vision, mostly due to the lack of 3D objectannotations in real images. Previous work tackles this problem by eithersolving an optimization task given 2D keypoint positions, or training onsynthetic data with ground truth 3D information. In this work, we propose 3DINterpreter Network (3D-INN), an end-to-end framework which sequentiallyestimates 2D keypoint heatmaps and 3D object structure, trained on both real2D-annotated images and synthetic 3D data. This is made possible mainly by twotechnical innovations. First, we propose a Projection Layer, which projectsestimated 3D structure to 2D space, so that 3D-INN can be trained to predict 3Dstructural parameters supervised by 2D annotations on real images. Second,heatmaps of keypoints serve as an intermediate representation connecting realand synthetic data, enabling 3D-INN to benefit from the variation and abundanceof synthetic 3D objects, without suffering from the difference between thestatistics of real and synthesized images due to imperfect rendering. Thenetwork achieves state-of-the-art performance on both 2D keypoint estimationand 3D structure recovery. We also show that the recovered 3D information canbe used in other vision applications, such as 3D rendering and image retrieval.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'journal': 'ECCV (6), 365-382',\n",
       "  'citations': '231',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1604.08685v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6568776694831795699&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 281: {'ID': 281,\n",
       "  'title': 'Adversarial Discriminative Domain Adaptation',\n",
       "  'authors': ['Eric Tzeng', 'Kate Saenko', 'Trevor Darrell', 'Judy Hoffman'],\n",
       "  'published': '2017-02-17T18:10:53Z',\n",
       "  'updated': '2017-02-17T18:10:53Z',\n",
       "  'abstract': 'Adversarial learning methods are a promising approach to training robust deepnetworks, and can generate complex samples across diverse domains. They alsocan improve recognition despite the presence of domain shift or dataset bias:several adversarial approaches to unsupervised domain adaptation have recentlybeen introduced, which reduce the difference between the training and testdomain distributions and thus improve generalization performance. Priorgenerative approaches show compelling visualizations, but are not optimal ondiscriminative tasks and can be limited to smaller shifts. Prior discriminativeapproaches could handle larger domain shifts, but imposed tied weights on themodel and did not exploit a GAN-based loss. We first outline a novelgeneralized framework for adversarial adaptation, which subsumes recentstate-of-the-art approaches as special cases, and we use this generalized viewto better relate the prior approaches. We propose a previously unexploredinstance of our general framework which combines discriminative modeling,untied weight sharing, and a GAN loss, which we call Adversarial DiscriminativeDomain Adaptation (ADDA). We show that ADDA is more effective yet considerablysimpler than competing domain-adversarial methods, and demonstrate the promiseof our approach by exceeding state-of-the-art unsupervised adaptation resultson standard cross-domain digit classification tasks and a new more difficultcross-modality object classification task.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '1351',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1702.05464v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14531515109021297962&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 282: {'ID': 282,\n",
       "  'title': 'Instance-aware Semantic Segmentation via Multi-task Network Cascades',\n",
       "  'authors': ['Jian Sun', 'Jifeng Dai', 'Kaiming He'],\n",
       "  'published': '2015-12-14T17:17:23Z',\n",
       "  'updated': '2015-12-14T17:17:23Z',\n",
       "  'abstract': 'Semantic segmentation research has recently witnessed rapid progress, butmany leading methods are unable to identify object instances. In this paper, wepresent Multi-task Network Cascades for instance-aware semantic segmentation.Our model consists of three networks, respectively differentiating instances,estimating masks, and categorizing objects. These networks form a cascadedstructure, and are designed to share their convolutional features. We developan algorithm for the nontrivial end-to-end training of this causal, cascadedstructure. Our solution is a clean, single-step training framework and can begeneralized to cascades that have more stages. We demonstrate state-of-the-artinstance-aware semantic segmentation accuracy on PASCAL VOC. Meanwhile, ourmethod takes only 360ms testing an image using VGG-16, which is two orders ofmagnitude faster than previous systems for this challenging problem. As a byproduct, our method also achieves compelling object detection results whichsurpass the competitive Fast/Faster R-CNN systems.  The method described in this paper is the foundation of our submissions tothe MS COCO 2015 segmentation competition, where we won the 1st place.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '791',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1512.04412v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=7342374495730352680&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 283: {'ID': 283,\n",
       "  'title': 'Learning to Reason: End-to-End Module Networks for Visual Question  Answering',\n",
       "  'authors': ['Jacob Andreas',\n",
       "   'Ronghang Hu',\n",
       "   'Kate Saenko',\n",
       "   'Marcus Rohrbach',\n",
       "   'Trevor Darrell'],\n",
       "  'published': '2017-04-18T20:57:32Z',\n",
       "  'updated': '2017-09-11T22:22:59Z',\n",
       "  'abstract': 'Natural language questions are inherently compositional, and many are mosteasily answered by reasoning about their decomposition into modularsub-problems. For example, to answer \"is there an equal number of balls andboxes?\" we can look for balls, look for boxes, count them, and compare theresults. The recently proposed Neural Module Network (NMN) architectureimplements this approach to question answering by parsing questions intolinguistic substructures and assembling question-specific deep networks fromsmaller modules that each solve one subtask. However, existing NMNimplementations rely on brittle off-the-shelf parsers, and are restricted tothe module configurations proposed by these parsers rather than learning themfrom data. In this paper, we propose End-to-End Module Networks (N2NMNs), whichlearn to reason by directly predicting instance-specific network layoutswithout the aid of a parser. Our model learns to generate network structures(by imitating expert demonstrations) while simultaneously learning networkparameters (using the downstream task loss). Experimental results on the newCLEVR dataset targeted at compositional question answering show that N2NMNsachieve an error reduction of nearly 50% relative to state-of-the-artattentional approaches, while discovering interpretable network architecturesspecialized for each question.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 804-813',\n",
       "  'citations': '280',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1704.05526v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=7936052564820157532&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 284: {'ID': 284,\n",
       "  'title': 'GANimation: Anatomically-aware Facial Animation from a Single Image',\n",
       "  'authors': ['Albert Pumarola',\n",
       "   'Alberto Sanfeliu',\n",
       "   'Antonio Agudo',\n",
       "   'Francesc Moreno-Noguer',\n",
       "   'Aleix M. Martinez'],\n",
       "  'published': '2018-07-24T17:47:09Z',\n",
       "  'updated': '2018-08-28T23:46:23Z',\n",
       "  'abstract': 'Recent advances in Generative Adversarial Networks (GANs) have shownimpressive results for task of facial expression synthesis. The most successfularchitecture is StarGAN, that conditions GANs generation process with images ofa specific domain, namely a set of images of persons sharing the sameexpression. While effective, this approach can only generate a discrete numberof expressions, determined by the content of the dataset. To address thislimitation, in this paper, we introduce a novel GAN conditioning scheme basedon Action Units (AU) annotations, which describes in a continuous manifold theanatomical facial movements defining a human expression. Our approach allowscontrolling the magnitude of activation of each AU and combine several of them.Additionally, we propose a fully unsupervised strategy to train the model, thatonly requires images annotated with their activated AUs, and exploit attentionmechanisms that make our network robust to changing backgrounds and lightingconditions. Extensive evaluation show that our approach goes beyond competingconditional generators both in the capability to synthesize a much wider rangeof expressions ruled by anatomically feasible muscle movements, as in thecapacity of dealing with images in the wild.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 818-833',\n",
       "  'citations': '187',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1807.09251v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=15397536897136412791&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 285: {'ID': 285,\n",
       "  'title': 'Discriminative Unsupervised Feature Learning with Exemplar Convolutional  Neural Networks',\n",
       "  'authors': ['Philipp Fischer',\n",
       "   'Jost Tobias Springenberg',\n",
       "   'Martin Riedmiller',\n",
       "   'Alexey Dosovitskiy',\n",
       "   'Thomas Brox'],\n",
       "  'published': '2014-06-26T15:07:14Z',\n",
       "  'updated': '2015-06-19T11:43:36Z',\n",
       "  'abstract': \"Deep convolutional networks have proven to be very successful in learningtask specific features that allow for unprecedented performance on variouscomputer vision tasks. Training of such networks follows mostly the supervisedlearning paradigm, where sufficiently many input-output pairs are required fortraining. Acquisition of large training sets is one of the key challenges, whenapproaching a new task. In this paper, we aim for generic feature learning andpresent an approach for training a convolutional network using only unlabeleddata. To this end, we train the network to discriminate between a set ofsurrogate classes. Each surrogate class is formed by applying a variety oftransformations to a randomly sampled 'seed' image patch. In contrast tosupervised network training, the resulting feature representation is not classspecific. It rather provides robustness to the transformations that have beenapplied during training. This generic feature representation allows forclassification results that outperform the state of the art for unsupervisedlearning on several popular datasets (STL-10, CIFAR-10, Caltech-101,Caltech-256). While such generic features cannot compete with class specificfeatures from supervised training on a classification task, we show that theyare advantageous on geometric matching problems, where they also outperform theSIFT descriptor.\",\n",
       "  'categories': ['cs.LG', 'cs.CV', 'cs.NE'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 38 (9), 1734-1747',\n",
       "  'citations': '212',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1406.6909v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=13852925172647616643&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 286: {'ID': 286,\n",
       "  'title': 'Supervised Discrete Hashing',\n",
       "  'authors': ['Wei Liu', 'Heng Tao Shen', 'Chunhua Shen', 'Fumin Shen'],\n",
       "  'published': '2015-03-05T07:06:02Z',\n",
       "  'updated': '2015-04-19T02:42:16Z',\n",
       "  'abstract': 'This paper has been withdrawn by the authour.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '727',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1503.01557v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=7283542758697728805&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 287: {'ID': 287,\n",
       "  'title': 'Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial  Networks',\n",
       "  'authors': ['Konstantinos Bousmalis',\n",
       "   'Dumitru Erhan',\n",
       "   'Nathan Silberman',\n",
       "   'Dilip Krishnan',\n",
       "   'David Dohan'],\n",
       "  'published': '2016-12-16T10:50:36Z',\n",
       "  'updated': '2017-08-23T12:35:56Z',\n",
       "  'abstract': 'Collecting well-annotated image datasets to train modern machine learningalgorithms is prohibitively expensive for many tasks. One appealing alternativeis rendering synthetic data where ground-truth annotations are generatedautomatically. Unfortunately, models trained purely on rendered images oftenfail to generalize to real images. To address this shortcoming, prior workintroduced unsupervised domain adaptation algorithms that attempt to maprepresentations between the two domains or learn to extract features that aredomain-invariant. In this work, we present a new approach that learns, in anunsupervised manner, a transformation in the pixel space from one domain to theother. Our generative adversarial network (GAN)-based method adaptssource-domain images to appear as if drawn from the target domain. Our approachnot only produces plausible samples, but also outperforms the state-of-the-arton a number of unsupervised domain adaptation scenarios by large margins.Finally, we demonstrate that the adaptation process generalizes to objectclasses unseen during training.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '727',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1612.05424v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=11774400096015894119&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 288: {'ID': 288,\n",
       "  'title': 'Predicting Deep Zero-Shot Convolutional Neural Networks using Textual  Descriptions',\n",
       "  'authors': ['Sanja Fidler',\n",
       "   'Ruslan Salakhutdinov',\n",
       "   'Jimmy Ba',\n",
       "   'Kevin Swersky'],\n",
       "  'published': '2015-06-01T14:37:06Z',\n",
       "  'updated': '2015-09-25T16:20:44Z',\n",
       "  'abstract': 'One of the main challenges in Zero-Shot Learning of visual categories isgathering semantic attributes to accompany images. Recent work has shown thatlearning from textual descriptions, such as Wikipedia articles, avoids theproblem of having to explicitly define these attributes. We present a new modelthat can classify unseen categories from their textual description.Specifically, we use text features to predict the output weights of both theconvolutional and the fully connected layers in a deep convolutional neuralnetwork (CNN). We take advantage of the architecture of CNNs and learn featuresat different layers, rather than just learning an embedding space for bothmodalities, as is common with existing approaches. The proposed model alsoallows us to automatically generate a list of pseudo- attributes for eachvisual category consisting of words from Wikipedia articles. We train ourmodels end-to-end us- ing the Caltech-UCSD bird and flower datasets andevaluate both ROC and Precision-Recall curves. Our empirical results show thatthe proposed model significantly outperforms previous methods.',\n",
       "  'categories': ['cs.LG', 'cs.CV', 'cs.NE'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 4247-4255',\n",
       "  'citations': '267',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1506.00511v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2724062316492775050&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 289: {'ID': 289,\n",
       "  'title': 'A Large-Scale Car Dataset for Fine-Grained Categorization and  Verification',\n",
       "  'authors': ['Xiaoou Tang', 'Linjie Yang', 'Chen Change Loy', 'Ping Luo'],\n",
       "  'published': '2015-06-30T06:47:50Z',\n",
       "  'updated': '2015-09-24T09:04:24Z',\n",
       "  'abstract': 'Updated on 24/09/2015: This update provides preliminary experiment resultsfor fine-grained classification on the surveillance data of CompCars. Thetrain/test splits are provided in the updated dataset. See details in Section6.',\n",
       "  'categories': ['cs.CV', 'cs.AI'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '450',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1506.08959v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=15493017649905778902&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 290: {'ID': 290,\n",
       "  'title': 'Learning Descriptors for Object Recognition and 3D Pose Estimation',\n",
       "  'authors': ['Paul Wohlhart', 'Vincent Lepetit'],\n",
       "  'published': '2015-02-20T15:39:42Z',\n",
       "  'updated': '2015-04-13T13:53:07Z',\n",
       "  'abstract': 'Detecting poorly textured objects and estimating their 3D pose reliably isstill a very challenging problem. We introduce a simple but powerful approachto computing descriptors for object views that efficiently capture both theobject identity and 3D pose. By contrast with previous manifold-basedapproaches, we can rely on the Euclidean distance to evaluate the similaritybetween descriptors, and therefore use scalable Nearest Neighbor search methodsto efficiently handle a large number of objects under a large range of poses.To achieve this, we train a Convolutional Neural Network to compute thesedescriptors by enforcing simple similarity and dissimilarity constraintsbetween the descriptors. We show that our constraints nicely untangle theimages from different objects and different views into clusters that are notonly well-separated but also structured as the corresponding sets of poses: TheEuclidean distance between descriptors is large when the descriptors are fromdifferent objects, and directly related to the distance between the poses whenthe descriptors are from the same object. These important properties allow usto outperform state-of-the-art object views representations on challenging RGBand RGB-D data.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '310',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1502.05908v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=15030326828619706231&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 291: {'ID': 291,\n",
       "  'title': 'Beyond Sharing Weights for Deep Domain Adaptation',\n",
       "  'authors': ['Pascal Fua', 'Artem Rozantsev', 'Mathieu Salzmann'],\n",
       "  'published': '2016-03-21T14:20:41Z',\n",
       "  'updated': '2016-11-17T13:51:31Z',\n",
       "  'abstract': 'The performance of a classifier trained on data coming from a specific domaintypically degrades when applied to a related but different one. Whileannotating many samples from the new domain would address this issue, it isoften too expensive or impractical. Domain Adaptation has therefore emerged asa solution to this problem; It leverages annotated data from a source domain,in which it is abundant, to train a classifier to operate in a target domain,in which it is either sparse or even lacking altogether. In this context, therecent trend consists of learning deep architectures whose weights are sharedfor both domains, which essentially amounts to learning domain invariantfeatures.  Here, we show that it is more effective to explicitly model the shift fromone domain to the other. To this end, we introduce a two-stream architecture,where one operates in the source domain and the other in the target domain. Incontrast to other approaches, the weights in corresponding layers are relatedbut not shared. We demonstrate that this both yields higher accuracy thanstate-of-the-art methods on several object recognition and detection tasks andconsistently outperforms networks with shared weights in both supervised andunsupervised settings.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 41 (4), 801-814',\n",
       "  'citations': '180',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1603.06432v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=8276563417207656251&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 292: {'ID': 292,\n",
       "  'title': 'Image Captioning and Visual Question Answering Based on Attributes and  External Knowledge',\n",
       "  'authors': ['Peng Wang',\n",
       "   'Qi Wu',\n",
       "   'Anthony Dick',\n",
       "   'Chunhua Shen',\n",
       "   'Anton van den Hengel'],\n",
       "  'published': '2016-03-09T08:56:45Z',\n",
       "  'updated': '2016-12-16T11:44:34Z',\n",
       "  'abstract': 'Much recent progress in Vision-to-Language problems has been achieved througha combination of Convolutional Neural Networks (CNNs) and Recurrent NeuralNetworks (RNNs). This approach does not explicitly represent high-levelsemantic concepts, but rather seeks to progress directly from image features totext. In this paper we first propose a method of incorporating high-levelconcepts into the successful CNN-RNN approach, and show that it achieves asignificant improvement on the state-of-the-art in both image captioning andvisual question answering. We further show that the same mechanism can be usedto incorporate external knowledge, which is critically important for answeringhigh level visual questions. Specifically, we design a visual questionanswering model that combines an internal representation of the content of animage with information extracted from a general knowledge base to answer abroad range of image-based questions. It particularly allows questions to beasked about the contents of an image, even when the image itself does notcontain a complete answer. Our final model achieves the best reported resultson both image captioning and visual question answering on several benchmarkdatasets.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 40 (6), 1367-1381',\n",
       "  'citations': '162',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1603.02814v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=1163475044307286563&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 293: {'ID': 293,\n",
       "  'title': 'Person Re-Identification via Recurrent Feature Aggregation',\n",
       "  'authors': ['Yichao Yan',\n",
       "   'Bingbing Ni',\n",
       "   'Yan Yan',\n",
       "   'Xiaokang Yang',\n",
       "   'Zhichao Song',\n",
       "   'Chao Ma'],\n",
       "  'published': '2017-01-23T12:05:16Z',\n",
       "  'updated': '2017-01-23T12:05:16Z',\n",
       "  'abstract': 'We address the person re-identification problem by effectively exploiting aglobally discriminative feature representation from a sequence of tracked humanregions/patches. This is in contrast to previous person re-id works, which relyon either single frame based person to person patch matching, or graph basedsequence to sequence matching. We show that a progressive/sequential fusionframework based on long short term memory (LSTM) network aggregates theframe-wise human region representation at each time stamp and yields a sequencelevel human feature representation. Since LSTM nodes can remember and propagatepreviously accumulated good features and forget newly input inferior ones, evenwith simple hand-crafted features, the proposed recurrent feature aggregationnetwork (RFA-Net) is effective in generating highly discriminative sequencelevel human representations. Extensive experimental results on two personre-identification benchmarks demonstrate that the proposed method performsfavorably against state-of-the-art person re-identification methods.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (6), 701-716',\n",
       "  'citations': '160',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1701.06351v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=4404478467572452526&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 294: {'ID': 294,\n",
       "  'title': 'Beyond Short Snippets: Deep Networks for Video Classification',\n",
       "  'authors': ['Rajat Monga',\n",
       "   'Sudheendra Vijayanarasimhan',\n",
       "   'George Toderici',\n",
       "   'Oriol Vinyals',\n",
       "   'Matthew Hausknecht',\n",
       "   'Joe Yue-Hei Ng'],\n",
       "  'published': '2015-03-31T04:34:12Z',\n",
       "  'updated': '2015-04-13T19:44:25Z',\n",
       "  'abstract': 'Convolutional neural networks (CNNs) have been extensively applied for imagerecognition problems giving state-of-the-art results on recognition, detection,segmentation and retrieval. In this work we propose and evaluate several deepneural network architectures to combine image information across a video overlonger time periods than previously attempted. We propose two methods capableof handling full length videos. The first method explores various convolutionaltemporal feature pooling architectures, examining the various design choiceswhich need to be made when adapting a CNN for this task. The second proposedmethod explicitly models the video as an ordered sequence of frames. For thispurpose we employ a recurrent neural network that uses Long Short-Term Memory(LSTM) cells which are connected to the output of the underlying CNN. Our bestnetworks exhibit significant performance improvements over previously publishedresults on the Sports 1 million dataset (73.1% vs. 60.9%) and the UCF-101datasets with (88.6% vs. 88.0%) and without additional optical flow information(82.6% vs. 72.8%).',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '1602',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1503.08909v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=13568866641154577152&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 295: {'ID': 295,\n",
       "  'title': 'Convolutional Two-Stream Network Fusion for Video Action Recognition',\n",
       "  'authors': ['Andrew Zisserman', 'Christoph Feichtenhofer', 'Axel Pinz'],\n",
       "  'published': '2016-04-22T08:51:17Z',\n",
       "  'updated': '2016-09-26T10:47:21Z',\n",
       "  'abstract': 'Recent applications of Convolutional Neural Networks (ConvNets) for humanaction recognition in videos have proposed different solutions forincorporating the appearance and motion information. We study a number of waysof fusing ConvNet towers both spatially and temporally in order to best takeadvantage of this spatio-temporal information. We make the following findings:(i) that rather than fusing at the softmax layer, a spatial and temporalnetwork can be fused at a convolution layer without loss of performance, butwith a substantial saving in parameters; (ii) that it is better to fuse suchnetworks spatially at the last convolutional layer than earlier, and thatadditionally fusing at the class prediction layer can boost accuracy; finally(iii) that pooling of abstract convolutional features over spatiotemporalneighbourhoods further boosts performance. Based on these studies we propose anew ConvNet architecture for spatiotemporal fusion of video snippets, andevaluate its performance on standard benchmarks where this architectureachieves state-of-the-art results.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '1342',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1604.06573v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=12361422087232908082&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 296: {'ID': 296,\n",
       "  'title': 'You Only Look Once: Unified, Real-Time Object Detection',\n",
       "  'authors': ['Santosh Divvala',\n",
       "   'Joseph Redmon',\n",
       "   'Ross Girshick',\n",
       "   'Ali Farhadi'],\n",
       "  'published': '2015-06-08T19:52:52Z',\n",
       "  'updated': '2016-05-09T22:22:11Z',\n",
       "  'abstract': 'We present YOLO, a new approach to object detection. Prior work on objectdetection repurposes classifiers to perform detection. Instead, we frame objectdetection as a regression problem to spatially separated bounding boxes andassociated class probabilities. A single neural network predicts bounding boxesand class probabilities directly from full images in one evaluation. Since thewhole detection pipeline is a single network, it can be optimized end-to-enddirectly on detection performance.  Our unified architecture is extremely fast. Our base YOLO model processesimages in real-time at 45 frames per second. A smaller version of the network,Fast YOLO, processes an astounding 155 frames per second while still achievingdouble the mAP of other real-time detectors. Compared to state-of-the-artdetection systems, YOLO makes more localization errors but is far less likelyto predict false detections where nothing exists. Finally, YOLO learns verygeneral representations of objects. It outperforms all other detection methods,including DPM and R-CNN, by a wide margin when generalizing from natural imagesto artwork on both the Picasso Dataset and the People-Art Dataset.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '9772',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1506.02640v5',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6382612685700818764&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 297: {'ID': 297,\n",
       "  'title': 'Self-critical Sequence Training for Image Captioning',\n",
       "  'authors': ['Etienne Marcheret',\n",
       "   'Jarret Ross',\n",
       "   'Steven J. Rennie',\n",
       "   'Youssef Mroueh',\n",
       "   'Vaibhava Goel'],\n",
       "  'published': '2016-12-02T04:37:22Z',\n",
       "  'updated': '2017-11-16T02:38:37Z',\n",
       "  'abstract': 'Recently it has been shown that policy-gradient methods for reinforcementlearning can be utilized to train deep end-to-end systems directly onnon-differentiable metrics for the task at hand. In this paper we consider theproblem of optimizing image captioning systems using reinforcement learning,and show that by carefully optimizing our systems using the test metrics of theMSCOCO task, significant gains in performance can be realized. Our systems arebuilt using a new optimization approach that we call self-critical sequencetraining (SCST). SCST is a form of the popular REINFORCE algorithm that, ratherthan estimating a \"baseline\" to normalize the rewards and reduce variance,utilizes the output of its own test-time inference algorithm to normalize therewards it experiences. Using this approach, estimating the reward signal (asactor-critic methods must do) and estimating normalization (as REINFORCEalgorithms typically do) is avoided, while at the same time harmonizing themodel with respect to its test-time inference procedure. Empirically we findthat directly optimizing the CIDEr metric with SCST and greedy decoding attest-time is highly effective. Our results on the MSCOCO evaluation severestablish a new state-of-the-art on the task, improving the best result interms of CIDEr from 104.9 to 114.7.',\n",
       "  'categories': ['cs.LG', 'cs.AI', 'cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '617',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1612.00563v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=4288352136730370416&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 298: {'ID': 298,\n",
       "  'title': 'Deeply supervised salient object detection with short connections',\n",
       "  'authors': ['Qibin Hou',\n",
       "   'Zhuowen Tu',\n",
       "   'Ali Borji',\n",
       "   'Ming-Ming Cheng',\n",
       "   'Philip Torr',\n",
       "   'Xiao-Wei Hu'],\n",
       "  'published': '2016-11-15T14:19:06Z',\n",
       "  'updated': '2018-03-16T01:46:40Z',\n",
       "  'abstract': 'Recent progress on saliency detection is substantial, benefiting mostly fromthe explosive development of Convolutional Neural Networks (CNNs). Semanticsegmentation and saliency detection algorithms developed lately have beenmostly based on Fully Convolutional Neural Networks (FCNs). There is still alarge room for improvement over the generic FCN models that do not explicitlydeal with the scale-space problem. Holistically-Nested Edge Detector (HED)provides a skip-layer structure with deep supervision for edge and boundarydetection, but the performance gain of HED on salience detection is notobvious. In this paper, we propose a new method for saliency detection byintroducing short connections to the skip-layer structures within the HEDarchitecture. Our framework provides rich multi-scale feature maps at eachlayer, a property that is critically needed to perform segment detection. Ourmethod produces state-of-the-art results on 5 widely tested salient objectdetection benchmarks, with advantages in terms of efficiency (0.15 seconds perimage), effectiveness, and simplicity over the existing algorithms.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '534',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1611.04849v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=16939402952199717847&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 299: {'ID': 299,\n",
       "  'title': 'The Cityscapes Dataset for Semantic Urban Scene Understanding',\n",
       "  'authors': ['Marius Cordts',\n",
       "   'Timo Rehfeld',\n",
       "   'Bernt Schiele',\n",
       "   'Markus Enzweiler',\n",
       "   'Stefan Roth',\n",
       "   'Uwe Franke',\n",
       "   'Sebastian Ramos',\n",
       "   'Mohamed Omran',\n",
       "   'Rodrigo Benenson'],\n",
       "  'published': '2016-04-06T16:34:33Z',\n",
       "  'updated': '2016-04-07T15:39:22Z',\n",
       "  'abstract': 'Visual understanding of complex urban street scenes is an enabling factor fora wide range of applications. Object detection has benefited enormously fromlarge-scale datasets, especially in the context of deep learning. For semanticurban scene understanding, however, no current dataset adequately captures thecomplexity of real-world urban scenes.  To address this, we introduce Cityscapes, a benchmark suite and large-scaledataset to train and test approaches for pixel-level and instance-levelsemantic labeling. Cityscapes is comprised of a large, diverse set of stereovideo sequences recorded in streets from 50 different cities. 5000 of theseimages have high quality pixel-level annotations; 20000 additional images havecoarse annotations to enable methods that leverage large volumes ofweakly-labeled data. Crucially, our effort exceeds previous attempts in termsof dataset size, annotation richness, scene variability, and complexity. Ouraccompanying empirical study provides an in-depth analysis of the datasetcharacteristics, as well as a performance evaluation of severalstate-of-the-art approaches based on our benchmark.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '3121',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1604.01685v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=1394466002617224745&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 300: {'ID': 300,\n",
       "  'title': 'Conditional Random Fields as Recurrent Neural Networks',\n",
       "  'authors': ['Sadeep Jayasumana',\n",
       "   'Vibhav Vineet',\n",
       "   'Philip H. S. Torr',\n",
       "   'Dalong Du',\n",
       "   'Zhizhong Su',\n",
       "   'Shuai Zheng',\n",
       "   'Chang Huang',\n",
       "   'Bernardino Romera-Paredes'],\n",
       "  'published': '2015-02-11T10:02:50Z',\n",
       "  'updated': '2016-04-13T23:26:45Z',\n",
       "  'abstract': 'Pixel-level labelling tasks, such as semantic segmentation, play a centralrole in image understanding. Recent approaches have attempted to harness thecapabilities of deep learning techniques for image recognition to tacklepixel-level labelling tasks. One central issue in this methodology is thelimited capacity of deep learning techniques to delineate visual objects. Tosolve this problem, we introduce a new form of convolutional neural networkthat combines the strengths of Convolutional Neural Networks (CNNs) andConditional Random Fields (CRFs)-based probabilistic graphical modelling. Tothis end, we formulate mean-field approximate inference for the ConditionalRandom Fields with Gaussian pairwise potentials as Recurrent Neural Networks.This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain adeep network that has desirable properties of both CNNs and CRFs. Importantly,our system fully integrates CRF modelling with CNNs, making it possible totrain the whole deep network end-to-end with the usual back-propagationalgorithm, avoiding offline post-processing methods for object delineation. Weapply the proposed method to the problem of semantic image segmentation,obtaining top results on the challenging Pascal VOC 2012 segmentationbenchmark.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 1529-1537',\n",
       "  'citations': '1997',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1502.03240v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=4680896688857314530&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 301: {'ID': 301,\n",
       "  'title': 'Deep Attributes Driven Multi-Camera Person Re-identification',\n",
       "  'authors': ['Junliang Xing',\n",
       "   'Wen Gao',\n",
       "   'Chi Su',\n",
       "   'Qi Tian',\n",
       "   'Shiliang Zhang'],\n",
       "  'published': '2016-05-11T02:05:22Z',\n",
       "  'updated': '2016-08-09T05:58:03Z',\n",
       "  'abstract': 'The visual appearance of a person is easily affected by many factors likepose variations, viewpoint changes and camera parameter differences. This makesperson Re-Identification (ReID) among multiple cameras a very challenging task.This work is motivated to learn mid-level human attributes which are robust tosuch visual appearance variations. And we propose a semi-supervised attributelearning framework which progressively boosts the accuracy of attributes onlyusing a limited number of labeled data. Specifically, this framework involves athree-stage training. A deep Convolutional Neural Network (dCNN) is firsttrained on an independent dataset labeled with attributes. Then it isfine-tuned on another dataset only labeled with person IDs using our definedtriplet loss. Finally, the updated dCNN predicts attribute labels for thetarget dataset, which is combined with the independent dataset for the finalround of fine-tuning. The predicted attributes, namely \\\\emph{deep attributes}exhibit superior generalization ability across different datasets. By directlyusing the deep attributes with simple Cosine distance, we have obtainedsurprisingly good accuracy on four person ReID datasets. Experiments also showthat a simple metric learning modular further boosts our method, making itsignificantly outperform many recent works.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (2), 475-491',\n",
       "  'citations': '286',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1605.03259v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6495936485477714779&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 302: {'ID': 302,\n",
       "  'title': 'Show and Tell: A Neural Image Caption Generator',\n",
       "  'authors': ['Oriol Vinyals',\n",
       "   'Samy Bengio',\n",
       "   'Dumitru Erhan',\n",
       "   'Alexander Toshev'],\n",
       "  'published': '2014-11-17T17:15:41Z',\n",
       "  'updated': '2015-04-20T22:26:11Z',\n",
       "  'abstract': 'Automatically describing the content of an image is a fundamental problem inartificial intelligence that connects computer vision and natural languageprocessing. In this paper, we present a generative model based on a deeprecurrent architecture that combines recent advances in computer vision andmachine translation and that can be used to generate natural sentencesdescribing an image. The model is trained to maximize the likelihood of thetarget description sentence given the training image. Experiments on severaldatasets show the accuracy of the model and the fluency of the language itlearns solely from image descriptions. Our model is often quite accurate, whichwe verify both qualitatively and quantitatively. For instance, while thecurrent state-of-the-art BLEU-1 score (the higher the better) on the Pascaldataset is 25, our approach yields 59, to be compared to human performancearound 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66,and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, weachieve a BLEU-4 of 27.7, which is the current state-of-the-art.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '3632',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1411.4555v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14219066212261539080&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 303: {'ID': 303,\n",
       "  'title': 'Learning Deep Features for Discriminative Localization',\n",
       "  'authors': ['Aditya Khosla',\n",
       "   'Antonio Torralba',\n",
       "   'Bolei Zhou',\n",
       "   'Aude Oliva',\n",
       "   'Agata Lapedriza'],\n",
       "  'published': '2015-12-14T01:32:33Z',\n",
       "  'updated': '2015-12-14T01:32:33Z',\n",
       "  'abstract': 'In this work, we revisit the global average pooling layer proposed in [13],and shed light on how it explicitly enables the convolutional neural network tohave remarkable localization ability despite being trained on image-levellabels. While this technique was previously proposed as a means forregularizing training, we find that it actually builds a generic localizabledeep representation that can be applied to a variety of tasks. Despite theapparent simplicity of global average pooling, we are able to achieve 37.1%top-5 error for object localization on ILSVRC 2014, which is remarkably closeto the 34.2% top-5 error achieved by a fully supervised CNN approach. Wedemonstrate that our network is able to localize the discriminative imageregions on a variety of tasks despite not being trained for them',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '2356',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1512.04150v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2686826201088684972&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 304: {'ID': 304,\n",
       "  'title': 'DeepDriving: Learning Affordance for Direct Perception in Autonomous  Driving',\n",
       "  'authors': ['Chenyi Chen', 'Jianxiong Xiao', 'Ari Seff', 'Alain Kornhauser'],\n",
       "  'published': '2015-05-01T19:31:13Z',\n",
       "  'updated': '2015-09-26T05:17:59Z',\n",
       "  'abstract': 'Today, there are two major paradigms for vision-based autonomous drivingsystems: mediated perception approaches that parse an entire scene to make adriving decision, and behavior reflex approaches that directly map an inputimage to a driving action by a regressor. In this paper, we propose a thirdparadigm: a direct perception approach to estimate the affordance for driving.We propose to map an input image to a small number of key perception indicatorsthat directly relate to the affordance of a road/traffic state for driving. Ourrepresentation provides a set of compact yet complete descriptions of the sceneto enable a simple controller to drive autonomously. Falling in between the twoextremes of mediated perception and behavior reflex, we argue that our directperception representation provides the right level of abstraction. Todemonstrate this, we train a deep Convolutional Neural Network using recordingfrom 12 hours of human driving in a video game and show that our model can workwell to drive a car in a very diverse set of virtual environments. We alsotrain a model for car distance estimation on the KITTI dataset. Results showthat our direct perception approach can generalize well to real driving images.Source code and data are available on our project website.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 2722-2730',\n",
       "  'citations': '962',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1505.00256v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6173219959812350972&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 305: {'ID': 305,\n",
       "  'title': 'BiSeNet: Bilateral Segmentation Network for Real-time Semantic  Segmentation',\n",
       "  'authors': ['Changxin Gao',\n",
       "   'Gang Yu',\n",
       "   'Nong Sang',\n",
       "   'Changqian Yu',\n",
       "   'Chao Peng',\n",
       "   'Jingbo Wang'],\n",
       "  'published': '2018-08-02T16:34:01Z',\n",
       "  'updated': '2018-08-02T16:34:01Z',\n",
       "  'abstract': 'Semantic segmentation requires both rich spatial information and sizeablereceptive field. However, modern approaches usually compromise spatialresolution to achieve real-time inference speed, which leads to poorperformance. In this paper, we address this dilemma with a novel BilateralSegmentation Network (BiSeNet). We first design a Spatial Path with a smallstride to preserve the spatial information and generate high-resolutionfeatures. Meanwhile, a Context Path with a fast downsampling strategy isemployed to obtain sufficient receptive field. On top of the two paths, weintroduce a new Feature Fusion Module to combine features efficiently. Theproposed architecture makes a right balance between the speed and segmentationperformance on Cityscapes, CamVid, and COCO-Stuff datasets. Specifically, for a2048x1024 input, we achieve 68.4% Mean IOU on the Cityscapes test dataset withspeed of 105 FPS on one NVIDIA Titan XP card, which is significantly fasterthan the existing methods with comparable performance.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 325-341',\n",
       "  'citations': '246',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1808.00897v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=3312073063271044600&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 306: {'ID': 306,\n",
       "  'title': 'Accurate Image Super-Resolution Using Very Deep Convolutional Networks',\n",
       "  'authors': ['Jiwon Kim', 'Kyoung Mu Lee', 'Jung Kwon Lee'],\n",
       "  'published': '2015-11-14T17:36:45Z',\n",
       "  'updated': '2016-11-11T08:40:47Z',\n",
       "  'abstract': 'We present a highly accurate single-image super-resolution (SR) method. Ourmethod uses a very deep convolutional network inspired by VGG-net used forImageNet classification \\\\cite{simonyan2015very}. We find increasing our networkdepth shows a significant improvement in accuracy. Our final model uses 20weight layers. By cascading small filters many times in a deep networkstructure, contextual information over large image regions is exploited in anefficient way. With very deep networks, however, convergence speed becomes acritical issue during training. We propose a simple yet effective trainingprocedure. We learn residuals only and use extremely high learning rates($10^4$ times higher than SRCNN \\\\cite{dong2015image}) enabled by adjustablegradient clipping. Our proposed method performs better than existing methods inaccuracy and visual improvements in our results are easily noticeable.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '2537',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1511.04587v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=8124948202418109550&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 307: {'ID': 307,\n",
       "  'title': 'Generation and Comprehension of Unambiguous Object Descriptions',\n",
       "  'authors': ['Kevin Murphy',\n",
       "   'Alexander Toshev',\n",
       "   'Jonathan Huang',\n",
       "   'Alan Yuille',\n",
       "   'Junhua Mao',\n",
       "   'Oana Camburu'],\n",
       "  'published': '2015-11-07T02:17:36Z',\n",
       "  'updated': '2016-04-11T01:11:56Z',\n",
       "  'abstract': 'We propose a method that can generate an unambiguous description (known as areferring expression) of a specific object or region in an image, and which canalso comprehend or interpret such an expression to infer which object is beingdescribed. We show that our method outperforms previous methods that generatedescriptions of objects without taking into account other potentially ambiguousobjects in the scene. Our model is inspired by recent successes of deeplearning methods for image captioning, but while image captioning is difficultto evaluate, our task allows for easy objective evaluation. We also present anew large-scale dataset for referring expressions, based on MS-COCO. We havereleased the dataset and a toolbox for visualization and evaluation, seehttps://github.com/mjhucla/Google_Refexp_toolbox',\n",
       "  'categories': ['cs.CV', 'cs.CL', 'cs.LG', 'cs.RO', 'I.2.6; I.2.7; I.2.10'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '326',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1511.02283v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=10404264718240229213&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 308: {'ID': 308,\n",
       "  'title': 'Pyramid Scene Parsing Network',\n",
       "  'authors': ['Jianping Shi',\n",
       "   'Xiaogang Wang',\n",
       "   'Xiaojuan Qi',\n",
       "   'Hengshuang Zhao',\n",
       "   'Jiaya Jia'],\n",
       "  'published': '2016-12-04T11:46:22Z',\n",
       "  'updated': '2017-04-27T12:15:17Z',\n",
       "  'abstract': 'Scene parsing is challenging for unrestricted open vocabulary and diversescenes. In this paper, we exploit the capability of global context informationby different-region-based context aggregation through our pyramid poolingmodule together with the proposed pyramid scene parsing network (PSPNet). Ourglobal prior representation is effective to produce good quality results on thescene parsing task, while PSPNet provides a superior framework for pixel-levelprediction tasks. The proposed approach achieves state-of-the-art performanceon various datasets. It came first in ImageNet scene parsing challenge 2016,PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields newrecord of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% onCityscapes.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '2767',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1612.01105v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14280592669518971589&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 309: {'ID': 309,\n",
       "  'title': 'Understanding Deep Image Representations by Inverting Them',\n",
       "  'authors': ['Aravindh Mahendran', 'Andrea Vedaldi'],\n",
       "  'published': '2014-11-26T18:51:52Z',\n",
       "  'updated': '2014-11-26T18:51:52Z',\n",
       "  'abstract': 'Image representations, from SIFT and Bag of Visual Words to ConvolutionalNeural Networks (CNNs), are a crucial component of almost any imageunderstanding system. Nevertheless, our understanding of them remains limited.In this paper we conduct a direct analysis of the visual information containedin representations by asking the following question: given an encoding of animage, to which extent is it possible to reconstruct the image itself? Toanswer this question we contribute a general framework to invertrepresentations. We show that this method can invert representations such asHOG and SIFT more accurately than recent alternatives while being applicable toCNNs too. We then use this technique to study the inverse of recentstate-of-the-art CNN image representations for the first time. Among ourfindings, we show that several layers in CNNs retain photographically accurateinformation about the image, with different degrees of geometric andphotometric invariance.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '1090',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1412.0035v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6027656395688479365&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 310: {'ID': 310,\n",
       "  'title': 'Unsupervised CNN for Single View Depth Estimation: Geometry to the  Rescue',\n",
       "  'authors': ['Ravi Garg', 'Gustavo Carneiro', 'Vijay Kumar BG', 'Ian Reid'],\n",
       "  'published': '2016-03-16T08:57:15Z',\n",
       "  'updated': '2016-07-29T03:20:46Z',\n",
       "  'abstract': 'A significant weakness of most current deep Convolutional Neural Networks isthe need to train them using vast amounts of manu- ally labelled data. In thiswork we propose a unsupervised framework to learn a deep convolutional neuralnetwork for single view depth predic- tion, without requiring a pre-trainingstage or annotated ground truth depths. We achieve this by training the networkin a manner analogous to an autoencoder. At training time we consider a pair ofimages, source and target, with small, known camera motion between the two suchas a stereo pair. We train the convolutional encoder for the task of predictingthe depth map for the source image. To do so, we explicitly generate an inversewarp of the target image using the predicted depth and known inter-viewdisplacement, to reconstruct the source image; the photomet- ric error in thereconstruction is the reconstruction loss for the encoder. The acquisition ofthis training data is considerably simpler than for equivalent systems,requiring no manual annotation, nor calibration of depth sensor to camera. Weshow that our network trained on less than half of the KITTI dataset (withoutany further augmentation) gives com- parable performance to that of the stateof art supervised methods for single view depth estimation.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (8), 740-756',\n",
       "  'citations': '630',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1603.04992v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14931725386274994615&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 311: {'ID': 311,\n",
       "  'title': 'Temporal Segment Networks: Towards Good Practices for Deep Action  Recognition',\n",
       "  'authors': ['Luc Van Gool',\n",
       "   'Yuanjun Xiong',\n",
       "   'Xiaoou Tang',\n",
       "   'Yu Qiao',\n",
       "   'Dahua Lin',\n",
       "   'Limin Wang',\n",
       "   'Zhe Wang'],\n",
       "  'published': '2016-08-02T15:06:50Z',\n",
       "  'updated': '2016-08-02T15:06:50Z',\n",
       "  'abstract': 'Deep convolutional networks have achieved great success for visualrecognition in still images. However, for action recognition in videos, theadvantage over traditional methods is not so evident. This paper aims todiscover the principles to design effective ConvNet architectures for actionrecognition in videos and learn these models given limited training samples.Our first contribution is temporal segment network (TSN), a novel framework forvideo-based action recognition. which is based on the idea of long-rangetemporal structure modeling. It combines a sparse temporal sampling strategyand video-level supervision to enable efficient and effective learning usingthe whole action video. The other contribution is our study on a series of goodpractices in learning ConvNets on video data with the help of temporal segmentnetwork. Our approach obtains the state-the-of-art performance on the datasetsof HMDB51 ( $ 69.4\\\\% $) and UCF101 ($ 94.2\\\\% $). We also visualize the learnedConvNet models, which qualitatively demonstrates the effectiveness of temporalsegment network and the proposed good practices.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (8), 20-36',\n",
       "  'citations': '1465',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1608.00859v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5243110286852111542&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 312: {'ID': 312,\n",
       "  'title': 'Volumetric and Multi-View CNNs for Object Classification on 3D Data',\n",
       "  'authors': ['Mengyuan Yan',\n",
       "   'Angela Dai',\n",
       "   'Matthias Niessner',\n",
       "   'Leonidas J. Guibas',\n",
       "   'Charles R. Qi',\n",
       "   'Hao Su'],\n",
       "  'published': '2016-04-12T07:10:43Z',\n",
       "  'updated': '2016-04-29T06:21:09Z',\n",
       "  'abstract': '3D shape models are becoming widely available and easier to capture, makingavailable 3D information crucial for progress in object classification. Currentstate-of-the-art methods rely on CNNs to address this problem. Recently, wewitness two types of CNNs being developed: CNNs based upon volumetricrepresentations versus CNNs based upon multi-view representations. Empiricalresults from these two types of CNNs exhibit a large gap, indicating thatexisting volumetric CNN architectures and approaches are unable to fullyexploit the power of 3D representations. In this paper, we aim to improve bothvolumetric CNNs and multi-view CNNs according to extensive analysis of existingapproaches. To this end, we introduce two distinct network architectures ofvolumetric CNNs. In addition, we examine multi-view CNNs, where we introducemulti-resolution filtering in 3D. Overall, we are able to outperform currentstate-of-the-art methods for both volumetric CNNs and multi-view CNNs. Weprovide extensive experiments designed to evaluate underlying design choices,thus providing a better understanding of the space of methods available forobject classification on 3D data.',\n",
       "  'categories': ['cs.CV', 'cs.AI'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '761',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1604.03265v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5558114292511619197&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 313: {'ID': 313,\n",
       "  'title': 'Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images',\n",
       "  'authors': ['Yinda Zhang',\n",
       "   'Wei Liu',\n",
       "   'Nanyang Wang',\n",
       "   'Yanwei Fu',\n",
       "   'Yu-Gang Jiang',\n",
       "   'Zhuwen Li'],\n",
       "  'published': '2018-04-05T02:24:03Z',\n",
       "  'updated': '2018-08-03T08:52:33Z',\n",
       "  'abstract': 'We propose an end-to-end deep learning architecture that produces a 3D shapein triangular mesh from a single color image. Limited by the nature of deepneural network, previous methods usually represent a 3D shape in volume orpoint cloud, and it is non-trivial to convert them to the more ready-to-usemesh model. Unlike the existing methods, our network represents 3D mesh in agraph-based convolutional neural network and produces correct geometry byprogressively deforming an ellipsoid, leveraging perceptual features extractedfrom the input image. We adopt a coarse-to-fine strategy to make the wholedeformation procedure stable, and define various of mesh related losses tocapture properties of different levels to guarantee visually appealing andphysically accurate 3D geometry. Extensive experiments show that our method notonly qualitatively produces mesh model with better details, but also achieveshigher 3D shape estimation accuracy compared to the state-of-the-art.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 52-67',\n",
       "  'citations': '220',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1804.01654v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=17608680451259990023&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 314: {'ID': 314,\n",
       "  'title': 'Flow-Guided Feature Aggregation for Video Object Detection',\n",
       "  'authors': ['Yichen Wei',\n",
       "   'Xizhou Zhu',\n",
       "   'Lu Yuan',\n",
       "   'Yujie Wang',\n",
       "   'Jifeng Dai'],\n",
       "  'published': '2017-03-29T13:21:28Z',\n",
       "  'updated': '2017-08-18T12:30:38Z',\n",
       "  'abstract': 'Extending state-of-the-art object detectors from image to video ischallenging. The accuracy of detection suffers from degenerated objectappearances in videos, e.g., motion blur, video defocus, rare poses, etc.Existing work attempts to exploit temporal information on box level, but suchmethods are not trained end-to-end. We present flow-guided feature aggregation,an accurate and end-to-end learning framework for video object detection. Itleverages temporal coherence on feature level instead. It improves theper-frame features by aggregation of nearby features along the motion paths,and thus improves the video recognition accuracy. Our method significantlyimproves upon strong single-frame baselines in ImageNet VID, especially formore challenging fast moving objects. Our framework is principled, and on parwith the best engineered systems winning the ImageNet VID challenges 2016,without additional bells-and-whistles. The proposed method, together with DeepFeature Flow, powered the winning entry of ImageNet VID challenges 2017. Thecode is available athttps://github.com/msracver/Flow-Guided-Feature-Aggregation.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 408-417',\n",
       "  'citations': '188',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1703.10025v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=15475443364070282374&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 315: {'ID': 315,\n",
       "  'title': 'Pixel-Level Domain Transfer',\n",
       "  'authors': ['Namil Kim',\n",
       "   'In So Kweon',\n",
       "   'Anthony S. Paek',\n",
       "   'Donggeun Yoo',\n",
       "   'Sunggyun Park'],\n",
       "  'published': '2016-03-24T05:20:59Z',\n",
       "  'updated': '2016-11-28T13:17:40Z',\n",
       "  'abstract': 'We present an image-conditional image generation model. The model transfersan input domain to a target domain in semantic level, and generates the targetimage in pixel level. To generate realistic target images, we employ thereal/fake-discriminator as in Generative Adversarial Nets, but also introduce anovel domain-discriminator to make the generated image relevant to the inputimage. We verify our model through a challenging task of generating a piece ofclothing from an input image of a dressed person. We present a high qualityclothing dataset containing the two domains, and succeed in demonstratingdecent results.',\n",
       "  'categories': ['cs.CV', 'cs.AI'],\n",
       "  'journal': 'ECCV (8), 517-532',\n",
       "  'citations': '195',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1603.07442v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=3197923341848919663&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 316: {'ID': 316,\n",
       "  'title': 'A Novel Performance Evaluation Methodology for Single-Target Trackers',\n",
       "  'authors': ['Fatih Porikli',\n",
       "   'Tomas Vojir',\n",
       "   'Jiri Matas',\n",
       "   'Roman Pflugfelder',\n",
       "   'Georg Nebehay',\n",
       "   'Gustavo Fernandez',\n",
       "   'Ales Leonardis',\n",
       "   'Luka Cehovin',\n",
       "   'Matej Kristan'],\n",
       "  'published': '2015-03-04T14:12:17Z',\n",
       "  'updated': '2016-01-08T15:27:11Z',\n",
       "  'abstract': 'This paper addresses the problem of single-target tracker performanceevaluation. We consider the performance measures, the dataset and theevaluation system to be the most important components of tracker evaluation andpropose requirements for each of them. The requirements are the basis of a newevaluation methodology that aims at a simple and easily interpretable trackercomparison. The ranking-based methodology addresses tracker equivalence interms of statistical significance and practical differences. A fully-annotateddataset with per-frame annotations with several visual attributes isintroduced. The diversity of its visual properties is maximized in a novel wayby clustering a large number of videos according to their visual attributes.This makes it the most sophistically constructed and annotated dataset to date.A multi-platform evaluation system allowing easy integration of third-partytrackers is presented as well. The proposed evaluation methodology was testedon the VOT2014 challenge on the new dataset and 38 trackers, making it thelargest benchmark to date. Most of the tested trackers are indeedstate-of-the-art since they outperform the standard baselines, resulting in ahighly-challenging benchmark. An exhaustive analysis of the dataset from theperspective of tracking difficulty is carried out. To facilitate trackercomparison a new performance visualization technique is proposed.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 38 (11), 2137\\xa0…',\n",
       "  'citations': '327',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1503.01313v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6911526552705231400&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 317: {'ID': 317,\n",
       "  'title': 'Person Re-identification by Saliency Learning',\n",
       "  'authors': ['Wanli Ouyang', 'Rui Zhao', 'Xiaogang Wang'],\n",
       "  'published': '2014-12-05T07:33:48Z',\n",
       "  'updated': '2014-12-05T07:33:48Z',\n",
       "  'abstract': 'Human eyes can recognize person identities based on small salient regions,i.e. human saliency is distinctive and reliable in pedestrian matching acrossdisjoint camera views. However, such valuable information is often hidden whencomputing similarities of pedestrian images with existing approaches. Inspiredby our user study result of human perception on human saliency, we propose anovel perspective for person re-identification based on learning human saliencyand matching saliency distribution. The proposed saliency learning and matchingframework consists of four steps: (1) To handle misalignment caused by drasticviewpoint change and pose variations, we apply adjacency constrained patchmatching to build dense correspondence between image pairs. (2) We propose twoalternative methods, i.e. K-Nearest Neighbors and One-class SVM, to estimate asaliency score for each image patch, through which distinctive features standout without using identity labels in the training procedure. (3) saliencymatching is proposed based on patch matching. Matching patches withinconsistent saliency brings penalty, and images of the same identity arerecognized by minimizing the saliency matching cost. (4) Furthermore, saliencymatching is tightly integrated with patch matching in a unified structuralRankSVM learning framework. The effectiveness of our approach is validated onthe VIPeR dataset and the CUHK01 dataset. Our approach outperforms thestate-of-the-art person re-identification methods on both datasets.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 39 (2), 356-370',\n",
       "  'citations': '133',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1412.1908v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=17429687016862468539&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 318: {'ID': 318,\n",
       "  'title': 'Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry  and Semantics',\n",
       "  'authors': ['Roberto Cipolla', 'Yarin Gal', 'Alex Kendall'],\n",
       "  'published': '2017-05-19T17:56:57Z',\n",
       "  'updated': '2018-04-24T06:42:35Z',\n",
       "  'abstract': \"Numerous deep learning applications benefit from multi-task learning withmultiple regression and classification objectives. In this paper we make theobservation that the performance of such systems is strongly dependent on therelative weighting between each task's loss. Tuning these weights by hand is adifficult and expensive process, making multi-task learning prohibitive inpractice. We propose a principled approach to multi-task deep learning whichweighs multiple loss functions by considering the homoscedastic uncertainty ofeach task. This allows us to simultaneously learn various quantities withdifferent units or scales in both classification and regression settings. Wedemonstrate our model learning per-pixel depth regression, semantic andinstance segmentation from a monocular input image. Perhaps surprisingly, weshow our model can learn multi-task weightings and outperform separate modelstrained individually on each task.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '529',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1705.07115v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2960014320829204839&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 319: {'ID': 319,\n",
       "  'title': 'DeepFool: a simple and accurate method to fool deep neural networks',\n",
       "  'authors': ['Alhussein Fawzi',\n",
       "   'Pascal Frossard',\n",
       "   'Seyed-Mohsen Moosavi-Dezfooli'],\n",
       "  'published': '2015-11-14T18:50:00Z',\n",
       "  'updated': '2016-07-04T04:49:44Z',\n",
       "  'abstract': 'State-of-the-art deep neural networks have achieved impressive results onmany image classification tasks. However, these same architectures have beenshown to be unstable to small, well sought, perturbations of the images.Despite the importance of this phenomenon, no effective methods have beenproposed to accurately compute the robustness of state-of-the-art deepclassifiers to such perturbations on large-scale datasets. In this paper, wefill this gap and propose the DeepFool algorithm to efficiently computeperturbations that fool deep networks, and thus reliably quantify therobustness of these classifiers. Extensive experimental results show that ourapproach outperforms recent methods in the task of computing adversarialperturbations and making classifiers more robust.',\n",
       "  'categories': ['cs.LG', 'cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '1565',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1511.04599v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14711824744254857883&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 320: {'ID': 320,\n",
       "  'title': 'Heterogeneous Face Attribute Estimation: A Deep Multi-Task Learning  Approach',\n",
       "  'authors': ['Anil K. Jain',\n",
       "   'Hu Han',\n",
       "   'Shiguang Shan',\n",
       "   'Fang Wang',\n",
       "   'Xilin Chen'],\n",
       "  'published': '2017-06-03T07:37:59Z',\n",
       "  'updated': '2017-09-28T11:11:57Z',\n",
       "  'abstract': 'Face attribute estimation has many potential applications in videosurveillance, face retrieval, and social media. While a number of methods havebeen proposed for face attribute estimation, most of them did not explicitlyconsider the attribute correlation and heterogeneity (e.g., ordinal vs. nominaland holistic vs. local) during feature representation learning. In this paper,we present a Deep Multi-Task Learning (DMTL) approach to jointly estimatemultiple heterogeneous attributes from a single face image. In DMTL, we tackleattribute correlation and heterogeneity with convolutional neural networks(CNNs) consisting of shared feature learning for all the attributes, andcategory-specific feature learning for heterogeneous attributes. We alsointroduce an unconstrained face database (LFW+), an extension of public-domainLFW, with heterogeneous demographic attributes (age, gender, and race) obtainedvia crowdsourcing. Experimental results on benchmarks with multiple faceattributes (MORPH II, LFW+, CelebA, LFWA, and FotW) show that the proposedapproach has superior performance compared to state of the art. Finally,evaluations on a public-domain face database (LAP) with a single attribute showthat the proposed approach has excellent generalization ability.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 40 (11), 2597\\xa0…',\n",
       "  'citations': '138',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1706.00906v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=3434468347319138443&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 321: {'ID': 321,\n",
       "  'title': 'Fast, Accurate, and Lightweight Super-Resolution with Cascading Residual  Network',\n",
       "  'authors': ['Namhyuk Ahn', 'Kyung-Ah Sohn', 'Byungkon Kang'],\n",
       "  'published': '2018-03-23T06:07:20Z',\n",
       "  'updated': '2018-10-04T21:47:19Z',\n",
       "  'abstract': 'In recent years, deep learning methods have been successfully applied tosingle-image super-resolution tasks. Despite their great performances, deeplearning methods cannot be easily applied to real-world applications due to therequirement of heavy computation. In this paper, we address this issue byproposing an accurate and lightweight deep network for image super-resolution.In detail, we design an architecture that implements a cascading mechanism upona residual network. We also present variant models of the proposed cascadingresidual network to further improve efficiency. Our extensive experiments showthat even with much fewer parameters and operations, our models achieveperformance comparable to that of state-of-the-art methods.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 252-268',\n",
       "  'citations': '157',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1803.08664v5',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=551313611766215361&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 322: {'ID': 322,\n",
       "  'title': 'Fast Algorithms for Convolutional Neural Networks',\n",
       "  'authors': ['Andrew Lavin', 'Scott Gray'],\n",
       "  'published': '2015-09-30T19:39:20Z',\n",
       "  'updated': '2015-11-10T20:08:41Z',\n",
       "  'abstract': \"Deep convolutional neural networks take GPU days of compute time to train onlarge data sets. Pedestrian detection for self driving cars requires very lowlatency. Image recognition for mobile phones is constrained by limitedprocessing resources. The success of convolutional neural networks in thesesituations is limited by how fast we can compute them. Conventional FFT basedconvolution is fast for large filters, but state of the art convolutionalneural networks use small, 3x3 filters. We introduce a new class of fastalgorithms for convolutional neural networks using Winograd's minimal filteringalgorithms. The algorithms compute minimal complexity convolution over smalltiles, which makes them fast with small filters and small batch sizes. Webenchmark a GPU implementation of our algorithm with the VGG network and showstate of the art throughput at batch sizes from 1 to 64.\",\n",
       "  'categories': ['cs.NE', 'cs.LG', 'I.2.6; F.2.1'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '448',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1509.09308v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=4107269088164149002&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 323: {'ID': 323,\n",
       "  'title': 'Large-Scale Image Retrieval with Attentive Deep Local Features',\n",
       "  'authors': ['Jack Sim',\n",
       "   'Tobias Weyand',\n",
       "   'Andre Araujo',\n",
       "   'Hyeonwoo Noh',\n",
       "   'Bohyung Han'],\n",
       "  'published': '2016-12-19T19:35:56Z',\n",
       "  'updated': '2018-02-03T02:19:16Z',\n",
       "  'abstract': 'We propose an attentive local feature descriptor suitable for large-scaleimage retrieval, referred to as DELF (DEep Local Feature). The new feature isbased on convolutional neural networks, which are trained only with image-levelannotations on a landmark image dataset. To identify semantically useful localfeatures for image retrieval, we also propose an attention mechanism forkeypoint selection, which shares most network layers with the descriptor. Thisframework can be used for image retrieval as a drop-in replacement for otherkeypoint detectors and descriptors, enabling more accurate feature matching andgeometric verification. Our system produces reliable confidence scores toreject false positives---in particular, it is robust against queries that haveno correct match in the database. To evaluate the proposed descriptor, weintroduce a new large-scale dataset, referred to as Google-Landmarks dataset,which involves challenges in both database and query such as backgroundclutter, partial occlusion, multiple landmarks, objects in variable scales,etc. We show that DELF outperforms the state-of-the-art global and localdescriptors in the large-scale setting by significant margins. Code and datasetcan be found at the project webpage:https://github.com/tensorflow/models/tree/master/research/delf .',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 3456-3465',\n",
       "  'citations': '215',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1612.06321v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=9521668325497245760&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 324: {'ID': 324,\n",
       "  'title': 'Unified Deep Supervised Domain Adaptation and Generalization',\n",
       "  'authors': ['Donald A. Adjeroh',\n",
       "   'Saeid Motiian',\n",
       "   'Gianfranco Doretto',\n",
       "   'Marco Piccirilli'],\n",
       "  'published': '2017-09-28T22:35:36Z',\n",
       "  'updated': '2017-09-28T22:35:36Z',\n",
       "  'abstract': 'This work provides a unified framework for addressing the problem of visualsupervised domain adaptation and generalization with deep models. The main ideais to exploit the Siamese architecture to learn an embedding subspace that isdiscriminative, and where mapped visual domains are semantically aligned andyet maximally separated. The supervised setting becomes attractive especiallywhen only few target data samples need to be labeled. In this scenario,alignment and separation of semantic probability distributions is difficultbecause of the lack of data. We found that by reverting to point-wisesurrogates of distribution distances and similarities provides an effectivesolution. In addition, the approach has a high speed of adaptation, whichrequires an extremely low number of labeled target training samples, even oneper category can be effective. The approach is extended to domaingeneralization. For both applications the experiments show very promisingresults.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 5715-5725',\n",
       "  'citations': '179',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1709.10190v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=8920876554647800563&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 325: {'ID': 325,\n",
       "  'title': 'PoseNet: A Convolutional Network for Real-Time 6-DOF Camera  Relocalization',\n",
       "  'authors': ['Matthew Grimes', 'Roberto Cipolla', 'Alex Kendall'],\n",
       "  'published': '2015-05-27T18:18:42Z',\n",
       "  'updated': '2016-02-18T13:52:18Z',\n",
       "  'abstract': 'We present a robust and real-time monocular six degree of freedomrelocalization system. Our system trains a convolutional neural network toregress the 6-DOF camera pose from a single RGB image in an end-to-end mannerwith no need of additional engineering or graph optimisation. The algorithm canoperate indoors and outdoors in real time, taking 5ms per frame to compute. Itobtains approximately 2m and 6 degree accuracy for large scale outdoor scenesand 0.5m and 10 degree accuracy indoors. This is achieved using an efficient 23layer deep convnet, demonstrating that convnets can be used to solvecomplicated out of image plane regression problems. This was made possible byleveraging transfer learning from large scale classification data. We show theconvnet localizes from high level features and is robust to difficult lighting,motion blur and different camera intrinsics where point based SIFT registrationfails. Furthermore we show how the pose feature that is produced generalizes toother scenes allowing us to regress pose with only a few dozen trainingexamples. PoseNet code, dataset and an online demonstration is available on ourproject webpage, at http://mi.eng.cam.ac.uk/projects/relocalisation/',\n",
       "  'categories': ['cs.CV', 'cs.NE', 'cs.RO'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 2938-2946',\n",
       "  'citations': '917',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1505.07427v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=7791289761831755523&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 326: {'ID': 326,\n",
       "  'title': 'DeepCut: Joint Subset Partition and Labeling for Multi Person Pose  Estimation',\n",
       "  'authors': ['Bjoern Andres',\n",
       "   'Bernt Schiele',\n",
       "   'Eldar Insafutdinov',\n",
       "   'Mykhaylo Andriluka',\n",
       "   'Siyu Tang',\n",
       "   'Leonid Pishchulin',\n",
       "   'Peter Gehler'],\n",
       "  'published': '2015-11-20T15:37:55Z',\n",
       "  'updated': '2016-04-26T04:26:29Z',\n",
       "  'abstract': 'This paper considers the task of articulated human pose estimation ofmultiple people in real world images. We propose an approach that jointlysolves the tasks of detection and pose estimation: it infers the number ofpersons in a scene, identifies occluded body parts, and disambiguates bodyparts between people in close proximity of each other. This joint formulationis in contrast to previous strategies, that address the problem by firstdetecting people and subsequently estimating their body pose. We propose apartitioning and labeling formulation of a set of body-part hypothesesgenerated with CNN-based part detectors. Our formulation, an instance of aninteger linear program, implicitly performs non-maximum suppression on the setof part candidates and groups them to form configurations of body partsrespecting geometric and appearance constraints. Experiments on four differentdatasets demonstrate state-of-the-art results for both single person and multiperson pose estimation. Models and code available athttp://pose.mpi-inf.mpg.de.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '481',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1511.06645v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2566321101337466182&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 327: {'ID': 327,\n",
       "  'title': 'Beyond Part Models: Person Retrieval with Refined Part Pooling (and a  Strong Convolutional Baseline)',\n",
       "  'authors': ['Yi Yang',\n",
       "   'Yifan Sun',\n",
       "   'Liang Zheng',\n",
       "   'Qi Tian',\n",
       "   'Shengjin Wang'],\n",
       "  'published': '2017-11-26T08:44:53Z',\n",
       "  'updated': '2018-01-09T07:11:53Z',\n",
       "  'abstract': 'Employing part-level features for pedestrian image description offersfine-grained information and has been verified as beneficial for personretrieval in very recent literature. A prerequisite of part discovery is thateach part should be well located. Instead of using external cues, e.g., poseestimation, to directly locate parts, this paper lays emphasis on the contentconsistency within each part.  Specifically, we target at learning discriminative part-informed features forperson retrieval and make two contributions. (i) A network named Part-basedConvolutional Baseline (PCB). Given an image input, it outputs a convolutionaldescriptor consisting of several part-level features. With a uniform partitionstrategy, PCB achieves competitive results with the state-of-the-art methods,proving itself as a strong convolutional baseline for person retrieval.  (ii) A refined part pooling (RPP) method. Uniform partition inevitably incursoutliers in each part, which are in fact more similar to other parts. RPPre-assigns these outliers to the parts they are closest to, resulting inrefined parts with enhanced within-part consistency. Experiment confirms thatRPP allows PCB to gain another round of performance boost. For instance, on theMarket-1501 dataset, we achieve (77.4+4.2)% mAP and (92.3+1.5)% rank-1accuracy, surpassing the state of the art by a large margin.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 480-496',\n",
       "  'citations': '489',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1711.09349v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=16920363862459764705&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 328: {'ID': 328,\n",
       "  'title': 'Unsupervised Learning of Visual Representations by Solving Jigsaw  Puzzles',\n",
       "  'authors': ['Paolo Favaro', 'Mehdi Noroozi'],\n",
       "  'published': '2016-03-30T15:27:37Z',\n",
       "  'updated': '2017-08-22T17:32:19Z',\n",
       "  'abstract': 'In this paper we study the problem of image representation learning withouthuman annotation. By following the principles of self-supervision, we build aconvolutional neural network (CNN) that can be trained to solve Jigsaw puzzlesas a pretext task, which requires no manual labeling, and then later repurposedto solve object classification and detection. To maintain the compatibilityacross tasks we introduce the context-free network (CFN), a siamese-ennead CNN.The CFN takes image tiles as input and explicitly limits the receptive field(or context) of its early processing units to one tile at a time. We show thatthe CFN includes fewer parameters than AlexNet while preserving the samesemantic learning capabilities. By training the CFN to solve Jigsaw puzzles, welearn both a feature mapping of object parts as well as their correct spatialarrangement. Our experimental evaluations show that the learned featurescapture semantically relevant content. Our proposed method for learning visualrepresentations outperforms state of the art methods in several transferlearning benchmarks.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (6), 69-84',\n",
       "  'citations': '465',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1603.09246v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=13080441875872699761&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 329: {'ID': 329,\n",
       "  'title': 'Learning without Forgetting',\n",
       "  'authors': ['Zhizhong Li', 'Derek Hoiem'],\n",
       "  'published': '2016-06-29T20:54:04Z',\n",
       "  'updated': '2017-02-14T22:32:30Z',\n",
       "  'abstract': 'When building a unified vision system or gradually adding new capabilities toa system, the usual assumption is that training data for all tasks is alwaysavailable. However, as the number of tasks grows, storing and retraining onsuch data becomes infeasible. A new problem arises where we add newcapabilities to a Convolutional Neural Network (CNN), but the training data forits existing capabilities are unavailable. We propose our Learning withoutForgetting method, which uses only new task data to train the network whilepreserving the original capabilities. Our method performs favorably compared tocommonly used feature extraction and fine-tuning adaption techniques andperforms similarly to multitask learning that uses original task data we assumeunavailable. A more surprising observation is that Learning without Forgettingmay be able to replace fine-tuning with similar old and new task datasets forimproved new task performance.',\n",
       "  'categories': ['cs.CV', 'cs.LG', 'stat.ML'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 40 (12), 2935\\xa0…',\n",
       "  'citations': '649',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1606.09282v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=11736167988707774176&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 330: {'ID': 330,\n",
       "  'title': 'Cross-stitch Networks for Multi-task Learning',\n",
       "  'authors': ['Abhinav Gupta',\n",
       "   'Ishan Misra',\n",
       "   'Abhinav Shrivastava',\n",
       "   'Martial Hebert'],\n",
       "  'published': '2016-04-12T19:43:25Z',\n",
       "  'updated': '2016-04-12T19:43:25Z',\n",
       "  'abstract': 'Multi-task learning in Convolutional Networks has displayed remarkablesuccess in the field of recognition. This success can be largely attributed tolearning shared representations from multiple supervisory tasks. However,existing multi-task approaches rely on enumerating multiple networkarchitectures specific to the tasks at hand, that do not generalize. In thispaper, we propose a principled approach to learn shared representations inConvNets using multi-task learning. Specifically, we propose a new sharingunit: \"cross-stitch\" unit. These units combine the activations from multiplenetworks and can be trained end-to-end. A network with cross-stitch units canlearn an optimal combination of shared and task-specific representations. Ourproposed method generalizes across multiple tasks and shows dramaticallyimproved performance over baseline methods for categories with few trainingexamples.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '394',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1604.03539v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=9201269953233977173&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 331: {'ID': 331,\n",
       "  'title': 'A Style-Based Generator Architecture for Generative Adversarial Networks',\n",
       "  'authors': ['Samuli Laine', 'Tero Karras', 'Timo Aila'],\n",
       "  'published': '2018-12-12T13:59:43Z',\n",
       "  'updated': '2019-03-29T11:08:46Z',\n",
       "  'abstract': 'We propose an alternative generator architecture for generative adversarialnetworks, borrowing from style transfer literature. The new architecture leadsto an automatically learned, unsupervised separation of high-level attributes(e.g., pose and identity when trained on human faces) and stochastic variationin the generated images (e.g., freckles, hair), and it enables intuitive,scale-specific control of the synthesis. The new generator improves thestate-of-the-art in terms of traditional distribution quality metrics, leads todemonstrably better interpolation properties, and also better disentangles thelatent factors of variation. To quantify interpolation quality anddisentanglement, we propose two new, automated methods that are applicable toany generator architecture. Finally, we introduce a new, highly varied andhigh-quality dataset of human faces.',\n",
       "  'categories': ['cs.NE', 'cs.LG', 'stat.ML'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '871',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1812.04948v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=9092299839549248053&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 332: {'ID': 332,\n",
       "  'title': 'Bottom-Up and Top-Down Attention for Image Captioning and Visual  Question Answering',\n",
       "  'authors': ['Damien Teney',\n",
       "   'Lei Zhang',\n",
       "   'Stephen Gould',\n",
       "   'Peter Anderson',\n",
       "   'Chris Buehler',\n",
       "   'Mark Johnson',\n",
       "   'Xiaodong He'],\n",
       "  'published': '2017-07-25T13:50:17Z',\n",
       "  'updated': '2018-03-14T05:24:23Z',\n",
       "  'abstract': 'Top-down visual attention mechanisms have been used extensively in imagecaptioning and visual question answering (VQA) to enable deeper imageunderstanding through fine-grained analysis and even multiple steps ofreasoning. In this work, we propose a combined bottom-up and top-down attentionmechanism that enables attention to be calculated at the level of objects andother salient image regions. This is the natural basis for attention to beconsidered. Within our approach, the bottom-up mechanism (based on FasterR-CNN) proposes image regions, each with an associated feature vector, whilethe top-down mechanism determines feature weightings. Applying this approach toimage captioning, our results on the MSCOCO test server establish a newstate-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability ofthe method, applying the same approach to VQA we obtain first place in the 2017VQA Challenge.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '1078',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1707.07998v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=7383633913245131178&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 333: {'ID': 333,\n",
       "  'title': 'Free-Form Image Inpainting with Gated Convolution',\n",
       "  'authors': ['Xin Lu',\n",
       "   'Xiaohui Shen',\n",
       "   'Thomas Huang',\n",
       "   'Zhe Lin',\n",
       "   'Jiahui Yu',\n",
       "   'Jimei Yang'],\n",
       "  'published': '2018-06-10T05:51:32Z',\n",
       "  'updated': '2019-10-22T03:06:37Z',\n",
       "  'abstract': 'We present a generative image inpainting system to complete images withfree-form mask and guidance. The system is based on gated convolutions learnedfrom millions of images without additional labelling efforts. The proposedgated convolution solves the issue of vanilla convolution that treats all inputpixels as valid ones, generalizes partial convolution by providing a learnabledynamic feature selection mechanism for each channel at each spatial locationacross all layers. Moreover, as free-form masks may appear anywhere in imageswith any shape, global and local GANs designed for a single rectangular maskare not applicable. Thus, we also present a patch-based GAN loss, namedSN-PatchGAN, by applying spectral-normalized discriminator on dense imagepatches. SN-PatchGAN is simple in formulation, fast and stable in training.Results on automatic image inpainting and user-guided extension demonstratethat our system generates higher-quality and more flexible results thanprevious methods. Our system helps user quickly remove distracting objects,modify image layouts, clear watermarks and edit faces. Code, demo and modelsare available at: https://github.com/JiahuiYu/generative_inpainting',\n",
       "  'categories': ['cs.CV', 'cs.GR', 'cs.LG'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 4471-4480',\n",
       "  'citations': '197',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1806.03589v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=10791632868951282786&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 334: {'ID': 334,\n",
       "  'title': 'Grad-CAM: Visual Explanations from Deep Networks via Gradient-based  Localization',\n",
       "  'authors': ['Ramakrishna Vedantam',\n",
       "   'Abhishek Das',\n",
       "   'Dhruv Batra',\n",
       "   'Ramprasaath R. Selvaraju',\n",
       "   'Devi Parikh',\n",
       "   'Michael Cogswell'],\n",
       "  'published': '2016-10-07T19:54:24Z',\n",
       "  'updated': '2019-12-03T02:13:03Z',\n",
       "  'abstract': 'We propose a technique for producing \"visual explanations\" for decisions froma large class of CNN-based models, making them more transparent. Our approach -Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients ofany target concept, flowing into the final convolutional layer to produce acoarse localization map highlighting important regions in the image forpredicting the concept. Grad-CAM is applicable to a wide variety of CNNmodel-families: (1) CNNs with fully-connected layers, (2) CNNs used forstructured outputs, (3) CNNs used in tasks with multimodal inputs orreinforcement learning, without any architectural changes or re-training. Wecombine Grad-CAM with fine-grained visualizations to create a high-resolutionclass-discriminative visualization and apply it to off-the-shelf imageclassification, captioning, and visual question answering (VQA) models,including ResNet-based architectures. In the context of image classificationmodels, our visualizations (a) lend insights into their failure modes, (b) arerobust to adversarial images, (c) outperform previous methods on localization,(d) are more faithful to the underlying model and (e) help achievegeneralization by identifying dataset bias. For captioning and VQA, we showthat even non-attention based models can localize inputs. We devise a way toidentify important neurons through Grad-CAM and combine it with neuron names toprovide textual explanations for model decisions. Finally, we design andconduct human studies to measure if Grad-CAM helps users establish appropriatetrust in predictions from models and show that Grad-CAM helps untrained userssuccessfully discern a \\'stronger\\' nodel from a \\'weaker\\' one even when both makeidentical predictions. Our code is available athttps://github.com/ramprs/grad-cam/, along with a demo athttp://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.LG'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 618-626',\n",
       "  'citations': '2183',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1610.02391v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=11803081582287838465&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 335: {'ID': 335,\n",
       "  'title': 'End-to-End Learning of Geometry and Context for Deep Stereo Regression',\n",
       "  'authors': ['Adam Bry',\n",
       "   'Saumitro Dasgupta',\n",
       "   'Alex Kendall',\n",
       "   'Hayk Martirosyan',\n",
       "   'Abraham Bachrach',\n",
       "   'Peter Henry',\n",
       "   'Ryan Kennedy'],\n",
       "  'published': '2017-03-13T10:00:52Z',\n",
       "  'updated': '2017-03-13T10:00:52Z',\n",
       "  'abstract': \"We propose a novel deep learning architecture for regressing disparity from arectified pair of stereo images. We leverage knowledge of the problem'sgeometry to form a cost volume using deep feature representations. We learn toincorporate contextual information using 3-D convolutions over this volume.Disparity values are regressed from the cost volume using a proposeddifferentiable soft argmin operation, which allows us to train our methodend-to-end to sub-pixel accuracy without any additional post-processing orregularization. We evaluate our method on the Scene Flow and KITTI datasets andon KITTI we set a new state-of-the-art benchmark, while being significantlyfaster than competing approaches.\",\n",
       "  'categories': ['cs.CV', 'cs.NE'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 66-75',\n",
       "  'citations': '333',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1703.04309v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=11888123919489276419&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 336: {'ID': 336,\n",
       "  'title': 'Taking a Deeper Look at Pedestrians',\n",
       "  'authors': ['Mohamed Omran',\n",
       "   'Bernt Schiele',\n",
       "   'Jan Hosang',\n",
       "   'Rodrigo Benenson'],\n",
       "  'published': '2015-01-23T13:07:56Z',\n",
       "  'updated': '2015-01-23T13:07:56Z',\n",
       "  'abstract': 'In this paper we study the use of convolutional neural networks (convnets)for the task of pedestrian detection. Despite their recent diverse successes,convnets historically underperform compared to other pedestrian detectors. Wedeliberately omit explicitly modelling the problem into the network (e.g. partsor occlusion modelling) and show that we can reach competitive performancewithout bells and whistles. In a wide range of experiments we analyse small andbig convnets, their architectural choices, parameters, and the influence ofdifferent training data, including pre-training on surrogate tasks.  We present the best convnet detectors on the Caltech and KITTI dataset. OnCaltech our convnets reach top performance both for the Caltech1x andCaltech10x training setup. Using additional data at training time our strongestconvnet model is competitive even to detectors that use additional data(optical flow) at test time.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '316',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1501.05790v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=12213861898695501044&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 337: {'ID': 337,\n",
       "  'title': 'Learning Depth from Single Monocular Images Using Deep Convolutional  Neural Fields',\n",
       "  'authors': ['Guosheng Lin', 'Fayao Liu', 'Chunhua Shen', 'Ian Reid'],\n",
       "  'published': '2015-02-26T01:26:22Z',\n",
       "  'updated': '2015-11-25T00:03:31Z',\n",
       "  'abstract': 'In this article, we tackle the problem of depth estimation from singlemonocular images. Compared with depth estimation using multiple images such asstereo depth perception, depth from monocular images is much more challenging.Prior work typically focuses on exploiting geometric priors or additionalsources of information, most using hand-crafted features. Recently, there ismounting evidence that features from deep convolutional neural networks (CNN)set new records for various vision applications. On the other hand, consideringthe continuous characteristic of the depth values, depth estimations can benaturally formulated as a continuous conditional random field (CRF) learningproblem. Therefore, here we present a deep convolutional neural field model forestimating depths from single monocular images, aiming to jointly explore thecapacity of deep CNN and continuous CRF. In particular, we propose a deepstructured learning scheme which learns the unary and pairwise potentials ofcontinuous CRF in a unified deep CNN framework. We then further propose anequally effective model based on fully convolutional networks and a novelsuperpixel pooling method, which is $\\\\sim 10$ times faster, to speedup thepatch-wise convolutions in the deep model. With this more efficient model, weare able to design deeper networks to pursue better performance. Experiments onboth indoor and outdoor scene datasets demonstrate that the proposed methodoutperforms state-of-the-art depth estimation approaches.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 38 (10), 2024\\xa0…',\n",
       "  'citations': '621',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1502.07411v6',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2562418167496300062&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 338: {'ID': 338,\n",
       "  'title': 'Group Normalization',\n",
       "  'authors': ['Yuxin Wu', 'Kaiming He'],\n",
       "  'published': '2018-03-22T17:57:16Z',\n",
       "  'updated': '2018-06-11T22:48:02Z',\n",
       "  'abstract': \"Batch Normalization (BN) is a milestone technique in the development of deeplearning, enabling various networks to train. However, normalizing along thebatch dimension introduces problems --- BN's error increases rapidly when thebatch size becomes smaller, caused by inaccurate batch statistics estimation.This limits BN's usage for training larger models and transferring features tocomputer vision tasks including detection, segmentation, and video, whichrequire small batches constrained by memory consumption. In this paper, wepresent Group Normalization (GN) as a simple alternative to BN. GN divides thechannels into groups and computes within each group the mean and variance fornormalization. GN's computation is independent of batch sizes, and its accuracyis stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GNhas 10.6% lower error than its BN counterpart when using a batch size of 2;when using typical batch sizes, GN is comparably good with BN and outperformsother normalization variants. Moreover, GN can be naturally transferred frompre-training to fine-tuning. GN can outperform its BN-based counterparts forobject detection and segmentation in COCO, and for video classification inKinetics, showing that GN can effectively replace the powerful BN in a varietyof tasks. GN can be easily implemented by a few lines of code in modernlibraries.\",\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 3-19',\n",
       "  'citations': '628',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1803.08494v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14814179610283147593&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 339: {'ID': 339,\n",
       "  'title': 'Multi-view Convolutional Neural Networks for 3D Shape Recognition',\n",
       "  'authors': ['Erik Learned-Miller',\n",
       "   'Evangelos Kalogerakis',\n",
       "   'Subhransu Maji',\n",
       "   'Hang Su'],\n",
       "  'published': '2015-05-05T04:51:19Z',\n",
       "  'updated': '2015-09-27T20:42:16Z',\n",
       "  'abstract': \"A longstanding question in computer vision concerns the representation of 3Dshapes for recognition: should 3D shapes be represented with descriptorsoperating on their native 3D formats, such as voxel grid or polygon mesh, orcan they be effectively represented with view-based descriptors? We addressthis question in the context of learning to recognize 3D shapes from acollection of their rendered views on 2D images. We first present a standardCNN architecture trained to recognize the shapes' rendered views independentlyof each other, and show that a 3D shape can be recognized even from a singleview at an accuracy far higher than using state-of-the-art 3D shapedescriptors. Recognition rates further increase when multiple views of theshapes are provided. In addition, we present a novel CNN architecture thatcombines information from multiple views of a 3D shape into a single andcompact shape descriptor offering even better recognition performance. The samearchitecture can be applied to accurately recognize human hand-drawn sketchesof shapes. We conclude that a collection of 2D views can be highly informativefor 3D shape recognition and is amenable to emerging CNN architectures andtheir derivatives.\",\n",
       "  'categories': ['cs.CV', 'cs.GR'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 945-953',\n",
       "  'citations': '1264',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1505.00880v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=4277219180010481756&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 340: {'ID': 340,\n",
       "  'title': 'Finding Action Tubes',\n",
       "  'authors': ['Georgia Gkioxari', 'Jitendra Malik'],\n",
       "  'published': '2014-11-21T21:38:15Z',\n",
       "  'updated': '2014-11-21T21:38:15Z',\n",
       "  'abstract': 'We address the problem of action detection in videos. Driven by the latestprogress in object detection from 2D images, we build action models using richfeature hierarchies derived from shape and kinematic cues. We incorporateappearance and motion in two ways. First, starting from image region proposalswe select those that are motion salient and thus are more likely to contain theaction. This leads to a significant reduction in the number of regions beingprocessed and allows for faster computations. Second, we extractspatio-temporal feature representations to build strong classifiers usingConvolutional Neural Networks. We link our predictions to produce detectionsconsistent in time, which we call action tubes. We show that our approachoutperforms other techniques in the task of action detection.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '461',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1411.6031v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=16858679220712660534&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 341: {'ID': 341,\n",
       "  'title': 'Everybody Dance Now',\n",
       "  'authors': ['Alexei A. Efros',\n",
       "   'Caroline Chan',\n",
       "   'Tinghui Zhou',\n",
       "   'Shiry Ginosar'],\n",
       "  'published': '2018-08-22T13:58:36Z',\n",
       "  'updated': '2019-08-27T21:10:54Z',\n",
       "  'abstract': 'This paper presents a simple method for \"do as I do\" motion transfer: given asource video of a person dancing, we can transfer that performance to a novel(amateur) target after only a few minutes of the target subject performingstandard moves. We approach this problem as video-to-video translation usingpose as an intermediate representation. To transfer the motion, we extractposes from the source subject and apply the learned pose-to-appearance mappingto generate the target subject. We predict two consecutive frames fortemporally coherent video results and introduce a separate pipeline forrealistic face synthesis. Although our method is quite simple, it producessurprisingly compelling results (see video). This motivates us to also providea forensics tool for reliable synthetic content detection, which is able todistinguish videos synthesized by our system from real data. In addition, werelease a first-of-its-kind open-source dataset of videos that can be legallyused for training and motion transfer.',\n",
       "  'categories': ['cs.GR', 'cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 5933-5942',\n",
       "  'citations': '187',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1808.07371v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6741146518985213994&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 342: {'ID': 342,\n",
       "  'title': 'Network Dissection: Quantifying Interpretability of Deep Visual  Representations',\n",
       "  'authors': ['Aditya Khosla',\n",
       "   'Antonio Torralba',\n",
       "   'Bolei Zhou',\n",
       "   'Aude Oliva',\n",
       "   'David Bau'],\n",
       "  'published': '2017-04-19T16:10:38Z',\n",
       "  'updated': '2017-04-19T16:10:38Z',\n",
       "  'abstract': 'We propose a general framework called Network Dissection for quantifying theinterpretability of latent representations of CNNs by evaluating the alignmentbetween individual hidden units and a set of semantic concepts. Given any CNNmodel, the proposed method draws on a broad data set of visual concepts toscore the semantics of hidden units at each intermediate convolutional layer.The units with semantics are given labels across a range of objects, parts,scenes, textures, materials, and colors. We use the proposed method to test thehypothesis that interpretability of units is equivalent to random linearcombinations of units, then we apply our method to compare the latentrepresentations of various networks when trained to solve different supervisedand self-supervised training tasks. We further analyze the effect of trainingiterations, compare networks trained with different initializations, examinethe impact of network depth and width, and measure the effect of dropout andbatch normalization on the interpretability of deep visual representations. Wedemonstrate that the proposed method can shed light on characteristics of CNNmodels and training methods that go beyond measurements of their discriminativepower.',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'I.2.10'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '500',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1704.05796v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=18069685615852396783&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 343: {'ID': 343,\n",
       "  'title': 'Bayesian CP Factorization of Incomplete Tensors with Automatic Rank  Determination',\n",
       "  'authors': ['Qibin Zhao', 'Andrzej Cichocki', 'Liqing Zhang'],\n",
       "  'published': '2014-01-25T05:08:33Z',\n",
       "  'updated': '2014-10-09T09:48:37Z',\n",
       "  'abstract': 'CANDECOMP/PARAFAC (CP) tensor factorization of incomplete data is a powerfultechnique for tensor completion through explicitly capturing the multilinearlatent factors. The existing CP algorithms require the tensor rank to bemanually specified, however, the determination of tensor rank remains achallenging problem especially for CP rank. In addition, existing approaches donot take into account uncertainty information of latent factors, as well asmissing entries. To address these issues, we formulate CP factorization using ahierarchical probabilistic model and employ a fully Bayesian treatment byincorporating a sparsity-inducing prior over multiple latent factors and theappropriate hyperpriors over all hyperparameters, resulting in automatic rankdetermination. To learn the model, we develop an efficient deterministicBayesian inference algorithm, which scales linearly with data size. Our methodis characterized as a tuning parameter-free approach, which can effectivelyinfer underlying multilinear factors with a low-rank constraint, while alsoproviding predictive distributions over missing entries. Extensive simulationson synthetic data illustrate the intrinsic capability of our method to recoverthe ground-truth of CP rank and prevent the overfitting problem, even when alarge amount of entries are missing. Moreover, the results from real-worldapplications, including image inpainting and facial image synthesis,demonstrate that our method outperforms state-of-the-art approaches for bothtensor factorization and tensor completion in terms of predictive performance.',\n",
       "  'categories': ['cs.LG', 'cs.CV', 'stat.ML'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 37 (9), 1751-1763',\n",
       "  'citations': '242',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1401.6497v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5204851493209171114&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 344: {'ID': 344,\n",
       "  'title': 'Holistically-Nested Edge Detection',\n",
       "  'authors': ['Zhuowen Tu', 'Saining Xie'],\n",
       "  'published': '2015-04-24T02:12:15Z',\n",
       "  'updated': '2015-10-04T02:15:38Z',\n",
       "  'abstract': 'We develop a new edge detection algorithm that tackles two important issuesin this long-standing vision problem: (1) holistic image training andprediction; and (2) multi-scale and multi-level feature learning. Our proposedmethod, holistically-nested edge detection (HED), performs image-to-imageprediction by means of a deep learning model that leverages fully convolutionalneural networks and deeply-supervised nets. HED automatically learns richhierarchical representations (guided by deep supervision on side responses)that are important in order to approach the human ability resolve thechallenging ambiguity in edge and object boundary detection. We significantlyadvance the state-of-the-art on the BSD500 dataset (ODS F-score of .782) andthe NYU Depth dataset (ODS F-score of .746), and do so with an improved speed(0.4 second per image) that is orders of magnitude faster than some recentCNN-based edge detection algorithms.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 1395-1403',\n",
       "  'citations': '1519',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1504.06375v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=18154299256265143241&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 345: {'ID': 345,\n",
       "  'title': 'DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial  Networks',\n",
       "  'authors': ['Volodymyr Budzan',\n",
       "   'Orest Kupyn',\n",
       "   'Dmytro Mishkin',\n",
       "   'Jiri Matas',\n",
       "   'Mykola Mykhailych'],\n",
       "  'published': '2017-11-19T19:46:18Z',\n",
       "  'updated': '2018-04-03T10:28:19Z',\n",
       "  'abstract': 'We present DeblurGAN, an end-to-end learned method for motion deblurring. Thelearning is based on a conditional GAN and the content loss . DeblurGANachieves state-of-the art performance both in the structural similarity measureand visual appearance. The quality of the deblurring model is also evaluated ina novel way on a real-world problem -- object detection on (de-)blurred images.The method is 5 times faster than the closest competitor -- DeepDeblur. We alsointroduce a novel method for generating synthetic motion blurred images fromsharp ones, allowing realistic dataset augmentation.  The model, code and the dataset are available athttps://github.com/KupynOrest/DeblurGAN',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '354',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1711.07064v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5089744073478166070&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 346: {'ID': 346,\n",
       "  'title': 'High-Resolution Image Inpainting using Multi-Scale Neural Patch  Synthesis',\n",
       "  'authors': ['Chao Yang',\n",
       "   'Xin Lu',\n",
       "   'Oliver Wang',\n",
       "   'Eli Shechtman',\n",
       "   'Hao Li',\n",
       "   'Zhe Lin'],\n",
       "  'published': '2016-11-30T01:58:54Z',\n",
       "  'updated': '2017-04-13T06:56:06Z',\n",
       "  'abstract': 'Recent advances in deep learning have shown exciting promise in filling largeholes in natural images with semantically plausible and context aware details,impacting fundamental image manipulation tasks such as object removal. Whilethese learning-based methods are significantly more effective in capturinghigh-level features than prior techniques, they can only handle verylow-resolution inputs due to memory limitations and difficulty in training.Even for slightly larger images, the inpainted regions would appear blurry andunpleasant boundaries become visible. We propose a multi-scale neural patchsynthesis approach based on joint optimization of image content and textureconstraints, which not only preserves contextual structures but also produceshigh-frequency details by matching and adapting patches with the most similarmid-layer feature correlations of a deep classification network. We evaluateour method on the ImageNet and Paris Streetview datasets and achievedstate-of-the-art inpainting accuracy. We show our approach produces sharper andmore coherent results than prior methods, especially for high-resolutionimages.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '370',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1611.09969v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=154146008869681014&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 347: {'ID': 347,\n",
       "  'title': 'Unsupervised Visual Representation Learning by Context Prediction',\n",
       "  'authors': ['Abhinav Gupta', 'Alexei A. Efros', 'Carl Doersch'],\n",
       "  'published': '2015-05-19T21:18:17Z',\n",
       "  'updated': '2016-01-16T22:09:45Z',\n",
       "  'abstract': 'This work explores the use of spatial context as a source of free andplentiful supervisory signal for training a rich visual representation. Givenonly a large, unlabeled image collection, we extract random pairs of patchesfrom each image and train a convolutional neural net to predict the position ofthe second patch relative to the first. We argue that doing well on this taskrequires the model to learn to recognize objects and their parts. Wedemonstrate that the feature representation learned using this within-imagecontext indeed captures visual similarity across images. For example, thisrepresentation allows us to perform unsupervised visual discovery of objectslike cats, people, and even birds from the Pascal VOC 2011 detection dataset.Furthermore, we show that the learned ConvNet can be used in the R-CNNframework and provides a significant boost over a randomly-initialized ConvNet,resulting in state-of-the-art performance among algorithms which use onlyPascal-provided training set annotations.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 1422-1430',\n",
       "  'citations': '709',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1505.05192v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2604183921270291244&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 348: {'ID': 348,\n",
       "  'title': 'Rethinking ImageNet Pre-training',\n",
       "  'authors': ['Piotr Dollár', 'Ross Girshick', 'Kaiming He'],\n",
       "  'published': '2018-11-21T18:55:58Z',\n",
       "  'updated': '2018-11-21T18:55:58Z',\n",
       "  'abstract': \"We report competitive results on object detection and instance segmentationon the COCO dataset using standard models trained from random initialization.The results are no worse than their ImageNet pre-training counterparts evenwhen using the hyper-parameters of the baseline system (Mask R-CNN) that wereoptimized for fine-tuning pre-trained models, with the sole exception ofincreasing the number of training iterations so the randomly initialized modelsmay converge. Training from random initialization is surprisingly robust; ourresults hold even when: (i) using only 10% of the training data, (ii) fordeeper and wider models, and (iii) for multiple tasks and metrics. Experimentsshow that ImageNet pre-training speeds up convergence early in training, butdoes not necessarily provide regularization or improve final target taskaccuracy. To push the envelope we demonstrate 50.9 AP on COCO object detectionwithout using any external data---a result on par with the top COCO 2017competition results that used ImageNet pre-training. These observationschallenge the conventional wisdom of ImageNet pre-training for dependent tasksand we expect these discoveries will encourage people to rethink the current defacto paradigm of `pre-training and fine-tuning' in computer vision.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 4918-4927',\n",
       "  'citations': '188',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1811.08883v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=8945175718646989267&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 349: {'ID': 349,\n",
       "  'title': 'Attention to Scale: Scale-aware Semantic Image Segmentation',\n",
       "  'authors': ['Yi Yang',\n",
       "   'Wei Xu',\n",
       "   'Jiang Wang',\n",
       "   'Alan L. Yuille',\n",
       "   'Liang-Chieh Chen'],\n",
       "  'published': '2015-11-10T23:53:57Z',\n",
       "  'updated': '2016-06-02T02:02:21Z',\n",
       "  'abstract': 'Incorporating multi-scale features in fully convolutional neural networks(FCNs) has been a key element to achieving state-of-the-art performance onsemantic image segmentation. One common way to extract multi-scale features isto feed multiple resized input images to a shared deep network and then mergethe resulting features for pixelwise classification. In this work, we proposean attention mechanism that learns to softly weight the multi-scale features ateach pixel location. We adapt a state-of-the-art semantic image segmentationmodel, which we jointly train with multi-scale input images and the attentionmodel. The proposed attention model not only outperforms average- andmax-pooling, but allows us to diagnostically visualize the importance offeatures at different positions and scales. Moreover, we show that adding extrasupervision to the output at each scale is essential to achieving excellentperformance when merging multi-scale features. We demonstrate the effectivenessof our model with extensive experiments on three challenging datasets,including PASCAL-Person-Part, PASCAL VOC 2012 and a subset of MS-COCO 2014.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '645',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1511.03339v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14655587414800838242&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 350: {'ID': 350,\n",
       "  'title': 'Videos as Space-Time Region Graphs',\n",
       "  'authors': ['Abhinav Gupta', 'Xiaolong Wang'],\n",
       "  'published': '2018-06-05T16:58:59Z',\n",
       "  'updated': '2018-12-21T23:56:25Z',\n",
       "  'abstract': 'How do humans recognize the action \"opening a book\" ? We argue that there aretwo important cues: modeling temporal shape dynamics and modeling functionalrelationships between humans and objects. In this paper, we propose torepresent videos as space-time region graphs which capture these two importantcues. Our graph nodes are defined by the object region proposals from differentframes in a long range video. These nodes are connected by two types ofrelations: (i) similarity relations capturing the long range dependenciesbetween correlated objects and (ii) spatial-temporal relations capturing theinteractions between nearby objects. We perform reasoning on this graphrepresentation via Graph Convolutional Networks. We achieve state-of-the-artresults on both Charades and Something-Something datasets. Especially forCharades, we obtain a huge 4.4% gain when our model is applied in complexenvironments.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 399-417',\n",
       "  'citations': '192',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1806.01810v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=8371622883937307202&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 351: {'ID': 351,\n",
       "  'title': 'Scene Graph Generation by Iterative Message Passing',\n",
       "  'authors': ['Li Fei-Fei', 'Christopher B. Choy', 'Yuke Zhu', 'Danfei Xu'],\n",
       "  'published': '2017-01-10T03:06:58Z',\n",
       "  'updated': '2017-04-12T04:11:32Z',\n",
       "  'abstract': 'Understanding a visual scene goes beyond recognizing individual objects inisolation. Relationships between objects also constitute rich semanticinformation about the scene. In this work, we explicitly model the objects andtheir relationships using scene graphs, a visually-grounded graphical structureof an image. We propose a novel end-to-end model that generates such structuredscene representation from an input image. The model solves the scene graphinference problem using standard RNNs and learns to iteratively improves itspredictions via message passing. Our joint inference model can take advantageof contextual cues to make better predictions on objects and theirrelationships. The experiments show that our model significantly outperformsprevious methods for generating scene graphs using Visual Genome dataset andinferring support relations with NYU Depth v2 dataset.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '315',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1701.02426v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=12812416044799283808&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 352: {'ID': 352,\n",
       "  'title': 'Social GAN: Socially Acceptable Trajectories with Generative Adversarial  Networks',\n",
       "  'authors': ['Li Fei-Fei',\n",
       "   'Silvio Savarese',\n",
       "   'Agrim Gupta',\n",
       "   'Justin Johnson',\n",
       "   'Alexandre Alahi'],\n",
       "  'published': '2018-03-29T01:24:02Z',\n",
       "  'updated': '2018-03-29T01:24:02Z',\n",
       "  'abstract': 'Understanding human motion behavior is critical for autonomous movingplatforms (like self-driving cars and social robots) if they are to navigatehuman-centric environments. This is challenging because human motion isinherently multimodal: given a history of human motion paths, there are manysocially plausible ways that people could move in the future. We tackle thisproblem by combining tools from sequence prediction and generative adversarialnetworks: a recurrent sequence-to-sequence model observes motion histories andpredicts future behavior, using a novel pooling mechanism to aggregateinformation across people. We predict socially plausible futures by trainingadversarially against a recurrent discriminator, and encourage diversepredictions with a novel variety loss. Through experiments on several datasetswe demonstrate that our approach outperforms prior work in terms of accuracy,variety, collision avoidance, and computational complexity.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '324',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1803.10892v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=129172247046928959&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 353: {'ID': 353,\n",
       "  'title': 'DualGAN: Unsupervised Dual Learning for Image-to-Image Translation',\n",
       "  'authors': ['Ping Tan', 'Hao Zhang', 'Minglun Gong', 'Zili Yi'],\n",
       "  'published': '2017-04-08T16:13:52Z',\n",
       "  'updated': '2018-10-09T20:42:00Z',\n",
       "  'abstract': 'Conditional Generative Adversarial Networks (GANs) for cross-domainimage-to-image translation have made much progress recently. Depending on thetask complexity, thousands to millions of labeled image pairs are needed totrain a conditional GAN. However, human labeling is expensive, evenimpractical, and large quantities of data may not always be available. Inspiredby dual learning from natural language translation, we develop a novel dual-GANmechanism, which enables image translators to be trained from two sets ofunlabeled images from two domains. In our architecture, the primal GAN learnsto translate images from domain U to those in domain V, while the dual GANlearns to invert the task. The closed loop made by the primal and dual tasksallows images from either domain to be translated and then reconstructed. Hencea loss function that accounts for the reconstruction error of images can beused to train the translators. Experiments on multiple image translation taskswith unlabeled data show considerable performance gain of DualGAN over a singleGAN. For some tasks, DualGAN can even achieve comparable or slightly betterresults than conditional GAN trained on fully labeled data.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 2849-2857',\n",
       "  'citations': '761',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1704.02510v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6550565919250210407&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 354: {'ID': 354,\n",
       "  'title': 'Deep Convolutional Neural Fields for Depth Estimation from a Single  Image',\n",
       "  'authors': ['Guosheng Lin', 'Fayao Liu', 'Chunhua Shen'],\n",
       "  'published': '2014-11-24T09:13:00Z',\n",
       "  'updated': '2014-12-18T04:11:14Z',\n",
       "  'abstract': 'We consider the problem of depth estimation from a single monocular image inthis work. It is a challenging task as no reliable depth cues are available,e.g., stereo correspondences, motions, etc. Previous efforts have been focusingon exploiting geometric priors or additional sources of information, with allusing hand-crafted features. Recently, there is mounting evidence that featuresfrom deep convolutional neural networks (CNN) are setting new records forvarious vision applications. On the other hand, considering the continuouscharacteristic of the depth values, depth estimations can be naturallyformulated into a continuous conditional random field (CRF) learning problem.Therefore, we in this paper present a deep convolutional neural field model forestimating depths from a single image, aiming to jointly explore the capacityof deep CNN and continuous CRF. Specifically, we propose a deep structuredlearning scheme which learns the unary and pairwise potentials of continuousCRF in a unified deep CNN framework.  The proposed method can be used for depth estimations of general scenes withno geometric priors nor any extra information injected. In our case, theintegral of the partition function can be analytically calculated, thus we canexactly solve the log-likelihood optimization. Moreover, solving the MAPproblem for predicting depths of a new image is highly efficient as closed-formsolutions exist. We experimentally demonstrate that the proposed methodoutperforms state-of-the-art depth estimation methods on both indoor andoutdoor scene datasets.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '596',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1411.6387v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5460753578310050253&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 355: {'ID': 355,\n",
       "  'title': 'Do We Really Need to Collect Millions of Faces for Effective Face  Recognition?',\n",
       "  'authors': ['Tal Hassner',\n",
       "   'Iacopo Masi',\n",
       "   'Anh Tuan Tran',\n",
       "   'Gerard Medioni',\n",
       "   'Jatuporn Toy Leksut'],\n",
       "  'published': '2016-03-23T02:57:15Z',\n",
       "  'updated': '2016-04-11T02:25:35Z',\n",
       "  'abstract': 'Face recognition capabilities have recently made extraordinary leaps. Thoughthis progress is at least partially due to ballooning training set sizes --huge numbers of face images downloaded and labeled for identity -- it is notclear if the formidable task of collecting so many images is truly necessary.We propose a far more accessible means of increasing training data sizes forface recognition systems. Rather than manually harvesting and labeling morefaces, we simply synthesize them. We describe novel methods of enriching anexisting dataset with important facial appearance variations by manipulatingthe faces it contains. We further apply this synthesis approach when matchingquery images represented using a standard convolutional neural network. Theeffect of training and testing with synthesized images is extensively tested onthe LFW and IJB-A (verification and identification) benchmarks and Janus CS2.The performances obtained by our approach match state of the art resultsreported by systems trained on millions of downloaded images.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (5), 579-596',\n",
       "  'citations': '269',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1603.07057v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=11085869434810770683&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 356: {'ID': 356,\n",
       "  'title': 'Synthetic Data for Text Localisation in Natural Images',\n",
       "  'authors': ['Andrew Zisserman', 'Andrea Vedaldi', 'Ankush Gupta'],\n",
       "  'published': '2016-04-22T13:23:08Z',\n",
       "  'updated': '2016-04-22T13:23:08Z',\n",
       "  'abstract': 'In this paper we introduce a new method for text detection in natural images.The method comprises two contributions: First, a fast and scalable engine togenerate synthetic images of text in clutter. This engine overlays synthetictext to existing background images in a natural way, accounting for the local3D scene geometry. Second, we use the synthetic images to train aFully-Convolutional Regression Network (FCRN) which efficiently performs textdetection and bounding-box regression at all locations and multiple scales inan image. We discuss the relation of FCRN to the recently-introduced YOLOdetector, as well as other end-to-end object detection systems based on deeplearning. The resulting detection network significantly out performs currentmethods for text detection in natural images, achieving an F-measure of 84.2%on the standard ICDAR 2013 benchmark. Furthermore, it can process 15 images persecond on a GPU.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '575',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1604.06646v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=4348461873934081485&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 357: {'ID': 357,\n",
       "  'title': 'CIDEr: Consensus-based Image Description Evaluation',\n",
       "  'authors': ['Ramakrishna Vedantam', 'Devi Parikh', 'C. Lawrence Zitnick'],\n",
       "  'published': '2014-11-20T23:54:35Z',\n",
       "  'updated': '2015-06-03T01:42:20Z',\n",
       "  'abstract': 'Automatically describing an image with a sentence is a long-standingchallenge in computer vision and natural language processing. Due to recentprogress in object detection, attribute classification, action recognition,etc., there is renewed interest in this area. However, evaluating the qualityof descriptions has proven to be challenging. We propose a novel paradigm forevaluating image descriptions that uses human consensus. This paradigm consistsof three main parts: a new triplet-based method of collecting human annotationsto measure consensus, a new automated metric (CIDEr) that captures consensus,and two new datasets: PASCAL-50S and ABSTRACT-50S that contain 50 sentencesdescribing each image. Our simple metric captures human judgment of consensusbetter than existing metrics across sentences generated by various sources. Wealso evaluate five state-of-the-art image description approaches using this newprotocol and provide a benchmark for future comparisons. A version of CIDErnamed CIDEr-D is available as a part of MS COCO evaluation server to enablesystematic evaluation and benchmarking.',\n",
       "  'categories': ['cs.CV', 'cs.CL', 'cs.IR'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '1260',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1411.5726v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2428149707553201565&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 358: {'ID': 358,\n",
       "  'title': 'MobileNetV2: Inverted Residuals and Linear Bottlenecks',\n",
       "  'authors': ['Mark Sandler',\n",
       "   'Menglong Zhu',\n",
       "   'Andrey Zhmoginov',\n",
       "   'Liang-Chieh Chen',\n",
       "   'Andrew Howard'],\n",
       "  'published': '2018-01-13T04:46:26Z',\n",
       "  'updated': '2019-03-21T19:44:34Z',\n",
       "  'abstract': 'In this paper we describe a new mobile architecture, MobileNetV2, thatimproves the state of the art performance of mobile models on multiple tasksand benchmarks as well as across a spectrum of different model sizes. We alsodescribe efficient ways of applying these mobile models to object detection ina novel framework we call SSDLite. Additionally, we demonstrate how to buildmobile semantic segmentation models through a reduced form of DeepLabv3 whichwe call Mobile DeepLabv3.  The MobileNetV2 architecture is based on an inverted residual structure wherethe input and output of the residual block are thin bottleneck layers oppositeto traditional residual models which use expanded representations in the inputan MobileNetV2 uses lightweight depthwise convolutions to filter features inthe intermediate expansion layer. Additionally, we find that it is important toremove non-linearities in the narrow layers in order to maintainrepresentational power. We demonstrate that this improves performance andprovide an intuition that led to this design. Finally, our approach allowsdecoupling of the input/output domains from the expressiveness of thetransformation, which provides a convenient framework for further analysis. Wemeasure our performance on Imagenet classification, COCO object detection, VOCimage segmentation. We evaluate the trade-offs between accuracy, and number ofoperations measured by multiply-adds (MAdd), as well as the number ofparameters',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '2085',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1801.04381v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5034558864053164025&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 359: {'ID': 359,\n",
       "  'title': 'Switching Convolutional Neural Network for Crowd Counting',\n",
       "  'authors': ['R. Venkatesh Babu', 'Shiv Surya', 'Deepak Babu Sam'],\n",
       "  'published': '2017-08-01T08:30:28Z',\n",
       "  'updated': '2017-08-03T11:02:02Z',\n",
       "  'abstract': 'We propose a novel crowd counting model that maps a given crowd scene to itsdensity. Crowd analysis is compounded by myriad of factors like inter-occlusionbetween people due to extreme crowding, high similarity of appearance betweenpeople and background elements, and large variability of camera view-points.Current state-of-the art approaches tackle these factors by using multi-scaleCNN architectures, recurrent networks and late fusion of features frommulti-column CNN with different receptive fields. We propose switchingconvolutional neural network that leverages variation of crowd density withinan image to improve the accuracy and localization of the predicted crowd count.Patches from a grid within a crowd scene are relayed to independent CNNregressors based on crowd count prediction quality of the CNN establishedduring training. The independent CNN regressors are designed to have differentreceptive fields and a switch classifier is trained to relay the crowd scenepatch to the best CNN regressor. We perform extensive experiments on all majorcrowd counting datasets and evidence better performance compared to currentstate-of-the-art methods. We provide interpretable representations of themultichotomy of space of crowd scene patches inferred from the switch. It isobserved that the switch relays an image patch to a particular CNN column basedon density of crowd.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'CVPR, 4031-4039',\n",
       "  'citations': '321',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1708.00199v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=18278994156144407180&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 360: {'ID': 360,\n",
       "  'title': 'Rank Pooling for Action Recognition',\n",
       "  'authors': ['Basura Fernando',\n",
       "   'Efstratios Gavves',\n",
       "   'Tinne Tuytelaars',\n",
       "   'Amir Ghodrati',\n",
       "   'Jose Oramas'],\n",
       "  'published': '2015-12-06T22:30:53Z',\n",
       "  'updated': '2016-05-16T00:41:05Z',\n",
       "  'abstract': 'We propose a function-based temporal pooling method that captures the latentstructure of the video sequence data - e.g. how frame-level features evolveover time in a video. We show how the parameters of a function that has beenfit to the video data can serve as a robust new video representation. As aspecific example, we learn a pooling function via ranking machines. By learningto rank the frame-level features of a video in chronological order, we obtain anew representation that captures the video-wide temporal dynamics of a video,suitable for action recognition. Other than ranking functions, we exploredifferent parametric models that could also explain the temporal changes invideos. The proposed functional pooling methods, and rank pooling inparticular, is easy to interpret and implement, fast to compute and effectivein recognizing a wide variety of actions. We evaluate our method on variousbenchmarks for generic action, fine-grained action and gesture recognition.Results show that rank pooling brings an absolute improvement of 7-10 averagepooling baseline. At the same time, rank pooling is compatible with andcomplementary to several appearance and local motion based methods andfeatures, such as improved trajectories and deep learning features.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 39 (4), 773-787',\n",
       "  'citations': '197',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1512.01848v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=12693622606762977277&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 361: {'ID': 361,\n",
       "  'title': 'Person Re-Identification by Camera Correlation Aware Feature  Augmentation',\n",
       "  'authors': ['Wei-Shi Zheng',\n",
       "   'Ying-Cong Chen',\n",
       "   'Xiatian Zhu',\n",
       "   'Jian-Huang Lai'],\n",
       "  'published': '2017-03-26T16:18:48Z',\n",
       "  'updated': '2017-03-26T16:18:48Z',\n",
       "  'abstract': 'The challenge of person re-identification (re-id) is to match individualimages of the same person captured by different non-overlapping camera viewsagainst significant and unknown cross-view feature distortion. While a largenumber of distance metric/subspace learning models have been developed forre-id, the cross-view transformations they learned are view-generic and thuspotentially less effective in quantifying the feature distortion inherent toeach camera view. Learning view-specific feature transformations for re-id(i.e., view-specific re-id), an under-studied approach, becomes an alternativeresort for this problem. In this work, we formulate a novel view-specificperson re-identification framework from the feature augmentation point of view,called Camera coRrelation Aware Feature augmenTation (CRAFT). Specifically,CRAFT performs cross-view adaptation by automatically measuring cameracorrelation from cross-view visual data distribution and adaptively conductingfeature augmentation to transform the original features into a new adaptivespace. Through our augmentation framework, view-generic learning algorithms canbe readily generalized to learn and optimize view-specific sub-models whilstsimultaneously modelling view-generic discrimination information. Therefore,our framework not only inherits the strength of view-generic model learning butalso provides an effective way to take into account view specificcharacteristics. Our CRAFT framework can be extended to jointly learnview-specific feature transformations for person re-id across a large networkwith more than two cameras, a largely under-investigated but realistic re-idsetting. Additionally, we present a domain-generic deep person appearancerepresentation which is designed particularly to be towards view invariant forfacilitating cross-view adaptation by CRAFT.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 40 (2), 392-408',\n",
       "  'citations': '176',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1703.08837v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=12785612054446603350&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 362: {'ID': 362,\n",
       "  'title': 'ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic  Segmentation',\n",
       "  'authors': ['Kaiming He', 'Di Lin', 'Jifeng Dai', 'Jian Sun', 'Jiaya Jia'],\n",
       "  'published': '2016-04-18T13:46:23Z',\n",
       "  'updated': '2016-04-18T13:46:23Z',\n",
       "  'abstract': 'Large-scale data is of crucial importance for learning semantic segmentationmodels, but annotating per-pixel masks is a tedious and inefficient procedure.We note that for the topic of interactive image segmentation, scribbles arevery widely used in academic research and commercial software, and arerecognized as one of the most user-friendly ways of interacting. In this paper,we propose to use scribbles to annotate images, and develop an algorithm totrain convolutional networks for semantic segmentation supervised by scribbles.Our algorithm is based on a graphical model that jointly propagates informationfrom scribbles to unmarked pixels and learns network parameters. We presentcompetitive object semantic segmentation results on the PASCAL VOC dataset byusing scribbles as annotations. Scribbles are also favored for annotating stuff(e.g., water, sky, grass) that has no well-defined shape, and our method showsexcellent results on the PASCAL-CONTEXT dataset thanks to extra inexpensivescribble annotations. Our scribble annotations on PASCAL VOC are available athttp://research.microsoft.com/en-us/um/people/jifdai/downloads/scribble_sup',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '301',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1604.05144v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=1344368443978557455&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 363: {'ID': 363,\n",
       "  'title': 'Deformable Part Models are Convolutional Neural Networks',\n",
       "  'authors': ['Jitendra Malik',\n",
       "   'Ross Girshick',\n",
       "   'Trevor Darrell',\n",
       "   'Forrest Iandola'],\n",
       "  'published': '2014-09-18T18:34:10Z',\n",
       "  'updated': '2014-10-01T18:44:14Z',\n",
       "  'abstract': 'Deformable part models (DPMs) and convolutional neural networks (CNNs) aretwo widely used tools for visual recognition. They are typically viewed asdistinct approaches: DPMs are graphical models (Markov random fields), whileCNNs are \"black-box\" non-linear classifiers. In this paper, we show that a DPMcan be formulated as a CNN, thus providing a novel synthesis of the two ideas.Our construction involves unrolling the DPM inference algorithm and mappingeach step to an equivalent (and at times novel) CNN layer. From thisperspective, it becomes natural to replace the standard image features used inDPM with a learned feature extractor. We call the resulting model DeepPyramidDPM and experimentally validate it on PASCAL VOC. DeepPyramid DPM significantlyoutperforms DPMs based on histograms of oriented gradients features (HOG) andslightly outperforms a comparable version of the recently introduced R-CNNdetection system, while running an order of magnitude faster.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '354',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1409.5403v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14100084883870768044&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 364: {'ID': 364,\n",
       "  'title': 'Fully Convolutional Instance-aware Semantic Segmentation',\n",
       "  'authors': ['Xiangyang Ji',\n",
       "   'Yichen Wei',\n",
       "   'Haozhi Qi',\n",
       "   'Yi Li',\n",
       "   'Jifeng Dai'],\n",
       "  'published': '2016-11-23T09:53:57Z',\n",
       "  'updated': '2017-04-10T09:00:54Z',\n",
       "  'abstract': 'We present the first fully convolutional end-to-end solution forinstance-aware semantic segmentation task. It inherits all the merits of FCNsfor semantic segmentation and instance mask proposal. It performs instance maskprediction and classification jointly. The underlying convolutionalrepresentation is fully shared between the two sub-tasks, as well as betweenall regions of interest. The proposed network is highly integrated and achievesstate-of-the-art performance in both accuracy and efficiency. It wins the COCO2016 segmentation competition by a large margin. Code would be released at\\\\url{https://github.com/daijifeng001/TA-FCN}.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '481',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1611.07709v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=8993319314849501947&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 365: {'ID': 365,\n",
       "  'title': 'Deep Ordinal Regression Network for Monocular Depth Estimation',\n",
       "  'authors': ['Huan Fu',\n",
       "   'Dacheng Tao',\n",
       "   'Kayhan Batmanghelich',\n",
       "   'Mingming Gong',\n",
       "   'Chaohui Wang'],\n",
       "  'published': '2018-06-06T22:36:23Z',\n",
       "  'updated': '2018-06-06T22:36:23Z',\n",
       "  'abstract': 'Monocular depth estimation, which plays a crucial role in understanding 3Dscene geometry, is an ill-posed problem. Recent methods have gained significantimprovement by exploring image-level information and hierarchical features fromdeep convolutional neural networks (DCNNs). These methods model depthestimation as a regression problem and train the regression networks byminimizing mean squared error, which suffers from slow convergence andunsatisfactory local solutions. Besides, existing depth estimation networksemploy repeated spatial pooling operations, resulting in undesirablelow-resolution feature maps. To obtain high-resolution depth maps,skip-connections or multi-layer deconvolution networks are required, whichcomplicates network training and consumes much more computations. To eliminateor at least largely reduce these problems, we introduce a spacing-increasingdiscretization (SID) strategy to discretize depth and recast depth networklearning as an ordinal regression problem. By training the network using anordinary regression loss, our method achieves much higher accuracy and\\\\dd{faster convergence in synch}. Furthermore, we adopt a multi-scale networkstructure which avoids unnecessary spatial pooling and captures multi-scaleinformation in parallel.  The method described in this paper achieves state-of-the-art results on fourchallenging benchmarks, i.e., KITTI [17], ScanNet [9], Make3D [50], and NYUDepth v2 [42], and win the 1st prize in Robust Vision Challenge 2018. Code hasbeen made available at: https://github.com/hufu6371/DORN.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '311',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1806.02446v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14501834367390829598&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 366: {'ID': 366,\n",
       "  'title': 'Multi-task Self-Supervised Visual Learning',\n",
       "  'authors': ['Andrew Zisserman', 'Carl Doersch'],\n",
       "  'published': '2017-08-25T18:52:17Z',\n",
       "  'updated': '2017-08-25T18:52:17Z',\n",
       "  'abstract': 'We investigate methods for combining multiple self-supervised tasks--i.e.,supervised tasks where data can be collected without manual labeling--in orderto train a single visual representation. First, we provide an apples-to-applescomparison of four different self-supervised tasks using the very deepResNet-101 architecture. We then combine tasks to jointly train a network. Wealso explore lasso regularization to encourage the network to factorize theinformation in its representation, and methods for \"harmonizing\" network inputsin order to learn a more unified representation. We evaluate all methods onImageNet classification, PASCAL VOC detection, and NYU depth prediction. Ourresults show that deeper networks work better, and that combining tasks--evenvia a naive multi-head architecture--always improves performance. Our bestjoint network nearly matches the PASCAL performance of a model pre-trained onImageNet classification, and matches the ImageNet network on NYU depthprediction.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 2051-2060',\n",
       "  'citations': '227',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1708.07860v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=16321364009430803753&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 367: {'ID': 367,\n",
       "  'title': 'A Discriminative CNN Video Representation for Event Detection',\n",
       "  'authors': ['Yi Yang', 'Alexander G. Hauptmann', 'Zhongwen Xu'],\n",
       "  'published': '2014-11-14T18:37:31Z',\n",
       "  'updated': '2014-11-14T18:37:31Z',\n",
       "  'abstract': 'In this paper, we propose a discriminative video representation for eventdetection over a large scale video dataset when only limited hardware resourcesare available. The focus of this paper is to effectively leverage deepConvolutional Neural Networks (CNNs) to advance event detection, where onlyframe level static descriptors can be extracted by the existing CNN toolkit.This paper makes two contributions to the inference of CNN videorepresentation. First, while average pooling and max pooling have long been thestandard approaches to aggregating frame level static features, we show thatperformance can be significantly improved by taking advantage of an appropriateencoding method. Second, we propose using a set of latent concept descriptorsas the frame descriptor, which enriches visual information while keeping itcomputationally affordable. The integration of the two contributions results ina new state-of-the-art performance in event detection over the largest videodatasets. Compared to improved Dense Trajectories, which has been recognized asthe best video representation for event detection, our new representationimproves the Mean Average Precision (mAP) from 27.6% to 36.8% for the TRECVIDMEDTest 14 dataset and from 34.0% to 44.6% for the TRECVID MEDTest 13 dataset.This work is the core part of the winning solution of our CMU-Informedia teamin TRECVID MED 2014 competition.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '406',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1411.4006v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2586135584119783347&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 368: {'ID': 368,\n",
       "  'title': 'A Fast and Accurate Unconstrained Face Detector',\n",
       "  'authors': ['Anil K. Jain', 'Shengcai Liao', 'Stan Z. Li'],\n",
       "  'published': '2014-08-06T15:17:33Z',\n",
       "  'updated': '2015-09-07T08:17:34Z',\n",
       "  'abstract': 'We propose a method to address challenges in unconstrained face detection,such as arbitrary pose variations and occlusions. First, a new image featurecalled Normalized Pixel Difference (NPD) is proposed. NPD feature is computedas the difference to sum ratio between two pixel values, inspired by the WeberFraction in experimental psychology. The new feature is scale invariant,bounded, and is able to reconstruct the original image. Second, we propose adeep quadratic tree to learn the optimal subset of NPD features and theircombinations, so that complex face manifolds can be partitioned by the learnedrules. This way, only a single soft-cascade classifier is needed to handleunconstrained face detection. Furthermore, we show that the NPD features can beefficiently obtained from a look up table, and the detection template can beeasily scaled, making the proposed face detector very fast. Experimentalresults on three public face datasets (FDDB, GENKI, and CMU-MIT) show that theproposed method achieves state-of-the-art performance in detectingunconstrained faces with arbitrary pose variations and occlusions in clutteredscenes.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 38 (2), 211-223',\n",
       "  'citations': '242',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1408.1656v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6235353306704874594&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 369: {'ID': 369,\n",
       "  'title': 'Image-based localization using LSTMs for structured feature correlation',\n",
       "  'authors': ['Sebastian Hilsenbeck',\n",
       "   'Daniel Cremers',\n",
       "   'Caner Hazirbas',\n",
       "   'Laura Leal-Taixé',\n",
       "   'Torsten Sattler',\n",
       "   'Florian Walch'],\n",
       "  'published': '2016-11-23T17:22:27Z',\n",
       "  'updated': '2017-08-20T22:07:43Z',\n",
       "  'abstract': 'In this work we propose a new CNN+LSTM architecture for camera poseregression for indoor and outdoor scenes. CNNs allow us to learn suitablefeature representations for localization that are robust against motion blurand illumination changes. We make use of LSTM units on the CNN output, whichplay the role of a structured dimensionality reduction on the feature vector,leading to drastic improvements in localization performance. We provideextensive quantitative comparison of CNN-based and SIFT-based localizationmethods, showing the weaknesses and strengths of each. Furthermore, we presenta new large-scale indoor dataset with accurate ground truth from a laserscanner. Experimental results on both indoor and outdoor public datasets showour method outperforms existing deep architectures, and can localize images inhard conditions, e.g., in the presence of mostly textureless surfaces, whereclassic SIFT-based methods fail.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 627-637',\n",
       "  'citations': '208',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1611.07890v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=7641720320082495183&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 370: {'ID': 370,\n",
       "  'title': 'Adversarial Examples for Semantic Segmentation and Object Detection',\n",
       "  'authors': ['Zhishuai Zhang',\n",
       "   'Jianyu Wang',\n",
       "   'Lingxi Xie',\n",
       "   'Alan Yuille',\n",
       "   'Yuyin Zhou',\n",
       "   'Cihang Xie'],\n",
       "  'published': '2017-03-24T21:26:16Z',\n",
       "  'updated': '2017-07-21T17:27:17Z',\n",
       "  'abstract': 'It has been well demonstrated that adversarial examples, i.e., natural imageswith visually imperceptible perturbations added, generally exist for deepnetworks to fail on image classification. In this paper, we extend adversarialexamples to semantic segmentation and object detection which are much moredifficult. Our observation is that both segmentation and detection are based onclassifying multiple targets on an image (e.g., the basic target is a pixel ora receptive field in segmentation, and an object proposal in detection), whichinspires us to optimize a loss function over a set of pixels/proposals forgenerating adversarial perturbations. Based on this idea, we propose a novelalgorithm named Dense Adversary Generation (DAG), which generates a largefamily of adversarial examples, and applies to a wide range of state-of-the-artdeep networks for segmentation and detection. We also find that the adversarialperturbations can be transferred across networks with different training data,based on different architectures, and even for different recognition tasks. Inparticular, the transferability across networks with the same architecture ismore significant than in other cases. Besides, summing up heterogeneousperturbations often leads to better transfer performance, which provides aneffective method of black-box adversarial attack.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 1369-1378',\n",
       "  'citations': '281',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1703.08603v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=4402033815466351900&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 371: {'ID': 371,\n",
       "  'title': 'Frustum PointNets for 3D Object Detection from RGB-D Data',\n",
       "  'authors': ['Leonidas J. Guibas',\n",
       "   'Wei Liu',\n",
       "   'Chenxia Wu',\n",
       "   'Charles R. Qi',\n",
       "   'Hao Su'],\n",
       "  'published': '2017-11-22T19:52:18Z',\n",
       "  'updated': '2018-04-13T00:30:24Z',\n",
       "  'abstract': 'In this work, we study 3D object detection from RGB-D data in both indoor andoutdoor scenes. While previous methods focus on images or 3D voxels, oftenobscuring natural 3D patterns and invariances of 3D data, we directly operateon raw point clouds by popping up RGB-D scans. However, a key challenge of thisapproach is how to efficiently localize objects in point clouds of large-scalescenes (region proposal). Instead of solely relying on 3D proposals, our methodleverages both mature 2D object detectors and advanced 3D deep learning forobject localization, achieving efficiency as well as high recall for even smallobjects. Benefited from learning directly in raw point clouds, our method isalso able to precisely estimate 3D bounding boxes even under strong occlusionor with very sparse points. Evaluated on KITTI and SUN RGB-D 3D detectionbenchmarks, our method outperforms the state of the art by remarkable marginswhile having real-time capability.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '534',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1711.08488v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=4885970701564432144&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 372: {'ID': 372,\n",
       "  'title': 'Efficient and robust approximate nearest neighbor search using  Hierarchical Navigable Small World graphs',\n",
       "  'authors': ['Yu. A. Malkov', 'D. A. Yashunin'],\n",
       "  'published': '2016-03-30T19:29:44Z',\n",
       "  'updated': '2018-08-14T19:29:07Z',\n",
       "  'abstract': 'We present a new approach for the approximate K-nearest neighbor search basedon navigable small world graphs with controllable hierarchy (Hierarchical NSW,HNSW). The proposed solution is fully graph-based, without any need foradditional search structures, which are typically used at the coarse searchstage of the most proximity graph techniques. Hierarchical NSW incrementallybuilds a multi-layer structure consisting from hierarchical set of proximitygraphs (layers) for nested subsets of the stored elements. The maximum layer inwhich an element is present is selected randomly with an exponentially decayingprobability distribution. This allows producing graphs similar to thepreviously studied Navigable Small World (NSW) structures while additionallyhaving the links separated by their characteristic distance scales. Startingsearch from the upper layer together with utilizing the scale separation booststhe performance compared to NSW and allows a logarithmic complexity scaling.Additional employment of a heuristic for selecting proximity graph neighborssignificantly increases performance at high recall and in case of highlyclustered data. Performance evaluation has demonstrated that the proposedgeneral metric space search index is able to strongly outperform previousopensource state-of-the-art vector-only approaches. Similarity of the algorithmto the skip list structure allows straightforward balanced distributedimplementation.',\n",
       "  'categories': ['cs.DS', 'cs.CV', 'cs.IR', 'cs.SI'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence',\n",
       "  'citations': '196',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1603.09320v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=16046989618488798653&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 373: {'ID': 373,\n",
       "  'title': 'A Unified Multi-scale Deep Convolutional Neural Network for Fast Object  Detection',\n",
       "  'authors': ['Quanfu Fan',\n",
       "   'Zhaowei Cai',\n",
       "   'Rogerio S. Feris',\n",
       "   'Nuno Vasconcelos'],\n",
       "  'published': '2016-07-25T05:15:31Z',\n",
       "  'updated': '2016-07-25T05:15:31Z',\n",
       "  'abstract': 'A unified deep neural network, denoted the multi-scale CNN (MS-CNN), isproposed for fast multi-scale object detection. The MS-CNN consists of aproposal sub-network and a detection sub-network. In the proposal sub-network,detection is performed at multiple output layers, so that receptive fieldsmatch objects of different scales. These complementary scale-specific detectorsare combined to produce a strong multi-scale object detector. The unifiednetwork is learned end-to-end, by optimizing a multi-task loss. Featureupsampling by deconvolution is also explored, as an alternative to inputupsampling, to reduce the memory and computation costs. State-of-the-art objectdetection performance, at up to 15 fps, is reported on datasets, such as KITTIand Caltech, containing a substantial number of small objects.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (4), 354-370',\n",
       "  'citations': '839',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1607.07155v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=16810968307175371363&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 374: {'ID': 374,\n",
       "  'title': 'Neural Activation Constellations: Unsupervised Part Model Discovery with  Convolutional Networks',\n",
       "  'authors': ['Marcel Simon', 'Erik Rodner'],\n",
       "  'published': '2015-04-30T16:06:50Z',\n",
       "  'updated': '2015-12-05T15:53:09Z',\n",
       "  'abstract': 'Part models of object categories are essential for challenging recognitiontasks, where differences in categories are subtle and only reflected inappearances of small parts of the object. We present an approach that is ableto learn part models in a completely unsupervised manner, without partannotations and even without given bounding boxes during learning. The key ideais to find constellations of neural activation patterns computed usingconvolutional neural networks. In our experiments, we outperform existingapproaches for fine-grained recognition on the CUB200-2011, NA birds, OxfordPETS, and Oxford Flowers dataset in case no part or bounding box annotationsare available and achieve state-of-the-art performance for the Stanford Dogdataset. We also show the benefits of neural constellation models as a dataaugmentation technique for fine-tuning. Furthermore, our paper unites the areasof generic and fine-grained classification, since our approach is suitable forboth scenarios. The source code of our method is available online athttp://www.inf-cv.uni-jena.de/part_discovery',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 1143-1151',\n",
       "  'citations': '297',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1504.08289v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=4285624108192289380&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 375: {'ID': 375,\n",
       "  'title': 'Acquisition of Localization Confidence for Accurate Object Detection',\n",
       "  'authors': ['Tete Xiao',\n",
       "   'Ruixuan Luo',\n",
       "   'Yuning Jiang',\n",
       "   'Jiayuan Mao',\n",
       "   'Borui Jiang'],\n",
       "  'published': '2018-07-30T21:36:20Z',\n",
       "  'updated': '2018-07-30T21:36:20Z',\n",
       "  'abstract': 'Modern CNN-based object detectors rely on bounding box regression andnon-maximum suppression to localize objects. While the probabilities for classlabels naturally reflect classification confidence, localization confidence isabsent. This makes properly localized bounding boxes degenerate duringiterative regression or even suppressed during NMS. In the paper we proposeIoU-Net learning to predict the IoU between each detected bounding box and thematched ground-truth. The network acquires this confidence of localization,which improves the NMS procedure by preserving accurately localized boundingboxes. Furthermore, an optimization-based bounding box refinement method isproposed, where the predicted IoU is formulated as the objective. Extensiveexperiments on the MS-COCO dataset show the effectiveness of IoU-Net, as wellas its compatibility with and adaptivity to several state-of-the-art objectdetectors.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 784-799',\n",
       "  'citations': '165',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1807.11590v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14154791864857863721&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 376: {'ID': 376,\n",
       "  'title': 'Training a Feedback Loop for Hand Pose Estimation',\n",
       "  'authors': ['Paul Wohlhart', 'Vincent Lepetit', 'Markus Oberweger'],\n",
       "  'published': '2016-09-30T12:35:26Z',\n",
       "  'updated': '2016-09-30T12:35:26Z',\n",
       "  'abstract': 'We propose an entirely data-driven approach to estimating the 3D pose of ahand given a depth image. We show that we can correct the mistakes made by aConvolutional Neural Network trained to predict an estimate of the 3D pose byusing a feedback loop. The components of this feedback loop are also DeepNetworks, optimized using training data. They remove the need for fitting a 3Dmodel to the input data, which requires both a carefully designed fittingfunction and algorithm. We show that our approach outperforms state-of-the-artmethods, and is efficient as our implementation runs at over 400 fps on asingle GPU.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 3316-3324',\n",
       "  'citations': '216',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1609.09698v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5562345415917778971&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 377: {'ID': 377,\n",
       "  'title': 'Stacked Hourglass Networks for Human Pose Estimation',\n",
       "  'authors': ['Alejandro Newell', 'Jia Deng', 'Kaiyu Yang'],\n",
       "  'published': '2016-03-22T19:56:42Z',\n",
       "  'updated': '2016-07-26T19:19:37Z',\n",
       "  'abstract': 'This work introduces a novel convolutional network architecture for the taskof human pose estimation. Features are processed across all scales andconsolidated to best capture the various spatial relationships associated withthe body. We show how repeated bottom-up, top-down processing used inconjunction with intermediate supervision is critical to improving theperformance of the network. We refer to the architecture as a \"stackedhourglass\" network based on the successive steps of pooling and upsampling thatare done to produce a final set of predictions. State-of-the-art results areachieved on the FLIC and MPII benchmarks outcompeting all recent methods.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (8), 483-499',\n",
       "  'citations': '1825',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1603.06937v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14584233315522536803&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 378: {'ID': 378,\n",
       "  'title': 'Convolutional Feature Masking for Joint Object and Stuff Segmentation',\n",
       "  'authors': ['Jian Sun', 'Jifeng Dai', 'Kaiming He'],\n",
       "  'published': '2014-12-03T11:45:34Z',\n",
       "  'updated': '2015-04-02T04:12:26Z',\n",
       "  'abstract': 'The topic of semantic segmentation has witnessed considerable progress due tothe powerful features learned by convolutional neural networks (CNNs). Thecurrent leading approaches for semantic segmentation exploit shape informationby extracting CNN features from masked image regions. This strategy introducesartificial boundaries on the images and may impact the quality of the extractedfeatures. Besides, the operations on the raw image domain require to computethousands of networks on a single image, which is time-consuming. In thispaper, we propose to exploit shape information via masking convolutionalfeatures. The proposal segments (e.g., super-pixels) are treated as masks onthe convolutional feature maps. The CNN features of segments are directlymasked out from these maps and used to train classifiers for recognition. Wefurther propose a joint method to handle objects and \"stuff\" (e.g., grass, sky,water) in the same framework. State-of-the-art results are demonstrated onbenchmarks of PASCAL VOC and new PASCAL-CONTEXT, with a compellingcomputational speed.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '346',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1412.1283v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=16351799615487824726&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 379: {'ID': 379,\n",
       "  'title': 'Optical Flow Estimation using a Spatial Pyramid Network',\n",
       "  'authors': ['Anurag Ranjan', 'Michael J. Black'],\n",
       "  'published': '2016-11-03T00:10:42Z',\n",
       "  'updated': '2016-11-21T19:01:19Z',\n",
       "  'abstract': 'We learn to compute optical flow by combining a classical spatial-pyramidformulation with deep learning. This estimates large motions in acoarse-to-fine approach by warping one image of a pair at each pyramid level bythe current flow estimate and computing an update to the flow. Instead of thestandard minimization of an objective function at each pyramid level, we trainone deep network per level to compute the flow update. Unlike the recentFlowNet approach, the networks do not need to deal with large motions; theseare dealt with by the pyramid. This has several advantages. First, our SpatialPyramid Network (SPyNet) is much simpler and 96% smaller than FlowNet in termsof model parameters. This makes it more efficient and appropriate for embeddedapplications. Second, since the flow at each pyramid level is small (&lt; 1pixel), a convolutional approach applied to pairs of warped images isappropriate. Third, unlike FlowNet, the learned convolution filters appearsimilar to classical spatio-temporal filters, giving insight into the methodand how to improve it. Our results are more accurate than FlowNet on moststandard benchmarks, suggesting a new direction of combining classical flowmethods with deep learning.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '332',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1611.00850v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=1776399510420952171&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 380: {'ID': 380,\n",
       "  'title': 'Back to Basics: Unsupervised Learning of Optical Flow via Brightness  Constancy and Motion Smoothness',\n",
       "  'authors': ['Konstantinos G. Derpanis', 'Jason J. Yu', 'Adam W. Harley'],\n",
       "  'published': '2016-08-20T15:25:31Z',\n",
       "  'updated': '2016-08-20T15:25:31Z',\n",
       "  'abstract': 'Recently, convolutional networks (convnets) have proven useful for predictingoptical flow. Much of this success is predicated on the availability of largedatasets that require expensive and involved data acquisition and laborious la-beling. To bypass these challenges, we propose an unsuper- vised approach(i.e., without leveraging groundtruth flow) to train a convnet end-to-end forpredicting optical flow be- tween two images. We use a loss function thatcombines a data term that measures photometric constancy over time with aspatial term that models the expected variation of flow across the image.Together these losses form a proxy measure for losses based on the groundtruthflow. Empiri- cally, we show that a strong convnet baseline trained with theproposed unsupervised approach outperforms the same network trained withsupervision on the KITTI dataset.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV Workshops (3), 3-10',\n",
       "  'citations': '196',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1608.05842v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14093304429035111418&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 381: {'ID': 381,\n",
       "  'title': 'End-to-end Recovery of Human Shape and Pose',\n",
       "  'authors': ['Angjoo Kanazawa',\n",
       "   'David W. Jacobs',\n",
       "   'Jitendra Malik',\n",
       "   'Michael J. Black'],\n",
       "  'published': '2017-12-18T18:57:40Z',\n",
       "  'updated': '2018-06-23T23:12:25Z',\n",
       "  'abstract': 'We describe Human Mesh Recovery (HMR), an end-to-end framework forreconstructing a full 3D mesh of a human body from a single RGB image. Incontrast to most current methods that compute 2D or 3D joint locations, weproduce a richer and more useful mesh representation that is parameterized byshape and 3D joint angles. The main objective is to minimize the reprojectionloss of keypoints, which allow our model to be trained using images in-the-wildthat only have ground truth 2D annotations. However, the reprojection lossalone leaves the model highly under constrained. In this work we address thisproblem by introducing an adversary trained to tell whether a human bodyparameter is real or not using a large database of 3D human meshes. We showthat HMR can be trained with and without using any paired 2D-to-3D supervision.We do not rely on intermediate 2D keypoint detections and infer 3D pose andshape parameters directly from image pixels. Our model runs in real-time givena bounding box containing the person. We demonstrate our approach on variousimages in-the-wild and out-perform previous optimization based methods thatoutput 3D meshes and show competitive results on tasks such as 3D jointlocation estimation and part segmentation.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '375',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1712.06584v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=1235713119272923345&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 382: {'ID': 382,\n",
       "  'title': 'Detecting Text in Natural Image with Connectionist Text Proposal Network',\n",
       "  'authors': ['Zhi Tian', 'Yu Qiao', 'Weilin Huang', 'Pan He', 'Tong He'],\n",
       "  'published': '2016-09-12T21:12:46Z',\n",
       "  'updated': '2016-09-12T21:12:46Z',\n",
       "  'abstract': 'We propose a novel Connectionist Text Proposal Network (CTPN) that accuratelylocalizes text lines in natural image. The CTPN detects a text line in asequence of fine-scale text proposals directly in convolutional feature maps.We develop a vertical anchor mechanism that jointly predicts location andtext/non-text score of each fixed-width proposal, considerably improvinglocalization accuracy. The sequential proposals are naturally connected by arecurrent neural network, which is seamlessly incorporated into theconvolutional network, resulting in an end-to-end trainable model. This allowsthe CTPN to explore rich context information of image, making it powerful todetect extremely ambiguous text. The CTPN works reliably on multi-scale andmulti- language text without further post-processing, departing from previousbottom-up methods requiring multi-step post-processing. It achieves 0.88 and0.61 F-measure on the ICDAR 2013 and 2015 benchmarks, surpass- ing recentresults [8, 35] by a large margin. The CTPN is computationally efficient with0:14s/image, by using the very deep VGG16 model [27]. Online demo is availableat: http://textdet.com/.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (8), 56-72',\n",
       "  'citations': '430',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1609.03605v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=1617435073920306658&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 383: {'ID': 383,\n",
       "  'title': 'VQA: Visual Question Answering',\n",
       "  'authors': ['C. Lawrence Zitnick',\n",
       "   'Margaret Mitchell',\n",
       "   'Jiasen Lu',\n",
       "   'Aishwarya Agrawal',\n",
       "   'Dhruv Batra',\n",
       "   'Devi Parikh',\n",
       "   'Stanislaw Antol'],\n",
       "  'published': '2015-05-03T20:07:39Z',\n",
       "  'updated': '2016-10-27T03:50:19Z',\n",
       "  'abstract': 'We propose the task of free-form and open-ended Visual Question Answering(VQA). Given an image and a natural language question about the image, the taskis to provide an accurate natural language answer. Mirroring real-worldscenarios, such as helping the visually impaired, both the questions andanswers are open-ended. Visual questions selectively target different areas ofan image, including background details and underlying context. As a result, asystem that succeeds at VQA typically needs a more detailed understanding ofthe image and complex reasoning than a system producing generic image captions.Moreover, VQA is amenable to automatic evaluation, since many open-endedanswers contain only a few words or a closed set of answers that can beprovided in a multiple-choice format. We provide a dataset containing ~0.25Mimages, ~0.76M questions, and ~10M answers (www.visualqa.org), and discuss theinformation it provides. Numerous baselines and methods for VQA are providedand compared with human performance. Our VQA demo is available on CloudCV(http://cloudcv.org/vqa).',\n",
       "  'categories': ['cs.CL', 'cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 2425-2433',\n",
       "  'citations': '1932',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1505.00468v7',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=11651121946809783243&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 384: {'ID': 384,\n",
       "  'title': 'Fast Edge Detection Using Structured Forests',\n",
       "  'authors': ['Piotr Dollár', 'C. Lawrence Zitnick'],\n",
       "  'published': '2014-06-20T22:28:29Z',\n",
       "  'updated': '2014-11-25T02:49:28Z',\n",
       "  'abstract': 'Edge detection is a critical component of many vision systems, includingobject detectors and image segmentation algorithms. Patches of edges exhibitwell-known forms of local structure, such as straight lines or T-junctions. Inthis paper we take advantage of the structure present in local image patches tolearn both an accurate and computationally efficient edge detector. Weformulate the problem of predicting local edge masks in a structured learningframework applied to random decision forests. Our novel approach to learningdecision trees robustly maps the structured labels to a discrete space on whichstandard information gain measures may be evaluated. The result is an approachthat obtains realtime performance that is orders of magnitude faster than manycompeting state-of-the-art approaches, while also achieving state-of-the-artedge detection results on the BSDS500 Segmentation dataset and NYU Depthdataset. Finally, we show the potential of our approach as a general purposeedge detector by showing our learned edge models generalize well acrossdatasets.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis & Machine Intelligence, 1558-1570',\n",
       "  'citations': '643',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1406.5549v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=17952985400304605291&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 385: {'ID': 385,\n",
       "  'title': 'Geometric deep learning on graphs and manifolds using mixture model CNNs',\n",
       "  'authors': ['Jan Svoboda',\n",
       "   'Michael M. Bronstein',\n",
       "   'Emanuele Rodolà',\n",
       "   'Federico Monti',\n",
       "   'Davide Boscaini',\n",
       "   'Jonathan Masci'],\n",
       "  'published': '2016-11-25T10:05:03Z',\n",
       "  'updated': '2016-12-06T21:38:12Z',\n",
       "  'abstract': 'Deep learning has achieved a remarkable performance breakthrough in severalfields, most notably in speech recognition, natural language processing, andcomputer vision. In particular, convolutional neural network (CNN)architectures currently produce state-of-the-art performance on a variety ofimage analysis tasks such as object detection and recognition. Most of deeplearning research has so far focused on dealing with 1D, 2D, or 3DEuclidean-structured data such as acoustic signals, images, or videos.Recently, there has been an increasing interest in geometric deep learning,attempting to generalize deep learning methods to non-Euclidean structured datasuch as graphs and manifolds, with a variety of applications from the domainsof network analysis, computational social science, or computer graphics. Inthis paper, we propose a unified framework allowing to generalize CNNarchitectures to non-Euclidean domains (graphs and manifolds) and learn local,stationary, and compositional task-specific features. We show that variousnon-Euclidean CNN methods previously proposed in the literature can beconsidered as particular instances of our framework. We test the proposedmethod on standard tasks from the realms of image-, graph- and 3D shapeanalysis and show that it consistently outperforms previous approaches.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '587',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1611.08402v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14362062008359202020&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 386: {'ID': 386,\n",
       "  'title': 'Go-ICP: A Globally Optimal Solution to 3D ICP Point-Set Registration',\n",
       "  'authors': ['Dylan Campbell', 'Hongdong Li', 'Yunde Jia', 'Jiaolong Yang'],\n",
       "  'published': '2016-05-11T09:15:11Z',\n",
       "  'updated': '2016-05-11T09:15:11Z',\n",
       "  'abstract': 'The Iterative Closest Point (ICP) algorithm is one of the most widely usedmethods for point-set registration. However, being based on local iterativeoptimization, ICP is known to be susceptible to local minima. Its performancecritically relies on the quality of the initialization and only localoptimality is guaranteed. This paper presents the first globally optimalalgorithm, named Go-ICP, for Euclidean (rigid) registration of two 3Dpoint-sets under the L2 error metric defined in ICP. The Go-ICP method is basedon a branch-and-bound (BnB) scheme that searches the entire 3D motion spaceSE(3). By exploiting the special structure of SE(3) geometry, we derive novelupper and lower bounds for the registration error function. Local ICP isintegrated into the BnB scheme, which speeds up the new method whileguaranteeing global optimality. We also discuss extensions, addressing theissue of outlier robustness. The evaluation demonstrates that the proposedmethod is able to produce reliable registration results regardless of theinitialization. Go-ICP can be applied in scenarios where an optimal solution isdesirable or where a good initialization is not always available.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 38 (11), 2241\\xa0…',\n",
       "  'citations': '269',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1605.03344v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6139288535705251889&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 387: {'ID': 387,\n",
       "  'title': 'Towards Diverse and Natural Image Descriptions via a Conditional GAN',\n",
       "  'authors': ['Raquel Urtasun', 'Sanja Fidler', 'Bo Dai', 'Dahua Lin'],\n",
       "  'published': '2017-03-17T14:33:41Z',\n",
       "  'updated': '2017-08-11T05:02:36Z',\n",
       "  'abstract': 'Despite the substantial progress in recent years, the image captioningtechniques are still far from being perfect.Sentences produced by existingmethods, e.g. those based on RNNs, are often overly rigid and lacking invariability. This issue is related to a learning principle widely used inpractice, that is, to maximize the likelihood of training samples. Thisprinciple encourages high resemblance to the \"ground-truth\" captions whilesuppressing other reasonable descriptions. Conventional evaluation metrics,e.g. BLEU and METEOR, also favor such restrictive methods. In this paper, weexplore an alternative approach, with the aim to improve the naturalness anddiversity -- two essential properties of human expression. Specifically, wepropose a new framework based on Conditional Generative Adversarial Networks(CGAN), which jointly learns a generator to produce descriptions conditioned onimages and an evaluator to assess how well a description fits the visualcontent. It is noteworthy that training a sequence generator is nontrivial. Weovercome the difficulty by Policy Gradient, a strategy stemming fromReinforcement Learning, which allows the generator to receive early feedbackalong the way. We tested our method on two large datasets, where it performedcompetitively against real people in our user study and outperformed othermethods on various tasks.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 2970-2979',\n",
       "  'citations': '229',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1703.06029v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=10176759937431265501&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 388: {'ID': 388,\n",
       "  'title': 'Filtered Channel Features for Pedestrian Detection',\n",
       "  'authors': ['Rodrigo Benenson', 'Shanshan Zhang', 'Bernt Schiele'],\n",
       "  'published': '2015-01-23T10:19:33Z',\n",
       "  'updated': '2015-01-23T10:19:33Z',\n",
       "  'abstract': 'This paper starts from the observation that multiple top performingpedestrian detectors can be modelled by using an intermediate layer filteringlow-level features in combination with a boosted decision forest. Based on thisobservation we propose a unifying framework and experimentally exploredifferent filter families. We report extensive results enabling a systematicanalysis.  Using filtered channel features we obtain top performance on the challengingCaltech and KITTI datasets, while using only HOG+LUV as low-level features.When adding optical flow features we further improve detection quality andreport the best known results on the Caltech dataset, reaching 93% recall at 1FPPI.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'CVPR, 1751-1760',\n",
       "  'citations': '327',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1501.05759v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2043372113368539904&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 389: {'ID': 389,\n",
       "  'title': 'Harmonious Attention Network for Person Re-Identification',\n",
       "  'authors': ['Shaogang Gong', 'Xiatian Zhu', 'Wei Li'],\n",
       "  'published': '2018-02-22T16:04:55Z',\n",
       "  'updated': '2018-02-22T16:04:55Z',\n",
       "  'abstract': 'Existing person re-identification (re-id) methods either assume theavailability of well-aligned person bounding box images as model input or relyon constrained attention selection mechanisms to calibrate misaligned images.They are therefore sub-optimal for re-id matching in arbitrarily aligned personimages potentially with large human pose variations and unconstrainedauto-detection errors. In this work, we show the advantages of jointly learningattention selection and feature representation in a Convolutional NeuralNetwork (CNN) by maximising the complementary information of different levelsof visual attention subject to re-id discriminative learning constraints.Specifically, we formulate a novel Harmonious Attention CNN (HA-CNN) model forjoint learning of soft pixel attention and hard regional attention along withsimultaneous optimisation of feature representations, dedicated to optimiseperson re-id in uncontrolled (misaligned) images. Extensive comparativeevaluations validate the superiority of this new HA-CNN model for person re-idover a wide variety of state-of-the-art methods on three large-scale benchmarksincluding CUHK03, Market-1501, and DukeMTMC-ReID.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '457',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1802.08122v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=7374410551099388706&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 390: {'ID': 390,\n",
       "  'title': 'Learning to rank in person re-identification with metric ensembles',\n",
       "  'authors': ['Anton van den Hengel',\n",
       "   'Chunhua Shen',\n",
       "   'Sakrapee Paisitkriangkrai'],\n",
       "  'published': '2015-03-05T05:25:57Z',\n",
       "  'updated': '2015-03-05T05:25:57Z',\n",
       "  'abstract': 'We propose an effective structured learning based approach to the problem ofperson re-identification which outperforms the current state-of-the-art on mostbenchmark data sets evaluated. Our framework is built on the basis of multiplelow-level hand-crafted and high-level visual features. We then formulate twooptimization algorithms, which directly optimize evaluation measures commonlyused in person re-identification, also known as the Cumulative MatchingCharacteristic (CMC) curve. Our new approach is practical to many real-worldsurveillance applications as the re-identification performance can beconcentrated in the range of most practical importance. The combination ofthese factors leads to a person re-identification system which outperforms mostexisting algorithms. More importantly, we advance state-of-the-art results onperson re-identification by improving the rank-$1$ recognition rates from$40\\\\%$ to $50\\\\%$ on the iLIDS benchmark, $16\\\\%$ to $18\\\\%$ on the PRID2011benchmark, $43\\\\%$ to $46\\\\%$ on the VIPeR benchmark, $34\\\\%$ to $53\\\\%$ on theCUHK01 benchmark and $21\\\\%$ to $62\\\\%$ on the CUHK03 benchmark.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '400',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1503.01543v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5953615090367886073&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 391: {'ID': 391,\n",
       "  'title': 'Appearance-Based Gaze Estimation in the Wild',\n",
       "  'authors': ['Mario Fritz',\n",
       "   'Yusuke Sugano',\n",
       "   'Xucong Zhang',\n",
       "   'Andreas Bulling'],\n",
       "  'published': '2015-04-11T11:52:33Z',\n",
       "  'updated': '2015-04-11T11:52:33Z',\n",
       "  'abstract': 'Appearance-based gaze estimation is believed to work well in real-worldsettings, but existing datasets have been collected under controlled laboratoryconditions and methods have been not evaluated across multiple datasets. Inthis work we study appearance-based gaze estimation in the wild. We present theMPIIGaze dataset that contains 213,659 images we collected from 15 participantsduring natural everyday laptop use over more than three months. Our dataset issignificantly more variable than existing ones with respect to appearance andillumination. We also present a method for in-the-wild appearance-based gazeestimation using multimodal convolutional neural networks that significantlyoutperforms state-of-the art methods in the most challenging cross-datasetevaluation. We present an extensive evaluation of several state-of-the-artimage-based gaze estimation algorithms on three current datasets, including ourown. This evaluation provides clear insights and allows us to identify keyresearch challenges of gaze estimation in the wild.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '355',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1504.02863v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=8284646831790188439&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 392: {'ID': 392,\n",
       "  'title': 'Richer Convolutional Features for Edge Detection',\n",
       "  'authors': ['Xiaowei Hu',\n",
       "   'Yun Liu',\n",
       "   'Ming-Ming Cheng',\n",
       "   'Xiang Bai',\n",
       "   'Kai Wang'],\n",
       "  'published': '2016-12-07T02:57:03Z',\n",
       "  'updated': '2019-07-03T04:44:45Z',\n",
       "  'abstract': 'In this paper, we propose an accurate edge detector using richerconvolutional features (RCF). Since objects in nature images have variousscales and aspect ratios, the automatically learned rich hierarchicalrepresentations by CNNs are very critical and effective to detect edges andobject boundaries. And the convolutional features gradually become coarser withreceptive fields increasing. Based on these observations, our proposed networkarchitecture makes full use of multiscale and multi-level information toperform the image-to-image edge prediction by combining all of the usefulconvolutional features into a holistic framework. It is the first attempt toadopt such rich convolutional features in computer vision tasks. Using VGG16network, we achieve \\\\sArt results on several available datasets. Whenevaluating on the well-known BSDS500 benchmark, we achieve ODS F-measure of\\\\textbf{.811} while retaining a fast speed (\\\\textbf{8} FPS). Besides, our fastversion of RCF achieves ODS F-measure of \\\\textbf{.806} with \\\\textbf{30} FPS.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 41 (8), 1939-1946',\n",
       "  'citations': '297',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1612.02103v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=16811934375694575588&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 393: {'ID': 393,\n",
       "  'title': 'Temporal Action Detection with Structured Segment Networks',\n",
       "  'authors': ['Yue Zhao',\n",
       "   'Yuanjun Xiong',\n",
       "   'Xiaoou Tang',\n",
       "   'Zhirong Wu',\n",
       "   'Dahua Lin',\n",
       "   'Limin Wang'],\n",
       "  'published': '2017-04-20T16:51:45Z',\n",
       "  'updated': '2017-09-18T08:43:11Z',\n",
       "  'abstract': 'Detecting actions in untrimmed videos is an important yet challenging task.In this paper, we present the structured segment network (SSN), a novelframework which models the temporal structure of each action instance via astructured temporal pyramid. On top of the pyramid, we further introduce adecomposed discriminative model comprising two classifiers, respectively forclassifying actions and determining completeness. This allows the framework toeffectively distinguish positive proposals from background or incomplete ones,thus leading to both accurate recognition and localization. These componentsare integrated into a unified network that can be efficiently trained in anend-to-end fashion. Additionally, a simple yet effective temporal actionproposal scheme, dubbed temporal actionness grouping (TAG) is devised togenerate high quality action proposals. On two challenging benchmarks, THUMOS14and ActivityNet, our method remarkably outperforms previous state-of-the-artmethods, demonstrating superior accuracy and strong adaptivity in handlingactions with various temporal structures.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 2914-2923',\n",
       "  'citations': '314',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1704.06228v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=16265582985843476869&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 394: {'ID': 394,\n",
       "  'title': 'Constrained Convolutional Neural Networks for Weakly Supervised  Segmentation',\n",
       "  'authors': ['Deepak Pathak', 'Philipp Krähenbühl', 'Trevor Darrell'],\n",
       "  'published': '2015-06-11T12:30:17Z',\n",
       "  'updated': '2015-10-18T23:51:40Z',\n",
       "  'abstract': 'We present an approach to learn a dense pixel-wise labeling from image-leveltags. Each image-level tag imposes constraints on the output labeling of aConvolutional Neural Network (CNN) classifier. We propose Constrained CNN(CCNN), a method which uses a novel loss function to optimize for any set oflinear constraints on the output space (i.e. predicted label distribution) of aCNN. Our loss formulation is easy to optimize and can be incorporated directlyinto standard stochastic gradient descent optimization. The key idea is tophrase the training objective as a biconvex optimization for linear models,which we then relax to nonlinear deep networks. Extensive experimentsdemonstrate the generality of our new learning framework. The constrained lossyields state-of-the-art results on weakly supervised semantic imagesegmentation. We further demonstrate that adding slightly more supervision cangreatly improve the performance of the learning algorithm.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 1796-1804',\n",
       "  'citations': '371',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1506.03648v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=18113115400192563138&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 395: {'ID': 395,\n",
       "  'title': 'GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera  Pose',\n",
       "  'authors': ['Zhichao Yin', 'Jianping Shi'],\n",
       "  'published': '2018-03-06T16:09:21Z',\n",
       "  'updated': '2018-03-12T03:02:07Z',\n",
       "  'abstract': 'We propose GeoNet, a jointly unsupervised learning framework for monoculardepth, optical flow and ego-motion estimation from videos. The three componentsare coupled by the nature of 3D scene geometry, jointly learned by ourframework in an end-to-end manner. Specifically, geometric relationships areextracted over the predictions of individual modules and then combined as animage reconstruction loss, reasoning about static and dynamic scene partsseparately. Furthermore, we propose an adaptive geometric consistency loss toincrease robustness towards outliers and non-Lambertian regions, which resolvesocclusions and texture ambiguities effectively. Experimentation on the KITTIdriving dataset reveals that our scheme achieves state-of-the-art results inall of the three tasks, performing better than previously unsupervised methodsand comparably with supervised ones.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '304',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1803.02276v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=1957429302279516892&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 396: {'ID': 396,\n",
       "  'title': 'NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis',\n",
       "  'authors': ['Gang Wang', 'Amir Shahroudy', 'Tian-Tsong Ng', 'Jun Liu'],\n",
       "  'published': '2016-04-11T06:44:53Z',\n",
       "  'updated': '2016-04-11T06:44:53Z',\n",
       "  'abstract': 'Recent approaches in depth-based human activity analysis achieved outstandingperformance and proved the effectiveness of 3D representation forclassification of action classes. Currently available depth-based andRGB+D-based action recognition benchmarks have a number of limitations,including the lack of training samples, distinct class labels, camera views andvariety of subjects. In this paper we introduce a large-scale dataset for RGB+Dhuman action recognition with more than 56 thousand video samples and 4 millionframes, collected from 40 distinct subjects. Our dataset contains 60 differentaction classes including daily, mutual, and health-related actions. Inaddition, we propose a new recurrent neural network structure to model thelong-term temporal correlation of the features for each body part, and utilizethem for better action classification. Experimental results show the advantagesof applying deep learning methods over state-of-the-art hand-crafted featureson the suggested cross-subject and cross-view evaluation criteria for ourdataset. The introduction of this large scale dataset will enable the communityto apply, develop and adapt various data-hungry learning techniques for thetask of depth-based and RGB+D-based human activity analysis.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '755',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1604.02808v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=10407150499851480949&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 397: {'ID': 397,\n",
       "  'title': 'Hypercolumns for Object Segmentation and Fine-grained Localization',\n",
       "  'authors': ['Pablo Arbeláez',\n",
       "   'Bharath Hariharan',\n",
       "   'Ross Girshick',\n",
       "   'Jitendra Malik'],\n",
       "  'published': '2014-11-21T03:12:33Z',\n",
       "  'updated': '2015-04-25T23:08:59Z',\n",
       "  'abstract': 'Recognition algorithms based on convolutional networks (CNNs) typically usethe output of the last layer as feature representation. However, theinformation in this layer may be too coarse to allow precise localization. Onthe contrary, earlier layers may be precise in localization but will notcapture semantics. To get the best of both worlds, we define the hypercolumn ata pixel as the vector of activations of all CNN units above that pixel. Usinghypercolumns as pixel descriptors, we show results on three fine-grainedlocalization tasks: simultaneous detection and segmentation[22], where weimprove state-of-the-art from 49.7[22] mean AP^r to 60.0, keypointlocalization, where we get a 3.3 point boost over[20] and part labeling, wherewe show a 6.6 point gain over a strong baseline.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '1178',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1411.5752v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=17698122047951614619&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 398: {'ID': 398,\n",
       "  'title': 'Attribute2Image: Conditional Image Generation from Visual Attributes',\n",
       "  'authors': ['Kihyuk Sohn', 'Xinchen Yan', 'Jimei Yang', 'Honglak Lee'],\n",
       "  'published': '2015-12-02T04:07:28Z',\n",
       "  'updated': '2016-10-08T08:55:32Z',\n",
       "  'abstract': 'This paper investigates a novel problem of generating images from visualattributes. We model the image as a composite of foreground and background anddevelop a layered generative model with disentangled latent variables that canbe learned end-to-end using a variational auto-encoder. We experiment withnatural images of faces and birds and demonstrate that the proposed models arecapable of generating realistic and diverse samples with disentangled latentrepresentations. We use a general energy minimization algorithm for posteriorinference of latent variables given novel images. Therefore, the learnedgenerative models show excellent quantitative and visual results in the tasksof attribute-conditioned image reconstruction and completion.',\n",
       "  'categories': ['cs.LG', 'cs.AI', 'cs.CV'],\n",
       "  'journal': 'ECCV (4), 776-791',\n",
       "  'citations': '508',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1512.00570v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5700939689460967442&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 399: {'ID': 399,\n",
       "  'title': 'End-to-end Learning of Driving Models from Large-scale Video Datasets',\n",
       "  'authors': ['Huazhe Xu', 'Yang Gao', 'Trevor Darrell', 'Fisher Yu'],\n",
       "  'published': '2016-12-04T07:02:52Z',\n",
       "  'updated': '2017-07-23T10:10:56Z',\n",
       "  'abstract': 'Robust perception-action models should be learned from training data withdiverse visual appearances and realistic behaviors, yet current approaches todeep visuomotor policy learning have been generally limited to in-situ modelslearned from a single vehicle or a simulation environment. We advocate learninga generic vehicle motion model from large scale crowd-sourced video data, anddevelop an end-to-end trainable architecture for learning to predict adistribution over future vehicle egomotion from instantaneous monocular cameraobservations and previous vehicle state. Our model incorporates a novelFCN-LSTM architecture, which can be learned from large-scale crowd-sourcedvehicle action data, and leverages available scene segmentation side tasks toimprove performance under a privileged learning paradigm.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '394',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1612.01079v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=7247109571287718894&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 400: {'ID': 400,\n",
       "  'title': 'Learning Deep CNN Denoiser Prior for Image Restoration',\n",
       "  'authors': ['Kai Zhang', 'Shuhang Gu', 'Wangmeng Zuo', 'Lei Zhang'],\n",
       "  'published': '2017-04-11T12:30:46Z',\n",
       "  'updated': '2017-04-11T12:30:46Z',\n",
       "  'abstract': 'Model-based optimization methods and discriminative learning methods havebeen the two dominant strategies for solving various inverse problems inlow-level vision. Typically, those two kinds of methods have their respectivemerits and drawbacks, e.g., model-based optimization methods are flexible forhandling different inverse problems but are usually time-consuming withsophisticated priors for the purpose of good performance; in the meanwhile,discriminative learning methods have fast testing speed but their applicationrange is greatly restricted by the specialized task. Recent works have revealedthat, with the aid of variable splitting techniques, denoiser prior can beplugged in as a modular part of model-based optimization methods to solve otherinverse problems (e.g., deblurring). Such an integration induces considerableadvantage when the denoiser is obtained via discriminative learning. However,the study of integration with fast discriminative denoiser prior is stilllacking. To this end, this paper aims to train a set of fast and effective CNN(convolutional neural network) denoisers and integrate them into model-basedoptimization method to solve other inverse problems. Experimental resultsdemonstrate that the learned set of denoisers not only achieve promisingGaussian denoising results but also can be used as prior to deliver goodperformance for various low-level vision applications.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '591',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1704.03264v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=11733015574117803846&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 401: {'ID': 401,\n",
       "  'title': 'Unsupervised Monocular Depth Estimation with Left-Right Consistency',\n",
       "  'authors': ['Clément Godard', 'Gabriel J. Brostow', 'Oisin Mac Aodha'],\n",
       "  'published': '2016-09-13T04:48:31Z',\n",
       "  'updated': '2017-04-12T14:40:50Z',\n",
       "  'abstract': 'Learning based methods have shown very promising results for the task ofdepth estimation in single images. However, most existing approaches treatdepth prediction as a supervised regression problem and as a result, requirevast quantities of corresponding ground truth depth data for training. Justrecording quality depth data in a range of environments is a challengingproblem. In this paper, we innovate beyond existing approaches, replacing theuse of explicit depth data during training with easier-to-obtain binocularstereo footage.  We propose a novel training objective that enables our convolutional neuralnetwork to learn to perform single image depth estimation, despite the absenceof ground truth depth data. Exploiting epipolar geometry constraints, wegenerate disparity images by training our network with an image reconstructionloss. We show that solving for image reconstruction alone results in poorquality depth images. To overcome this problem, we propose a novel trainingloss that enforces consistency between the disparities produced relative toboth the left and right images, leading to improved performance and robustnesscompared to existing approaches. Our method produces state of the art resultsfor monocular depth estimation on the KITTI driving dataset, even outperformingsupervised methods that have been trained with ground truth depth.',\n",
       "  'categories': ['cs.CV', 'cs.LG', 'stat.ML'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '1001',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1609.03677v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=17687680169908617872&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 402: {'ID': 402,\n",
       "  'title': 'Gaussian Processes for Data-Efficient Learning in Robotics and Control',\n",
       "  'authors': ['Carl Edward Rasmussen', 'Marc Peter Deisenroth', 'Dieter Fox'],\n",
       "  'published': '2015-02-10T11:09:38Z',\n",
       "  'updated': '2017-10-10T18:25:45Z',\n",
       "  'abstract': 'Autonomous learning has been a promising direction in control and roboticsfor more than a decade since data-driven learning allows to reduce the amountof engineering knowledge, which is otherwise required. However, autonomousreinforcement learning (RL) approaches typically require many interactions withthe system to learn controllers, which is a practical limitation in realsystems, such as robots, where many interactions can be impractical and timeconsuming. To address this problem, current learning approaches typicallyrequire task-specific knowledge in form of expert demonstrations, realisticsimulators, pre-shaped policies, or specific knowledge about the underlyingdynamics. In this article, we follow a different approach and speed up learningby extracting more information from data. In particular, we learn aprobabilistic, non-parametric Gaussian process transition model of the system.By explicitly incorporating model uncertainty into long-term planning andcontroller learning our approach reduces the effects of model errors, a keyproblem in model-based learning. Compared to state-of-the art RL ourmodel-based policy search method achieves an unprecedented speed of learning.We demonstrate its applicability to autonomous learning in real robot andcontrol tasks.',\n",
       "  'categories': ['stat.ML', 'cs.LG', 'cs.RO', 'cs.SY'],\n",
       "  'journal': 'IEEE Annals of the History of Computing, 408-423',\n",
       "  'citations': '391',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1502.02860v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=16265150371053637003&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 403: {'ID': 403,\n",
       "  'title': 'Active Object Localization with Deep Reinforcement Learning',\n",
       "  'authors': ['Juan C. Caicedo', 'Svetlana Lazebnik'],\n",
       "  'published': '2015-11-18T22:55:46Z',\n",
       "  'updated': '2015-11-18T22:55:46Z',\n",
       "  'abstract': 'We present an active detection model for localizing objects in scenes. Themodel is class-specific and allows an agent to focus attention on candidateregions for identifying the correct location of a target object. This agentlearns to deform a bounding box using simple transformation actions, with thegoal of determining the most specific location of target objects followingtop-down reasoning. The proposed localization agent is trained using deepreinforcement learning, and evaluated on the Pascal VOC 2007 dataset. We showthat agents guided by the proposed model are able to localize a single instanceof an object after analyzing only between 11 and 25 regions in an image, andobtain the best detection results among systems that do not use objectproposals for object localization.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 2488-2496',\n",
       "  'citations': '270',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1511.06015v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=16986174613012654953&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 404: {'ID': 404,\n",
       "  'title': 'Deep Image Retrieval: Learning global representations for image search',\n",
       "  'authors': ['Albert Gordo', 'Diane Larlus', 'Jerome Revaud', 'Jon Almazan'],\n",
       "  'published': '2016-04-05T16:48:17Z',\n",
       "  'updated': '2016-07-28T10:44:17Z',\n",
       "  'abstract': 'We propose a novel approach for instance-level image retrieval. It produces aglobal and compact fixed-length representation for each image by aggregatingmany region-wise descriptors. In contrast to previous works employingpre-trained deep networks as a black box to produce features, our methodleverages a deep architecture trained for the specific task of image retrieval.Our contribution is twofold: (i) we leverage a ranking framework to learnconvolution and projection weights that are used to build the region features;and (ii) we employ a region proposal network to learn which regions should bepooled to form the final global descriptor. We show that using clean trainingdata is key to the success of our approach. To that aim, we use a large scalebut noisy landmark dataset and develop an automatic cleaning approach. Theproposed architecture produces a global image representation in a singleforward pass. Our approach significantly outperforms previous approaches basedon global descriptors on standard datasets. It even surpasses most prior worksbased on costly local descriptor indexing and spatial verification. Additionalmaterial is available at www.xrce.xerox.com/Deep-Image-Retrieval.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (6), 241-257',\n",
       "  'citations': '461',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1604.01325v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=7563585562026238541&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 405: {'ID': 405,\n",
       "  'title': 'Visual Saliency Based on Multiscale Deep Features',\n",
       "  'authors': ['Yizhou Yu', 'Guanbin Li'],\n",
       "  'published': '2015-03-30T13:21:09Z',\n",
       "  'updated': '2015-04-10T06:40:46Z',\n",
       "  'abstract': 'Visual saliency is a fundamental problem in both cognitive and computationalsciences, including computer vision. In this CVPR 2015 paper, we discover thata high-quality visual saliency model can be trained with multiscale featuresextracted using a popular deep learning architecture, convolutional neuralnetworks (CNNs), which have had many successes in visual recognition tasks. Forlearning such saliency models, we introduce a neural network architecture,which has fully connected layers on top of CNNs responsible for extractingfeatures at three different scales. We then propose a refinement method toenhance the spatial coherence of our saliency results. Finally, aggregatingmultiple saliency maps computed for different levels of image segmentation canfurther boost the performance, yielding saliency maps better than thosegenerated from a single segmentation. To promote further research andevaluation of visual saliency models, we also construct a new large database of4447 challenging images and their pixelwise saliency annotation. Experimentalresults demonstrate that our proposed method is capable of achievingstate-of-the-art performance on all public benchmarks, improving the F-Measureby 5.0% and 13.2% respectively on the MSRA-B dataset and our new dataset(HKU-IS), and lowering the mean absolute error by 5.7% and 35.1% respectivelyon these two datasets.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '622',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1503.08663v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=17997083431600641924&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 406: {'ID': 406,\n",
       "  'title': 'Where To Look: Focus Regions for Visual Question Answering',\n",
       "  'authors': ['Saurabh Singh', 'Kevin J. Shih', 'Derek Hoiem'],\n",
       "  'published': '2015-11-23T20:17:18Z',\n",
       "  'updated': '2016-01-10T13:26:23Z',\n",
       "  'abstract': 'We present a method that learns to answer visual questions by selecting imageregions relevant to the text-based query. Our method exhibits significantimprovements in answering questions such as \"what color,\" where it is necessaryto evaluate a specific location, and \"what room,\" where it selectivelyidentifies informative image regions. Our model is tested on the VQA datasetwhich is the largest human-annotated visual question answering dataset to ourknowledge.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '301',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1511.07394v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6170652068032197021&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 407: {'ID': 407,\n",
       "  'title': 'Revisiting Unreasonable Effectiveness of Data in Deep Learning Era',\n",
       "  'authors': ['Chen Sun',\n",
       "   'Saurabh Singh',\n",
       "   'Abhinav Shrivastava',\n",
       "   'Abhinav Gupta'],\n",
       "  'published': '2017-07-10T17:54:31Z',\n",
       "  'updated': '2017-08-04T01:33:22Z',\n",
       "  'abstract': \"The success of deep learning in vision can be attributed to: (a) models withhigh capacity; (b) increased computational power; and (c) availability oflarge-scale labeled data. Since 2012, there have been significant advances inrepresentation capabilities of the models and computational capabilities ofGPUs. But the size of the biggest dataset has surprisingly remained constant.What will happen if we increase the dataset size by 10x or 100x? This papertakes a step towards clearing the clouds of mystery surrounding therelationship between `enormous data' and visual deep learning. By exploitingthe JFT-300M dataset which has more than 375M noisy labels for 300M images, weinvestigate how the performance of current vision tasks would change if thisdata was used for representation learning. Our paper delivers some surprising(and some expected) findings. First, we find that the performance on visiontasks increases logarithmically based on volume of training data size. Second,we show that representation learning (or pre-training) still holds a lot ofpromise. One can improve performance on many vision tasks by just training abetter base model. Finally, as expected, we present new state-of-the-artresults for different vision tasks including image classification, objectdetection, semantic segmentation and human pose estimation. Our sincere hope isthat this inspires vision community to not undervalue the data and developcollective efforts in building larger datasets.\",\n",
       "  'categories': ['cs.CV', 'cs.AI'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 843-852',\n",
       "  'citations': '519',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1707.02968v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=4481926339453519807&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 408: {'ID': 408,\n",
       "  'title': 'Temporal Action Localization in Untrimmed Videos via Multi-stage CNNs',\n",
       "  'authors': ['Shih-Fu Chang', 'Zheng Shou', 'Dongang Wang'],\n",
       "  'published': '2016-01-09T16:41:21Z',\n",
       "  'updated': '2016-04-21T22:15:22Z',\n",
       "  'abstract': 'We address temporal action localization in untrimmed long videos. This isimportant because videos in real applications are usually unconstrained andcontain multiple action instances plus video content of background scenes orother activities. To address this challenging issue, we exploit theeffectiveness of deep networks in temporal action localization via threesegment-based 3D ConvNets: (1) a proposal network identifies candidate segmentsin a long video that may contain actions; (2) a classification network learnsone-vs-all action classification model to serve as initialization for thelocalization network; and (3) a localization network fine-tunes on the learnedclassification network to localize each action instance. We propose a novelloss function for the localization network to explicitly consider temporaloverlap and therefore achieve high temporal localization accuracy. Only theproposal network and the localization network are used during prediction. Ontwo large-scale benchmarks, our approach achieves significantly superiorperformances compared with other state-of-the-art systems: mAP increases from1.7% to 7.4% on MEXaction2 and increases from 15.0% to 19.0% on THUMOS 2014,when the overlap threshold for evaluation is set to 0.5.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '426',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1601.02129v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=4402437974533685345&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 409: {'ID': 409,\n",
       "  'title': 'CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard  Examples',\n",
       "  'authors': ['Ondřej Chum', 'Filip Radenović', 'Giorgos Tolias'],\n",
       "  'published': '2016-04-08T19:04:35Z',\n",
       "  'updated': '2016-09-07T16:46:58Z',\n",
       "  'abstract': 'Convolutional Neural Networks (CNNs) achieve state-of-the-art performance inmany computer vision tasks. However, this achievement is preceded by extrememanual annotation in order to perform either training from scratch orfine-tuning for the target task. In this work, we propose to fine-tune CNN forimage retrieval from a large collection of unordered images in a fullyautomated manner. We employ state-of-the-art retrieval andStructure-from-Motion (SfM) methods to obtain 3D models, which are used toguide the selection of the training data for CNN fine-tuning. We show that bothhard positive and hard negative examples enhance the final performance inparticular object retrieval with compact codes.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (1), 3-20',\n",
       "  'citations': '393',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1604.02426v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=9984313522583654475&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 410: {'ID': 410,\n",
       "  'title': 'CornerNet: Detecting Objects as Paired Keypoints',\n",
       "  'authors': ['Hei Law', 'Jia Deng'],\n",
       "  'published': '2018-08-03T16:05:55Z',\n",
       "  'updated': '2019-03-18T18:58:40Z',\n",
       "  'abstract': 'We propose CornerNet, a new approach to object detection where we detect anobject bounding box as a pair of keypoints, the top-left corner and thebottom-right corner, using a single convolution neural network. By detectingobjects as paired keypoints, we eliminate the need for designing a set ofanchor boxes commonly used in prior single-stage detectors. In addition to ournovel formulation, we introduce corner pooling, a new type of pooling layerthat helps the network better localize corners. Experiments show that CornerNetachieves a 42.2% AP on MS COCO, outperforming all existing one-stage detectors.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 734-750',\n",
       "  'citations': '409',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1808.01244v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5999650257677576183&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 411: {'ID': 411,\n",
       "  'title': 'Pose-driven Deep Convolutional Model for Person Re-identification',\n",
       "  'authors': ['Jianing Li',\n",
       "   'Junliang Xing',\n",
       "   'Wen Gao',\n",
       "   'Chi Su',\n",
       "   'Qi Tian',\n",
       "   'Shiliang Zhang'],\n",
       "  'published': '2017-09-25T05:36:39Z',\n",
       "  'updated': '2017-09-25T05:36:39Z',\n",
       "  'abstract': 'Feature extraction and matching are two crucial components in personRe-Identification (ReID). The large pose deformations and the complex viewvariations exhibited by the captured person images significantly increase thedifficulty of learning and matching of the features from person images. Toovercome these difficulties, in this work we propose a Pose-driven DeepConvolutional (PDC) model to learn improved feature extraction and matchingmodels from end to end. Our deep architecture explicitly leverages the humanpart cues to alleviate the pose variations and learn robust featurerepresentations from both the global image and different local parts. To matchthe features from global human body and local body parts, a pose driven featureweighting sub-network is further designed to learn adaptive feature fusions.Extensive experimental analyses and results on three popular datasetsdemonstrate significant performance improvements of our model over allpublished state-of-the-art methods.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 3960-3969',\n",
       "  'citations': '379',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1709.08325v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6238497758044032613&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 412: {'ID': 412,\n",
       "  'title': 'Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning  Challenge',\n",
       "  'authors': ['Oriol Vinyals',\n",
       "   'Samy Bengio',\n",
       "   'Dumitru Erhan',\n",
       "   'Alexander Toshev'],\n",
       "  'published': '2016-09-21T17:40:57Z',\n",
       "  'updated': '2016-09-21T17:40:57Z',\n",
       "  'abstract': 'Automatically describing the content of an image is a fundamental problem inartificial intelligence that connects computer vision and natural languageprocessing. In this paper, we present a generative model based on a deeprecurrent architecture that combines recent advances in computer vision andmachine translation and that can be used to generate natural sentencesdescribing an image. The model is trained to maximize the likelihood of thetarget description sentence given the training image. Experiments on severaldatasets show the accuracy of the model and the fluency of the language itlearns solely from image descriptions. Our model is often quite accurate, whichwe verify both qualitatively and quantitatively. Finally, given the recentsurge of interest in this task, a competition was organized in 2015 using thenewly released COCO dataset. We describe and analyze the various improvementswe applied to our own baseline and show the resulting performance in thecompetition, which we won ex-aequo with a team from Microsoft Research, andprovide an open source implementation in TensorFlow.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 39 (4), 652-663',\n",
       "  'citations': '456',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1609.06647v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5524345091130944955&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 413: {'ID': 413,\n",
       "  'title': 'Channel Pruning for Accelerating Very Deep Neural Networks',\n",
       "  'authors': ['Yihui He', 'Xiangyu Zhang', 'Jian Sun'],\n",
       "  'published': '2017-07-19T15:52:14Z',\n",
       "  'updated': '2017-08-21T09:44:26Z',\n",
       "  'abstract': 'In this paper, we introduce a new channel pruning method to accelerate verydeep convolutional neural networks.Given a trained CNN model, we propose aniterative two-step algorithm to effectively prune each layer, by a LASSOregression based channel selection and least square reconstruction. We furthergeneralize this algorithm to multi-layer and multi-branch cases. Our methodreduces the accumulated error and enhance the compatibility with variousarchitectures. Our pruned VGG-16 achieves the state-of-the-art results by 5xspeed-up along with only 0.3% increase of error. More importantly, our methodis able to accelerate modern networks like ResNet, Xception and suffers only1.4%, 1.0% accuracy loss under 2x speed-up respectively, which is significant.Code has been made publicly available.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 1389-1397',\n",
       "  'citations': '694',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1707.06168v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=56253794075327747&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 414: {'ID': 414,\n",
       "  'title': 'ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on  Weakly-Supervised Classification and Localization of Common Thorax Diseases',\n",
       "  'authors': ['Zhiyong Lu',\n",
       "   'Xiaosong Wang',\n",
       "   'Ronald M. Summers',\n",
       "   'Le Lu',\n",
       "   'Mohammadhadi Bagheri',\n",
       "   'Yifan Peng'],\n",
       "  'published': '2017-05-05T17:31:12Z',\n",
       "  'updated': '2017-12-14T19:35:31Z',\n",
       "  'abstract': 'The chest X-ray is one of the most commonly accessible radiologicalexaminations for screening and diagnosis of many lung diseases. A tremendousnumber of X-ray imaging studies accompanied by radiological reports areaccumulated and stored in many modern hospitals\\' Picture Archiving andCommunication Systems (PACS). On the other side, it is still an open questionhow this type of hospital-size knowledge database containing invaluable imaginginformatics (i.e., loosely labeled) can be used to facilitate the data-hungrydeep learning paradigms in building truly large-scale high precisioncomputer-aided diagnosis (CAD) systems.  In this paper, we present a new chest X-ray database, namely \"ChestX-ray8\",which comprises 108,948 frontal-view X-ray images of 32,717 unique patientswith the text-mined eight disease image labels (where each image can havemulti-labels), from the associated radiological reports using natural languageprocessing. Importantly, we demonstrate that these commonly occurring thoracicdiseases can be detected and even spatially-located via a unifiedweakly-supervised multi-label image classification and disease localizationframework, which is validated using our proposed dataset. Although the initialquantitative results are promising as reported, deep convolutional neuralnetwork based \"reading chest X-rays\" (i.e., recognizing and locating the commondisease patterns trained with only image-level labels) remains a strenuous taskfor fully-automated high precision CAD systems. Data download link:https://nihcc.app.box.com/v/ChestXray-NIHCC',\n",
       "  'categories': ['cs.CV', 'cs.CL'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '743',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1705.02315v5',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5116144877511284006&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 415: {'ID': 415,\n",
       "  'title': 'Discriminative Scale Space Tracking',\n",
       "  'authors': ['Gustav Häger',\n",
       "   'Martin Danelljan',\n",
       "   'Fahad Shahbaz Khan',\n",
       "   'Michael Felsberg'],\n",
       "  'published': '2016-09-20T12:57:08Z',\n",
       "  'updated': '2016-09-20T12:57:08Z',\n",
       "  'abstract': 'Accurate scale estimation of a target is a challenging research problem invisual object tracking. Most state-of-the-art methods employ an exhaustivescale search to estimate the target size. The exhaustive search strategy iscomputationally expensive and struggles when encountered with large scalevariations. This paper investigates the problem of accurate and robust scaleestimation in a tracking-by-detection framework. We propose a novel scaleadaptive tracking approach by learning separate discriminative correlationfilters for translation and scale estimation. The explicit scale filter islearned online using the target appearance sampled at a set of differentscales. Contrary to standard approaches, our method directly learns theappearance change induced by variations in the target scale. Additionally, weinvestigate strategies to reduce the computational cost of our approach.  Extensive experiments are performed on the OTB and the VOT2014 datasets.Compared to the standard exhaustive scale search, our approach achieves a gainof 2.5% in average overlap precision on the OTB dataset. Additionally, ourmethod is computationally efficient, operating at a 50% higher frame ratecompared to the exhaustive scale search. Our method obtains the top rank inperformance by outperforming 19 state-of-the-art trackers on OTB and 37state-of-the-art trackers on VOT2014.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 39 (8), 1561-1575',\n",
       "  'citations': '538',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1609.06141v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=17781280863589185981&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 416: {'ID': 416,\n",
       "  'title': 'Integral Human Pose Regression',\n",
       "  'authors': ['Yichen Wei',\n",
       "   'Shuang Liang',\n",
       "   'Fangyin Wei',\n",
       "   'Bin Xiao',\n",
       "   'Xiao Sun'],\n",
       "  'published': '2017-11-22T11:15:06Z',\n",
       "  'updated': '2018-09-18T08:29:46Z',\n",
       "  'abstract': 'State-of-the-art human pose estimation methods are based on heat maprepresentation. In spite of the good performance, the representation has a fewissues in nature, such as not differentiable and quantization error. This workshows that a simple integral operation relates and unifies the heat maprepresentation and joint regression, thus avoiding the above issues. It isdifferentiable, efficient, and compatible with any heat map based methods. Itseffectiveness is convincingly validated via comprehensive ablation experimentsunder various settings, specifically on 3D pose estimation, for the first time.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 529-545',\n",
       "  'citations': '153',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1711.08229v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=13367121804975374310&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 417: {'ID': 417,\n",
       "  'title': 'Instance-sensitive Fully Convolutional Networks',\n",
       "  'authors': ['Yi Li', 'Kaiming He', 'Jifeng Dai', 'Jian Sun', 'Shaoqing Ren'],\n",
       "  'published': '2016-03-29T08:37:26Z',\n",
       "  'updated': '2016-03-29T08:37:26Z',\n",
       "  'abstract': 'Fully convolutional networks (FCNs) have been proven very successful forsemantic segmentation, but the FCN outputs are unaware of object instances. Inthis paper, we develop FCNs that are capable of proposing instance-levelsegment candidates. In contrast to the previous FCN that generates one scoremap, our FCN is designed to compute a small set of instance-sensitive scoremaps, each of which is the outcome of a pixel-wise classifier of a relativeposition to instances. On top of these instance-sensitive score maps, a simpleassembling module is able to output instance candidate at each position. Incontrast to the recent DeepMask method for segmenting instances, our methoddoes not have any high-dimensional layer related to the mask resolution, butinstead exploits image local coherence for estimating instances. We presentcompetitive results of instance segment proposal on both PASCAL VOC and MSCOCO.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (6), 534-549',\n",
       "  'citations': '264',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1603.08678v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=15993495353255568373&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 418: {'ID': 418,\n",
       "  'title': 'Learning from Synthetic Humans',\n",
       "  'authors': ['Michael J. Black',\n",
       "   'Javier Romero',\n",
       "   'Cordelia Schmid',\n",
       "   'Naureen Mahmood',\n",
       "   'Xavier Martin',\n",
       "   'Ivan Laptev',\n",
       "   'Gül Varol'],\n",
       "  'published': '2017-01-05T16:27:46Z',\n",
       "  'updated': '2018-01-19T12:34:53Z',\n",
       "  'abstract': 'Estimating human pose, shape, and motion from images and videos arefundamental challenges with many applications. Recent advances in 2D human poseestimation use large amounts of manually-labeled training data for learningconvolutional neural networks (CNNs). Such data is time consuming to acquireand difficult to extend. Moreover, manual labeling of 3D pose, depth and motionis impractical. In this work we present SURREAL (Synthetic hUmans foR REALtasks): a new large-scale dataset with synthetically-generated but realisticimages of people rendered from 3D sequences of human motion capture data. Wegenerate more than 6 million frames together with ground truth pose, depthmaps, and segmentation masks. We show that CNNs trained on our syntheticdataset allow for accurate human depth estimation and human part segmentationin real RGB images. Our results and the new dataset open up new possibilitiesfor advancing person analysis using cheap and large-scale synthetic data.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '316',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1701.01370v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=3070187880069123504&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 419: {'ID': 419,\n",
       "  'title': 'A Siamese Long Short-Term Memory Architecture for Human  Re-Identification',\n",
       "  'authors': ['Jiwen Lu',\n",
       "   'Gang Wang',\n",
       "   'Dong Xu',\n",
       "   'Rahul Rama Varior',\n",
       "   'Bing Shuai'],\n",
       "  'published': '2016-07-28T09:43:52Z',\n",
       "  'updated': '2016-07-28T09:43:52Z',\n",
       "  'abstract': 'Matching pedestrians across multiple camera views known as humanre-identification (re-identification) is a challenging problem in visualsurveillance. In the existing works concentrating on feature extraction,representations are formed locally and independent of other regions. We presenta novel siamese Long Short-Term Memory (LSTM) architecture that can processimage regions sequentially and enhance the discriminative capability of localfeature representation by leveraging contextual information. The feedbackconnections and internal gating mechanism of the LSTM cells enable our model tomemorize the spatial dependencies and selectively propagate relevant contextualinformation through the network. We demonstrate improved performance comparedto the baseline algorithm with no LSTM units and promising results compared tostate-of-the-art methods on Market-1501, CUHK03 and VIPeR datasets.Visualization of the internal mechanism of LSTM cells shows meaningful patternscan be learned by our method.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (7), 135-153',\n",
       "  'citations': '387',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1607.08381v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=3996088614932093432&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 420: {'ID': 420,\n",
       "  'title': 'Feedforward semantic segmentation with zoom-out features',\n",
       "  'authors': ['Mohammadreza Mostajabi',\n",
       "   'Gregory Shakhnarovich',\n",
       "   'Payman Yadollahpour'],\n",
       "  'published': '2014-12-02T03:31:51Z',\n",
       "  'updated': '2014-12-02T03:31:51Z',\n",
       "  'abstract': 'We introduce a purely feed-forward architecture for semantic segmentation. Wemap small image elements (superpixels) to rich feature representationsextracted from a sequence of nested regions of increasing extent. These regionsare obtained by \"zooming out\" from the superpixel all the way to scene-levelresolution. This approach exploits statistical structure in the image and inthe label space without setting up explicit structured prediction mechanisms,and thus avoids complex and expensive inference. Instead superpixels areclassified by a feedforward multilayer network. Our architecture achieves newstate of the art performance in semantic segmentation, obtaining 64.4% averageaccuracy on the PASCAL VOC 2012 test set.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '387',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1412.0774v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=18258985651051922896&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 421: {'ID': 421,\n",
       "  'title': 'Full Resolution Image Compression with Recurrent Neural Networks',\n",
       "  'authors': ['Joel Shor',\n",
       "   'George Toderici',\n",
       "   'David Minnen',\n",
       "   'Damien Vincent',\n",
       "   'Sung Jin Hwang',\n",
       "   'Michele Covell',\n",
       "   'Nick Johnston'],\n",
       "  'published': '2016-08-18T01:05:09Z',\n",
       "  'updated': '2017-07-07T16:26:16Z',\n",
       "  'abstract': 'This paper presents a set of full-resolution lossy image compression methodsbased on neural networks. Each of the architectures we describe can providevariable compression rates during deployment without requiring retraining ofthe network: each network need only be trained once. All of our architecturesconsist of a recurrent neural network (RNN)-based encoder and decoder, abinarizer, and a neural network for entropy coding. We compare RNN types (LSTM,associative LSTM) and introduce a new hybrid of GRU and ResNet. We also study\"one-shot\" versus additive reconstruction architectures and introduce a newscaled-additive framework. We compare to previous work, showing improvements of4.3%-8.8% AUC (area under the rate-distortion curve), depending on theperceptual metric used. As far as we know, this is the first neural networkarchitecture that is able to outperform JPEG at image compression across mostbitrates on the rate-distortion curve on the Kodak dataset images, with andwithout the aid of entropy coding.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '338',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1608.05148v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=12135576749020741024&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 422: {'ID': 422,\n",
       "  'title': 'Joint 3D Face Reconstruction and Dense Alignment with Position Map  Regression Network',\n",
       "  'authors': ['Yanfeng Wang', 'Yao Feng', 'Xi Zhou', 'Xiaohu Shao', 'Fan Wu'],\n",
       "  'published': '2018-03-21T10:27:04Z',\n",
       "  'updated': '2018-03-21T10:27:04Z',\n",
       "  'abstract': 'We propose a straightforward method that simultaneously reconstructs the 3Dfacial structure and provides dense alignment. To achieve this, we design a 2Drepresentation called UV position map which records the 3D shape of a completeface in UV space, then train a simple Convolutional Neural Network to regressit from a single 2D image. We also integrate a weight mask into the lossfunction during training to improve the performance of the network. Our methoddoes not rely on any prior face model, and can reconstruct full facial geometryalong with semantic meaning. Meanwhile, our network is very light-weighted andspends only 9.8ms to process an image, which is extremely faster than previousworks. Experiments on multiple challenging datasets show that our methodsurpasses other state-of-the-art methods on both reconstruction and alignmenttasks by a large margin.',\n",
       "  'categories': ['cs.CV', 'cs.GR'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 534-551',\n",
       "  'citations': '160',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1803.07835v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2161439188175601394&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 423: {'ID': 423,\n",
       "  'title': 'Factors of Transferability for a Generic ConvNet Representation',\n",
       "  'authors': ['Josephine Sullivan',\n",
       "   'Ali Sharif Razavian',\n",
       "   'Stefan Carlsson',\n",
       "   'Atsuto Maki',\n",
       "   'Hossein Azizpour'],\n",
       "  'published': '2014-06-22T21:57:46Z',\n",
       "  'updated': '2015-07-15T10:02:19Z',\n",
       "  'abstract': 'Evidence is mounting that Convolutional Networks (ConvNets) are the mosteffective representation learning method for visual recognition tasks. In thecommon scenario, a ConvNet is trained on a large labeled dataset (source) andthe feed-forward units activation of the trained network, at a certain layer ofthe network, is used as a generic representation of an input image for a taskwith relatively smaller training set (target). Recent studies have shown thisform of representation transfer to be suitable for a wide range of targetvisual recognition tasks. This paper introduces and investigates severalfactors affecting the transferability of such representations. It includesparameters for training of the source ConvNet such as its architecture,distribution of the training data, etc. and also the parameters of featureextraction such as layer of the trained ConvNet, dimensionality reduction, etc.Then, by optimizing these factors, we show that significant improvements can beachieved on various (17) visual recognition tasks. We further show that thesevisual recognition tasks can be categorically ordered based on their distancefrom the source task such that a correlation between the performance of tasksand their distance from the source task w.r.t. the proposed factors isobserved.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 38 (9), 1790-1802',\n",
       "  'citations': '195',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1406.5774v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5791133606522462238&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 424: {'ID': 424,\n",
       "  'title': 'Photo-Realistic Single Image Super-Resolution Using a Generative  Adversarial Network',\n",
       "  'authors': ['Lucas Theis',\n",
       "   'Alejandro Acosta',\n",
       "   'Zehan Wang',\n",
       "   'Christian Ledig',\n",
       "   'Andrew Aitken',\n",
       "   'Johannes Totz',\n",
       "   'Alykhan Tejani',\n",
       "   'Ferenc Huszar',\n",
       "   'Wenzhe Shi',\n",
       "   'Andrew Cunningham',\n",
       "   'Jose Caballero'],\n",
       "  'published': '2016-09-15T19:53:07Z',\n",
       "  'updated': '2017-05-25T11:25:41Z',\n",
       "  'abstract': 'Despite the breakthroughs in accuracy and speed of single imagesuper-resolution using faster and deeper convolutional neural networks, onecentral problem remains largely unsolved: how do we recover the finer texturedetails when we super-resolve at large upscaling factors? The behavior ofoptimization-based super-resolution methods is principally driven by the choiceof the objective function. Recent work has largely focused on minimizing themean squared reconstruction error. The resulting estimates have high peaksignal-to-noise ratios, but they are often lacking high-frequency details andare perceptually unsatisfying in the sense that they fail to match the fidelityexpected at the higher resolution. In this paper, we present SRGAN, agenerative adversarial network (GAN) for image super-resolution (SR). To ourknowledge, it is the first framework capable of inferring photo-realisticnatural images for 4x upscaling factors. To achieve this, we propose aperceptual loss function which consists of an adversarial loss and a contentloss. The adversarial loss pushes our solution to the natural image manifoldusing a discriminator network that is trained to differentiate between thesuper-resolved images and original photo-realistic images. In addition, we usea content loss motivated by perceptual similarity instead of similarity inpixel space. Our deep residual network is able to recover photo-realistictextures from heavily downsampled images on public benchmarks. An extensivemean-opinion-score (MOS) test shows hugely significant gains in perceptualquality using SRGAN. The MOS scores obtained with SRGAN are closer to those ofthe original high-resolution images than to those obtained with anystate-of-the-art method.',\n",
       "  'categories': ['cs.CV', 'stat.ML'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '3741',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1609.04802v5',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=1219263946448760936&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 425: {'ID': 425,\n",
       "  'title': 'SIFT Meets CNN: A Decade Survey of Instance Retrieval',\n",
       "  'authors': ['Qi Tian', 'Yi Yang', 'Liang Zheng'],\n",
       "  'published': '2016-08-05T08:50:58Z',\n",
       "  'updated': '2017-05-23T08:10:33Z',\n",
       "  'abstract': 'In the early days, content-based image retrieval (CBIR) was studied withglobal features. Since 2003, image retrieval based on local descriptors (defacto SIFT) has been extensively studied for over a decade due to the advantageof SIFT in dealing with image transformations. Recently, image representationsbased on the convolutional neural network (CNN) have attracted increasinginterest in the community and demonstrated impressive performance. Given thistime of rapid evolution, this article provides a comprehensive survey ofinstance retrieval over the last decade. Two broad categories, SIFT-based andCNN-based methods, are presented. For the former, according to the codebooksize, we organize the literature into using large/medium-sized/small codebooks.For the latter, we discuss three lines of methods, i.e., using pre-trained orfine-tuned CNN models, and hybrid methods. The first two perform a single-passof an image to the network, while the last category employs a patch-basedfeature extraction scheme. This survey presents milestones in modern instanceretrieval, reviews a broad selection of previous works in different categories,and provides insights on the connection between SIFT and CNN-based methods.After analyzing and comparing retrieval performance of different categories onseveral datasets, we discuss promising directions towards generic andspecialized instance retrieval.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 40 (5), 1224-1244',\n",
       "  'citations': '348',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1608.01807v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=16393680164066993823&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 426: {'ID': 426,\n",
       "  'title': 'Visual Relationship Detection with Language Priors',\n",
       "  'authors': ['Li Fei-Fei', 'Michael Bernstein', 'Ranjay Krishna', 'Cewu Lu'],\n",
       "  'published': '2016-07-31T05:54:13Z',\n",
       "  'updated': '2016-07-31T05:54:13Z',\n",
       "  'abstract': 'Visual relationships capture a wide variety of interactions between pairs ofobjects in images (e.g. \"man riding bicycle\" and \"man pushing bicycle\").Consequently, the set of possible relationships is extremely large and it isdifficult to obtain sufficient training examples for all possiblerelationships. Because of this limitation, previous work on visual relationshipdetection has concentrated on predicting only a handful of relationships.Though most relationships are infrequent, their objects (e.g. \"man\" and\"bicycle\") and predicates (e.g. \"riding\" and \"pushing\") independently occurmore frequently. We propose a model that uses this insight to train visualmodels for objects and predicates individually and later combines them togetherto predict multiple relationships per image. We improve on prior work byleveraging language priors from semantic word embeddings to finetune thelikelihood of a predicted relationship. Our model can scale to predictthousands of types of relationships from a few examples. Additionally, welocalize the objects in the predicted relationships as bounding boxes in theimage. We further demonstrate that understanding relationships can improvecontent based image retrieval.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (1), 852-869',\n",
       "  'citations': '415',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1608.00187v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=518833812754147671&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 427: {'ID': 427,\n",
       "  'title': 'Rethinking the Inception Architecture for Computer Vision',\n",
       "  'authors': ['Christian Szegedy',\n",
       "   'Sergey Ioffe',\n",
       "   'Vincent Vanhoucke',\n",
       "   'Zbigniew Wojna',\n",
       "   'Jonathon Shlens'],\n",
       "  'published': '2015-12-02T03:44:38Z',\n",
       "  'updated': '2015-12-11T20:27:50Z',\n",
       "  'abstract': 'Convolutional networks are at the core of most state-of-the-art computervision solutions for a wide variety of tasks. Since 2014 very deepconvolutional networks started to become mainstream, yielding substantial gainsin various benchmarks. Although increased model size and computational costtend to translate to immediate quality gains for most tasks (as long as enoughlabeled data is provided for training), computational efficiency and lowparameter count are still enabling factors for various use cases such as mobilevision and big-data scenarios. Here we explore ways to scale up networks inways that aim at utilizing the added computation as efficiently as possible bysuitably factorized convolutions and aggressive regularization. We benchmarkour methods on the ILSVRC 2012 classification challenge validation setdemonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6%top-5 error for single frame evaluation using a network with a computationalcost of 5 billion multiply-adds per inference and with using less than 25million parameters. With an ensemble of 4 models and multi-crop evaluation, wereport 3.5% top-5 error on the validation set (3.6% error on the test set) and17.3% top-1 error on the validation set.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '8499',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1512.00567v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=1692140599533045894&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 428: {'ID': 428,\n",
       "  'title': 'Deformable Convolutional Networks',\n",
       "  'authors': ['Yichen Wei',\n",
       "   'Haozhi Qi',\n",
       "   'Han Hu',\n",
       "   'Yi Li',\n",
       "   'Guodong Zhang',\n",
       "   'Yuwen Xiong',\n",
       "   'Jifeng Dai'],\n",
       "  'published': '2017-03-17T21:58:20Z',\n",
       "  'updated': '2017-06-05T10:08:50Z',\n",
       "  'abstract': 'Convolutional neural networks (CNNs) are inherently limited to modelgeometric transformations due to the fixed geometric structures in its buildingmodules. In this work, we introduce two new modules to enhance thetransformation modeling capacity of CNNs, namely, deformable convolution anddeformable RoI pooling. Both are based on the idea of augmenting the spatialsampling locations in the modules with additional offsets and learning theoffsets from target tasks, without additional supervision. The new modules canreadily replace their plain counterparts in existing CNNs and can be easilytrained end-to-end by standard back-propagation, giving rise to deformableconvolutional networks. Extensive experiments validate the effectiveness of ourapproach on sophisticated vision tasks of object detection and semanticsegmentation. The code would be released.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 764-773',\n",
       "  'citations': '1051',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1703.06211v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6058204364274666350&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 429: {'ID': 429,\n",
       "  'title': 'Learning a Predictable and Generative Vector Representation for Objects',\n",
       "  'authors': ['Abhinav Gupta',\n",
       "   'David F. Fouhey',\n",
       "   'Rohit Girdhar',\n",
       "   'Mikel Rodriguez'],\n",
       "  'published': '2016-03-29T04:36:18Z',\n",
       "  'updated': '2016-08-31T15:39:29Z',\n",
       "  'abstract': 'What is a good vector representation of an object? We believe that it shouldbe generative in 3D, in the sense that it can produce new 3D objects; as wellas be predictable from 2D, in the sense that it can be perceived from 2Dimages. We propose a novel architecture, called the TL-embedding network, tolearn an embedding space with these properties. The network consists of twocomponents: (a) an autoencoder that ensures the representation is generative;and (b) a convolutional network that ensures the representation is predictable.This enables tackling a number of tasks including voxel prediction from 2Dimages and 3D model retrieval. Extensive experimental analysis demonstrates theusefulness and versatility of this embedding.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (6), 484-499',\n",
       "  'citations': '344',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1603.08637v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5113567481337158484&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 430: {'ID': 430,\n",
       "  'title': 'Learning a Discriminative Null Space for Person Re-identification',\n",
       "  'authors': ['Shaogang Gong', 'Tao Xiang', 'Li Zhang'],\n",
       "  'published': '2016-03-07T16:26:07Z',\n",
       "  'updated': '2016-03-07T16:26:07Z',\n",
       "  'abstract': \"Most existing person re-identification (re-id) methods focus on learning theoptimal distance metrics across camera views. Typically a person's appearanceis represented using features of thousands of dimensions, whilst only hundredsof training samples are available due to the difficulties in collecting matchedtraining images. With the number of training samples much smaller than thefeature dimension, the existing methods thus face the classic small sample size(SSS) problem and have to resort to dimensionality reduction techniques and/ormatrix regularisation, which lead to loss of discriminative power. In thiswork, we propose to overcome the SSS problem in re-id distance metric learningby matching people in a discriminative null space of the training data. In thisnull space, images of the same person are collapsed into a single point thusminimising the within-class scatter to the extreme and maximising the relativebetween-class separation simultaneously. Importantly, it has a fixed dimension,a closed-form solution and is very efficient to compute. Extensive experimentscarried out on five person re-identification benchmarks including VIPeR,PRID2011, CUHK01, CUHK03 and Market1501 show that such a simple approach beatsthe state-of-the-art alternatives, often by a big margin.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '551',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1603.02139v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6697095554433865478&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 431: {'ID': 431,\n",
       "  'title': 'Multi-Directional Multi-Level Dual-Cross Patterns for Robust Face  Recognition',\n",
       "  'authors': ['Dacheng Tao',\n",
       "   'Jonghyun Choi',\n",
       "   'Changxing Ding',\n",
       "   'Larry S. Davis'],\n",
       "  'published': '2014-01-21T13:24:16Z',\n",
       "  'updated': '2015-08-06T02:28:49Z',\n",
       "  'abstract': 'To perform unconstrained face recognition robust to variations inillumination, pose and expression, this paper presents a new scheme to extract\"Multi-Directional Multi-Level Dual-Cross Patterns\" (MDML-DCPs) from faceimages. Specifically, the MDMLDCPs scheme exploits the first derivative ofGaussian operator to reduce the impact of differences in illumination and thencomputes the DCP feature at both the holistic and component levels. DCP is anovel face image descriptor inspired by the unique textural structure of humanfaces. It is computationally efficient and only doubles the cost of computinglocal binary patterns, yet is extremely robust to pose and expressionvariations. MDML-DCPs comprehensively yet efficiently encodes the invariantcharacteristics of a face image from multiple levels into patterns that arehighly discriminative of inter-personal differences but robust tointra-personal variations. Experimental results on the FERET, CAS-PERL-R1, FRGC2.0, and LFW databases indicate that DCP outperforms the state-of-the-art localdescriptors (e.g. LBP, LTP, LPQ, POEM, tLBP, and LGXP) for both faceidentification and face verification tasks. More impressively, the bestperformance is achieved on the challenging LFW and FRGC 2.0 databases bydeploying MDML-DCPs in a simple recognition scheme.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 38 (3), 518-531',\n",
       "  'citations': '291',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1401.5311v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2237293194243300825&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 432: {'ID': 432,\n",
       "  'title': 'Ask Your Neurons: A Neural-based Approach to Answering Questions about  Images',\n",
       "  'authors': ['Marcus Rohrbach', 'Mateusz Malinowski', 'Mario Fritz'],\n",
       "  'published': '2015-05-05T18:39:29Z',\n",
       "  'updated': '2015-10-01T12:13:20Z',\n",
       "  'abstract': 'We address a question answering task on real-world images that is set up as aVisual Turing Test. By combining latest advances in image representation andnatural language processing, we propose Neural-Image-QA, an end-to-endformulation to this problem for which all parts are trained jointly. Incontrast to previous efforts, we are facing a multi-modal problem where thelanguage output (answer) is conditioned on visual and natural language input(image and question). Our approach Neural-Image-QA doubles the performance ofthe previous best approach on this problem. We provide additional insights intothe problem by analyzing how much information is contained only in the languagepart for which we provide a new human baseline. To study human consensus, whichis related to the ambiguities inherent in this challenging task, we propose twonovel metrics and collect additional answers which extends the original DAQUARdataset to DAQUAR-Consensus.',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.CL'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 1-9',\n",
       "  'citations': '464',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1505.01121v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6398699668058993420&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 433: {'ID': 433,\n",
       "  'title': 'Virtual Adversarial Training: A Regularization Method for Supervised and  Semi-Supervised Learning',\n",
       "  'authors': ['Takeru Miyato',\n",
       "   'Masanori Koyama',\n",
       "   'Shin-ichi Maeda',\n",
       "   'Shin Ishii'],\n",
       "  'published': '2017-04-13T02:45:27Z',\n",
       "  'updated': '2018-06-27T04:52:47Z',\n",
       "  'abstract': 'We propose a new regularization method based on virtual adversarial loss: anew measure of local smoothness of the conditional label distribution giveninput. Virtual adversarial loss is defined as the robustness of the conditionallabel distribution around each input data point against local perturbation.Unlike adversarial training, our method defines the adversarial directionwithout label information and is hence applicable to semi-supervised learning.Because the directions in which we smooth the model are only \"virtually\"adversarial, we call our method virtual adversarial training (VAT). Thecomputational cost of VAT is relatively low. For neural networks, theapproximated gradient of virtual adversarial loss can be computed with no morethan two pairs of forward- and back-propagations. In our experiments, weapplied VAT to supervised and semi-supervised learning tasks on multiplebenchmark datasets. With a simple enhancement of the algorithm based on theentropy minimization principle, our VAT achieves state-of-the-art performancefor semi-supervised learning tasks on SVHN and CIFAR-10.',\n",
       "  'categories': ['stat.ML', 'cs.LG'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 41 (8), 1979-1993',\n",
       "  'citations': '547',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1704.03976v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=10593080354202030680&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 434: {'ID': 434,\n",
       "  'title': 'MUTAN: Multimodal Tucker Fusion for Visual Question Answering',\n",
       "  'authors': ['Hedi Ben-younes',\n",
       "   'Nicolas Thome',\n",
       "   'Matthieu Cord',\n",
       "   'Rémi Cadene'],\n",
       "  'published': '2017-05-18T16:23:22Z',\n",
       "  'updated': '2017-05-18T16:23:22Z',\n",
       "  'abstract': 'Bilinear models provide an appealing framework for mixing and merginginformation in Visual Question Answering (VQA) tasks. They help to learn highlevel associations between question meaning and visual concepts in the image,but they suffer from huge dimensionality issues. We introduce MUTAN, amultimodal tensor-based Tucker decomposition to efficiently parametrizebilinear interactions between visual and textual representations. Additionallyto the Tucker framework, we design a low-rank matrix-based decomposition toexplicitly constrain the interaction rank. With MUTAN, we control thecomplexity of the merging scheme while keeping nice interpretable fusionrelations. We show how our MUTAN model generalizes some of the latest VQAarchitectures, providing state-of-the-art results.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 2612-2620',\n",
       "  'citations': '227',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1705.06676v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=7053181340342045630&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 435: {'ID': 435,\n",
       "  'title': 'Person Re-Identification by Discriminative Selection in Video Ranking',\n",
       "  'authors': ['Shaogang Gong', 'Xiatian Zhu', 'Shengjin Wang', 'Taiqing Wang'],\n",
       "  'published': '2016-01-23T10:33:45Z',\n",
       "  'updated': '2016-01-23T10:33:45Z',\n",
       "  'abstract': 'Current person re-identification (ReID) methods typically rely onsingle-frame imagery features, whilst ignoring space-time information fromimage sequences often available in the practical surveillance scenarios.Single-frame (single-shot) based visual appearance matching is inherentlylimited for person ReID in public spaces due to the challenging visualambiguity and uncertainty arising from non-overlapping camera views whereviewing condition changes can cause significant people appearance variations.In this work, we present a novel model to automatically select the mostdiscriminative video fragments from noisy/incomplete image sequences of peoplefrom which reliable space-time and appearance features can be computed, whilstsimultaneously learning a video ranking function for person ReID. Using thePRID$2011$, iLIDS-VID, and HDA+ image sequence datasets, we extensivelyconducted comparative evaluations to demonstrate the advantages of the proposedmodel over contemporary gait recognition, holistic image sequence matching andstate-of-the-art single-/multi-shot ReID methods.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 38 (12), 2501\\xa0…',\n",
       "  'citations': '165',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1601.06260v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=9944591201180030734&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 436: {'ID': 436,\n",
       "  'title': 'Bayesian Models of Graphs, Arrays and Other Exchangeable Random  Structures',\n",
       "  'authors': ['Peter Orbanz', 'Daniel M. Roy'],\n",
       "  'published': '2013-12-30T20:40:18Z',\n",
       "  'updated': '2015-02-13T20:01:00Z',\n",
       "  'abstract': \"The natural habitat of most Bayesian methods is data represented byexchangeable sequences of observations, for which de Finetti's theorem providesthe theoretical foundation. Dirichlet process clustering, Gaussian processregression, and many other parametric and nonparametric Bayesian models fallwithin the remit of this framework; many problems arising in modern dataanalysis do not. This article provides an introduction to Bayesian models ofgraphs, matrices, and other data that can be modeled by random structures. Wedescribe results in probability theory that generalize de Finetti's theorem tosuch data and discuss their relevance to nonparametric Bayesian modeling. Withthe basic ideas in place, we survey example models available in the literature;applications of such models include collaborative filtering, link prediction,and graph and network analysis. We also highlight connections to recentdevelopments in graph theory and probability, and sketch the more generalmathematical foundation of Bayesian methods for other types of data beyondsequences and arrays.\",\n",
       "  'categories': ['math.ST', 'stat.ML', 'stat.TH'],\n",
       "  'journal': 'IEEE Annals of the History of Computing, 437-461',\n",
       "  'citations': '178',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1312.7857v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=9522578236048541142&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 437: {'ID': 437,\n",
       "  'title': 'Trunk-Branch Ensemble Convolutional Neural Networks for Video-based Face  Recognition',\n",
       "  'authors': ['Dacheng Tao', 'Changxing Ding'],\n",
       "  'published': '2016-07-19T07:14:28Z',\n",
       "  'updated': '2017-05-17T09:12:19Z',\n",
       "  'abstract': 'Human faces in surveillance videos often suffer from severe image blur,dramatic pose variations, and occlusion. In this paper, we propose acomprehensive framework based on Convolutional Neural Networks (CNN) toovercome challenges in video-based face recognition (VFR). First, to learnblur-robust face representations, we artificially blur training data composedof clear still images to account for a shortfall in real-world video trainingdata. Using training data composed of both still images and artificiallyblurred data, CNN is encouraged to learn blur-insensitive featuresautomatically. Second, to enhance robustness of CNN features to pose variationsand occlusion, we propose a Trunk-Branch Ensemble CNN model (TBE-CNN), whichextracts complementary information from holistic face images and patchescropped around facial components. TBE-CNN is an end-to-end model that extractsfeatures efficiently by sharing the low- and middle-level convolutional layersbetween the trunk and branch networks. Third, to further promote thediscriminative power of the representations learnt by TBE-CNN, we propose animproved triplet loss function. Systematic experiments justify theeffectiveness of the proposed techniques. Most impressively, TBE-CNN achievesstate-of-the-art performance on three popular video face databases: PaSC, COXFace, and YouTube Faces. With the proposed techniques, we also obtain the firstplace in the BTAS 2016 Video Person Recognition Evaluation.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 40 (4), 1002-1014',\n",
       "  'citations': '216',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1607.05427v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=7712565699770130730&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 438: {'ID': 438,\n",
       "  'title': 'FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks',\n",
       "  'authors': ['Eddy Ilg',\n",
       "   'Nikolaus Mayer',\n",
       "   'Margret Keuper',\n",
       "   'Tonmoy Saikia',\n",
       "   'Alexey Dosovitskiy',\n",
       "   'Thomas Brox'],\n",
       "  'published': '2016-12-06T17:52:47Z',\n",
       "  'updated': '2016-12-06T17:52:47Z',\n",
       "  'abstract': 'The FlowNet demonstrated that optical flow estimation can be cast as alearning problem. However, the state of the art with regard to the quality ofthe flow has still been defined by traditional methods. Particularly on smalldisplacements and real-world data, FlowNet cannot compete with variationalmethods. In this paper, we advance the concept of end-to-end learning ofoptical flow and make it work really well. The large improvements in qualityand speed are caused by three major contributions: first, we focus on thetraining data and show that the schedule of presenting data during training isvery important. Second, we develop a stacked architecture that includes warpingof the second image with intermediate optical flow. Third, we elaborate onsmall displacements by introducing a sub-network specializing on small motions.FlowNet 2.0 is only marginally slower than the original FlowNet but decreasesthe estimation error by more than 50%. It performs on par with state-of-the-artmethods, while running at interactive frame rates. Moreover, we present fastervariants that allow optical flow computation at up to 140fps with accuracymatching the original FlowNet.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '1149',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1612.01925v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=10327949235117821487&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 439: {'ID': 439,\n",
       "  'title': 'Focal Loss for Dense Object Detection',\n",
       "  'authors': ['Ross Girshick',\n",
       "   'Piotr Dollár',\n",
       "   'Priya Goyal',\n",
       "   'Kaiming He',\n",
       "   'Tsung-Yi Lin'],\n",
       "  'published': '2017-08-07T06:32:42Z',\n",
       "  'updated': '2018-02-07T18:44:44Z',\n",
       "  'abstract': 'The highest accuracy object detectors to date are based on a two-stageapproach popularized by R-CNN, where a classifier is applied to a sparse set ofcandidate object locations. In contrast, one-stage detectors that are appliedover a regular, dense sampling of possible object locations have the potentialto be faster and simpler, but have trailed the accuracy of two-stage detectorsthus far. In this paper, we investigate why this is the case. We discover thatthe extreme foreground-background class imbalance encountered during trainingof dense detectors is the central cause. We propose to address this classimbalance by reshaping the standard cross entropy loss such that itdown-weights the loss assigned to well-classified examples. Our novel FocalLoss focuses training on a sparse set of hard examples and prevents the vastnumber of easy negatives from overwhelming the detector during training. Toevaluate the effectiveness of our loss, we design and train a simple densedetector we call RetinaNet. Our results show that when trained with the focalloss, RetinaNet is able to match the speed of previous one-stage detectorswhile surpassing the accuracy of all existing state-of-the-art two-stagedetectors. Code is at: https://github.com/facebookresearch/Detectron.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 2980-2988',\n",
       "  'citations': '3549',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1708.02002v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=1010598353453664537&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 440: {'ID': 440,\n",
       "  'title': 'Soft-NMS -- Improving Object Detection With One Line of Code',\n",
       "  'authors': ['Bharat Singh',\n",
       "   'Navaneeth Bodla',\n",
       "   'Rama Chellappa',\n",
       "   'Larry S. Davis'],\n",
       "  'published': '2017-04-14T18:00:03Z',\n",
       "  'updated': '2017-08-08T17:49:18Z',\n",
       "  'abstract': 'Non-maximum suppression is an integral part of the object detection pipeline.First, it sorts all detection boxes on the basis of their scores. The detectionbox M with the maximum score is selected and all other detection boxes with asignificant overlap (using a pre-defined threshold) with M are suppressed. Thisprocess is recursively applied on the remaining boxes. As per the design of thealgorithm, if an object lies within the predefined overlap threshold, it leadsto a miss. To this end, we propose Soft-NMS, an algorithm which decays thedetection scores of all other objects as a continuous function of their overlapwith M. Hence, no object is eliminated in this process. Soft-NMS obtainsconsistent improvements for the coco-style mAP metric on standard datasets likePASCAL VOC 2007 (1.7% for both R-FCN and Faster-RCNN) and MS-COCO (1.3% forR-FCN and 1.1% for Faster-RCNN) by just changing the NMS algorithm without anyadditional hyper-parameters. Using Deformable-RFCN, Soft-NMS improvesstate-of-the-art in object detection from 39.8% to 40.9% with a single model.Further, the computational complexity of Soft-NMS is the same as traditionalNMS and hence it can be efficiently implemented. Since Soft-NMS does notrequire any extra training and is simple to implement, it can be easilyintegrated into any object detection pipeline. Code for Soft-NMS is publiclyavailable on GitHub (http://bit.ly/2nJLNMu).',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 5561-5569',\n",
       "  'citations': '396',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1704.04503v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=16880712016776516810&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 441: {'ID': 441,\n",
       "  'title': 'BB8: A Scalable, Accurate, Robust to Partial Occlusion Method for  Predicting the 3D Poses of Challenging Objects without Using Depth',\n",
       "  'authors': ['Vincent Lepetit', 'Mahdi Rad'],\n",
       "  'published': '2017-03-31T13:56:35Z',\n",
       "  'updated': '2018-03-26T16:08:36Z',\n",
       "  'abstract': 'We introduce a novel method for 3D object detection and pose estimation fromcolor images only. We first use segmentation to detect the objects of interestin 2D even in presence of partial occlusions and cluttered background. Bycontrast with recent patch-based methods, we rely on a \"holistic\" approach: Weapply to the detected objects a Convolutional Neural Network (CNN) trained topredict their 3D poses in the form of 2D projections of the corners of their 3Dbounding boxes. This, however, is not sufficient for handling objects from therecent T-LESS dataset: These objects exhibit an axis of rotational symmetry,and the similarity of two images of such an object under two different posesmakes training the CNN challenging. We solve this problem by restricting therange of poses used for training, and by introducing a classifier to identifythe range of a pose at run-time before estimating it. We also use an optionaladditional step that refines the predicted poses. We improve thestate-of-the-art on the LINEMOD dataset from 73.7% to 89.3% of correctlyregistered RGB frames. We are also the first to report results on the Occlusiondataset using color images only. We obtain 54% of frames passing the Pose 6Dcriterion on average on several sequences of the T-LESS dataset, compared tothe 67% of the state-of-the-art on the same sequences which uses both color anddepth. The full approach is also scalable, as a single network can be trainedfor multiple objects simultaneously.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 3828-3836',\n",
       "  'citations': '234',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1703.10896v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=9748825849583978942&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 442: {'ID': 442,\n",
       "  'title': 'From Facial Parts Responses to Face Detection: A Deep Learning Approach',\n",
       "  'authors': ['Xiaoou Tang', 'Ping Luo', 'Shuo Yang', 'Chen Change Loy'],\n",
       "  'published': '2015-09-22T02:59:31Z',\n",
       "  'updated': '2015-09-22T02:59:31Z',\n",
       "  'abstract': 'In this paper, we propose a novel deep convolutional network (DCN) thatachieves outstanding performance on FDDB, PASCAL Face, and AFW. Specifically,our method achieves a high recall rate of 90.99% on the challenging FDDBbenchmark, outperforming the state-of-the-art method by a large margin of2.91%. Importantly, we consider finding faces from a new perspective throughscoring facial parts responses by their spatial structure and arrangement. Thescoring mechanism is carefully formulated considering challenging cases wherefaces are only partially visible. This consideration allows our network todetect faces under severe occlusion and unconstrained pose variation, which arethe main difficulty and bottleneck of most existing face detection approaches.We show that despite the use of DCN, our network can achieve practical runtimespeed.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 3676-3684',\n",
       "  'citations': '438',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1509.06451v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=1818335115841631894&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 443: {'ID': 443,\n",
       "  'title': 'Memory Aware Synapses: Learning what (not) to forget',\n",
       "  'authors': ['Francesca Babiloni',\n",
       "   'Mohamed Elhoseiny',\n",
       "   'Rahaf Aljundi',\n",
       "   'Tinne Tuytelaars',\n",
       "   'Marcus Rohrbach'],\n",
       "  'published': '2017-11-27T09:48:44Z',\n",
       "  'updated': '2018-10-05T08:40:30Z',\n",
       "  'abstract': \"Humans can learn in a continuous manner. Old rarely utilized knowledge can beoverwritten by new incoming information while important, frequently usedknowledge is prevented from being erased. In artificial learning systems,lifelong learning so far has focused mainly on accumulating knowledge overtasks and overcoming catastrophic forgetting. In this paper, we argue that,given the limited model capacity and the unlimited new information to belearned, knowledge has to be preserved or erased selectively. Inspired byneuroplasticity, we propose a novel approach for lifelong learning, coinedMemory Aware Synapses (MAS). It computes the importance of the parameters of aneural network in an unsupervised and online manner. Given a new sample whichis fed to the network, MAS accumulates an importance measure for each parameterof the network, based on how sensitive the predicted output function is to achange in this parameter. When learning a new task, changes to importantparameters can then be penalized, effectively preventing important knowledgerelated to previous tasks from being overwritten. Further, we show aninteresting connection between a local version of our method and Hebb'srule,which is a model for the learning process in the brain. We test our methodon a sequence of object recognition tasks and on the challenging problem oflearning an embedding for predicting $&lt;$subject, predicate, object$&gt;$ triplets.We show state-of-the-art performance and, for the first time, the ability toadapt the importance of the parameters based on unlabeled data towards what thenetwork needs (not) to forget, which may vary depending on test conditions.\",\n",
       "  'categories': ['cs.CV', 'cs.AI', 'stat.ML'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 139-154',\n",
       "  'citations': '156',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1711.09601v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=15731974644747466357&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 444: {'ID': 444,\n",
       "  'title': 'Exploring Visual Relationship for Image Captioning',\n",
       "  'authors': ['Yingwei Pan', 'Tao Mei', 'Ting Yao', 'Yehao Li'],\n",
       "  'published': '2018-09-19T07:50:17Z',\n",
       "  'updated': '2018-09-19T07:50:17Z',\n",
       "  'abstract': 'It is always well believed that modeling relationships between objects wouldbe helpful for representing and eventually describing an image. Nevertheless,there has not been evidence in support of the idea on image descriptiongeneration. In this paper, we introduce a new design to explore the connectionsbetween objects for image captioning under the umbrella of attention-basedencoder-decoder framework. Specifically, we present Graph ConvolutionalNetworks plus Long Short-Term Memory (dubbed as GCN-LSTM) architecture thatnovelly integrates both semantic and spatial object relationships into imageencoder. Technically, we build graphs over the detected objects in an imagebased on their spatial and semantic connections. The representations of eachregion proposed on objects are then refined by leveraging graph structurethrough GCN. With the learnt region-level features, our GCN-LSTM capitalizes onLSTM-based captioning framework with attention mechanism for sentencegeneration. Extensive experiments are conducted on COCO image captioningdataset, and superior results are reported when comparing to state-of-the-artapproaches. More remarkably, GCN-LSTM increases CIDEr-D performance from 120.1%to 128.7% on COCO testing set.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 684-699',\n",
       "  'citations': '148',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1809.07041v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5139774831408313026&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 445: {'ID': 445,\n",
       "  'title': 'Label-Embedding for Image Classification',\n",
       "  'authors': ['Florent Perronnin',\n",
       "   'Zaid Harchaoui',\n",
       "   'Cordelia Schmid',\n",
       "   'Zeynep Akata'],\n",
       "  'published': '2015-03-30T14:04:34Z',\n",
       "  'updated': '2015-10-01T10:48:38Z',\n",
       "  'abstract': 'Attributes act as intermediate representations that enable parameter sharingbetween classes, a must when training data is scarce. We propose to viewattribute-based image classification as a label-embedding problem: each classis embedded in the space of attribute vectors. We introduce a function thatmeasures the compatibility between an image and a label embedding. Theparameters of this function are learned on a training set of labeled samples toensure that, given an image, the correct classes rank higher than the incorrectones. Results on the Animals With Attributes and Caltech-UCSD-Birds datasetsshow that the proposed framework outperforms the standard Direct AttributePrediction baseline in a zero-shot learning scenario. Label embedding enjoys abuilt-in ability to leverage alternative sources of information instead of orin addition to attributes, such as e.g. class hierarchies or textualdescriptions. Moreover, label embedding encompasses the whole range of learningsettings from zero-shot learning to regular learning with a large number oflabeled examples.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 38 (7), 1425-1438',\n",
       "  'citations': '319',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1503.08677v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14583193252252595475&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 446: {'ID': 446,\n",
       "  'title': 'MemNet: A Persistent Memory Network for Image Restoration',\n",
       "  'authors': ['Ying Tai', 'Jian Yang', 'Xiaoming Liu', 'Chunyan Xu'],\n",
       "  'published': '2017-08-07T17:20:58Z',\n",
       "  'updated': '2017-08-07T17:20:58Z',\n",
       "  'abstract': 'Recently, very deep convolutional neural networks (CNNs) have been attractingconsiderable attention in image restoration. However, as the depth grows, thelong-term dependency problem is rarely realized for these very deep models,which results in the prior states/layers having little influence on thesubsequent ones. Motivated by the fact that human thoughts have persistency, wepropose a very deep persistent memory network (MemNet) that introduces a memoryblock, consisting of a recursive unit and a gate unit, to explicitly minepersistent memory through an adaptive learning process. The recursive unitlearns multi-level representations of the current state under differentreceptive fields. The representations and the outputs from the previous memoryblocks are concatenated and sent to the gate unit, which adaptively controlshow much of the previous states should be reserved, and decides how much of thecurrent state should be stored. We apply MemNet to three image restorationtasks, i.e., image denosing, super-resolution and JPEG deblocking.Comprehensive experiments demonstrate the necessity of the MemNet and itsunanimous superiority on all three tasks over the state of the arts. Code isavailable at https://github.com/tyshiwo/MemNet.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 4539-4547',\n",
       "  'citations': '451',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1708.02209v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14700333538957670002&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 447: {'ID': 447,\n",
       "  'title': 'Laplacian Pyramid Reconstruction and Refinement for Semantic  Segmentation',\n",
       "  'authors': ['Charless C. Fowlkes', 'Golnaz Ghiasi'],\n",
       "  'published': '2016-05-08T02:25:12Z',\n",
       "  'updated': '2016-07-30T21:21:58Z',\n",
       "  'abstract': 'CNN architectures have terrific recognition performance but rely on spatialpooling which makes it difficult to adapt them to tasks that require dense,pixel-accurate labeling. This paper makes two contributions: (1) We demonstratethat while the apparent spatial resolution of convolutional feature maps islow, the high-dimensional feature representation contains significant sub-pixellocalization information. (2) We describe a multi-resolution reconstructionarchitecture based on a Laplacian pyramid that uses skip connections fromhigher resolution feature maps and multiplicative gating to successively refinesegment boundaries reconstructed from lower-resolution maps. This approachyields state-of-the-art semantic segmentation results on the PASCAL VOC andCityscapes segmentation benchmarks without resorting to more complexrandom-field inference or instance detection driven architectures.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (3), 519-534',\n",
       "  'citations': '282',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1605.02264v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6132709390859446333&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 448: {'ID': 448,\n",
       "  'title': 'Eye Tracking for Everyone',\n",
       "  'authors': ['Harini Kannan',\n",
       "   'Wojciech Matusik',\n",
       "   'Aditya Khosla',\n",
       "   'Petr Kellnhofer',\n",
       "   'Antonio Torralba',\n",
       "   'Suchendra Bhandarkar',\n",
       "   'Kyle Krafka'],\n",
       "  'published': '2016-06-18T23:53:54Z',\n",
       "  'updated': '2016-06-18T23:53:54Z',\n",
       "  'abstract': \"From scientific research to commercial applications, eye tracking is animportant tool across many domains. Despite its range of applications, eyetracking has yet to become a pervasive technology. We believe that we can putthe power of eye tracking in everyone's palm by building eye tracking softwarethat works on commodity hardware such as mobile phones and tablets, without theneed for additional sensors or devices. We tackle this problem by introducingGazeCapture, the first large-scale dataset for eye tracking, containing datafrom over 1450 people consisting of almost 2.5M frames. Using GazeCapture, wetrain iTracker, a convolutional neural network for eye tracking, which achievesa significant reduction in error over previous approaches while running in realtime (10-15fps) on a modern mobile device. Our model achieves a predictionerror of 1.71cm and 2.53cm without calibration on mobile phones and tabletsrespectively. With calibration, this is reduced to 1.34cm and 2.12cm. Further,we demonstrate that the features learned by iTracker generalize well to otherdatasets, achieving state-of-the-art results. The code, data, and models areavailable at http://gazecapture.csail.mit.edu.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '361',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1606.05814v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=9163144060397946813&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 449: {'ID': 449,\n",
       "  'title': 'A Large Dataset to Train Convolutional Networks for Disparity, Optical  Flow, and Scene Flow Estimation',\n",
       "  'authors': ['Philipp Fischer',\n",
       "   'Eddy Ilg',\n",
       "   'Nikolaus Mayer',\n",
       "   'Daniel Cremers',\n",
       "   'Philip Häusser',\n",
       "   'Alexey Dosovitskiy',\n",
       "   'Thomas Brox'],\n",
       "  'published': '2015-12-07T17:35:00Z',\n",
       "  'updated': '2015-12-07T17:35:00Z',\n",
       "  'abstract': 'Recent work has shown that optical flow estimation can be formulated as asupervised learning task and can be successfully solved with convolutionalnetworks. Training of the so-called FlowNet was enabled by a largesynthetically generated dataset. The present paper extends the concept ofoptical flow estimation via convolutional networks to disparity and scene flowestimation. To this end, we propose three synthetic stereo video datasets withsufficient realism, variation, and size to successfully train large networks.Our datasets are the first large-scale datasets to enable training andevaluating scene flow methods. Besides the datasets, we present a convolutionalnetwork for real-time disparity estimation that provides state-of-the-artresults. By combining a flow and disparity estimation network and training itjointly, we demonstrate the first scene flow estimation with a convolutionalnetwork.',\n",
       "  'categories': ['cs.CV', 'cs.LG', 'stat.ML', 'I.2.6; I.2.10; I.4.8'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '864',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1512.02134v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5567272789746439114&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 450: {'ID': 450,\n",
       "  'title': 'View Synthesis by Appearance Flow',\n",
       "  'authors': ['Alexei A. Efros',\n",
       "   'Weilun Sun',\n",
       "   'Tinghui Zhou',\n",
       "   'Jitendra Malik',\n",
       "   'Shubham Tulsiani'],\n",
       "  'published': '2016-05-11T19:16:24Z',\n",
       "  'updated': '2017-02-11T20:33:50Z',\n",
       "  'abstract': 'We address the problem of novel view synthesis: given an input image,synthesizing new images of the same object or scene observed from arbitraryviewpoints. We approach this as a learning task but, critically, instead oflearning to synthesize pixels from scratch, we learn to copy them from theinput image. Our approach exploits the observation that the visual appearanceof different views of the same instance is highly correlated, and suchcorrelation could be explicitly learned by training a convolutional neuralnetwork (CNN) to predict appearance flows -- 2-D coordinate vectors specifyingwhich pixels in the input view could be used to reconstruct the target view.Furthermore, the proposed framework easily generalizes to multiple input viewsby learning how to optimally combine single-view predictions. We show that forboth objects and scenes, our approach is able to synthesize novel views ofhigher perceptual quality than previous CNN-based techniques.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (4), 286-301',\n",
       "  'citations': '318',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1605.03557v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=13487039873891435297&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 451: {'ID': 451,\n",
       "  'title': 'Trainable Nonlinear Reaction Diffusion: A Flexible Framework for Fast  and Effective Image Restoration',\n",
       "  'authors': ['Thomas Pock', 'Yunjin Chen'],\n",
       "  'published': '2015-08-12T08:40:48Z',\n",
       "  'updated': '2016-08-20T04:48:54Z',\n",
       "  'abstract': 'Image restoration is a long-standing problem in low-level computer visionwith many interesting applications. We describe a flexible learning frameworkbased on the concept of nonlinear reaction diffusion models for various imagerestoration problems. By embodying recent improvements in nonlinear diffusionmodels, we propose a dynamic nonlinear reaction diffusion model withtime-dependent parameters (\\\\ie, linear filters and influence functions). Incontrast to previous nonlinear diffusion models, all the parameters, includingthe filters and the influence functions, are simultaneously learned fromtraining data through a loss based approach. We call this approach TNRD --\\\\textit{Trainable Nonlinear Reaction Diffusion}. The TNRD approach isapplicable for a variety of image restoration tasks by incorporatingappropriate reaction force. We demonstrate its capabilities with threerepresentative applications, Gaussian image denoising, single image superresolution and JPEG deblocking. Experiments show that our trained nonlineardiffusion models largely benefit from the training of the parameters andfinally lead to the best reported performance on common test datasets for thetested applications. Our trained models preserve the structural simplicity ofdiffusion models and take only a small number of diffusion steps, thus arehighly efficient. Moreover, they are also well-suited for parallel computationon GPUs, which makes the inference procedure extremely fast.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 39 (6), 1256-1272',\n",
       "  'citations': '501',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1508.02848v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=12833313690015594282&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 452: {'ID': 452,\n",
       "  'title': 'SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image  Segmentation',\n",
       "  'authors': ['Roberto Cipolla', 'Vijay Badrinarayanan', 'Alex Kendall'],\n",
       "  'published': '2015-11-02T15:51:03Z',\n",
       "  'updated': '2016-10-10T21:11:59Z',\n",
       "  'abstract': 'We present a novel and practical deep fully convolutional neural networkarchitecture for semantic pixel-wise segmentation termed SegNet. This coretrainable segmentation engine consists of an encoder network, a correspondingdecoder network followed by a pixel-wise classification layer. The architectureof the encoder network is topologically identical to the 13 convolutionallayers in the VGG16 network. The role of the decoder network is to map the lowresolution encoder feature maps to full input resolution feature maps forpixel-wise classification. The novelty of SegNet lies is in the manner in whichthe decoder upsamples its lower resolution input feature map(s). Specifically,the decoder uses pooling indices computed in the max-pooling step of thecorresponding encoder to perform non-linear upsampling. This eliminates theneed for learning to upsample. The upsampled maps are sparse and are thenconvolved with trainable filters to produce dense feature maps. We compare ourproposed architecture with the widely adopted FCN and also with the well knownDeepLab-LargeFOV, DeconvNet architectures. This comparison reveals the memoryversus accuracy trade-off involved in achieving good segmentation performance.  SegNet was primarily motivated by scene understanding applications. Hence, itis designed to be efficient both in terms of memory and computational timeduring inference. It is also significantly smaller in the number of trainableparameters than other competing architectures. We also performed a controlledbenchmark of SegNet and other architectures on both road scenes and SUN RGB-Dindoor scene segmentation tasks. We show that SegNet provides good performancewith competitive inference time and more efficient inference memory-wise ascompared to other architectures. We also provide a Caffe implementation ofSegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.',\n",
       "  'categories': ['cs.CV', 'cs.LG', 'cs.NE'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 39 (12), 2481\\xa0…',\n",
       "  'citations': '5060',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1511.00561v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=8599254436676320906&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 453: {'ID': 453,\n",
       "  'title': 'SSD-6D: Making RGB-based 3D detection and 6D pose estimation great again',\n",
       "  'authors': ['Nassir Navab',\n",
       "   'Fabian Manhardt',\n",
       "   'Federico Tombari',\n",
       "   'Wadim Kehl',\n",
       "   'Slobodan Ilic'],\n",
       "  'published': '2017-11-27T21:17:51Z',\n",
       "  'updated': '2017-11-27T21:17:51Z',\n",
       "  'abstract': 'We present a novel method for detecting 3D model instances and estimatingtheir 6D poses from RGB data in a single shot. To this end, we extend thepopular SSD paradigm to cover the full 6D pose space and train on syntheticmodel data only. Our approach competes or surpasses current state-of-the-artmethods that leverage RGB-D data on multiple challenging datasets. Furthermore,our method produces these results at around 10Hz, which is many times fasterthan the related methods. For the sake of reproducibility, we make our trainednetworks and detection code publicly available.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 1521-1529',\n",
       "  'citations': '281',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1711.10006v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=12941097279663990686&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 454: {'ID': 454,\n",
       "  'title': 'Coarse-to-Fine Volumetric Prediction for Single-Image 3D Human Pose',\n",
       "  'authors': ['Konstantinos G. Derpanis',\n",
       "   'Kostas Daniilidis',\n",
       "   'Georgios Pavlakos',\n",
       "   'Xiaowei Zhou'],\n",
       "  'published': '2016-11-23T15:06:18Z',\n",
       "  'updated': '2017-07-26T12:10:16Z',\n",
       "  'abstract': 'This paper addresses the challenge of 3D human pose estimation from a singlecolor image. Despite the general success of the end-to-end learning paradigm,top performing approaches employ a two-step solution consisting of aConvolutional Network (ConvNet) for 2D joint localization and a subsequentoptimization step to recover 3D pose. In this paper, we identify therepresentation of 3D pose as a critical issue with current ConvNet approachesand make two important contributions towards validating the value of end-to-endlearning for this task. First, we propose a fine discretization of the 3D spacearound the subject and train a ConvNet to predict per voxel likelihoods foreach joint. This creates a natural representation for 3D pose and greatlyimproves performance over the direct regression of joint coordinates. Second,to further improve upon initial estimates, we employ a coarse-to-fineprediction scheme. This step addresses the large dimensionality increase andenables iterative refinement and repeated processing of the image features. Theproposed approach outperforms all state-of-the-art methods on standardbenchmarks achieving a relative error reduction greater than 30% on average.Additionally, we investigate using our volumetric representation in a relatedarchitecture which is suboptimal compared to our end-to-end approach, but is ofpractical interest, since it enables training when no image with corresponding3D groundtruth is available, and allows us to present compelling results forin-the-wild images.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '339',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1611.07828v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5441238285159300924&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 455: {'ID': 455,\n",
       "  'title': 'Learning Cooperative Visual Dialog Agents with Deep Reinforcement  Learning',\n",
       "  'authors': ['Abhishek Das',\n",
       "   'Stefan Lee',\n",
       "   'Satwik Kottur',\n",
       "   'Dhruv Batra',\n",
       "   'José M. F. Moura'],\n",
       "  'published': '2017-03-20T03:50:57Z',\n",
       "  'updated': '2017-03-21T17:41:23Z',\n",
       "  'abstract': \"We introduce the first goal-driven training for visual question answering anddialog agents. Specifically, we pose a cooperative 'image guessing' gamebetween two agents -- Qbot and Abot -- who communicate in natural languagedialog so that Qbot can select an unseen image from a lineup of images. We usedeep reinforcement learning (RL) to learn the policies of these agentsend-to-end -- from pixels to multi-agent multi-round dialog to game reward.  We demonstrate two experimental results.  First, as a 'sanity check' demonstration of pure RL (from scratch), we showresults on a synthetic world, where the agents communicate in ungroundedvocabulary, i.e., symbols with no pre-specified meanings (X, Y, Z). We findthat two bots invent their own communication protocol and start using certainsymbols to ask/answer about certain visual attributes (shape/color/style).Thus, we demonstrate the emergence of grounded language and communication among'visual' dialog agents with no human supervision.  Second, we conduct large-scale real-image experiments on the VisDial dataset,where we pretrain with supervised dialog data and show that the RL 'fine-tuned'agents significantly outperform SL agents. Interestingly, the RL Qbot learns toask questions that Abot is good at, ultimately resulting in more informativedialog and a better team.\",\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 2951-2960',\n",
       "  'citations': '255',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1703.06585v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6747222074397803563&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 456: {'ID': 456,\n",
       "  'title': 'Advances in Variational Inference',\n",
       "  'authors': ['Cheng Zhang',\n",
       "   'Judith Butepage',\n",
       "   'Stephan Mandt',\n",
       "   'Hedvig Kjellstrom'],\n",
       "  'published': '2017-11-15T14:46:27Z',\n",
       "  'updated': '2018-10-23T17:05:19Z',\n",
       "  'abstract': 'Many modern unsupervised or semi-supervised machine learning algorithms relyon Bayesian probabilistic models. These models are usually intractable and thusrequire approximate inference. Variational inference (VI) lets us approximate ahigh-dimensional Bayesian posterior with a simpler variational distribution bysolving an optimization problem. This approach has been successfully used invarious models and large-scale applications. In this review, we give anoverview of recent trends in variational inference. We first introduce standardmean field variational inference, then review recent advances focusing on thefollowing aspects: (a) scalable VI, which includes stochastic approximations,(b) generic VI, which extends the applicability of VI to a large class ofotherwise intractable models, such as non-conjugate models, (c) accurate VI,which includes variational models beyond the mean field approximation or withatypical divergences, and (d) amortized VI, which implements the inference overlocal latent variables with inference networks. Finally, we provide a summaryof promising future research directions.',\n",
       "  'categories': ['cs.LG', 'stat.ML'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 41 (8), 2008-2026',\n",
       "  'citations': '148',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1711.05597v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=273927933594303479&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 457: {'ID': 457,\n",
       "  'title': 'CNN-RNN: A Unified Framework for Multi-label Image Classification',\n",
       "  'authors': ['Yi Yang',\n",
       "   'Jiang Wang',\n",
       "   'Zhiheng Huang',\n",
       "   'Wei Xu',\n",
       "   'Junhua Mao',\n",
       "   'Chang Huang'],\n",
       "  'published': '2016-04-15T17:10:54Z',\n",
       "  'updated': '2016-04-15T17:10:54Z',\n",
       "  'abstract': 'While deep convolutional neural networks (CNNs) have shown a great success insingle-label image classification, it is important to note that real worldimages generally contain multiple labels, which could correspond to differentobjects, scenes, actions and attributes in an image. Traditional approaches tomulti-label image classification learn independent classifiers for eachcategory and employ ranking or thresholding on the classification results.These techniques, although working well, fail to explicitly exploit the labeldependencies in an image. In this paper, we utilize recurrent neural networks(RNNs) to address this problem. Combined with CNNs, the proposed CNN-RNNframework learns a joint image-label embedding to characterize the semanticlabel dependency as well as the image-label relevance, and it can be trainedend-to-end from scratch to integrate both information in a unified framework.Experimental results on public benchmark datasets demonstrate that the proposedarchitecture achieves better performance than the state-of-the-art multi-labelclassification model',\n",
       "  'categories': ['cs.CV', 'cs.LG', 'cs.NE'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '536',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1604.04573v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=11947203399516238933&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 458: {'ID': 458,\n",
       "  'title': 'MoFA: Model-based Deep Convolutional Face Autoencoder for Unsupervised  Monocular Reconstruction',\n",
       "  'authors': ['Florian Bernard',\n",
       "   'Michael Zollhöfer',\n",
       "   'Pablo Garrido',\n",
       "   'Hyeongwoo Kim',\n",
       "   'Christian Theobalt',\n",
       "   'Ayush Tewari',\n",
       "   'Patrick Pérez'],\n",
       "  'published': '2017-03-30T17:29:42Z',\n",
       "  'updated': '2017-12-07T20:38:13Z',\n",
       "  'abstract': 'In this work we propose a novel model-based deep convolutional autoencoderthat addresses the highly challenging problem of reconstructing a 3D human facefrom a single in-the-wild color image. To this end, we combine a convolutionalencoder network with an expert-designed generative model that serves asdecoder. The core innovation is our new differentiable parametric decoder thatencapsulates image formation analytically based on a generative model. Ourdecoder takes as input a code vector with exactly defined semantic meaning thatencodes detailed face pose, shape, expression, skin reflectance and sceneillumination. Due to this new way of combining CNN-based with model-based facereconstruction, the CNN-based encoder learns to extract semantically meaningfulparameters from a single monocular input image. For the first time, a CNNencoder and an expert-designed generative model can be trained end-to-end in anunsupervised manner, which renders training on very large (unlabeled) realworld data feasible. The obtained reconstructions compare favorably to currentstate-of-the-art approaches in terms of quality and richness of representation.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 3715-3724',\n",
       "  'citations': '194',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1703.10580v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=1588561877699027150&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 459: {'ID': 459,\n",
       "  'title': 'Escape from Cells: Deep Kd-Networks for the Recognition of 3D Point  Cloud Models',\n",
       "  'authors': ['Victor Lempitsky', 'Roman Klokov'],\n",
       "  'published': '2017-04-04T23:52:40Z',\n",
       "  'updated': '2017-10-26T08:51:12Z',\n",
       "  'abstract': 'We present a new deep learning architecture (called Kd-network) that isdesigned for 3D model recognition tasks and works with unstructured pointclouds. The new architecture performs multiplicative transformations and shareparameters of these transformations according to the subdivisions of the pointclouds imposed onto them by Kd-trees. Unlike the currently dominantconvolutional architectures that usually require rasterization on uniformtwo-dimensional or three-dimensional grids, Kd-networks do not rely on suchgrids in any way and therefore avoid poor scaling behaviour. In a series ofexperiments with popular shape recognition benchmarks, Kd-networks demonstratecompetitive performance in a number of shape recognition tasks such as shapeclassification, shape retrieval and shape part segmentation.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 863-872',\n",
       "  'citations': '368',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1704.01222v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5349888641934729215&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 460: {'ID': 460,\n",
       "  'title': 'Deep Networks for Image Super-Resolution with Sparse Prior',\n",
       "  'authors': ['Ding Liu',\n",
       "   'Thomas Huang',\n",
       "   'Zhaowen Wang',\n",
       "   'Jianchao Yang',\n",
       "   'Wei Han'],\n",
       "  'published': '2015-07-31T14:56:30Z',\n",
       "  'updated': '2015-10-15T19:02:26Z',\n",
       "  'abstract': 'Deep learning techniques have been successfully applied in many areas ofcomputer vision, including low-level image restoration problems. For imagesuper-resolution, several models based on deep neural networks have beenrecently proposed and attained superior performance that overshadows allprevious handcrafted models. The question then arises whether large-capacityand data-driven models have become the dominant solution to the ill-posedsuper-resolution problem. In this paper, we argue that domain expertiserepresented by the conventional sparse coding model is still valuable, and itcan be combined with the key ingredients of deep learning to achieve furtherimproved results. We show that a sparse coding model particularly designed forsuper-resolution can be incarnated as a neural network, and trained in acascaded structure from end to end. The interpretation of the network based onsparse coding leads to much more efficient and effective training, as well as areduced model size. Our model is evaluated on a wide range of images, and showsclear advantage over existing state-of-the-art methods in terms of bothrestoration accuracy and human subjective quality.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 370-378',\n",
       "  'citations': '523',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1507.08905v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=16776231602849831052&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 461: {'ID': 461,\n",
       "  'title': 'Precomputed Real-Time Texture Synthesis with Markovian Generative  Adversarial Networks',\n",
       "  'authors': ['Chuan Li', 'Michael Wand'],\n",
       "  'published': '2016-04-15T07:32:18Z',\n",
       "  'updated': '2016-04-15T07:32:18Z',\n",
       "  'abstract': 'This paper proposes Markovian Generative Adversarial Networks (MGANs), amethod for training generative neural networks for efficient texture synthesis.While deep neural network approaches have recently demonstrated remarkableresults in terms of synthesis quality, they still come at considerablecomputational costs (minutes of run-time for low-res images). Our paperaddresses this efficiency issue. Instead of a numerical deconvolution inprevious work, we precompute a feed-forward, strided convolutional network thatcaptures the feature statistics of Markovian patches and is able to directlygenerate outputs of arbitrary dimensions. Such network can directly decodebrown noise to realistic texture, or photos to artistic paintings. Withadversarial training, we obtain quality comparable to recent neural texturesynthesis methods. As no optimization is required any longer at generationtime, our run-time performance (0.25M pixel images at 25Hz) surpasses previousneural texture synthesizers by a significant margin (at least 500 timesfaster). We apply this idea to texture synthesis, style transfer, and videostylization.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (3), 702-716',\n",
       "  'citations': '662',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1604.04382v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=7302114364954488947&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 462: {'ID': 462,\n",
       "  'title': 'Recurrent Network Models for Human Dynamics',\n",
       "  'authors': ['Katerina Fragkiadaki',\n",
       "   'Sergey Levine',\n",
       "   'Jitendra Malik',\n",
       "   'Panna Felsen'],\n",
       "  'published': '2015-08-02T18:59:52Z',\n",
       "  'updated': '2015-09-29T01:28:23Z',\n",
       "  'abstract': 'We propose the Encoder-Recurrent-Decoder (ERD) model for recognition andprediction of human body pose in videos and motion capture. The ERD model is arecurrent neural network that incorporates nonlinear encoder and decodernetworks before and after recurrent layers. We test instantiations of ERDarchitectures in the tasks of motion capture (mocap) generation, body poselabeling and body pose forecasting in videos. Our model handles mocap trainingdata across multiple subjects and activity domains, and synthesizes novelmotions while avoid drifting for long periods of time. For human pose labeling,ERD outperforms a per frame body part detector by resolving left-right bodypart confusions. For video pose forecasting, ERD predicts body jointdisplacements across a temporal horizon of 400ms and outperforms a first ordermotion model based on optical flow. ERDs extend previous Long Short Term Memory(LSTM) models in the literature to jointly learn representations and theirdynamics. Our experiments show such representation learning is crucial for bothlabeling and prediction in space-time. We find this is a distinguishing featurebetween the spatio-temporal visual domain in comparison to 1D text, speech orhandwriting, where straightforward hard coded representations have shownexcellent results when directly combined with recurrent units.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 4346-4354',\n",
       "  'citations': '346',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1508.00271v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=4892488825670482291&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 463: {'ID': 463,\n",
       "  'title': 'Generating Visual Explanations',\n",
       "  'authors': ['Bernt Schiele',\n",
       "   'Jeff Donahue',\n",
       "   'Trevor Darrell',\n",
       "   'Zeynep Akata',\n",
       "   'Marcus Rohrbach',\n",
       "   'Lisa Anne Hendricks'],\n",
       "  'published': '2016-03-28T19:54:12Z',\n",
       "  'updated': '2016-03-28T19:54:12Z',\n",
       "  'abstract': 'Clearly explaining a rationale for a classification decision to an end-usercan be as important as the decision itself. Existing approaches for deep visualrecognition are generally opaque and do not output any justification text;contemporary vision-language models can describe image content but fail to takeinto account class-discriminative image aspects which justify visualpredictions. We propose a new model that focuses on the discriminatingproperties of the visible object, jointly predicts a class label, and explainswhy the predicted label is appropriate for the image. We propose a novel lossfunction based on sampling and reinforcement learning that learns to generatesentences that realize a global sentence property, such as class specificity.Our results on a fine-grained bird species classification dataset show that ourmodel is able to generate explanations which are not only consistent with animage but also more discriminative than descriptions produced by existingcaptioning methods.',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.CL'],\n",
       "  'journal': 'ECCV (4), 3-19',\n",
       "  'citations': '287',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1603.08507v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2280945937357547322&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 464: {'ID': 464,\n",
       "  'title': 'Focal Loss for Dense Object Detection',\n",
       "  'authors': ['Ross Girshick',\n",
       "   'Piotr Dollár',\n",
       "   'Priya Goyal',\n",
       "   'Kaiming He',\n",
       "   'Tsung-Yi Lin'],\n",
       "  'published': '2017-08-07T06:32:42Z',\n",
       "  'updated': '2018-02-07T18:44:44Z',\n",
       "  'abstract': 'The highest accuracy object detectors to date are based on a two-stageapproach popularized by R-CNN, where a classifier is applied to a sparse set ofcandidate object locations. In contrast, one-stage detectors that are appliedover a regular, dense sampling of possible object locations have the potentialto be faster and simpler, but have trailed the accuracy of two-stage detectorsthus far. In this paper, we investigate why this is the case. We discover thatthe extreme foreground-background class imbalance encountered during trainingof dense detectors is the central cause. We propose to address this classimbalance by reshaping the standard cross entropy loss such that itdown-weights the loss assigned to well-classified examples. Our novel FocalLoss focuses training on a sparse set of hard examples and prevents the vastnumber of easy negatives from overwhelming the detector during training. Toevaluate the effectiveness of our loss, we design and train a simple densedetector we call RetinaNet. Our results show that when trained with the focalloss, RetinaNet is able to match the speed of previous one-stage detectorswhile surpassing the accuracy of all existing state-of-the-art two-stagedetectors. Code is at: https://github.com/facebookresearch/Detectron.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence',\n",
       "  'citations': '3549',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1708.02002v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=1010598353453664537&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 465: {'ID': 465,\n",
       "  'title': 'Deep Direct Regression for Multi-Oriented Scene Text Detection',\n",
       "  'authors': ['Wenhao He', 'Cheng-Lin Liu', 'Xu-Yao Zhang', 'Fei Yin'],\n",
       "  'published': '2017-03-24T05:54:11Z',\n",
       "  'updated': '2017-03-24T05:54:11Z',\n",
       "  'abstract': 'In this paper, we first provide a new perspective to divide existing highperformance object detection methods into direct and indirect regressions.Direct regression performs boundary regression by predicting the offsets from agiven point, while indirect regression predicts the offsets from some boundingbox proposals. Then we analyze the drawbacks of the indirect regression, whichthe recent state-of-the-art detection structures like Faster-RCNN and SSDfollows, for multi-oriented scene text detection, and point out the potentialsuperiority of direct regression. To verify this point of view, we propose adeep direct regression based method for multi-oriented scene text detection.Our detection framework is simple and effective with a fully convolutionalnetwork and one-step post processing. The fully convolutional network isoptimized in an end-to-end way and has bi-task outputs where one is pixel-wiseclassification between text and non-text, and the other is direct regression todetermine the vertex coordinates of quadrilateral text boundaries. The proposedmethod is particularly beneficial for localizing incidental scene texts. On theICDAR2015 Incidental Scene Text benchmark, our method achieves the F1-measureof 81%, which is a new state-of-the-art and significantly outperforms previousapproaches. On other standard datasets with focused scene texts, our methodalso reaches the state-of-the-art performance.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 745-753',\n",
       "  'citations': '190',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1703.08289v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5667947859893950973&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 466: {'ID': 466,\n",
       "  'title': 'Learning Deep Object Detectors from 3D Models',\n",
       "  'authors': ['Kate Saenko', 'Xingchao Peng', 'Baochen Sun', 'Karim Ali'],\n",
       "  'published': '2014-12-22T20:10:31Z',\n",
       "  'updated': '2015-10-12T01:01:39Z',\n",
       "  'abstract': 'Crowdsourced 3D CAD models are becoming easily accessible online, and canpotentially generate an infinite number of training images for almost anyobject category.We show that augmenting the training data of contemporary DeepConvolutional Neural Net (DCNN) models with such synthetic data can beeffective, especially when real training data is limited or not well matched tothe target domain. Most freely available CAD models capture 3D shape but areoften missing other low level cues, such as realistic object texture, pose, orbackground. In a detailed analysis, we use synthetic CAD-rendered images toprobe the ability of DCNN to learn without these cues, with surprisingfindings. In particular, we show that when the DCNN is fine-tuned on the targetdetection task, it exhibits a large degree of invariance to missing low-levelcues, but, when pretrained on generic ImageNet classification, it learns betterwhen the low-level cues are simulated. We show that our synthetic DCNN trainingapproach significantly outperforms previous methods on the PASCAL VOC2007dataset when learning in the few-shot scenario and improves performance in adomain shift scenario on the Office benchmark.',\n",
       "  'categories': ['cs.CV', 'cs.LG', 'cs.NE'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 1278-1286',\n",
       "  'citations': '251',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1412.7122v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=10987351959071838326&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 467: {'ID': 467,\n",
       "  'title': 'Learning Deconvolution Network for Semantic Segmentation',\n",
       "  'authors': ['Seunghoon Hong', 'Hyeonwoo Noh', 'Bohyung Han'],\n",
       "  'published': '2015-05-17T07:33:28Z',\n",
       "  'updated': '2015-05-17T07:33:28Z',\n",
       "  'abstract': 'We propose a novel semantic segmentation algorithm by learning adeconvolution network. We learn the network on top of the convolutional layersadopted from VGG 16-layer net. The deconvolution network is composed ofdeconvolution and unpooling layers, which identify pixel-wise class labels andpredict segmentation masks. We apply the trained network to each proposal in aninput image, and construct the final semantic segmentation map by combining theresults from all proposals in a simple manner. The proposed algorithm mitigatesthe limitations of the existing methods based on fully convolutional networksby integrating deep deconvolution network and proposal-wise prediction; oursegmentation method typically identifies detailed structures and handlesobjects in multiple scales naturally. Our network demonstrates outstandingperformance in PASCAL VOC 2012 dataset, and we achieve the best accuracy(72.5%) among the methods trained with no external data through ensemble withthe fully convolutional network.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 1520-1528',\n",
       "  'citations': '2654',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1505.04366v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=4896002303003783815&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 468: {'ID': 468,\n",
       "  'title': 'Learning to See by Moving',\n",
       "  'authors': ['Jitendra Malik', 'Joao Carreira', 'Pulkit Agrawal'],\n",
       "  'published': '2015-05-07T06:03:01Z',\n",
       "  'updated': '2015-09-14T16:59:36Z',\n",
       "  'abstract': 'The dominant paradigm for feature learning in computer vision relies ontraining neural networks for the task of object recognition using millions ofhand labelled images. Is it possible to learn useful features for a diverse setof visual tasks using any other form of supervision? In biology, livingorganisms developed the ability of visual perception for the purpose of movingand acting in the world. Drawing inspiration from this observation, in thiswork we investigate if the awareness of egomotion can be used as a supervisorysignal for feature learning. As opposed to the knowledge of class labels,information about egomotion is freely available to mobile agents. We show thatgiven the same number of training images, features learnt using egomotion assupervision compare favourably to features learnt using class-label assupervision on visual tasks of scene recognition, object recognition, visualodometry and keypoint matching.',\n",
       "  'categories': ['cs.CV', 'cs.NE', 'cs.RO'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 37-45',\n",
       "  'citations': '384',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1505.01596v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=15713100577731372816&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 469: {'ID': 469,\n",
       "  'title': 'Deep Sliding Shapes for Amodal 3D Object Detection in RGB-D Images',\n",
       "  'authors': ['Shuran Song', 'Jianxiong Xiao'],\n",
       "  'published': '2015-11-07T04:34:18Z',\n",
       "  'updated': '2016-03-09T19:21:23Z',\n",
       "  'abstract': 'We focus on the task of amodal 3D object detection in RGB-D images, whichaims to produce a 3D bounding box of an object in metric form at its fullextent. We introduce Deep Sliding Shapes, a 3D ConvNet formulation that takes a3D volumetric scene from a RGB-D image as input and outputs 3D object boundingboxes. In our approach, we propose the first 3D Region Proposal Network (RPN)to learn objectness from geometric shapes and the first joint ObjectRecognition Network (ORN) to extract geometric features in 3D and colorfeatures in 2D. In particular, we handle objects of various sizes by trainingan amodal RPN at two different scales and an ORN to regress 3D bounding boxes.Experiments show that our algorithm outperforms the state-of-the-art by 13.8 inmAP and is 200x faster than the original Sliding Shapes. All source code andpre-trained models will be available at GitHub.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '396',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1511.02300v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=9144348630084657649&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 470: {'ID': 470,\n",
       "  'title': 'Staple: Complementary Learners for Real-Time Tracking',\n",
       "  'authors': ['Jack Valmadre',\n",
       "   'Luca Bertinetto',\n",
       "   'Stuart Golodetz',\n",
       "   'Ondrej Miksik',\n",
       "   'Philip Torr'],\n",
       "  'published': '2015-12-04T09:56:48Z',\n",
       "  'updated': '2016-04-13T17:58:11Z',\n",
       "  'abstract': 'Correlation Filter-based trackers have recently achieved excellentperformance, showing great robustness to challenging situations exhibitingmotion blur and illumination changes. However, since the model that they learndepends strongly on the spatial layout of the tracked object, they arenotoriously sensitive to deformation. Models based on colour statistics havecomplementary traits: they cope well with variation in shape, but suffer whenillumination is not consistent throughout a sequence. Moreover, colourdistributions alone can be insufficiently discriminative. In this paper, weshow that a simple tracker combining complementary cues in a ridge regressionframework can operate faster than 80 FPS and outperform not only all entries inthe popular VOT14 competition, but also recent and far more sophisticatedtrackers according to multiple benchmarks.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '957',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1512.01355v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=7127186926810478238&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 471: {'ID': 471,\n",
       "  'title': 'Render for CNN: Viewpoint Estimation in Images Using CNNs Trained with  Rendered 3D Model Views',\n",
       "  'authors': ['Yangyan Li', 'Leonidas Guibas', 'Charles R. Qi', 'Hao Su'],\n",
       "  'published': '2015-05-21T08:16:06Z',\n",
       "  'updated': '2015-05-21T08:16:06Z',\n",
       "  'abstract': 'Object viewpoint estimation from 2D images is an essential task in computervision. However, two issues hinder its progress: scarcity of training data withviewpoint annotations, and a lack of powerful features. Inspired by the growingavailability of 3D models, we propose a framework to address both issues bycombining render-based image synthesis and CNNs. We believe that 3D models havethe potential in generating a large number of images of high variation, whichcan be well exploited by deep CNN with a high learning capacity. Towards thisgoal, we propose a scalable and overfit-resistant image synthesis pipeline,together with a novel CNN specifically tailored for the viewpoint estimationtask. Experimentally, we show that the viewpoint estimation from our pipelinecan significantly outperform state-of-the-art methods on PASCAL 3D+ benchmark.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 2686-2694',\n",
       "  'citations': '521',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1505.05641v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=1209553997502402606&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 472: {'ID': 472,\n",
       "  'title': 'LIFT: Learned Invariant Feature Transform',\n",
       "  'authors': ['Kwang Moo Yi',\n",
       "   'Eduard Trulls',\n",
       "   'Vincent Lepetit',\n",
       "   'Pascal Fua'],\n",
       "  'published': '2016-03-30T10:33:18Z',\n",
       "  'updated': '2016-07-29T15:29:39Z',\n",
       "  'abstract': 'We introduce a novel Deep Network architecture that implements the fullfeature point handling pipeline, that is, detection, orientation estimation,and feature description. While previous works have successfully tackled eachone of these problems individually, we show how to learn to do all three in aunified manner while preserving end-to-end differentiability. We thendemonstrate that our Deep pipeline outperforms state-of-the-art methods on anumber of benchmark datasets, without the need of retraining.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (6), 467-483',\n",
       "  'citations': '460',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1603.09114v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=8648671042395507368&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 473: {'ID': 473,\n",
       "  'title': 'Deep CORAL: Correlation Alignment for Deep Domain Adaptation',\n",
       "  'authors': ['Kate Saenko', 'Baochen Sun'],\n",
       "  'published': '2016-07-06T17:35:55Z',\n",
       "  'updated': '2016-07-06T17:35:55Z',\n",
       "  'abstract': 'Deep neural networks are able to learn powerful representations from largequantities of labeled input data, however they cannot always generalize wellacross changes in input distributions. Domain adaptation algorithms have beenproposed to compensate for the degradation in performance due to domain shift.In this paper, we address the case when the target domain is unlabeled,requiring unsupervised adaptation. CORAL is a \"frustratingly easy\" unsuperviseddomain adaptation method that aligns the second-order statistics of the sourceand target distributions with a linear transformation. Here, we extend CORAL tolearn a nonlinear transformation that aligns correlations of layer activationsin deep neural networks (Deep CORAL). Experiments on standard benchmarkdatasets show state-of-the-art performance.',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.LG', 'cs.NE'],\n",
       "  'journal': 'ECCV Workshops (3), 443-450',\n",
       "  'citations': '514',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1607.01719v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=11997819380814894542&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 474: {'ID': 474,\n",
       "  'title': '3D ShapeNets: A Deep Representation for Volumetric Shapes',\n",
       "  'authors': ['Xiaoou Tang',\n",
       "   'Zhirong Wu',\n",
       "   'Aditya Khosla',\n",
       "   'Linguang Zhang',\n",
       "   'Shuran Song',\n",
       "   'Jianxiong Xiao',\n",
       "   'Fisher Yu'],\n",
       "  'published': '2014-06-22T03:31:52Z',\n",
       "  'updated': '2015-04-15T16:46:05Z',\n",
       "  'abstract': \"3D shape is a crucial but heavily underutilized cue in today's computervision systems, mostly due to the lack of a good generic shape representation.With the recent availability of inexpensive 2.5D depth sensors (e.g. MicrosoftKinect), it is becoming increasingly important to have a powerful 3D shaperepresentation in the loop. Apart from category recognition, recovering full 3Dshapes from view-based 2.5D depth maps is also a critical part of visualunderstanding. To this end, we propose to represent a geometric 3D shape as aprobability distribution of binary variables on a 3D voxel grid, using aConvolutional Deep Belief Network. Our model, 3D ShapeNets, learns thedistribution of complex 3D shapes across different object categories andarbitrary poses from raw CAD data, and discovers hierarchical compositionalpart representations automatically. It naturally supports joint objectrecognition and shape completion from 2.5D depth maps, and it enables activeobject recognition through view planning. To train our 3D deep learning model,we construct ModelNet -- a large-scale 3D CAD model dataset. Extensiveexperiments show that our 3D deep representation enables significantperformance improvement over the-state-of-the-arts in a variety of tasks.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '1826',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1406.5670v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=10724931209147252484&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 475: {'ID': 475,\n",
       "  'title': 'Colorful Image Colorization',\n",
       "  'authors': ['Alexei A. Efros', 'Phillip Isola', 'Richard Zhang'],\n",
       "  'published': '2016-03-28T19:58:19Z',\n",
       "  'updated': '2016-10-05T18:01:05Z',\n",
       "  'abstract': 'Given a grayscale photograph as input, this paper attacks the problem ofhallucinating a plausible color version of the photograph. This problem isclearly underconstrained, so previous approaches have either relied onsignificant user interaction or resulted in desaturated colorizations. Wepropose a fully automatic approach that produces vibrant and realisticcolorizations. We embrace the underlying uncertainty of the problem by posingit as a classification task and use class-rebalancing at training time toincrease the diversity of colors in the result. The system is implemented as afeed-forward pass in a CNN at test time and is trained on over a million colorimages. We evaluate our algorithm using a \"colorization Turing test,\" askinghuman participants to choose between a generated and ground truth color image.Our method successfully fools humans on 32% of the trials, significantly higherthan previous methods. Moreover, we show that colorization can be a powerfulpretext task for self-supervised feature learning, acting as a cross-channelencoder. This approach results in state-of-the-art performance on severalfeature learning benchmarks.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (3), 649-666',\n",
       "  'citations': '1090',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1603.08511v5',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=7106912543889350821&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 476: {'ID': 476,\n",
       "  'title': 'Tracking The Untrackable: Learning To Track Multiple Cues with Long-Term  Dependencies',\n",
       "  'authors': ['Alexandre Alahi', 'Silvio Savarese', 'Amir Sadeghian'],\n",
       "  'published': '2017-01-08T03:29:26Z',\n",
       "  'updated': '2017-04-03T21:42:58Z',\n",
       "  'abstract': 'The majority of existing solutions to the Multi-Target Tracking (MTT) problemdo not combine cues in a coherent end-to-end fashion over a long period oftime. However, we present an online method that encodes long-term temporaldependencies across multiple cues. One key challenge of tracking methods is toaccurately track occluded targets or those which share similar appearanceproperties with surrounding objects. To address this challenge, we present astructure of Recurrent Neural Networks (RNN) that jointly reasons on multiplecues over a temporal window. We are able to correct many data associationerrors and recover observations from an occluded state. We demonstrate therobustness of our data-driven approach by tracking multiple targets using theirappearance, motion, and even interactions. Our method outperforms previousworks on multiple publicly available datasets including the challenging MOTbenchmark.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 300-311',\n",
       "  'citations': '260',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1701.01909v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=11696122762972714338&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 477: {'ID': 477,\n",
       "  'title': 'BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks  for Semantic Segmentation',\n",
       "  'authors': ['Jian Sun', 'Jifeng Dai', 'Kaiming He'],\n",
       "  'published': '2015-03-05T14:06:53Z',\n",
       "  'updated': '2015-05-18T09:00:40Z',\n",
       "  'abstract': 'Recent leading approaches to semantic segmentation rely on deep convolutionalnetworks trained with human-annotated, pixel-level segmentation masks. Suchpixel-accurate supervision demands expensive labeling effort and limits theperformance of deep networks that usually benefit from more training data. Inthis paper, we propose a method that achieves competitive accuracy but onlyrequires easily obtained bounding box annotations. The basic idea is to iteratebetween automatically generating region proposals and training convolutionalnetworks. These two steps gradually recover segmentation masks for improvingthe networks, and vise versa. Our method, called BoxSup, produces competitiveresults supervised by boxes only, on par with strong baselines fully supervisedby masks under the same setting. By leveraging a large amount of boundingboxes, BoxSup further unleashes the power of deep convolutional networks andyields state-of-the-art results on PASCAL VOC 2012 and PASCAL-CONTEXT.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 1635-1643',\n",
       "  'citations': '512',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1503.01640v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=10583411756105923851&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 478: {'ID': 478,\n",
       "  'title': 'Unsupervised Learning of Depth and Ego-Motion from Video',\n",
       "  'authors': ['Matthew Brown',\n",
       "   'Tinghui Zhou',\n",
       "   'David G. Lowe',\n",
       "   'Noah Snavely'],\n",
       "  'published': '2017-04-25T17:44:33Z',\n",
       "  'updated': '2017-08-01T02:23:45Z',\n",
       "  'abstract': 'We present an unsupervised learning framework for the task of monocular depthand camera motion estimation from unstructured video sequences. We achieve thisby simultaneously training depth and camera pose estimation networks using thetask of view synthesis as the supervisory signal. The networks are thus coupledvia the view synthesis objective during training, but can be appliedindependently at test time. Empirical evaluation on the KITTI datasetdemonstrates the effectiveness of our approach: 1) monocular depth performingcomparably with supervised methods that use either ground-truth pose or depthfor training, and 2) pose estimation performing favorably with established SLAMsystems under comparable input settings.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '859',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1704.07813v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=11440379944978300844&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 479: {'ID': 479,\n",
       "  'title': 'Squeeze-and-Excitation Networks',\n",
       "  'authors': ['Gang Sun', 'Li Shen', 'Jie Hu', 'Samuel Albanie', 'Enhua Wu'],\n",
       "  'published': '2017-09-05T17:42:13Z',\n",
       "  'updated': '2019-05-16T05:32:17Z',\n",
       "  'abstract': 'The central building block of convolutional neural networks (CNNs) is theconvolution operator, which enables networks to construct informative featuresby fusing both spatial and channel-wise information within local receptivefields at each layer. A broad range of prior research has investigated thespatial component of this relationship, seeking to strengthen therepresentational power of a CNN by enhancing the quality of spatial encodingsthroughout its feature hierarchy. In this work, we focus instead on the channelrelationship and propose a novel architectural unit, which we term the\"Squeeze-and-Excitation\" (SE) block, that adaptively recalibrates channel-wisefeature responses by explicitly modelling interdependencies between channels.We show that these blocks can be stacked together to form SENet architecturesthat generalise extremely effectively across different datasets. We furtherdemonstrate that SE blocks bring significant improvements in performance forexisting state-of-the-art CNNs at slight additional computational cost.Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017classification submission which won first place and reduced the top-5 error to2.251%, surpassing the winning entry of 2016 by a relative improvement of ~25%.Models and code are available at https://github.com/hujie-frank/SENet.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '3314',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1709.01507v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=11424287065250598243&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 480: {'ID': 480,\n",
       "  'title': 'End-to-end Learning of Action Detection from Frame Glimpses in Videos',\n",
       "  'authors': ['Greg Mori', 'Olga Russakovsky', 'Li Fei-Fei', 'Serena Yeung'],\n",
       "  'published': '2015-11-22T09:41:50Z',\n",
       "  'updated': '2017-03-13T07:33:15Z',\n",
       "  'abstract': \"In this work we introduce a fully end-to-end approach for action detection invideos that learns to directly predict the temporal bounds of actions. Ourintuition is that the process of detecting actions is naturally one ofobservation and refinement: observing moments in video, and refining hypothesesabout when an action is occurring. Based on this insight, we formulate ourmodel as a recurrent neural network-based agent that interacts with a videoover time. The agent observes video frames and decides both where to look nextand when to emit a prediction. Since backpropagation is not adequate in thisnon-differentiable setting, we use REINFORCE to learn the agent's decisionpolicy. Our model achieves state-of-the-art results on the THUMOS'14 andActivityNet datasets while observing only a fraction (2% or less) of the videoframes.\",\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '381',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1511.06984v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2818271568652020089&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 481: {'ID': 481,\n",
       "  'title': 'Part-Aligned Bilinear Representations for Person Re-identification',\n",
       "  'authors': ['Tao Mei',\n",
       "   'Jingdong Wang',\n",
       "   'Kyoung Mu Lee',\n",
       "   'Yumin Suh',\n",
       "   'Siyu Tang'],\n",
       "  'published': '2018-04-19T11:35:19Z',\n",
       "  'updated': '2018-04-19T11:35:19Z',\n",
       "  'abstract': 'We propose a novel network that learns a part-aligned representation forperson re-identification. It handles the body part misalignment problem, thatis, body parts are misaligned across human detections due to pose/viewpointchange and unreliable detection. Our model consists of a two-stream network(one stream for appearance map extraction and the other one for body part mapextraction) and a bilinear-pooling layer that generates and spatially pools apart-aligned map. Each local feature of the part-aligned map is obtained by abilinear mapping of the corresponding local appearance and body partdescriptors. Our new representation leads to a robust image matchingsimilarity, which is equivalent to an aggregation of the local similarities ofthe corresponding body parts combined with the weighted appearance similarity.This part-aligned representation reduces the part misalignment problemsignificantly. Our approach is also advantageous over other pose-guidedrepresentations (e.g., extracting representations over the bounding box of eachbody part) by learning part descriptors optimal for person re-identification.For training the network, our approach does not require any part annotation onthe person re-identification dataset. Instead, we simply initialize the partsub-stream using a pre-trained sub-network of an existing pose estimationnetwork, and train the whole network to minimize the re-identification loss. Wevalidate the effectiveness of our approach by demonstrating its superiorityover the state-of-the-art methods on the standard benchmark datasets, includingMarket-1501, CUHK03, CUHK01 and DukeMTMC, and standard video dataset MARS.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 402-419',\n",
       "  'citations': '149',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1804.07094v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=7341079685534342246&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 482: {'ID': 482,\n",
       "  'title': 'Fine-tuning CNN Image Retrieval with No Human Annotation',\n",
       "  'authors': ['Ondřej Chum', 'Filip Radenović', 'Giorgos Tolias'],\n",
       "  'published': '2017-11-03T21:29:55Z',\n",
       "  'updated': '2018-07-10T05:25:34Z',\n",
       "  'abstract': 'Image descriptors based on activations of Convolutional Neural Networks(CNNs) have become dominant in image retrieval due to their discriminativepower, compactness of representation, and search efficiency. Training of CNNs,either from scratch or fine-tuning, requires a large amount of annotated data,where a high quality of annotation is often crucial. In this work, we proposeto fine-tune CNNs for image retrieval on a large collection of unordered imagesin a fully automated manner. Reconstructed 3D models obtained by thestate-of-the-art retrieval and structure-from-motion methods guide theselection of the training data. We show that both hard-positive andhard-negative examples, selected by exploiting the geometry and the camerapositions available from the 3D models, enhance the performance ofparticular-object retrieval. CNN descriptor whitening discriminatively learnedfrom the same training data outperforms commonly used PCA whitening. We proposea novel trainable Generalized-Mean (GeM) pooling layer that generalizes max andaverage pooling and show that it boosts retrieval performance. Applying theproposed method to the VGG network achieves state-of-the-art performance on thestandard benchmarks: Oxford Buildings, Paris, and Holidays datasets.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 41 (7), 1655-1668',\n",
       "  'citations': '199',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1711.02512v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=3378547499304586925&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 483: {'ID': 483,\n",
       "  'title': 'Top-down Neural Attention by Excitation Backprop',\n",
       "  'authors': ['Jonathan Brandt',\n",
       "   'Jianming Zhang',\n",
       "   'Xiaohui Shen',\n",
       "   'Stan Sclaroff',\n",
       "   'Zhe Lin'],\n",
       "  'published': '2016-08-01T17:49:57Z',\n",
       "  'updated': '2016-08-01T17:49:57Z',\n",
       "  'abstract': 'We aim to model the top-down attention of a Convolutional Neural Network(CNN) classifier for generating task-specific attention maps. Inspired by atop-down human visual attention model, we propose a new backpropagation scheme,called Excitation Backprop, to pass along top-down signals downwards in thenetwork hierarchy via a probabilistic Winner-Take-All process. Furthermore, weintroduce the concept of contrastive attention to make the top-down attentionmaps more discriminative. In experiments, we demonstrate the accuracy andgeneralizability of our method in weakly supervised localization tasks on theMS COCO, PASCAL VOC07 and ImageNet datasets. The usefulness of our method isfurther validated in the text-to-region association task. On the Flickr30kEntities dataset, we achieve promising performance in phrase localization byleveraging the top-down attention of a CNN model that has been trained onweakly labeled web images.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (4), 543-559',\n",
       "  'citations': '316',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1608.00507v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=10241180141320929341&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 484: {'ID': 484,\n",
       "  'title': 'PointNet: Deep Learning on Point Sets for 3D Classification and  Segmentation',\n",
       "  'authors': ['Leonidas J. Guibas', 'Kaichun Mo', 'Charles R. Qi', 'Hao Su'],\n",
       "  'published': '2016-12-02T08:40:40Z',\n",
       "  'updated': '2017-04-10T22:25:25Z',\n",
       "  'abstract': 'Point cloud is an important type of geometric data structure. Due to itsirregular format, most researchers transform such data to regular 3D voxelgrids or collections of images. This, however, renders data unnecessarilyvoluminous and causes issues. In this paper, we design a novel type of neuralnetwork that directly consumes point clouds and well respects the permutationinvariance of points in the input. Our network, named PointNet, provides aunified architecture for applications ranging from object classification, partsegmentation, to scene semantic parsing. Though simple, PointNet is highlyefficient and effective. Empirically, it shows strong performance on par oreven better than state of the art. Theoretically, we provide analysis towardsunderstanding of what the network has learnt and why the network is robust withrespect to input perturbation and corruption.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '2512',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1612.00593v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=1958213547177282284&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 485: {'ID': 485,\n",
       "  'title': 'StarGAN: Unified Generative Adversarial Networks for Multi-Domain  Image-to-Image Translation',\n",
       "  'authors': ['Yunjey Choi',\n",
       "   'Minje Choi',\n",
       "   'Munyoung Kim',\n",
       "   'Jung-Woo Ha',\n",
       "   'Jaegul Choo',\n",
       "   'Sunghun Kim'],\n",
       "  'published': '2017-11-24T15:37:30Z',\n",
       "  'updated': '2018-09-21T08:17:49Z',\n",
       "  'abstract': \"Recent studies have shown remarkable success in image-to-image translationfor two domains. However, existing approaches have limited scalability androbustness in handling more than two domains, since different models should bebuilt independently for every pair of image domains. To address thislimitation, we propose StarGAN, a novel and scalable approach that can performimage-to-image translations for multiple domains using only a single model.Such a unified model architecture of StarGAN allows simultaneous training ofmultiple datasets with different domains within a single network. This leads toStarGAN's superior quality of translated images compared to existing models aswell as the novel capability of flexibly translating an input image to anydesired target domain. We empirically demonstrate the effectiveness of ourapproach on a facial attribute transfer and a facial expression synthesistasks.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '964',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1711.09020v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=15940143923298105219&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 486: {'ID': 486,\n",
       "  'title': 'Accelerating the Super-Resolution Convolutional Neural Network',\n",
       "  'authors': ['Xiaoou Tang', 'Chao Dong', 'Chen Change Loy'],\n",
       "  'published': '2016-08-01T09:44:41Z',\n",
       "  'updated': '2016-08-01T09:44:41Z',\n",
       "  'abstract': 'As a successful deep model applied in image super-resolution (SR), theSuper-Resolution Convolutional Neural Network (SRCNN) has demonstrated superiorperformance to the previous hand-crafted models either in speed and restorationquality. However, the high computational cost still hinders it from practicalusage that demands real-time performance (24 fps). In this paper, we aim ataccelerating the current SRCNN, and propose a compact hourglass-shape CNNstructure for faster and better SR. We re-design the SRCNN structure mainly inthree aspects. First, we introduce a deconvolution layer at the end of thenetwork, then the mapping is learned directly from the original low-resolutionimage (without interpolation) to the high-resolution one. Second, wereformulate the mapping layer by shrinking the input feature dimension beforemapping and expanding back afterwards. Third, we adopt smaller filter sizes butmore mapping layers. The proposed model achieves a speed up of more than 40times with even superior restoration quality. Further, we present the parametersettings that can achieve real-time performance on a generic CPU while stillmaintaining good performance. A corresponding transfer strategy is alsoproposed for fast training and testing across different upscaling factors.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (2), 391-407',\n",
       "  'citations': '962',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1608.00367v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14172147191910078930&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 487: {'ID': 487,\n",
       "  'title': 'Convolutional neural network architecture for geometric matching',\n",
       "  'authors': ['Relja Arandjelović', 'Ignacio Rocco', 'Josef Sivic'],\n",
       "  'published': '2017-03-16T13:03:54Z',\n",
       "  'updated': '2017-04-13T22:32:43Z',\n",
       "  'abstract': 'We address the problem of determining correspondences between two images inagreement with a geometric model such as an affine or thin-plate splinetransformation, and estimating its parameters. The contributions of this workare three-fold. First, we propose a convolutional neural network architecturefor geometric matching. The architecture is based on three main components thatmimic the standard steps of feature extraction, matching and simultaneousinlier detection and model parameter estimation, while being trainableend-to-end. Second, we demonstrate that the network parameters can be trainedfrom synthetically generated imagery without the need for manual annotation andthat our matching layer significantly increases generalization capabilities tonever seen before images. Finally, we show that the same model can perform bothinstance-level and category-level matching giving state-of-the-art results onthe challenging Proposal Flow dataset.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 41 (11), 2553\\xa0…',\n",
       "  'citations': '188',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1703.05593v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=8324362856621568563&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 488: {'ID': 488,\n",
       "  'title': 'Deeply-Learned Part-Aligned Representations for Person Re-Identification',\n",
       "  'authors': ['Xi Li', 'Liming Zhao', 'Jingdong Wang', 'Yueting Zhuang'],\n",
       "  'published': '2017-07-23T07:25:07Z',\n",
       "  'updated': '2017-07-23T07:25:07Z',\n",
       "  'abstract': 'In this paper, we address the problem of person re-identification, whichrefers to associating the persons captured from different cameras. We propose asimple yet effective human part-aligned representation for handling the bodypart misalignment problem. Our approach decomposes the human body into regions(parts) which are discriminative for person matching, accordingly computes therepresentations over the regions, and aggregates the similarities computedbetween the corresponding regions of a pair of probe and gallery images as theoverall matching score. Our formulation, inspired by attention models, is adeep neural network modeling the three steps together, which is learnt throughminimizing the triplet loss function without requiring body part labelinginformation. Unlike most existing deep learning algorithms that learn a globalor spatial partition-based local representation, our approach performs humanbody partition, and thus is more robust to pose changes and various humanspatial distributions in the person bounding box. Our approach showsstate-of-the-art results over standard datasets, Market-$1501$, CUHK$03$,CUHK$01$ and VIPeR.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 3219-3228',\n",
       "  'citations': '390',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1707.07256v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=959453658463980412&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 489: {'ID': 489,\n",
       "  'title': 'Progressive Neural Architecture Search',\n",
       "  'authors': ['Li Fei-Fei',\n",
       "   'Kevin Murphy',\n",
       "   'Chenxi Liu',\n",
       "   'Jonathan Huang',\n",
       "   'Alan Yuille',\n",
       "   'Wei Hua',\n",
       "   'Barret Zoph',\n",
       "   'Maxim Neumann',\n",
       "   'Li-Jia Li',\n",
       "   'Jonathon Shlens'],\n",
       "  'published': '2017-12-02T06:23:16Z',\n",
       "  'updated': '2018-07-26T19:51:26Z',\n",
       "  'abstract': 'We propose a new method for learning the structure of convolutional neuralnetworks (CNNs) that is more efficient than recent state-of-the-art methodsbased on reinforcement learning and evolutionary algorithms. Our approach usesa sequential model-based optimization (SMBO) strategy, in which we search forstructures in order of increasing complexity, while simultaneously learning asurrogate model to guide the search through structure space. Direct comparisonunder the same search space shows that our method is up to 5 times moreefficient than the RL method of Zoph et al. (2018) in terms of number of modelsevaluated, and 8 times faster in terms of total compute. The structures wediscover in this way achieve state of the art classification accuracies onCIFAR-10 and ImageNet.',\n",
       "  'categories': ['cs.CV', 'cs.LG', 'stat.ML'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 19-34',\n",
       "  'citations': '612',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1712.00559v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2934282391900627386&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 490: {'ID': 490,\n",
       "  'title': 'Beyond triplet loss: a deep quadruplet network for person  re-identification',\n",
       "  'authors': ['Xiaotang Chen', 'Weihua Chen', 'Kaiqi Huang', 'Jianguo Zhang'],\n",
       "  'published': '2017-04-06T06:09:55Z',\n",
       "  'updated': '2017-04-06T06:09:55Z',\n",
       "  'abstract': 'Person re-identification (ReID) is an important task in wide area videosurveillance which focuses on identifying people across different cameras.Recently, deep learning networks with a triplet loss become a common frameworkfor person ReID. However, the triplet loss pays main attentions on obtainingcorrect orders on the training set. It still suffers from a weakergeneralization capability from the training set to the testing set, thusresulting in inferior performance. In this paper, we design a quadruplet loss,which can lead to the model output with a larger inter-class variation and asmaller intra-class variation compared to the triplet loss. As a result, ourmodel has a better generalization ability and can achieve a higher performanceon the testing set. In particular, a quadruplet deep network using amargin-based online hard negative mining is proposed based on the quadrupletloss for the person ReID. In extensive experiments, the proposed networkoutperforms most of the state-of-the-art algorithms on representative datasetswhich clearly demonstrates the effectiveness of our proposed method.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '519',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1704.01719v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=15352923940543802650&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 491: {'ID': 491,\n",
       "  'title': 'Describing Videos by Exploiting Temporal Structure',\n",
       "  'authors': ['Christopher Pal',\n",
       "   'Hugo Larochelle',\n",
       "   'Kyunghyun Cho',\n",
       "   'Atousa Torabi',\n",
       "   'Aaron Courville',\n",
       "   'Nicolas Ballas',\n",
       "   'Li Yao'],\n",
       "  'published': '2015-02-27T19:30:40Z',\n",
       "  'updated': '2015-10-01T00:12:46Z',\n",
       "  'abstract': 'Recent progress in using recurrent neural networks (RNNs) for imagedescription has motivated the exploration of their application for videodescription. However, while images are static, working with videos requiresmodeling their dynamic temporal structure and then properly integrating thatinformation into a natural language description. In this context, we propose anapproach that successfully takes into account both the local and globaltemporal structure of videos to produce descriptions. First, our approachincorporates a spatial temporal 3-D convolutional neural network (3-D CNN)representation of the short temporal dynamics. The 3-D CNN representation istrained on video action recognition tasks, so as to produce a representationthat is tuned to human motion and behavior. Second we propose a temporalattention mechanism that allows to go beyond local temporal modeling and learnsto automatically select the most relevant temporal segments given thetext-generating RNN. Our approach exceeds the current state-of-art for bothBLEU and METEOR metrics on the Youtube2Text dataset. We also present results ona new, larger and more challenging dataset of paired video and natural languagedescriptions.',\n",
       "  'categories': ['stat.ML', 'cs.AI', 'cs.CL', 'cs.CV', 'cs.LG'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 4507-4515',\n",
       "  'citations': '731',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1502.08029v5',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=17225606232504528023&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 492: {'ID': 492,\n",
       "  'title': 'HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis',\n",
       "  'authors': ['Lu Sheng',\n",
       "   'Xihui Liu',\n",
       "   'Junjie Yan',\n",
       "   'Jing Shao',\n",
       "   'Xiaogang Wang',\n",
       "   'Haiyu Zhao',\n",
       "   'Maoqing Tian',\n",
       "   'Shuai Yi'],\n",
       "  'published': '2017-09-28T13:06:55Z',\n",
       "  'updated': '2017-09-28T13:06:55Z',\n",
       "  'abstract': 'Pedestrian analysis plays a vital role in intelligent video surveillance andis a key component for security-centric computer vision systems. Despite thatthe convolutional neural networks are remarkable in learning discriminativefeatures from images, the learning of comprehensive features of pedestrians forfine-grained tasks remains an open problem. In this study, we propose a newattention-based deep neural network, named as HydraPlus-Net (HP-net), thatmulti-directionally feeds the multi-level attention maps to different featurelayers. The attentive deep features learned from the proposed HP-net bringunique advantages: (1) the model is capable of capturing multiple attentionsfrom low-level to semantic-level, and (2) it explores the multi-scaleselectiveness of attentive features to enrich the final feature representationsfor a pedestrian image. We demonstrate the effectiveness and generality of theproposed HP-net for pedestrian analysis on two tasks, i.e. pedestrian attributerecognition and person re-identification. Intensive experimental results havebeen provided to prove that the HP-net outperforms the state-of-the-art methodson various datasets.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 350-359',\n",
       "  'citations': '217',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1709.09930v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=221889866849508274&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 493: {'ID': 493,\n",
       "  'title': 'ArcFace: Additive Angular Margin Loss for Deep Face Recognition',\n",
       "  'authors': ['Jiankang Deng', 'Stefanos Zafeiriou', 'Niannan Xue', 'Jia Guo'],\n",
       "  'published': '2018-01-23T18:39:19Z',\n",
       "  'updated': '2019-02-09T15:14:58Z',\n",
       "  'abstract': 'One of the main challenges in feature learning using Deep ConvolutionalNeural Networks (DCNNs) for large-scale face recognition is the design ofappropriate loss functions that enhance discriminative power. Centre losspenalises the distance between the deep features and their corresponding classcentres in the Euclidean space to achieve intra-class compactness. SphereFaceassumes that the linear transformation matrix in the last fully connected layercan be used as a representation of the class centres in an angular space andpenalises the angles between the deep features and their corresponding weightsin a multiplicative way. Recently, a popular line of research is to incorporatemargins in well-established loss functions in order to maximise face classseparability. In this paper, we propose an Additive Angular Margin Loss(ArcFace) to obtain highly discriminative features for face recognition. Theproposed ArcFace has a clear geometric interpretation due to the exactcorrespondence to the geodesic distance on the hypersphere. We present arguablythe most extensive experimental evaluation of all the recent state-of-the-artface recognition methods on over 10 face recognition benchmarks including a newlarge-scale image database with trillion level of pairs and a large-scale videodataset. We show that ArcFace consistently outperforms the state-of-the-art andcan be easily implemented with negligible computational overhead. We releaseall refined training data, training codes, pre-trained models and traininglogs, which will help reproduce the results in this paper.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '644',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1801.07698v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14066082468781799933&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 494: {'ID': 494,\n",
       "  'title': 'Generative Image Inpainting with Contextual Attention',\n",
       "  'authors': ['Xin Lu',\n",
       "   'Xiaohui Shen',\n",
       "   'Zhe Lin',\n",
       "   'Jiahui Yu',\n",
       "   'Jimei Yang',\n",
       "   'Thomas S. Huang'],\n",
       "  'published': '2018-01-24T08:04:55Z',\n",
       "  'updated': '2018-03-21T21:46:22Z',\n",
       "  'abstract': 'Recent deep learning based approaches have shown promising results for thechallenging task of inpainting large missing regions in an image. These methodscan generate visually plausible image structures and textures, but often createdistorted structures or blurry textures inconsistent with surrounding areas.This is mainly due to ineffectiveness of convolutional neural networks inexplicitly borrowing or copying information from distant spatial locations. Onthe other hand, traditional texture and patch synthesis approaches areparticularly suitable when it needs to borrow textures from the surroundingregions. Motivated by these observations, we propose a new deep generativemodel-based approach which can not only synthesize novel image structures butalso explicitly utilize surrounding image features as references during networktraining to make better predictions. The model is a feed-forward, fullyconvolutional neural network which can process images with multiple holes atarbitrary locations and with variable sizes during the test time. Experimentson multiple datasets including faces (CelebA, CelebA-HQ), textures (DTD) andnatural images (ImageNet, Places2) demonstrate that our proposed approachgenerates higher-quality inpainting results than existing ones. Code, demo andmodels are available at: https://github.com/JiahuiYu/generative_inpainting.',\n",
       "  'categories': ['cs.CV', 'cs.GR'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '476',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1801.07892v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=11541279513087894235&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 495: {'ID': 495,\n",
       "  'title': 'Effective Face Frontalization in Unconstrained Images',\n",
       "  'authors': ['Tal Hassner', 'Eran Paz', 'Shai Harel', 'Roee Enbar'],\n",
       "  'published': '2014-11-28T18:22:56Z',\n",
       "  'updated': '2014-11-28T18:22:56Z',\n",
       "  'abstract': '\"Frontalization\" is the process of synthesizing frontal facing views of facesappearing in single unconstrained photos. Recent reports have suggested thatthis process may substantially boost the performance of face recognitionsystems. This, by transforming the challenging problem of recognizing facesviewed from unconstrained viewpoints to the easier problem of recognizing facesin constrained, forward facing poses. Previous frontalization methods did thisby attempting to approximate 3D facial shapes for each query image. We observethat 3D face shape estimation from unconstrained photos may be a harder problemthan frontalization and can potentially introduce facial misalignments.Instead, we explore the simpler approach of using a single, unmodified, 3Dsurface as an approximation to the shape of all input faces. We show that thisleads to a straightforward, efficient and easy to implement method forfrontalization. More importantly, it produces aesthetic new frontal views andis surprisingly effective when used for face recognition and gender estimation.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '469',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1411.7964v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=7708865079910174306&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 496: {'ID': 496,\n",
       "  'title': 'Convolutional Pose Machines',\n",
       "  'authors': ['Yaser Sheikh',\n",
       "   'Takeo Kanade',\n",
       "   'Shih-En Wei',\n",
       "   'Varun Ramakrishna'],\n",
       "  'published': '2016-01-30T16:15:28Z',\n",
       "  'updated': '2016-04-12T03:31:53Z',\n",
       "  'abstract': 'Pose Machines provide a sequential prediction framework for learning richimplicit spatial models. In this work we show a systematic design for howconvolutional networks can be incorporated into the pose machine framework forlearning image features and image-dependent spatial models for the task of poseestimation. The contribution of this paper is to implicitly model long-rangedependencies between variables in structured prediction tasks such asarticulated pose estimation. We achieve this by designing a sequentialarchitecture composed of convolutional networks that directly operate on beliefmaps from previous stages, producing increasingly refined estimates for partlocations, without the need for explicit graphical model-style inference. Ourapproach addresses the characteristic difficulty of vanishing gradients duringtraining by providing a natural learning objective function that enforcesintermediate supervision, thereby replenishing back-propagated gradients andconditioning the learning procedure. We demonstrate state-of-the-artperformance and outperform competing methods on standard benchmarks includingthe MPII, LSP, and FLIC datasets.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '1501',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1602.00134v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=16225467191686938474&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 497: {'ID': 497,\n",
       "  'title': 'Learning to Compare: Relation Network for Few-Shot Learning',\n",
       "  'authors': ['Philip H. S. Torr',\n",
       "   'Tao Xiang',\n",
       "   'Li Zhang',\n",
       "   'Yongxin Yang',\n",
       "   'Flood Sung',\n",
       "   'Timothy M. Hospedales'],\n",
       "  'published': '2017-11-16T11:01:51Z',\n",
       "  'updated': '2018-03-27T10:55:50Z',\n",
       "  'abstract': 'We present a conceptually simple, flexible, and general framework forfew-shot learning, where a classifier must learn to recognise new classes givenonly few examples from each. Our method, called the Relation Network (RN), istrained end-to-end from scratch. During meta-learning, it learns to learn adeep distance metric to compare a small number of images within episodes, eachof which is designed to simulate the few-shot setting. Once trained, a RN isable to classify images of new classes by computing relation scores betweenquery images and the few examples of each new class without further updatingthe network. Besides providing improved performance on few-shot learning, ourframework is easily extended to zero-shot learning. Extensive experiments onfive benchmarks demonstrate that our simple approach provides a unified andeffective approach for both of these two tasks.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '625',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1711.06025v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=8676534722509316447&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 498: {'ID': 498,\n",
       "  'title': 'HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale  Visual Recognition',\n",
       "  'authors': ['Robinson Piramuthu',\n",
       "   'Dennis DeCoste',\n",
       "   'Zhicheng Yan',\n",
       "   'Hao Zhang',\n",
       "   'Wei Di',\n",
       "   'Vignesh Jagadeesh',\n",
       "   'Yizhou Yu'],\n",
       "  'published': '2014-10-03T01:17:20Z',\n",
       "  'updated': '2015-05-16T03:36:32Z',\n",
       "  'abstract': 'In image classification, visual separability between different objectcategories is highly uneven, and some categories are more difficult todistinguish than others. Such difficult categories demand more dedicatedclassifiers. However, existing deep convolutional neural networks (CNN) aretrained as flat N-way classifiers, and few efforts have been made to leveragethe hierarchical structure of categories. In this paper, we introducehierarchical deep CNNs (HD-CNNs) by embedding deep CNNs into a categoryhierarchy. An HD-CNN separates easy classes using a coarse category classifierwhile distinguishing difficult classes using fine category classifiers. DuringHD-CNN training, component-wise pretraining is followed by global finetuningwith a multinomial logistic loss regularized by a coarse category consistencyterm. In addition, conditional executions of fine category classifiers andlayer parameter compression make HD-CNNs scalable for large-scale visualrecognition. We achieve state-of-the-art results on both CIFAR100 andlarge-scale ImageNet 1000-class benchmark datasets. In our experiments, webuild up three different HD-CNNs and they lower the top-1 error of the standardCNNs by 2.65%, 3.1% and 1.1%, respectively.',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.LG', 'cs.NE', 'stat.ML'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 2740-2748',\n",
       "  'citations': '197',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1410.0736v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=8575196851568618448&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 499: {'ID': 499,\n",
       "  'title': 'From Captions to Visual Concepts and Back',\n",
       "  'authors': ['Jianfeng Gao',\n",
       "   'Li Deng',\n",
       "   'John C. Platt',\n",
       "   'Margaret Mitchell',\n",
       "   'C. Lawrence Zitnick',\n",
       "   'Piotr Dollár',\n",
       "   'Hao Fang',\n",
       "   'Forrest Iandola',\n",
       "   'Geoffrey Zweig',\n",
       "   'Saurabh Gupta',\n",
       "   'Xiaodong He',\n",
       "   'Rupesh Srivastava'],\n",
       "  'published': '2014-11-18T18:23:45Z',\n",
       "  'updated': '2015-04-14T18:05:07Z',\n",
       "  'abstract': 'This paper presents a novel approach for automatically generating imagedescriptions: visual detectors, language models, and multimodal similaritymodels learnt directly from a dataset of image captions. We use multipleinstance learning to train visual detectors for words that commonly occur incaptions, including many different parts of speech such as nouns, verbs, andadjectives. The word detector outputs serve as conditional inputs to amaximum-entropy language model. The language model learns from a set of over400,000 image descriptions to capture the statistics of word usage. We captureglobal semantics by re-ranking caption candidates using sentence-level featuresand a deep multimodal similarity model. Our system is state-of-the-art on theofficial Microsoft COCO benchmark, producing a BLEU-4 score of 29.1%. Whenhuman judges compare the system captions to ones written by other people on ourheld-out test set, the system captions have equal or better quality 34% of thetime.',\n",
       "  'categories': ['cs.CV', 'cs.CL'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '959',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1411.4952v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=1678451061419717601&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 500: {'ID': 500,\n",
       "  'title': 'Combining Markov Random Fields and Convolutional Neural Networks for  Image Synthesis',\n",
       "  'authors': ['Chuan Li', 'Michael Wand'],\n",
       "  'published': '2016-01-18T16:31:37Z',\n",
       "  'updated': '2016-01-18T16:31:37Z',\n",
       "  'abstract': 'This paper studies a combination of generative Markov random field (MRF)models and discriminatively trained deep convolutional neural networks (dCNNs)for synthesizing 2D images. The generative MRF acts on higher-levels of a dCNNfeature pyramid, controling the image layout at an abstract level. We apply themethod to both photographic and non-photo-realistic (artwork) synthesis tasks.The MRF regularizer prevents over-excitation artifacts and reduces implausiblefeature mixtures common to previous dCNN inversion approaches, permittingsynthezing photographic content with increased visual plausibility. Unlikestandard MRF-based texture synthesis, the combined system can both match andadapt local features with considerable variability, yielding results far out ofreach of classic generative MRF methods.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '386',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1601.04589v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14693487110597042194&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 501: {'ID': 501,\n",
       "  'title': 'Deep Neural Networks are Easily Fooled: High Confidence Predictions for  Unrecognizable Images',\n",
       "  'authors': ['Anh Nguyen', 'Jason Yosinski', 'Jeff Clune'],\n",
       "  'published': '2014-12-05T05:29:43Z',\n",
       "  'updated': '2015-04-02T23:12:56Z',\n",
       "  'abstract': 'Deep neural networks (DNNs) have recently been achieving state-of-the-artperformance on a variety of pattern-recognition tasks, most notably visualclassification problems. Given that DNNs are now able to classify objects inimages with near-human-level performance, questions naturally arise as to whatdifferences remain between computer and human vision. A recent study revealedthat changing an image (e.g. of a lion) in a way imperceptible to humans cancause a DNN to label the image as something else entirely (e.g. mislabeling alion a library). Here we show a related result: it is easy to produce imagesthat are completely unrecognizable to humans, but that state-of-the-art DNNsbelieve to be recognizable objects with 99.99% confidence (e.g. labeling withcertainty that white noise static is a lion). Specifically, we takeconvolutional neural networks trained to perform well on either the ImageNet orMNIST datasets and then find images with evolutionary algorithms or gradientascent that DNNs label with high confidence as belonging to each dataset class.It is possible to produce images totally unrecognizable to human eyes that DNNsbelieve with near certainty are familiar objects, which we call \"foolingimages\" (more generally, fooling examples). Our results shed light oninteresting differences between human vision and current DNNs, and raisequestions about the generality of DNN computer vision.',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.NE'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '1753',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1412.1897v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=3471582876361710884&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 502: {'ID': 502,\n",
       "  'title': 'Revisiting Visual Question Answering Baselines',\n",
       "  'authors': ['Laurens van der Maaten', 'Armand Joulin', 'Allan Jabri'],\n",
       "  'published': '2016-06-27T18:07:58Z',\n",
       "  'updated': '2016-11-22T21:26:06Z',\n",
       "  'abstract': 'Visual question answering (VQA) is an interesting learning setting forevaluating the abilities and shortcomings of current systems for imageunderstanding. Many of the recently proposed VQA systems include attention ormemory mechanisms designed to support \"reasoning\". For multiple-choice VQA,nearly all of these systems train a multi-class classifier on image andquestion features to predict an answer. This paper questions the value of thesecommon practices and develops a simple alternative model based on binaryclassification. Instead of treating answers as competing choices, our modelreceives the answer as input and predicts whether or not animage-question-answer triplet is correct. We evaluate our model on the Visual7WTelling and the VQA Real Multiple Choice tasks, and find that even simpleversions of our model perform competitively. Our best model achievesstate-of-the-art performance on the Visual7W Telling task and comparessurprisingly well with the most complex systems proposed for the VQA RealMultiple Choice task. We explore variants of the model and study itstransferability between both datasets. We also present an error analysis of ourmodel that suggests a key problem of current VQA systems lies in the lack ofvisual grounding of concepts that occur in the questions and answers. Overall,our results suggest that the performance of current VQA systems is notsignificantly better than that of systems designed to exploit dataset biases.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (8), 727-739',\n",
       "  'citations': '182',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1606.08390v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=1258687269261432931&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 503: {'ID': 503,\n",
       "  'title': 'Simultaneous Feature Learning and Hash Coding with Deep Neural Networks',\n",
       "  'authors': ['Shuicheng Yan', 'Ye Liu', 'Yan Pan', 'Hanjiang Lai'],\n",
       "  'published': '2015-04-14T03:14:24Z',\n",
       "  'updated': '2015-04-14T03:14:24Z',\n",
       "  'abstract': 'Similarity-preserving hashing is a widely-used method for nearest neighboursearch in large-scale image retrieval tasks. For most existing hashing methods,an image is first encoded as a vector of hand-engineering visual features,followed by another separate projection or quantization step that generatesbinary codes. However, such visual feature vectors may not be optimallycompatible with the coding process, thus producing sub-optimal hashing codes.In this paper, we propose a deep architecture for supervised hashing, in whichimages are mapped into binary codes via carefully designed deep neuralnetworks. The pipeline of the proposed deep architecture consists of threebuilding blocks: 1) a sub-network with a stack of convolution layers to producethe effective intermediate image features; 2) a divide-and-encode module todivide the intermediate image features into multiple branches, each encodedinto one hash bit; and 3) a triplet ranking loss designed to characterize thatone image is more similar to the second image than to the third one. Extensiveevaluations on several benchmark image datasets show that the proposedsimultaneous feature learning and hash coding pipeline brings substantialimprovements over other state-of-the-art supervised or unsupervised hashingmethods.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '558',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1504.03410v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=17482208703764341924&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 504: {'ID': 504,\n",
       "  'title': 'Unsupervised Learning of Visual Representations using Videos',\n",
       "  'authors': ['Abhinav Gupta', 'Xiaolong Wang'],\n",
       "  'published': '2015-05-04T15:50:53Z',\n",
       "  'updated': '2015-10-06T17:05:49Z',\n",
       "  'abstract': 'Is strong supervision necessary for learning a good visual representation? Dowe really need millions of semantically-labeled images to train a ConvolutionalNeural Network (CNN)? In this paper, we present a simple yet surprisinglypowerful approach for unsupervised learning of CNN. Specifically, we usehundreds of thousands of unlabeled videos from the web to learn visualrepresentations. Our key idea is that visual tracking provides the supervision.That is, two patches connected by a track should have similar visualrepresentation in deep feature space since they probably belong to the sameobject or object part. We design a Siamese-triplet network with a ranking lossfunction to train this CNN representation. Without using a single image fromImageNet, just using 100K unlabeled videos and the VOC 2012 dataset, we trainan ensemble of unsupervised networks that achieves 52% mAP (no bounding boxregression). This performance comes tantalizingly close to itsImageNet-supervised counterpart, an ensemble which achieves a mAP of 54.4%. Wealso show that our unsupervised network can perform competitively in othertasks such as surface-normal estimation.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 2794-2802',\n",
       "  'citations': '651',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1505.00687v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=15788792053693868696&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 505: {'ID': 505,\n",
       "  'title': 'EnhanceNet: Single Image Super-Resolution Through Automated Texture  Synthesis',\n",
       "  'authors': ['Mehdi S. M. Sajjadi', 'Bernhard Schölkopf', 'Michael Hirsch'],\n",
       "  'published': '2016-12-23T10:16:26Z',\n",
       "  'updated': '2017-07-30T21:52:23Z',\n",
       "  'abstract': 'Single image super-resolution is the task of inferring a high-resolutionimage from a single low-resolution input. Traditionally, the performance ofalgorithms for this task is measured using pixel-wise reconstruction measuressuch as peak signal-to-noise ratio (PSNR) which have been shown to correlatepoorly with the human perception of image quality. As a result, algorithmsminimizing these metrics tend to produce over-smoothed images that lackhigh-frequency textures and do not look natural despite yielding high PSNRvalues.  We propose a novel application of automated texture synthesis in combinationwith a perceptual loss focusing on creating realistic textures rather thanoptimizing for a pixel-accurate reproduction of ground truth images duringtraining. By using feed-forward fully convolutional neural networks in anadversarial training setting, we achieve a significant boost in image qualityat high magnification ratios. Extensive experiments on a number of datasetsshow the effectiveness of our approach, yielding state-of-the-art results inboth quantitative and qualitative benchmarks.',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'stat.ML'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 4491-4500',\n",
       "  'citations': '375',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1612.07919v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=308024524496163151&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 506: {'ID': 506,\n",
       "  'title': 'Weakly Supervised Object Localization with Multi-fold Multiple Instance  Learning',\n",
       "  'authors': ['Cordelia Schmid', 'Ramazan Gokberk Cinbis', 'Jakob Verbeek'],\n",
       "  'published': '2015-03-03T14:06:02Z',\n",
       "  'updated': '2016-02-22T20:26:43Z',\n",
       "  'abstract': 'Object category localization is a challenging problem in computer vision.Standard supervised training requires bounding box annotations of objectinstances. This time-consuming annotation process is sidestepped in weaklysupervised learning. In this case, the supervised information is restricted tobinary labels that indicate the absence/presence of object instances in theimage, without their locations. We follow a multiple-instance learning approachthat iteratively trains the detector and infers the object locations in thepositive training images. Our main contribution is a multi-fold multipleinstance learning procedure, which prevents training from prematurely lockingonto erroneous object locations. This procedure is particularly important whenusing high-dimensional representations, such as Fisher vectors andconvolutional neural network features. We also propose a window refinementmethod, which improves the localization accuracy by incorporating an objectnessprior. We present a detailed experimental evaluation using the PASCAL VOC 2007dataset, which verifies the effectiveness of our approach.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 39 (1), 189-203',\n",
       "  'citations': '268',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1503.00949v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2768949281506611828&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 507: {'ID': 507,\n",
       "  'title': 'Graph R-CNN for Scene Graph Generation',\n",
       "  'authors': ['Jiasen Lu',\n",
       "   'Jianwei Yang',\n",
       "   'Stefan Lee',\n",
       "   'Dhruv Batra',\n",
       "   'Devi Parikh'],\n",
       "  'published': '2018-08-01T06:50:19Z',\n",
       "  'updated': '2018-08-01T06:50:19Z',\n",
       "  'abstract': 'We propose a novel scene graph generation model called Graph R-CNN, that isboth effective and efficient at detecting objects and their relations inimages. Our model contains a Relation Proposal Network (RePN) that efficientlydeals with the quadratic number of potential relations between objects in animage. We also propose an attentional Graph Convolutional Network (aGCN) thateffectively captures contextual information between objects and relations.Finally, we introduce a new evaluation metric that is more holistic andrealistic than existing metrics. We report state-of-the-art performance onscene graph generation as evaluated using both existing and our proposedmetrics.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 670-685',\n",
       "  'citations': '170',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1808.00191v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=15993138764460245346&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 508: {'ID': 508,\n",
       "  'title': 'A Survey on Learning to Hash',\n",
       "  'authors': ['Jingdong Wang',\n",
       "   'Nicu Sebe',\n",
       "   'Ting Zhang',\n",
       "   'Jingkuan Song',\n",
       "   'Heng Tao Shen'],\n",
       "  'published': '2016-06-01T09:23:48Z',\n",
       "  'updated': '2017-04-22T00:19:47Z',\n",
       "  'abstract': 'Nearest neighbor search is a problem of finding the data points from thedatabase such that the distances from them to the query point are the smallest.Learning to hash is one of the major solutions to this problem and has beenwidely studied recently. In this paper, we present a comprehensive survey ofthe learning to hash algorithms, categorize them according to the manners ofpreserving the similarities into: pairwise similarity preserving, multiwisesimilarity preserving, implicit similarity preserving, as well as quantization,and discuss their relations. We separate quantization from pairwise similaritypreserving as the objective function is very different though quantization, aswe show, can be derived from preserving the pairwise similarities. In addition,we present the evaluation protocols, and the general performance analysis, andpoint out that the quantization algorithms perform superiorly in terms ofsearch accuracy, search time cost, and space cost. Finally, we introduce a fewemerging topics.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 40 (4), 769-790',\n",
       "  'citations': '440',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1606.00185v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2127760446220938839&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 509: {'ID': 509,\n",
       "  'title': 'Semantic Object Parsing with Graph LSTM',\n",
       "  'authors': ['Xiaodan Liang',\n",
       "   'Xiaohui Shen',\n",
       "   'Liang Lin',\n",
       "   'Shuicheng Yan',\n",
       "   'Jiashi Feng'],\n",
       "  'published': '2016-03-23T03:31:02Z',\n",
       "  'updated': '2016-03-23T03:31:02Z',\n",
       "  'abstract': 'By taking the semantic object parsing task as an exemplar applicationscenario, we propose the Graph Long Short-Term Memory (Graph LSTM) network,which is the generalization of LSTM from sequential data or multi-dimensionaldata to general graph-structured data. Particularly, instead of evenly andfixedly dividing an image to pixels or patches in existing multi-dimensionalLSTM structures (e.g., Row, Grid and Diagonal LSTMs), we take eacharbitrary-shaped superpixel as a semantically consistent node, and adaptivelyconstruct an undirected graph for each image, where the spatial relations ofthe superpixels are naturally used as edges. Constructed on such an adaptivegraph topology, the Graph LSTM is more naturally aligned with the visualpatterns in the image (e.g., object boundaries or appearance similarities) andprovides a more economical information propagation route. Furthermore, for eachoptimization step over Graph LSTM, we propose to use a confidence-driven schemeto update the hidden and memory states of nodes progressively till all nodesare updated. In addition, for each node, the forgets gates are adaptivelylearned to capture different degrees of semantic correlation with neighboringnodes. Comprehensive evaluations on four diverse semantic object parsingdatasets well demonstrate the significant superiority of our Graph LSTM overother state-of-the-art solutions.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (1), 125-143',\n",
       "  'citations': '182',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1603.07063v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=16869082826714764614&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 510: {'ID': 510,\n",
       "  'title': 'Temporal Relational Reasoning in Videos',\n",
       "  'authors': ['Bolei Zhou', 'Alex Andonian', 'Aude Oliva', 'Antonio Torralba'],\n",
       "  'published': '2017-11-22T20:31:19Z',\n",
       "  'updated': '2018-07-25T03:03:32Z',\n",
       "  'abstract': 'Temporal relational reasoning, the ability to link meaningful transformationsof objects or entities over time, is a fundamental property of intelligentspecies. In this paper, we introduce an effective and interpretable networkmodule, the Temporal Relation Network (TRN), designed to learn and reason abouttemporal dependencies between video frames at multiple time scales. We evaluateTRN-equipped networks on activity recognition tasks using three recent videodatasets - Something-Something, Jester, and Charades - which fundamentallydepend on temporal relational reasoning. Our results demonstrate that theproposed TRN gives convolutional neural networks a remarkable capacity todiscover temporal relations in videos. Through only sparsely sampled videoframes, TRN-equipped networks can accurately predict human-object interactionsin the Something-Something dataset and identify various human gestures on theJester dataset with very competitive performance. TRN-equipped networks alsooutperform two-stream networks and 3D convolution networks in recognizing dailyactivities in the Charades dataset. Further analyses show that the models learnintuitive and interpretable visual common sense knowledge in videos.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 803-818',\n",
       "  'citations': '238',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1711.08496v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14863300584090962308&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 511: {'ID': 511,\n",
       "  'title': 'Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset',\n",
       "  'authors': ['Andrew Zisserman', 'Joao Carreira'],\n",
       "  'published': '2017-05-22T13:57:53Z',\n",
       "  'updated': '2018-02-12T17:10:11Z',\n",
       "  'abstract': 'The paucity of videos in current action classification datasets (UCF-101 andHMDB-51) has made it difficult to identify good video architectures, as mostmethods obtain similar performance on existing small-scale benchmarks. Thispaper re-evaluates state-of-the-art architectures in light of the new KineticsHuman Action Video dataset. Kinetics has two orders of magnitude more data,with 400 human action classes and over 400 clips per class, and is collectedfrom realistic, challenging YouTube videos. We provide an analysis on howcurrent architectures fare on the task of action classification on this datasetand how much performance improves on the smaller benchmark datasets afterpre-training on Kinetics.  We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on2D ConvNet inflation: filters and pooling kernels of very deep imageclassification ConvNets are expanded into 3D, making it possible to learnseamless spatio-temporal feature extractors from video while leveragingsuccessful ImageNet architecture designs and even their parameters. We showthat, after pre-training on Kinetics, I3D models considerably improve upon thestate-of-the-art in action classification, reaching 80.9% on HMDB-51 and 98.0%on UCF-101.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '1583',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1705.07750v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=9581219496538221166&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 512: {'ID': 512,\n",
       "  'title': 'Image Super-Resolution Using Deep Convolutional Networks',\n",
       "  'authors': ['Xiaoou Tang', 'Chao Dong', 'Chen Change Loy', 'Kaiming He'],\n",
       "  'published': '2014-12-31T08:35:09Z',\n",
       "  'updated': '2015-07-31T09:13:32Z',\n",
       "  'abstract': 'We propose a deep learning method for single image super-resolution (SR). Ourmethod directly learns an end-to-end mapping between the low/high-resolutionimages. The mapping is represented as a deep convolutional neural network (CNN)that takes the low-resolution image as the input and outputs thehigh-resolution one. We further show that traditional sparse-coding-based SRmethods can also be viewed as a deep convolutional network. But unliketraditional methods that handle each component separately, our method jointlyoptimizes all layers. Our deep CNN has a lightweight structure, yetdemonstrates state-of-the-art restoration quality, and achieves fast speed forpractical on-line usage. We explore different network structures and parametersettings to achieve trade-offs between performance and speed. Moreover, weextend our network to cope with three color channels simultaneously, and showbetter overall reconstruction quality.',\n",
       "  'categories': ['cs.CV', 'cs.NE', 'I.4.5; I.2.6'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 38 (2), 295-307',\n",
       "  'citations': '3228',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1501.00092v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=13705212607084119344&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 513: {'ID': 513,\n",
       "  'title': 'Convolutional Channel Features',\n",
       "  'authors': ['Bin Yang', 'Stan Z. Li', 'Junjie Yan', 'Zhen Lei'],\n",
       "  'published': '2015-04-28T03:44:39Z',\n",
       "  'updated': '2015-09-24T17:22:41Z',\n",
       "  'abstract': 'Deep learning methods are powerful tools but often suffer from expensivecomputation and limited flexibility. An alternative is to combine light-weightmodels with deep representations. As successful cases exist in several visualproblems, a unified framework is absent. In this paper, we revisit two widelyused approaches in computer vision, namely filtered channel features andConvolutional Neural Networks (CNN), and absorb merits from both by proposingan integrated method called Convolutional Channel Features (CCF). CCF transferslow-level features from pre-trained CNN models to feed the boosting forestmodel. With the combination of CNN features and boosting forest, CCF benefitsfrom the richer capacity in feature representation compared with channelfeatures, as well as lower cost in computation and storage compared withend-to-end CNN methods. We show that CCF serves as a good way of tailoringpre-trained CNN models to diverse tasks without fine-tuning the whole networkto each task by achieving state-of-the-art performances in pedestriandetection, face detection, edge detection and object proposal generation.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 82-90',\n",
       "  'citations': '267',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1504.07339v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=15348182900133754024&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 514: {'ID': 514,\n",
       "  'title': 'Image Super-Resolution Using Very Deep Residual Channel Attention  Networks',\n",
       "  'authors': ['Yun Fu',\n",
       "   'Yulun Zhang',\n",
       "   'Kai Li',\n",
       "   'Kunpeng Li',\n",
       "   'Bineng Zhong',\n",
       "   'Lichen Wang'],\n",
       "  'published': '2018-07-08T05:45:45Z',\n",
       "  'updated': '2018-07-12T21:57:37Z',\n",
       "  'abstract': 'Convolutional neural network (CNN) depth is of crucial importance for imagesuper-resolution (SR). However, we observe that deeper networks for image SRare more difficult to train. The low-resolution inputs and features containabundant low-frequency information, which is treated equally across channels,hence hindering the representational ability of CNNs. To solve these problems,we propose the very deep residual channel attention networks (RCAN).Specifically, we propose a residual in residual (RIR) structure to form verydeep network, which consists of several residual groups with long skipconnections. Each residual group contains some residual blocks with short skipconnections. Meanwhile, RIR allows abundant low-frequency information to bebypassed through multiple skip connections, making the main network focus onlearning high-frequency information. Furthermore, we propose a channelattention mechanism to adaptively rescale channel-wise features by consideringinterdependencies among channels. Extensive experiments show that our RCANachieves better accuracy and visual improvements against state-of-the-artmethods.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 286-301',\n",
       "  'citations': '513',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1807.02758v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=3748973811121591896&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 515: {'ID': 515,\n",
       "  'title': 'Quantized Convolutional Neural Networks for Mobile Devices',\n",
       "  'authors': ['Jiaxiang Wu',\n",
       "   'Cong Leng',\n",
       "   'Yuhang Wang',\n",
       "   'Jian Cheng',\n",
       "   'Qinghao Hu'],\n",
       "  'published': '2015-12-21T02:26:46Z',\n",
       "  'updated': '2016-05-16T00:37:35Z',\n",
       "  'abstract': \"Recently, convolutional neural networks (CNN) have demonstrated impressiveperformance in various computer vision tasks. However, high performancehardware is typically indispensable for the application of CNN models due tothe high computation complexity, which prohibits their further extensions. Inthis paper, we propose an efficient framework, namely Quantized CNN, tosimultaneously speed-up the computation and reduce the storage and memoryoverhead of CNN models. Both filter kernels in convolutional layers andweighting matrices in fully-connected layers are quantized, aiming atminimizing the estimation error of each layer's response. Extensive experimentson the ILSVRC-12 benchmark demonstrate 4~6x speed-up and 15~20x compressionwith merely one percentage loss of classification accuracy. With our quantizedCNN model, even mobile devices can accurately classify images within onesecond.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '525',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1512.06473v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=4949425006385395544&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 516: {'ID': 516,\n",
       "  'title': 'Dilated Residual Networks',\n",
       "  'authors': ['Vladlen Koltun', 'Fisher Yu', 'Thomas Funkhouser'],\n",
       "  'published': '2017-05-28T09:01:44Z',\n",
       "  'updated': '2017-05-28T09:01:44Z',\n",
       "  'abstract': \"Convolutional networks for image classification progressively reduceresolution until the image is represented by tiny feature maps in which thespatial structure of the scene is no longer discernible. Such loss of spatialacuity can limit image classification accuracy and complicate the transfer ofthe model to downstream applications that require detailed scene understanding.These problems can be alleviated by dilation, which increases the resolution ofoutput feature maps without reducing the receptive field of individual neurons.We show that dilated residual networks (DRNs) outperform their non-dilatedcounterparts in image classification without increasing the model's depth orcomplexity. We then study gridding artifacts introduced by dilation, develop anapproach to removing these artifacts (`degridding'), and show that this furtherincreases the performance of DRNs. In addition, we show that the accuracyadvantage of DRNs is further magnified in downstream applications such asobject localization and semantic segmentation.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '556',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1705.09914v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2962359587881515950&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 517: {'ID': 517,\n",
       "  'title': 'EAST: An Efficient and Accurate Scene Text Detector',\n",
       "  'authors': ['Yuzhi Wang',\n",
       "   'Xinyu Zhou',\n",
       "   'Shuchang Zhou',\n",
       "   'He Wen',\n",
       "   'Weiran He',\n",
       "   'Jiajun Liang',\n",
       "   'Cong Yao'],\n",
       "  'published': '2017-04-11T06:04:12Z',\n",
       "  'updated': '2017-07-10T08:10:52Z',\n",
       "  'abstract': 'Previous approaches for scene text detection have already achieved promisingperformances across various benchmarks. However, they usually fall short whendealing with challenging scenarios, even when equipped with deep neural networkmodels, because the overall performance is determined by the interplay ofmultiple stages and components in the pipelines. In this work, we propose asimple yet powerful pipeline that yields fast and accurate text detection innatural scenes. The pipeline directly predicts words or text lines of arbitraryorientations and quadrilateral shapes in full images, eliminating unnecessaryintermediate steps (e.g., candidate aggregation and word partitioning), with asingle neural network. The simplicity of our pipeline allows concentratingefforts on designing loss functions and neural network architecture.Experiments on standard datasets including ICDAR 2015, COCO-Text and MSRA-TD500demonstrate that the proposed algorithm significantly outperformsstate-of-the-art methods in terms of both accuracy and efficiency. On the ICDAR2015 dataset, the proposed algorithm achieves an F-score of 0.7820 at 13.2fpsat 720p resolution.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '509',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1704.03155v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14855999552089039794&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 518: {'ID': 518,\n",
       "  'title': 'Inferring and Executing Programs for Visual Reasoning',\n",
       "  'authors': ['Bharath Hariharan',\n",
       "   'Ross Girshick',\n",
       "   'Li Fei-Fei',\n",
       "   'C. Lawrence Zitnick',\n",
       "   'Laurens van der Maaten',\n",
       "   'Justin Johnson',\n",
       "   'Judy Hoffman'],\n",
       "  'published': '2017-05-10T07:08:23Z',\n",
       "  'updated': '2017-05-10T07:08:23Z',\n",
       "  'abstract': 'Existing methods for visual reasoning attempt to directly map inputs tooutputs using black-box architectures without explicitly modeling theunderlying reasoning processes. As a result, these black-box models often learnto exploit biases in the data rather than learning to perform visual reasoning.Inspired by module networks, this paper proposes a model for visual reasoningthat consists of a program generator that constructs an explicit representationof the reasoning process to be performed, and an execution engine that executesthe resulting program to produce an answer. Both the program generator and theexecution engine are implemented by neural networks, and are trained using acombination of backpropagation and REINFORCE. Using the CLEVR benchmark forvisual reasoning, we show that our model significantly outperforms strongbaselines and generalizes better in a variety of settings.',\n",
       "  'categories': ['cs.CV', 'cs.CL', 'cs.LG'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 2989-2998',\n",
       "  'citations': '261',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1705.03633v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=3255123339234178763&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 519: {'ID': 519,\n",
       "  'title': 'YOLO9000: Better, Faster, Stronger',\n",
       "  'authors': ['Ali Farhadi', 'Joseph Redmon'],\n",
       "  'published': '2016-12-25T07:21:38Z',\n",
       "  'updated': '2016-12-25T07:21:38Z',\n",
       "  'abstract': \"We introduce YOLO9000, a state-of-the-art, real-time object detection systemthat can detect over 9000 object categories. First we propose variousimprovements to the YOLO detection method, both novel and drawn from priorwork. The improved model, YOLOv2, is state-of-the-art on standard detectiontasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods likeFaster RCNN with ResNet and SSD while still running significantly faster.Finally we propose a method to jointly train on object detection andclassification. Using this method we train YOLO9000 simultaneously on the COCOdetection dataset and the ImageNet classification dataset. Our joint trainingallows YOLO9000 to predict detections for object classes that don't havelabelled detection data. We validate our approach on the ImageNet detectiontask. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despiteonly having detection data for 44 of the 200 classes. On the 156 classes not inCOCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes;it predicts detections for more than 9000 different object categories. And itstill runs in real-time.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '4960',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1612.08242v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=10688536692178239560&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 520: {'ID': 520,\n",
       "  'title': 'CLEVR: A Diagnostic Dataset for Compositional Language and Elementary  Visual Reasoning',\n",
       "  'authors': ['Bharath Hariharan',\n",
       "   'Ross Girshick',\n",
       "   'C. Lawrence Zitnick',\n",
       "   'Li Fei-Fei',\n",
       "   'Laurens van der Maaten',\n",
       "   'Justin Johnson'],\n",
       "  'published': '2016-12-20T21:40:40Z',\n",
       "  'updated': '2016-12-20T21:40:40Z',\n",
       "  'abstract': 'When building artificial intelligence systems that can reason and answerquestions about visual data, we need diagnostic tests to analyze our progressand discover shortcomings. Existing benchmarks for visual question answeringcan help, but have strong biases that models can exploit to correctly answerquestions without reasoning. They also conflate multiple sources of error,making it hard to pinpoint model weaknesses. We present a diagnostic datasetthat tests a range of visual reasoning abilities. It contains minimal biasesand has detailed annotations describing the kind of reasoning each questionrequires. We use this dataset to analyze a variety of modern visual reasoningsystems, providing novel insights into their abilities and limitations.',\n",
       "  'categories': ['cs.CV', 'cs.CL', 'cs.LG'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '606',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1612.06890v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=1033880884200484288&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 521: {'ID': 521,\n",
       "  'title': 'Deep Fried Convnets',\n",
       "  'authors': ['Le Song',\n",
       "   'Nando de Freitas',\n",
       "   'Misha Denil',\n",
       "   'Ziyu Wang',\n",
       "   'Zichao Yang',\n",
       "   'Marcin Moczulski',\n",
       "   'Alex Smola'],\n",
       "  'published': '2014-12-22T20:53:30Z',\n",
       "  'updated': '2015-07-17T20:17:26Z',\n",
       "  'abstract': 'The fully connected layers of a deep convolutional neural network typicallycontain over 90% of the network parameters, and consume the majority of thememory required to store the network parameters. Reducing the number ofparameters while preserving essentially the same predictive performance iscritically important for operating deep neural networks in memory constrainedenvironments such as GPUs or embedded devices.  In this paper we show how kernel methods, in particular a single Fastfoodlayer, can be used to replace all fully connected layers in a deepconvolutional neural network. This novel Fastfood layer is also end-to-endtrainable in conjunction with convolutional layers, allowing us to combine theminto a new architecture, named deep fried convolutional networks, whichsubstantially reduces the memory footprint of convolutional networks trained onMNIST and ImageNet with no drop in predictive performance.',\n",
       "  'categories': ['cs.LG', 'cs.NE', 'stat.ML'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 1476-1483',\n",
       "  'citations': '221',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1412.7149v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5084451682192638940&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 522: {'ID': 522,\n",
       "  'title': 'Low-shot Visual Recognition by Shrinking and Hallucinating Features',\n",
       "  'authors': ['Bharath Hariharan', 'Ross Girshick'],\n",
       "  'published': '2016-06-09T04:28:07Z',\n",
       "  'updated': '2017-11-04T15:52:48Z',\n",
       "  'abstract': 'Low-shot visual learning---the ability to recognize novel object categoriesfrom very few examples---is a hallmark of human visual intelligence. Existingmachine learning approaches fail to generalize in the same way. To makeprogress on this foundational problem, we present a low-shot learning benchmarkon complex images that mimics challenges faced by recognition systems in thewild. We then propose a) representation regularization techniques, and b)techniques to hallucinate additional training examples for data-starvedclasses. Together, our methods improve the effectiveness of convolutionalnetworks in low-shot learning, improving the one-shot accuracy on novel classesby 2.3x on the challenging ImageNet dataset.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 3018-3027',\n",
       "  'citations': '267',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1606.02819v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=797357624345838266&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 523: {'ID': 523,\n",
       "  'title': 'Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on  Graphs',\n",
       "  'authors': ['Martin Simonovsky', 'Nikos Komodakis'],\n",
       "  'published': '2017-04-10T15:18:54Z',\n",
       "  'updated': '2017-08-08T09:31:17Z',\n",
       "  'abstract': 'A number of problems can be formulated as prediction on graph-structureddata. In this work, we generalize the convolution operator from regular gridsto arbitrary graphs while avoiding the spectral domain, which allows us tohandle graphs of varying size and connectivity. To move beyond a simplediffusion, filter weights are conditioned on the specific edge labels in theneighborhood of a vertex. Together with the proper choice of graph coarsening,we explore constructing deep neural networks for graph classification. Inparticular, we demonstrate the generality of our formulation in point cloudclassification, where we set the new state of the art, and on a graphclassification dataset, where we outperform other deep learning approaches. Thesource code is available at https://github.com/mys007/ecc',\n",
       "  'categories': ['cs.CV', 'cs.LG', 'cs.NE'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '341',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1704.02901v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=11073703403545981773&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 524: {'ID': 524,\n",
       "  'title': 'Exploring the Limits of Weakly Supervised Pretraining',\n",
       "  'authors': ['Ross Girshick',\n",
       "   'Yixuan Li',\n",
       "   'Vignesh Ramanathan',\n",
       "   'Manohar Paluri',\n",
       "   'Kaiming He',\n",
       "   'Laurens van der Maaten',\n",
       "   'Dhruv Mahajan',\n",
       "   'Ashwin Bharambe'],\n",
       "  'published': '2018-05-02T17:57:16Z',\n",
       "  'updated': '2018-05-02T17:57:16Z',\n",
       "  'abstract': 'State-of-the-art visual perception models for a wide range of tasks rely onsupervised pretraining. ImageNet classification is the de facto pretrainingtask for these models. Yet, ImageNet is now nearly ten years old and is bymodern standards \"small\". Even so, relatively little is known about thebehavior of pretraining with datasets that are multiple orders of magnitudelarger. The reasons are obvious: such datasets are difficult to collect andannotate. In this paper, we present a unique study of transfer learning withlarge convolutional networks trained to predict hashtags on billions of socialmedia images. Our experiments demonstrate that training for large-scale hashtagprediction leads to excellent results. We show improvements on several imageclassification and object detection tasks, and report the highest ImageNet-1ksingle-crop, top-1 accuracy to date: 85.4% (97.6% top-5). We also performextensive experiments that provide novel empirical data on the relationshipbetween large-scale pretraining and transfer learning performance.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 181-196',\n",
       "  'citations': '301',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1805.00932v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=8358288919170046195&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 525: {'ID': 525,\n",
       "  'title': 'Siamese Instance Search for Tracking',\n",
       "  'authors': ['Ran Tao', 'Efstratios Gavves', 'Arnold W. M. Smeulders'],\n",
       "  'published': '2016-05-19T09:24:40Z',\n",
       "  'updated': '2016-05-19T09:24:40Z',\n",
       "  'abstract': 'In this paper we present a tracker, which is radically different fromstate-of-the-art trackers: we apply no model updating, no occlusion detection,no combination of trackers, no geometric matching, and still deliverstate-of-the-art tracking performance, as demonstrated on the popular onlinetracking benchmark (OTB) and six very challenging YouTube videos. The presentedtracker simply matches the initial patch of the target in the first frame withcandidates in a new frame and returns the most similar patch by a learnedmatching function. The strength of the matching function comes from beingextensively trained generically, i.e., without any data of the target, using aSiamese deep neural network, which we design for tracking. Once learned, thematching function is used as is, without any adapting, to track previouslyunseen targets. It turns out that the learned matching function is so powerfulthat a simple tracker built upon it, coined Siamese INstance search Tracker,SINT, which only uses the original observation of the target from the firstframe, suffices to reach state-of-the-art performance. Further, we show theproposed tracker even allows for target re-identification after the target wasabsent for a complete video shot.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '553',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1605.05863v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=7171796048899667723&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 526: {'ID': 526,\n",
       "  'title': 'Learning from Simulated and Unsupervised Images through Adversarial  Training',\n",
       "  'authors': ['Josh Susskind',\n",
       "   'Oncel Tuzel',\n",
       "   'Ashish Shrivastava',\n",
       "   'Wenda Wang',\n",
       "   'Tomas Pfister',\n",
       "   'Russ Webb'],\n",
       "  'published': '2016-12-22T22:10:51Z',\n",
       "  'updated': '2017-07-19T21:24:52Z',\n",
       "  'abstract': \"With recent progress in graphics, it has become more tractable to trainmodels on synthetic images, potentially avoiding the need for expensiveannotations. However, learning from synthetic images may not achieve thedesired performance due to a gap between synthetic and real imagedistributions. To reduce this gap, we propose Simulated+Unsupervised (S+U)learning, where the task is to learn a model to improve the realism of asimulator's output using unlabeled real data, while preserving the annotationinformation from the simulator. We develop a method for S+U learning that usesan adversarial network similar to Generative Adversarial Networks (GANs), butwith synthetic images as inputs instead of random vectors. We make several keymodifications to the standard GAN algorithm to preserve annotations, avoidartifacts, and stabilize training: (i) a 'self-regularization' term, (ii) alocal adversarial loss, and (iii) updating the discriminator using a history ofrefined images. We show that this enables generation of highly realisticimages, which we demonstrate both qualitatively and with a user study. Wequantitatively evaluate the generated images by training models for gazeestimation and hand pose estimation. We show a significant improvement overusing synthetic images, and achieve state-of-the-art results on the MPIIGazedataset without any labeled real data.\",\n",
       "  'categories': ['cs.CV', 'cs.LG', 'cs.NE'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '1025',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1612.07828v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5960837935511106924&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 527: {'ID': 527,\n",
       "  'title': 'Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors',\n",
       "  'authors': ['Xiaoou Tang', 'Limin Wang', 'Yu Qiao'],\n",
       "  'published': '2015-05-19T04:36:42Z',\n",
       "  'updated': '2015-05-19T04:36:42Z',\n",
       "  'abstract': 'Visual features are of vital importance for human action understanding invideos. This paper presents a new video representation, calledtrajectory-pooled deep-convolutional descriptor (TDD), which shares the meritsof both hand-crafted features and deep-learned features. Specifically, weutilize deep architectures to learn discriminative convolutional feature maps,and conduct trajectory-constrained pooling to aggregate these convolutionalfeatures into effective descriptors. To enhance the robustness of TDDs, wedesign two normalization methods to transform convolutional feature maps,namely spatiotemporal normalization and channel normalization. The advantagesof our features come from (i) TDDs are automatically learned and contain highdiscriminative capacity compared with those hand-crafted features; (ii) TDDstake account of the intrinsic characteristics of temporal dimension andintroduce the strategies of trajectory-constrained sampling and pooling foraggregating deep-learned features. We conduct experiments on two challengingdatasets: HMDB51 and UCF101. Experimental results show that TDDs outperformprevious hand-crafted features and deep-learned features. Our method alsoachieves superior performance to the state of the art on these datasets (HMDB5165.9%, UCF101 91.5%).',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '936',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1505.04868v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=15595982405325185015&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 528: {'ID': 528,\n",
       "  'title': 'Convolutional Neural Networks at Constrained Time Cost',\n",
       "  'authors': ['Jian Sun', 'Kaiming He'],\n",
       "  'published': '2014-12-04T16:00:47Z',\n",
       "  'updated': '2014-12-04T16:00:47Z',\n",
       "  'abstract': 'Though recent advanced convolutional neural networks (CNNs) have beenimproving the image recognition accuracy, the models are getting more complexand time-consuming. For real-world applications in industrial and commercialscenarios, engineers and developers are often faced with the requirement ofconstrained time budget. In this paper, we investigate the accuracy of CNNsunder constrained time cost. Under this constraint, the designs of the networkarchitectures should exhibit as trade-offs among the factors like depth,numbers of filters, filter sizes, etc. With a series of controlled comparisons,we progressively modify a baseline model while preserving its time complexity.This is also helpful for understanding the importance of the factors in networkdesigns. We present an architecture that achieves very competitive accuracy inthe ImageNet dataset (11.8% top-5 error, 10-view test), yet is 20% faster than\"AlexNet\" (16.0% top-5 error, 10-view test).',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '636',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1412.1710v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=8825404952865269038&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 529: {'ID': 529,\n",
       "  'title': 'Deeply learned face representations are sparse, selective, and robust',\n",
       "  'authors': ['Xiaogang Wang', 'Xiaoou Tang', 'Yi Sun'],\n",
       "  'published': '2014-12-03T10:37:13Z',\n",
       "  'updated': '2014-12-03T10:37:13Z',\n",
       "  'abstract': 'This paper designs a high-performance deep convolutional network (DeepID2+)for face recognition. It is learned with the identification-verificationsupervisory signal. By increasing the dimension of hidden representations andadding supervision to early convolutional layers, DeepID2+ achieves newstate-of-the-art on LFW and YouTube Faces benchmarks. Through empiricalstudies, we have discovered three properties of its deep neural activationscritical for the high performance: sparsity, selectiveness and robustness. (1)It is observed that neural activations are moderately sparse. Moderate sparsitymaximizes the discriminative power of the deep net as well as the distancebetween images. It is surprising that DeepID2+ still can achieve highrecognition accuracy even after the neural responses are binarized. (2) Itsneurons in higher layers are highly selective to identities andidentity-related attributes. We can identify different subsets of neurons whichare either constantly excited or inhibited when different identities orattributes are present. Although DeepID2+ is not taught to distinguishattributes during training, it has implicitly learned such high-level concepts.(3) It is much more robust to occlusions, although occlusion patterns are notincluded in the training set.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '704',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1412.1265v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14660449735839776202&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 530: {'ID': 530,\n",
       "  'title': 'Learning Transferable Architectures for Scalable Image Recognition',\n",
       "  'authors': ['Vijay Vasudevan',\n",
       "   'Barret Zoph',\n",
       "   'Jonathon Shlens',\n",
       "   'Quoc V. Le'],\n",
       "  'published': '2017-07-21T18:10:26Z',\n",
       "  'updated': '2018-04-11T05:12:21Z',\n",
       "  'abstract': 'Developing neural network image classification models often requiressignificant architecture engineering. In this paper, we study a method to learnthe model architectures directly on the dataset of interest. As this approachis expensive when the dataset is large, we propose to search for anarchitectural building block on a small dataset and then transfer the block toa larger dataset. The key contribution of this work is the design of a newsearch space (the \"NASNet search space\") which enables transferability. In ourexperiments, we search for the best convolutional layer (or \"cell\") on theCIFAR-10 dataset and then apply this cell to the ImageNet dataset by stackingtogether more copies of this cell, each with their own parameters to design aconvolutional architecture, named \"NASNet architecture\". We also introduce anew regularization technique called ScheduledDropPath that significantlyimproves generalization in the NASNet models. On CIFAR-10 itself, NASNetachieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNetachieves, among the published works, state-of-the-art accuracy of 82.7% top-1and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy thanthe best human-invented architectures while having 9 billion fewer FLOPS - areduction of 28% in computational demand from the previous state-of-the-artmodel. When evaluated at different levels of computational cost, accuracies ofNASNets exceed those of the state-of-the-art human-designed models. Forinstance, a small version of NASNet also achieves 74% top-1 accuracy, which is3.1% better than equivalently-sized, state-of-the-art models for mobileplatforms. Finally, the learned features by NASNet used with the Faster-RCNNframework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCOdataset.',\n",
       "  'categories': ['cs.CV', 'cs.LG', 'stat.ML'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '1569',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1707.07012v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=9456193458847397477&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 531: {'ID': 531,\n",
       "  'title': 'SVDNet for Pedestrian Retrieval',\n",
       "  'authors': ['Shengjin Wang', 'Yifan Sun', 'Weijian Deng', 'Liang Zheng'],\n",
       "  'published': '2017-03-16T16:11:05Z',\n",
       "  'updated': '2017-08-06T05:37:09Z',\n",
       "  'abstract': 'This paper proposes the SVDNet for retrieval problems, with focus on theapplication of person re-identification (re-ID). We view each weight vectorwithin a fully connected (FC) layer in a convolutional neuron network (CNN) asa projection basis. It is observed that the weight vectors are usually highlycorrelated. This problem leads to correlations among entries of the FCdescriptor, and compromises the retrieval performance based on the Euclideandistance. To address the problem, this paper proposes to optimize the deeprepresentation learning process with Singular Vector Decomposition (SVD).Specifically, with the restraint and relaxation iteration (RRI) trainingscheme, we are able to iteratively integrate the orthogonality constraint inCNN training, yielding the so-called SVDNet. We conduct experiments on theMarket-1501, CUHK03, and Duke datasets, and show that RRI effectively reducesthe correlation among the projection vectors, produces more discriminative FCdescriptors, and significantly improves the re-ID accuracy. On the Market-1501dataset, for instance, rank-1 accuracy is improved from 55.3% to 80.5% forCaffeNet, and from 73.8% to 82.3% for ResNet-50.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 3800-3808',\n",
       "  'citations': '427',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1703.05693v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=4883173656761999145&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 532: {'ID': 532,\n",
       "  'title': 'Weakly Supervised Deep Detection Networks',\n",
       "  'authors': ['Andrea Vedaldi', 'Hakan Bilen'],\n",
       "  'published': '2015-11-09T20:58:30Z',\n",
       "  'updated': '2016-12-19T09:44:29Z',\n",
       "  'abstract': 'Weakly supervised learning of object detection is an important problem inimage understanding that still does not have a satisfactory solution. In thispaper, we address this problem by exploiting the power of deep convolutionalneural networks pre-trained on large-scale image-level classification tasks. Wepropose a weakly supervised deep detection architecture that modifies one suchnetwork to operate at the level of image regions, performing simultaneouslyregion selection and classification. Trained as an image classifier, thearchitecture implicitly learns object detectors that are better thanalternative weakly supervised detection systems on the PASCAL VOC data. Themodel, which is a simple and elegant end-to-end architecture, outperformsstandard data augmentation and fine-tuning techniques for the task ofimage-level classification as well.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '357',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1511.02853v4',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5405093288658958492&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 533: {'ID': 533,\n",
       "  'title': 'Semantic Image Segmentation via Deep Parsing Network',\n",
       "  'authors': ['Ziwei Liu',\n",
       "   'Xiaoou Tang',\n",
       "   'Ping Luo',\n",
       "   'Chen Change Loy',\n",
       "   'Xiaoxiao Li'],\n",
       "  'published': '2015-09-09T04:39:34Z',\n",
       "  'updated': '2015-09-24T14:15:17Z',\n",
       "  'abstract': 'This paper addresses semantic image segmentation by incorporating richinformation into Markov Random Field (MRF), including high-order relations andmixture of label contexts. Unlike previous works that optimized MRFs usingiterative algorithm, we solve MRF by proposing a Convolutional Neural Network(CNN), namely Deep Parsing Network (DPN), which enables deterministicend-to-end computation in a single forward pass. Specifically, DPN extends acontemporary CNN architecture to model unary terms and additional layers arecarefully devised to approximate the mean field algorithm (MF) for pairwiseterms. It has several appealing properties. First, different from the recentworks that combined CNN and MRF, where many iterations of MF were required foreach training image during back-propagation, DPN is able to achieve highperformance by approximating one iteration of MF. Second, DPN representsvarious types of pairwise terms, making many existing works as its specialcases. Third, DPN makes MF easier to be parallelized and speeded up inGraphical Processing Unit (GPU). DPN is thoroughly evaluated on the PASCAL VOC2012 dataset, where a single DPN model yields a new state-of-the-artsegmentation accuracy.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 1377-1385',\n",
       "  'citations': '478',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1509.02634v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=18281955767933637624&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 534: {'ID': 534,\n",
       "  'title': 'Deep Visual-Semantic Alignments for Generating Image Descriptions',\n",
       "  'authors': ['Li Fei-Fei', 'Andrej Karpathy'],\n",
       "  'published': '2014-12-07T02:36:07Z',\n",
       "  'updated': '2015-04-14T05:02:53Z',\n",
       "  'abstract': 'We present a model that generates natural language descriptions of images andtheir regions. Our approach leverages datasets of images and their sentencedescriptions to learn about the inter-modal correspondences between languageand visual data. Our alignment model is based on a novel combination ofConvolutional Neural Networks over image regions, bidirectional RecurrentNeural Networks over sentences, and a structured objective that aligns the twomodalities through a multimodal embedding. We then describe a MultimodalRecurrent Neural Network architecture that uses the inferred alignments tolearn to generate novel descriptions of image regions. We demonstrate that ouralignment model produces state of the art results in retrieval experiments onFlickr8K, Flickr30K and MSCOCO datasets. We then show that the generateddescriptions significantly outperform retrieval baselines on both full imagesand on a new dataset of region-level annotations.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '3530',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1412.2306v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=1254599041474819707&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 535: {'ID': 535,\n",
       "  'title': 'Feature Pyramid Networks for Object Detection',\n",
       "  'authors': ['Bharath Hariharan',\n",
       "   'Ross Girshick',\n",
       "   'Serge Belongie',\n",
       "   'Piotr Dollár',\n",
       "   'Kaiming He',\n",
       "   'Tsung-Yi Lin'],\n",
       "  'published': '2016-12-09T19:55:54Z',\n",
       "  'updated': '2017-04-19T22:46:32Z',\n",
       "  'abstract': 'Feature pyramids are a basic component in recognition systems for detectingobjects at different scales. But recent deep learning object detectors haveavoided pyramid representations, in part because they are compute and memoryintensive. In this paper, we exploit the inherent multi-scale, pyramidalhierarchy of deep convolutional networks to construct feature pyramids withmarginal extra cost. A top-down architecture with lateral connections isdeveloped for building high-level semantic feature maps at all scales. Thisarchitecture, called a Feature Pyramid Network (FPN), shows significantimprovement as a generic feature extractor in several applications. Using FPNin a basic Faster R-CNN system, our method achieves state-of-the-artsingle-model results on the COCO detection benchmark without bells andwhistles, surpassing all existing single-model entries including those from theCOCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPUand thus is a practical and accurate solution to multi-scale object detection.Code will be made publicly available.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '3879',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1612.03144v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=13353537855176955572&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 536: {'ID': 536,\n",
       "  'title': 'Human Action Recognition using Factorized Spatio-Temporal Convolutional  Networks',\n",
       "  'authors': ['Kui Jia', 'Lin Sun', 'Bertram E. Shi', 'Dit-Yan Yeung'],\n",
       "  'published': '2015-10-02T11:24:04Z',\n",
       "  'updated': '2015-10-02T11:24:04Z',\n",
       "  'abstract': 'Human actions in video sequences are three-dimensional (3D) spatio-temporalsignals characterizing both the visual appearance and motion dynamics of theinvolved humans and objects. Inspired by the success of convolutional neuralnetworks (CNN) for image classification, recent attempts have been made tolearn 3D CNNs for recognizing human actions in videos. However, partly due tothe high complexity of training 3D convolution kernels and the need for largequantities of training videos, only limited success has been reported. This hastriggered us to investigate in this paper a new deep architecture which canhandle 3D signals more effectively. Specifically, we propose factorizedspatio-temporal convolutional networks (FstCN) that factorize the original 3Dconvolution kernel learning as a sequential process of learning 2D spatialkernels in the lower layers (called spatial convolutional layers), followed bylearning 1D temporal kernels in the upper layers (called temporal convolutionallayers). We introduce a novel transformation and permutation operator to makefactorization in FstCN possible. Moreover, to address the issue of sequencealignment, we propose an effective training and inference strategy based onsampling multiple video clips from a given action video sequence. We havetested FstCN on two commonly used benchmark datasets (UCF-101 and HMDB-51).Without using auxiliary training videos to boost the performance, FstCNoutperforms existing CNN based methods and achieves comparable performance witha recent method that benefits from using auxiliary training videos.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 4597-4605',\n",
       "  'citations': '368',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1510.00562v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=9473835785663262260&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 537: {'ID': 537,\n",
       "  'title': 'Exploiting Feature and Class Relationships in Video Categorization with  Regularized Deep Neural Networks',\n",
       "  'authors': ['Jun Wang',\n",
       "   'Xiangyang Xue',\n",
       "   'Zuxuan Wu',\n",
       "   'Shih-Fu Chang',\n",
       "   'Yu-Gang Jiang'],\n",
       "  'published': '2015-02-25T15:41:48Z',\n",
       "  'updated': '2018-02-21T20:37:34Z',\n",
       "  'abstract': 'In this paper, we study the challenging problem of categorizing videosaccording to high-level semantics such as the existence of a particular humanaction or a complex event. Although extensive efforts have been devoted inrecent years, most existing works combined multiple video features using simplefusion strategies and neglected the utilization of inter-class semanticrelationships. This paper proposes a novel unified framework that jointlyexploits the feature relationships and the class relationships for improvedcategorization performance. Specifically, these two types of relationships areestimated and utilized by rigorously imposing regularizations in the learningprocess of a deep neural network (DNN). Such a regularized DNN (rDNN) can beefficiently realized using a GPU-based implementation with an affordabletraining cost. Through arming the DNN with better capability of harnessing boththe feature and the class relationships, the proposed rDNN is more suitable formodeling video semantics. With extensive experimental evaluations, we show thatrDNN produces superior performance over several state-of-the-art approaches. Onthe well-known Hollywood2 and Columbia Consumer Video benchmarks, we obtainvery competitive results: 66.9\\\\% and 73.5\\\\% respectively in terms of meanaverage precision. In addition, to substantially evaluate our rDNN andstimulate future research on large scale video categorization, we collect andrelease a new benchmark dataset, called FCVID, which contains 91,223 Internetvideos and 239 manually annotated categories.',\n",
       "  'categories': ['cs.CV', 'cs.MM'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 40 (2), 352-364',\n",
       "  'citations': '216',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1502.07209v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=16107215268362475707&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 538: {'ID': 538,\n",
       "  'title': 'A simple yet effective baseline for 3d human pose estimation',\n",
       "  'authors': ['Javier Romero',\n",
       "   'Julieta Martinez',\n",
       "   'Rayat Hossain',\n",
       "   'James J. Little'],\n",
       "  'published': '2017-05-08T21:48:37Z',\n",
       "  'updated': '2017-08-04T18:36:24Z',\n",
       "  'abstract': 'Following the success of deep convolutional networks, state-of-the-artmethods for 3d human pose estimation have focused on deep end-to-end systemsthat predict 3d joint locations given raw image pixels. Despite their excellentperformance, it is often not easy to understand whether their remaining errorstems from a limited 2d pose (visual) understanding, or from a failure to map2d poses into 3-dimensional positions. With the goal of understanding thesesources of error, we set out to build a system that given 2d joint locationspredicts 3d positions. Much to our surprise, we have found that, with currenttechnology, \"lifting\" ground truth 2d joint locations to 3d space is a taskthat can be solved with a remarkably low error rate: a relatively simple deepfeed-forward network outperforms the best reported result by about 30\\\\% onHuman3.6M, the largest publicly available 3d pose estimation benchmark.Furthermore, training our system on the output of an off-the-shelfstate-of-the-art 2d detector (\\\\ie, using images as input) yields state of theart results -- this includes an array of systems that have been trainedend-to-end specifically for this task. Our results indicate that a largeportion of the error of modern deep 3d pose estimation systems stems from theirvisual analysis, and suggests directions to further advance the state of theart in 3d human pose estimation.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 2640-2649',\n",
       "  'citations': '355',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1705.03098v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=2242125562200718555&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 539: {'ID': 539,\n",
       "  'title': 'HyperFace: A Deep Multi-task Learning Framework for Face Detection,  Landmark Localization, Pose Estimation, and Gender Recognition',\n",
       "  'authors': ['Vishal M. Patel', 'Rama Chellappa', 'Rajeev Ranjan'],\n",
       "  'published': '2016-03-03T20:30:53Z',\n",
       "  'updated': '2017-12-06T01:34:20Z',\n",
       "  'abstract': 'We present an algorithm for simultaneous face detection, landmarkslocalization, pose estimation and gender recognition using deep convolutionalneural networks (CNN). The proposed method called, HyperFace, fuses theintermediate layers of a deep CNN using a separate CNN followed by a multi-tasklearning algorithm that operates on the fused features. It exploits the synergyamong the tasks which boosts up their individual performances. Additionally, wepropose two variants of HyperFace: (1) HyperFace-ResNet that builds on theResNet-101 model and achieves significant improvement in performance, and (2)Fast-HyperFace that uses a high recall fast face detector for generating regionproposals to improve the speed of the algorithm. Extensive experiments showthat the proposed models are able to capture both global and local informationin faces and performs significantly better than many competitive algorithms foreach of these four tasks.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'IEEE Transactions on Pattern Analysis and Machine Intelligence 41 (1), 121-135',\n",
       "  'citations': '666',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1603.01249v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=8494434349347836067&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 540: {'ID': 540,\n",
       "  'title': 'ECO: Efficient Convolution Operators for Tracking',\n",
       "  'authors': ['Martin Danelljan',\n",
       "   'Goutam Bhat',\n",
       "   'Fahad Shahbaz Khan',\n",
       "   'Michael Felsberg'],\n",
       "  'published': '2016-11-28T16:26:27Z',\n",
       "  'updated': '2017-04-10T18:13:05Z',\n",
       "  'abstract': 'In recent years, Discriminative Correlation Filter (DCF) based methods havesignificantly advanced the state-of-the-art in tracking. However, in thepursuit of ever increasing tracking performance, their characteristic speed andreal-time capability have gradually faded. Further, the increasingly complexmodels, with massive number of trainable parameters, have introduced the riskof severe over-fitting. In this work, we tackle the key causes behind theproblems of computational complexity and over-fitting, with the aim ofsimultaneously improving both speed and performance.  We revisit the core DCF formulation and introduce: (i) a factorizedconvolution operator, which drastically reduces the number of parameters in themodel; (ii) a compact generative model of the training sample distribution,that significantly reduces memory and time complexity, while providing betterdiversity of samples; (iii) a conservative model update strategy with improvedrobustness and reduced complexity. We perform comprehensive experiments on fourbenchmarks: VOT2016, UAV123, OTB-2015, and TempleColor. When using expensivedeep features, our tracker provides a 20-fold speedup and achieves a 13.0%relative gain in Expected Average Overlap compared to the top ranked method inthe VOT2016 challenge. Moreover, our fast variant, using hand-crafted features,operates at 60 Hz on a single CPU, while obtaining 65.0% AUC on OTB-2015.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '877',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1611.09224v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=16858432041905061958&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 541: {'ID': 541,\n",
       "  'title': 'Context Encoding for Semantic Segmentation',\n",
       "  'authors': ['Hang Zhang',\n",
       "   'Kristin Dana',\n",
       "   'Amit Agrawal',\n",
       "   'Jianping Shi',\n",
       "   'Xiaogang Wang',\n",
       "   'Zhongyue Zhang',\n",
       "   'Ambrish Tyagi'],\n",
       "  'published': '2018-03-23T17:34:21Z',\n",
       "  'updated': '2018-03-23T17:34:21Z',\n",
       "  'abstract': 'Recent work has made significant progress in improving spatial resolution forpixelwise labeling with Fully Convolutional Network (FCN) framework byemploying Dilated/Atrous convolution, utilizing multi-scale features andrefining boundaries. In this paper, we explore the impact of global contextualinformation in semantic segmentation by introducing the Context EncodingModule, which captures the semantic context of scenes and selectivelyhighlights class-dependent featuremaps. The proposed Context Encoding Modulesignificantly improves semantic segmentation results with only marginal extracomputation cost over FCN. Our approach has achieved new state-of-the-artresults 51.7% mIoU on PASCAL-Context, 85.9% mIoU on PASCAL VOC 2012. Our singlemodel achieves a final score of 0.5567 on ADE20K test set, which surpass thewinning entry of COCO-Place Challenge in 2017. In addition, we also explore howthe Context Encoding Module can improve the feature representation ofrelatively shallow networks for the image classification on CIFAR-10 dataset.Our 14 layer network has achieved an error rate of 3.45%, which is comparablewith state-of-the-art approaches with over 10 times more layers. The sourcecode for the complete system are publicly available.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '331',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1803.08904v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=11819343174629820664&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 542: {'ID': 542,\n",
       "  'title': 'Towards 3D Human Pose Estimation in the Wild: a Weakly-supervised  Approach',\n",
       "  'authors': ['Xingyi Zhou',\n",
       "   'Yichen Wei',\n",
       "   'Xiangyang Xue',\n",
       "   'Qixing Huang',\n",
       "   'Xiao Sun'],\n",
       "  'published': '2017-04-08T06:21:48Z',\n",
       "  'updated': '2017-07-30T15:01:30Z',\n",
       "  'abstract': 'In this paper, we study the task of 3D human pose estimation in the wild.This task is challenging due to lack of training data, as existing datasets areeither in the wild images with 2D pose or in the lab images with 3D pose.  We propose a weakly-supervised transfer learning method that uses mixed 2Dand 3D labels in a unified deep neutral network that presents two-stagecascaded structure. Our network augments a state-of-the-art 2D pose estimationsub-network with a 3D depth regression sub-network. Unlike previous two stageapproaches that train the two sub-networks sequentially and separately, ourtraining is end-to-end and fully exploits the correlation between the 2D poseand depth estimation sub-tasks. The deep features are better learnt throughshared representations. In doing so, the 3D pose labels in controlled labenvironments are transferred to in the wild images. In addition, we introduce a3D geometric constraint to regularize the 3D pose prediction, which iseffective in the absence of ground truth depth labels. Our method achievescompetitive results on both 2D and 3D benchmarks.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 398-407',\n",
       "  'citations': '220',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1704.02447v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=6404586540386096916&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 543: {'ID': 543,\n",
       "  'title': 'Video Frame Interpolation via Adaptive Separable Convolution',\n",
       "  'authors': ['Feng Liu', 'Long Mai', 'Simon Niklaus'],\n",
       "  'published': '2017-08-05T00:18:03Z',\n",
       "  'updated': '2017-08-05T00:18:03Z',\n",
       "  'abstract': 'Standard video frame interpolation methods first estimate optical flowbetween input frames and then synthesize an intermediate frame guided bymotion. Recent approaches merge these two steps into a single convolutionprocess by convolving input frames with spatially adaptive kernels that accountfor motion and re-sampling simultaneously. These methods require large kernelsto handle large motion, which limits the number of pixels whose kernels can beestimated at once due to the large memory demand. To address this problem, thispaper formulates frame interpolation as local separable convolution over inputframes using pairs of 1D kernels. Compared to regular 2D kernels, the 1Dkernels require significantly fewer parameters to be estimated. Our methoddevelops a deep fully convolutional neural network that takes two input framesand estimates pairs of 1D kernels for all pixels simultaneously. Since ourmethod is able to estimate kernels and synthesizes the whole video frame atonce, it allows for the incorporation of perceptual loss to train the neuralnetwork to produce visually pleasing frames. This deep neural network istrained end-to-end using widely available video data without any humanannotation. Both qualitative and quantitative experiments show that our methodprovides a practical solution to high-quality video frame interpolation.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 261-270',\n",
       "  'citations': '210',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1708.01692v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=14673917183353087261&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 544: {'ID': 544,\n",
       "  'title': 'The Fast Bilateral Solver',\n",
       "  'authors': ['Ben Poole', 'Jonathan T. Barron'],\n",
       "  'published': '2015-11-10T21:19:34Z',\n",
       "  'updated': '2016-07-22T19:12:04Z',\n",
       "  'abstract': 'We present the bilateral solver, a novel algorithm for edge-aware smoothingthat combines the flexibility and speed of simple filtering approaches with theaccuracy of domain-specific optimization algorithms. Our technique is capableof matching or improving upon state-of-the-art results on several differentcomputer vision tasks (stereo, depth superresolution, colorization, andsemantic segmentation) while being 10-1000 times faster than competingapproaches. The bilateral solver is fast, robust, straightforward to generalizeto new domains, and simple to integrate into deep learning pipelines.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'ECCV (3), 617-632',\n",
       "  'citations': '205',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1511.03296v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=17564895359862908510&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 545: {'ID': 545,\n",
       "  'title': 'Deep Networks with Stochastic Depth',\n",
       "  'authors': ['Daniel Sedra',\n",
       "   'Kilian Weinberger',\n",
       "   'Zhuang Liu',\n",
       "   'Gao Huang',\n",
       "   'Yu Sun'],\n",
       "  'published': '2016-03-30T20:58:07Z',\n",
       "  'updated': '2016-07-28T23:24:16Z',\n",
       "  'abstract': 'Very deep convolutional networks with hundreds of layers have led tosignificant reductions in error on competitive benchmarks. Although theunmatched expressiveness of the many layers can be highly desirable at testtime, training very deep networks comes with its own set of challenges. Thegradients can vanish, the forward flow often diminishes, and the training timecan be painfully slow. To address these problems, we propose stochastic depth,a training procedure that enables the seemingly contradictory setup to trainshort networks and use deep networks at test time. We start with very deepnetworks but during training, for each mini-batch, randomly drop a subset oflayers and bypass them with the identity function. This simple approachcomplements the recent success of residual networks. It reduces training timesubstantially and improves the test error significantly on almost all data setsthat we used for evaluation. With stochastic depth we can increase the depth ofresidual networks even beyond 1200 layers and still yield meaningfulimprovements in test error (4.91% on CIFAR-10).',\n",
       "  'categories': ['cs.LG', 'cs.CV', 'cs.NE'],\n",
       "  'journal': 'ECCV (4), 646-661',\n",
       "  'citations': '860',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1603.09382v3',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=9293307870467438191&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 546: {'ID': 546,\n",
       "  'title': 'ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks',\n",
       "  'authors': ['Chao Dong',\n",
       "   'Xiaoou Tang',\n",
       "   'Yihao Liu',\n",
       "   'Yu Qiao',\n",
       "   'Ke Yu',\n",
       "   'Chen Change Loy',\n",
       "   'Jinjin Gu',\n",
       "   'Shixiang Wu',\n",
       "   'Xintao Wang'],\n",
       "  'published': '2018-09-01T16:21:03Z',\n",
       "  'updated': '2018-09-17T07:00:39Z',\n",
       "  'abstract': 'The Super-Resolution Generative Adversarial Network (SRGAN) is a seminal workthat is capable of generating realistic textures during single imagesuper-resolution. However, the hallucinated details are often accompanied withunpleasant artifacts. To further enhance the visual quality, we thoroughlystudy three key components of SRGAN - network architecture, adversarial lossand perceptual loss, and improve each of them to derive an Enhanced SRGAN(ESRGAN). In particular, we introduce the Residual-in-Residual Dense Block(RRDB) without batch normalization as the basic network building unit.Moreover, we borrow the idea from relativistic GAN to let the discriminatorpredict relative realness instead of the absolute value. Finally, we improvethe perceptual loss by using the features before activation, which couldprovide stronger supervision for brightness consistency and texture recovery.Benefiting from these improvements, the proposed ESRGAN achieves consistentlybetter visual quality with more realistic and natural textures than SRGAN andwon the first place in the PIRM2018-SR Challenge. The code is available athttps://github.com/xinntao/ESRGAN .',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 0-0',\n",
       "  'citations': '429',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1809.00219v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=9756581518416283122&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 547: {'ID': 547,\n",
       "  'title': 'Video Summarization with Long Short-term Memory',\n",
       "  'authors': ['Ke Zhang', 'Fei Sha', 'Kristen Grauman', 'Wei-Lun Chao'],\n",
       "  'published': '2016-05-26T00:46:35Z',\n",
       "  'updated': '2016-07-29T07:05:34Z',\n",
       "  'abstract': 'We propose a novel supervised learning technique for summarizing videos byautomatically selecting keyframes or key subshots. Casting the problem as astructured prediction problem on sequential data, our main idea is to use LongShort-Term Memory (LSTM), a special type of recurrent neural networks to modelthe variable-range dependencies entailed in the task of video summarization.Our learning models attain the state-of-the-art results on two benchmark videodatasets. Detailed analysis justifies the design of the models. In particular,we show that it is crucial to take into consideration the sequential structuresin videos and model them. Besides advances in modeling techniques, we introducetechniques to address the need of a large number of annotated data for trainingcomplex learning models. There, our main idea is to exploit the existence ofauxiliary annotated video datasets, albeit heterogeneous in visual styles andcontents. Specifically, we show domain adaptation techniques can improvesummarization by reducing the discrepancies in statistical properties acrossthose datasets.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'journal': 'ECCV (7), 766-782',\n",
       "  'citations': '303',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1605.08110v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=9387203786908782938&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 548: {'ID': 548,\n",
       "  'title': 'Finding Tiny Faces',\n",
       "  'authors': ['Deva Ramanan', 'Peiyun Hu'],\n",
       "  'published': '2016-12-13T21:28:02Z',\n",
       "  'updated': '2017-04-15T06:18:08Z',\n",
       "  'abstract': 'Though tremendous strides have been made in object recognition, one of theremaining open challenges is detecting small objects. We explore three aspectsof the problem in the context of finding small faces: the role of scaleinvariance, image resolution, and contextual reasoning. While most recognitionapproaches aim to be scale-invariant, the cues for recognizing a 3px tall faceare fundamentally different than those for recognizing a 300px tall face. Wetake a different approach and train separate detectors for different scales. Tomaintain efficiency, detectors are trained in a multi-task fashion: they makeuse of features extracted from multiple layers of single (deep) featurehierarchy. While training detectors for large objects is straightforward, thecrucial challenge remains training detectors for small objects. We show thatcontext is crucial, and define templates that make use of massively-largereceptive fields (where 99% of the template extends beyond the object ofinterest). Finally, we explore the role of scale in pre-trained deep networks,providing ways to extrapolate networks tuned for limited scales to ratherextreme ranges. We demonstrate state-of-the-art results onmassively-benchmarked face datasets (FDDB and WIDER FACE). In particular, whencompared to prior art on WIDER FACE, our results reduce error by a factor of 2(our models produce an AP of 82% while prior art ranges from 29-64%).',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE Conference on Computer Vision and Pattern\\xa0…',\n",
       "  'citations': '437',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1612.04402v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=5935623751055355825&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 549: {'ID': 549,\n",
       "  'title': 'Generating High-Quality Crowd Density Maps using Contextual Pyramid CNNs',\n",
       "  'authors': ['Vishal M. Patel', 'Vishwanath A. Sindagi'],\n",
       "  'published': '2017-08-02T22:54:21Z',\n",
       "  'updated': '2017-08-02T22:54:21Z',\n",
       "  'abstract': 'We present a novel method called Contextual Pyramid CNN (CP-CNN) forgenerating high-quality crowd density and count estimation by explicitlyincorporating global and local contextual information of crowd images. Theproposed CP-CNN consists of four modules: Global Context Estimator (GCE), LocalContext Estimator (LCE), Density Map Estimator (DME) and a Fusion-CNN (F-CNN).GCE is a VGG-16 based CNN that encodes global context and it is trained toclassify input images into different density classes, whereas LCE is anotherCNN that encodes local context information and it is trained to performpatch-wise classification of input images into different density classes. DMEis a multi-column architecture-based CNN that aims to generate high-dimensionalfeature maps from the input image which are fused with the contextualinformation estimated by GCE and LCE using F-CNN. To generate high resolutionand high-quality density maps, F-CNN uses a set of convolutional andfractionally-strided convolutional layers and it is trained along with the DMEin an end-to-end fashion using a combination of adversarial loss andpixel-level Euclidean loss. Extensive experiments on highly challengingdatasets show that the proposed method achieves significant improvements overthe state-of-the-art methods.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 1861-1870',\n",
       "  'citations': '254',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1708.00953v1',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=16765251335283320254&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 550: {'ID': 550,\n",
       "  'title': 'NetAdapt: Platform-Aware Neural Network Adaptation for Mobile  Applications',\n",
       "  'authors': ['Mark Sandler',\n",
       "   'Vivienne Sze',\n",
       "   'Tien-Ju Yang',\n",
       "   'Hartwig Adam',\n",
       "   'Alec Go',\n",
       "   'Bo Chen',\n",
       "   'Xiao Zhang',\n",
       "   'Andrew Howard'],\n",
       "  'published': '2018-04-09T20:45:26Z',\n",
       "  'updated': '2018-09-28T19:20:16Z',\n",
       "  'abstract': 'This work proposes an algorithm, called NetAdapt, that automatically adapts apre-trained deep neural network to a mobile platform given a resource budget.While many existing algorithms simplify networks based on the number of MACs orweights, optimizing those indirect metrics may not necessarily reduce thedirect metrics, such as latency and energy consumption. To solve this problem,NetAdapt incorporates direct metrics into its adaptation algorithm. Thesedirect metrics are evaluated using empirical measurements, so that detailedknowledge of the platform and toolchain is not required. NetAdapt automaticallyand progressively simplifies a pre-trained network until the resource budget ismet while maximizing the accuracy. Experiment results show that NetAdaptachieves better accuracy versus latency trade-offs on both mobile CPU andmobile GPU, compared with the state-of-the-art automated network simplificationalgorithms. For image classification on the ImageNet dataset, NetAdapt achievesup to a 1.7$\\\\times$ speedup in measured inference latency with equal or higheraccuracy on MobileNets (V1&amp;V2).',\n",
       "  'categories': ['cs.CV'],\n",
       "  'journal': 'Proceedings of the European Conference on Computer Vision (ECCV), 285-300',\n",
       "  'citations': '157',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1804.03230v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=8372404043311956921&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 551: {'ID': 551,\n",
       "  'title': 'Video Frame Synthesis using Deep Voxel Flow',\n",
       "  'authors': ['Ziwei Liu',\n",
       "   'Yiming Liu',\n",
       "   'Xiaoou Tang',\n",
       "   'Aseem Agarwala',\n",
       "   'Raymond A. Yeh'],\n",
       "  'published': '2017-02-08T15:20:14Z',\n",
       "  'updated': '2017-08-05T04:43:44Z',\n",
       "  'abstract': 'We address the problem of synthesizing new video frames in an existing video,either in-between existing frames (interpolation), or subsequent to them(extrapolation). This problem is challenging because video appearance andmotion can be highly complex. Traditional optical-flow-based solutions oftenfail where flow estimation is challenging, while newer neural-network-basedmethods that hallucinate pixel values directly often produce blurry results. Wecombine the advantages of these two methods by training a deep network thatlearns to synthesize video frames by flowing pixel values from existing ones,which we call deep voxel flow. Our method requires no human supervision, andany video can be used as training data by dropping, and then learning topredict, existing frames. The technique is efficient, and can be applied at anyvideo resolution. We demonstrate that our method produces results that bothquantitatively and qualitatively improve upon the state-of-the-art.',\n",
       "  'categories': ['cs.CV', 'cs.GR', 'cs.LG'],\n",
       "  'journal': 'Proceedings of the IEEE International Conference on Computer Vision, 4463-4471',\n",
       "  'citations': '282',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1702.02463v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=16555360278441828137&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 552: {'ID': 552,\n",
       "  'title': 'Photo Aesthetics Ranking Network with Attributes and Content Adaptation',\n",
       "  'authors': ['Charless Fowlkes',\n",
       "   'Xiaohui Shen',\n",
       "   'Radomir Mech',\n",
       "   'Shu Kong',\n",
       "   'Zhe Lin'],\n",
       "  'published': '2016-06-06T06:14:00Z',\n",
       "  'updated': '2016-07-27T00:20:07Z',\n",
       "  'abstract': 'Real-world applications could benefit from the ability to automaticallygenerate a fine-grained ranking of photo aesthetics. However, previous methodsfor image aesthetics analysis have primarily focused on the coarse, binarycategorization of images into high- or low-aesthetic categories. In this work,we propose to learn a deep convolutional neural network to rank photoaesthetics in which the relative ranking of photo aesthetics are directlymodeled in the loss function. Our model incorporates joint learning ofmeaningful photographic attributes and image content information which can helpregularize the complicated photo aesthetics rating problem.  To train and analyze this model, we have assembled a new aesthetics andattributes database (AADB) which contains aesthetic scores and meaningfulattributes assigned to each image by multiple human raters. Anonymized rateridentities are recorded across images allowing us to exploit intra-raterconsistency using a novel sampling strategy when computing the ranking loss oftraining image pairs. We show the proposed sampling strategy is very effectiveand robust in face of subjective judgement of image aesthetics by individualswith different aesthetic tastes. Experiments demonstrate that our unified modelcan generate aesthetic rankings that are more consistent with human ratings. Tofurther validate our model, we show that by simply thresholding the estimatedaesthetic scores, we are able to achieve state-or-the-art classificationperformance on the existing AVA dataset benchmark.',\n",
       "  'categories': ['cs.CV', 'cs.IR', 'cs.MM'],\n",
       "  'journal': 'ECCV (1), 662-679',\n",
       "  'citations': '184',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1606.01621v2',\n",
       "  'gscholar_url': 'http://scholar.google.com/scholar?oi=bibs&cluster=15329324647568906190&btnI=1&nossl=1&hl=en&oe=ASCII'},\n",
       " 553: {'ID': 553,\n",
       "  'title': 'The Local Group as a time machine: studying the high-redshift Universe  with nearby galaxies',\n",
       "  'authors': ['Alex Fitts',\n",
       "   'Michael Boylan-Kolchin',\n",
       "   'Benjamin D. Johnson',\n",
       "   'Daniel R. Weisz',\n",
       "   'James S. Bullock',\n",
       "   'Charlie Conroy'],\n",
       "  'published': '2015-04-24T20:00:33Z',\n",
       "  'updated': '2015-08-27T12:56:39Z',\n",
       "  'abstract': 'We infer the UV luminosities of Local Group galaxies at early cosmic times($z \\\\sim 2$ and $z \\\\sim 7$) by combining stellar population synthesis modelingwith star formation histories derived from deep color-magnitude diagramsconstructed from Hubble Space Telescope (HST) observations. Our analysisprovides a basis for understanding high-$z$ galaxies - including those that maybe unobservable even with the James Webb Space Telescope (JWST) - in thecontext of familiar, well-studied objects in the very low-$z$ Universe. We findthat, at the epoch of reionization, all Local Group dwarfs were less luminousthan the faintest galaxies detectable in deep HST observations of blank fields.We predict that JWST will observe $z \\\\sim 7$ progenitors of galaxies similar tothe Large Magellanic Cloud today; however, the HST Frontier Fields initiativemay already be observing such galaxies, highlighting the power of gravitationallensing. Consensus reionization models require an extrapolation of the observedblank-field luminosity function at $z \\\\approx 7$ by at least two orders ofmagnitude in order to maintain reionization. This scenario requires theprogenitors of the Fornax and Sagittarius dwarf spheroidal galaxies to becontributors to the ionizing background at $z \\\\sim 7$. Combined with numericalsimulations, our results argue for a break in the UV luminosity function from afaint-end slope of $\\\\alpha \\\\sim -2$ at $M_{\\\\rm UV} &lt; -13$ to $\\\\alpha \\\\sim -1.2$at lower luminosities. Applied to photometric samples at lower redshifts, ouranalysis suggests that HST observations in lensing fields at $z \\\\sim 2$ arecapable of probing galaxies with luminosities comparable to the expectedprogenitor of Fornax.',\n",
       "  'categories': ['astro-ph.CO', 'astro-ph.GA'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1504.06621v2'},\n",
       " 554: {'ID': 554,\n",
       "  'title': 'Cascade one-vs-rest detection network for fine-grained recognition  without part annotations',\n",
       "  'authors': ['Kin-Man Lam',\n",
       "   'Long Chen',\n",
       "   'Muwei Jian',\n",
       "   'ShengKe Wang',\n",
       "   'XiaoChun Cao',\n",
       "   'Hua Zhang',\n",
       "   'Junyu Dong'],\n",
       "  'published': '2017-02-28T08:45:15Z',\n",
       "  'updated': '2017-08-23T07:05:42Z',\n",
       "  'abstract': 'Fine-grained recognition is a challenging task due to the smallintra-category variances. Most of top-performing fine-grained recognitionmethods leverage parts of objects for better performance. Therefore, partannotations which are extremely computationally expensive are required. In thispaper, we propose a novel cascaded deep CNN detection framework forfine-grained recognition which is trained to detect the whole object withoutconsidering parts. Nevertheless, most of current top-performing detectionnetworks use the N+1 class (N object categories plus background) softmax loss,and the background category with much more training samples dominates thefeature learning progress so that the features are not good for objectcategories with fewer samples. To bridge this gap, we introduce a cascadedstructure to eliminate background and exploit a one-vs-rest loss to capturemore minute variances among different subordinate categories. Experiments showthat our proposed recognition framework achieves comparable performance withstate-of-the-art, part-free, fine-grained recognition methods on theCUB-200-2011 Bird dataset. Moreover, our method even outperforms most ofpart-based methods while does not need part annotations at the training stageand is free from any annotations at test stage.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1702.08692v2'},\n",
       " 555: {'ID': 555,\n",
       "  'title': 'What Should I Ask? Using Conversationally Informative Rewards for  Goal-Oriented Visual Dialog',\n",
       "  'authors': ['Richika Sharan',\n",
       "   'Matthew Turk',\n",
       "   'Vivek Kulkarni',\n",
       "   'Carlos Elmadjian',\n",
       "   'William Yang Wang',\n",
       "   'Pushkar Shukla'],\n",
       "  'published': '2019-07-28T06:15:35Z',\n",
       "  'updated': '2019-07-28T06:15:35Z',\n",
       "  'abstract': 'The ability to engage in goal-oriented conversations has allowed humans togain knowledge, reduce uncertainty, and perform tasks more efficiently.Artificial agents, however, are still far behind humans in having goal-drivenconversations. In this work, we focus on the task of goal-oriented visualdialogue, aiming to automatically generate a series of questions about an imagewith a single objective. This task is challenging since these questions mustnot only be consistent with a strategy to achieve a goal, but also consider thecontextual information in the image. We propose an end-to-end goal-orientedvisual dialogue system, that combines reinforcement learning with regularizedinformation gain. Unlike previous approaches that have been proposed for thetask, our work is motivated by the Rational Speech Act framework, which modelsthe process of human inquiry to reach a goal. We test the two versions of ourmodel on the GuessWhat?! dataset, obtaining significant results that outperformthe current state-of-the-art models in the task of generating questions to findan undisclosed object in an image.',\n",
       "  'categories': ['cs.CL', 'cs.AI', 'cs.CV', 'cs.MM'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1907.12021v1'},\n",
       " 556: {'ID': 556,\n",
       "  'title': 'Understanding Adversarial Examples from the Mutual Influence of Images  and Perturbations',\n",
       "  'authors': ['In-So Kweon', 'Tooba Imtiaz', 'Chaoning Zhang', 'Philipp Benz'],\n",
       "  'published': '2020-07-13T05:00:09Z',\n",
       "  'updated': '2020-07-13T05:00:09Z',\n",
       "  'abstract': 'A wide variety of works have explored the reason for the existence ofadversarial examples, but there is no consensus on the explanation. We proposeto treat the DNN logits as a vector for feature representation, and exploitthem to analyze the mutual influence of two independent inputs based on thePearson correlation coefficient (PCC). We utilize this vector representation tounderstand adversarial examples by disentangling the clean images andadversarial perturbations, and analyze their influence on each other. Ourresults suggest a new perspective towards the relationship between images anduniversal perturbations: Universal perturbations contain dominant features, andimages behave like noise to them. This feature perspective leads to a newmethod for generating targeted universal adversarial perturbations using randomsource images. We are the first to achieve the challenging task of a targeteduniversal attack without utilizing original training data. Our approach using aproxy dataset achieves comparable performance to the state-of-the-art baselineswhich utilize the original training dataset.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2007.06189v1'},\n",
       " 557: {'ID': 557,\n",
       "  'title': 'The Enigmatic (Almost) Dark Galaxy Coma P: Distance Measurement and  Stellar Populations from HST Imaging',\n",
       "  'authors': ['Katherine L. Rhode',\n",
       "   'Kristen B. W. McQuinn',\n",
       "   'John J. Salzer',\n",
       "   'Samantha W. Brunker',\n",
       "   'Lukas Leisman',\n",
       "   'Martha P. Haynes',\n",
       "   'Riccardo Giovanelli',\n",
       "   'Elizabeth A. K. Adams',\n",
       "   'Steven Janowiecki',\n",
       "   'Andrew E. Dolphin',\n",
       "   'Catherine Ball',\n",
       "   'John M. Cannon'],\n",
       "  'published': '2019-01-22T19:00:00Z',\n",
       "  'updated': '2019-01-22T19:00:00Z',\n",
       "  'abstract': 'We present Hubble Space Telescope (HST) observations of the low surfacebrightness (SB) galaxy Coma P. This system was first discovered in the AreciboLegacy Fast ALFA HI survey and was cataloged as an (almost) dark galaxy becauseit did not exhibit any obvious optical counterpart in the available survey data(e.g., Sloan Digital Sky Survey). Subsequent WIYN pODI imaging revealed anultra-low SB stellar component located at the center of the HI detection. Weuse the HST images to produce a deep color-magnitude diagram (CMD) of theresolved stellar population present in Coma P. We clearly detect a red stellarsequence that we interpret to be a red giant branch, and use it to infer a tipof the red giant branch (TRGB) distance of 5.50$^{+0.28}_{-0.53}$ Mpc. The newdistance is substantially lower than earlier estimates and shows that Coma P isan extreme dwarf galaxy. Our derived stellar mass is only 4.3 $\\\\times$ 10$^5$$M_\\\\odot$, meaning that Coma P has an extreme HI-to-stellar mass ratio of 81.We present a detailed analysis of the galaxy environment within which Coma Presides. We hypothesize that Coma P formed within a local void and has spentmost of its lifetime in a low-density environment. Over time, the gravitationalattraction of the galaxies located in the void wall has moved it to the edge,where it had a recent \"fly-by\" interaction with M64. We investigate thepossibility that Coma P is at a farther distance and conclude that theavailable data are best fit by a distance of 5.5 Mpc.',\n",
       "  'categories': ['astro-ph.GA'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1901.07557v1'},\n",
       " 558: {'ID': 558,\n",
       "  'title': 'Exploring Feature Reuse in DenseNet Architectures',\n",
       "  'authors': ['Andy Hess'],\n",
       "  'published': '2018-06-05T21:11:23Z',\n",
       "  'updated': '2018-06-05T21:11:23Z',\n",
       "  'abstract': 'Densely Connected Convolutional Networks (DenseNets) have been shown toachieve state-of-the-art results on image classification tasks while usingfewer parameters and computation than competing methods. Since each layer inthis architecture has full access to the feature maps of all previous layers,the network is freed from the burden of having to relearn previously usefulfeatures, thus alleviating issues with vanishing gradients. In this work weexplore the question: To what extent is it necessary to connect to all previouslayers in order to reap the benefits of feature reuse? To this end, weintroduce the notion of local dense connectivity and present evidence that lessconnectivity, allowing for increased growth rate at a fixed network capacity,can achieve a more efficient reuse of features and lead to higher accuracy indense architectures.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1806.01935v1'},\n",
       " 559: {'ID': 559,\n",
       "  'title': 'CLEVR-Dialog: A Diagnostic Dataset for Multi-Round Reasoning in Visual  Dialog',\n",
       "  'authors': ['Satwik Kottur',\n",
       "   'Dhruv Batra',\n",
       "   'Marcus Rohrbach',\n",
       "   'José M. F. Moura',\n",
       "   'Devi Parikh'],\n",
       "  'published': '2019-03-07T20:18:39Z',\n",
       "  'updated': '2019-09-18T18:04:43Z',\n",
       "  'abstract': \"Visual Dialog is a multimodal task of answering a sequence of questionsgrounded in an image, using the conversation history as context. It entailschallenges in vision, language, reasoning, and grounding. However, studyingthese subtasks in isolation on large, real datasets is infeasible as itrequires prohibitively-expensive complete annotation of the 'state' of allimages and dialogs.  We develop CLEVR-Dialog, a large diagnostic dataset for studying multi-roundreasoning in visual dialog. Specifically, we construct a dialog grammar that isgrounded in the scene graphs of the images from the CLEVR dataset. Thiscombination results in a dataset where all aspects of the visual dialog arefully annotated. In total, CLEVR-Dialog contains 5 instances of 10-rounddialogs for about 85k CLEVR images, totaling to 4.25M question-answer pairs.  We use CLEVR-Dialog to benchmark performance of standard visual dialogmodels; in particular, on visual coreference resolution (as a function of thecoreference distance). This is the first analysis of its kind for visual dialogmodels that was not possible without this dataset. We hope the findings fromCLEVR-Dialog will help inform the development of future models for visualdialog. Our dataset and code are publicly available.\",\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1903.03166v2'},\n",
       " 560: {'ID': 560,\n",
       "  'title': 'Deep Learning for Automatic Quality Grading of Mangoes: Methods and  Insights',\n",
       "  'authors': ['Shih-Lun Wu', 'Yu-Lun Hsu', 'Hsiao-Yen Tung'],\n",
       "  'published': '2020-11-23T13:09:47Z',\n",
       "  'updated': '2020-11-23T13:09:47Z',\n",
       "  'abstract': \"The quality grading of mangoes is a crucial task for mango growers as itvastly affects their profit. However, until today, this process still relies onlaborious efforts of humans, who are prone to fatigue and errors. To remedythis, the paper approaches the grading task with various convolutional neuralnetworks (CNN), a tried-and-tested deep learning technology in computer vision.The models involved include Mask R-CNN (for background removal), the numerouspast winners of the ImageNet challenge, namely AlexNet, VGGs, and ResNets; and,a family of self-defined convolutional autoencoder-classifiers (ConvAE-Clfs)inspired by the claimed benefit of multi-task learning in classification tasks.Transfer learning is also adopted in this work via utilizing the ImageNetpretrained weights. Besides elaborating on the preprocessing techniques,training details, and the resulting performance, we go one step further toprovide explainable insights into the model's working with the help of saliencymaps and principal component analysis (PCA). These insights provide a succinct,meaningful glimpse into the intricate deep learning black box, fostering trust,and can also be presented to humans in real-world use cases for reviewing thegrading results.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2011.11378v1'},\n",
       " 561: {'ID': 561,\n",
       "  'title': 'Sentiment Analysis of German Twitter',\n",
       "  'authors': ['Wladimir Sidorenko'],\n",
       "  'published': '2019-11-29T11:24:10Z',\n",
       "  'updated': '2019-11-29T11:24:10Z',\n",
       "  'abstract': 'This thesis explores the ways by how people express their opinions on GermanTwitter, examines current approaches to automatic mining of these feelings, andproposes novel methods, which outperform state-of-the-art techniques. For thispurpose, I introduce a new corpus of German tweets that have been manuallyannotated with sentiments, their targets and holders, as well as polar termsand their contextual modifiers. Using these data, I explore four major areas ofsentiment research: (i) generation of sentiment lexicons, (ii) fine-grainedopinion mining, (iii) message-level polarity classification, and (iv)discourse-aware sentiment analysis. In the first task, I compare three populargroups of lexicon generation methods: dictionary-, corpus-, andword-embedding-based ones, finding that dictionary-based systems generallyyield better lexicons than the last two groups. Apart from this, I propose alinear projection algorithm, whose results surpass many existing automaticlexicons. Afterwords, in the second task, I examine two common approaches toautomatic prediction of sentiments, sources, and targets: conditional randomfields and recurrent neural networks, obtaining higher scores with the formermodel and improving these results even further by redefining the structure ofCRF graphs. When dealing with message-level polarity classification, Ijuxtapose three major sentiment paradigms: lexicon-, machine-learning-, anddeep-learning-based systems, and try to unite the first and last of thesegroups by introducing a bidirectional neural network with lexicon-basedattention. Finally, in order to make the new classifier aware of discoursestructure, I let it separately analyze the elementary discourse units of eachmicroblog and infer the overall polarity of a message from the scores of itsEDUs with the help of two new approaches: latent-marginalized CRFs andRecursive Dirichlet Process.',\n",
       "  'categories': ['cs.CL'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1911.13062v1'},\n",
       " 562: {'ID': 562,\n",
       "  'title': 'Sparse2Dense: From direct sparse odometry to dense 3D reconstruction',\n",
       "  'authors': ['Patric Jensfelt', 'John Folkesson', 'Jiexiong Tang'],\n",
       "  'published': '2019-03-21T19:01:37Z',\n",
       "  'updated': '2019-03-21T19:01:37Z',\n",
       "  'abstract': 'In this paper, we proposed a new deep learning based dense monocular SLAMmethod. Compared to existing methods, the proposed framework constructs a dense3D model via a sparse to dense mapping using learned surface normals. Withsingle view learned depth estimation as prior for monocular visual odometry, weobtain both accurate positioning and high quality depth reconstruction. Thedepth and normal are predicted by a single network trained in a tightly coupledmanner.Experimental results show that our method significantly improves theperformance of visual tracking and depth prediction in comparison to thestate-of-the-art in deep monocular dense SLAM.',\n",
       "  'categories': ['cs.RO', 'cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1903.09199v1'},\n",
       " 563: {'ID': 563,\n",
       "  'title': 'Auto-adaptive Resonance Equalization using Dilated Residual Networks',\n",
       "  'authors': ['Maarten Grachten', 'Alexandre Tanguy', 'Emmanuel Deruty'],\n",
       "  'published': '2018-07-23T14:18:56Z',\n",
       "  'updated': '2018-07-23T14:18:56Z',\n",
       "  'abstract': 'In music and audio production, attenuation of spectral resonances is animportant step towards a technically correct result. In this paper we present atwo-component system to automate the task of resonance equalization. The firstcomponent is a dynamic equalizer that automatically detects resonances andoffers to attenuate them by a user-specified factor. The second component is adeep neural network that predicts the optimal attenuation factor based on thewindowed audio. The network is trained and validated on empirical data gatheredfrom an experiment in which sound engineers choose their preferred attenuationfactors for a set of tracks. We test two distinct network architectures for thepredictive model and find that a dilated residual network operating directly onthe audio signal is on a par with a network architecture that requires a prioraudio feature extraction stage. Both architectures predict human-preferredresonance attenuation factors significantly better than a baseline approach.',\n",
       "  'categories': ['cs.SD', 'cs.LG', 'eess.AS'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1807.08636v1'},\n",
       " 564: {'ID': 564,\n",
       "  'title': 'Best of Both Worlds: Transferring Knowledge from Discriminative Learning  to a Generative Visual Dialog Model',\n",
       "  'authors': ['Jiasen Lu',\n",
       "   'Jianwei Yang',\n",
       "   'Dhruv Batra',\n",
       "   'Devi Parikh',\n",
       "   'Anitha Kannan'],\n",
       "  'published': '2017-06-05T22:50:37Z',\n",
       "  'updated': '2017-10-27T20:27:07Z',\n",
       "  'abstract': 'We present a novel training framework for neural sequence models,particularly for grounded dialog generation. The standard training paradigm forthese models is maximum likelihood estimation (MLE), or minimizing thecross-entropy of the human responses. Across a variety of domains, a recurringproblem with MLE trained generative neural dialog models (G) is that they tendto produce \\'safe\\' and generic responses (\"I don\\'t know\", \"I can\\'t tell\"). Incontrast, discriminative dialog models (D) that are trained to rank a list ofcandidate human responses outperform their generative counterparts; in terms ofautomatic metrics, diversity, and informativeness of the responses. However, Dis not useful in practice since it cannot be deployed to have realconversations with users.  Our work aims to achieve the best of both worlds -- the practical usefulnessof G and the strong performance of D -- via knowledge transfer from D to G. Ourprimary contribution is an end-to-end trainable generative visual dialog model,where G receives gradients from D as a perceptual (not adversarial) loss of thesequence sampled from G. We leverage the recently proposed Gumbel-Softmax (GS)approximation to the discrete distribution -- specifically, an RNN augmentedwith a sequence of GS samplers, coupled with the straight-through gradientestimator to enable end-to-end differentiability. We also introduce a strongerencoder for visual dialog, and employ a self-attention mechanism for answerencoding along with a metric learning loss to aid D in better capturingsemantic similarities in answer responses. Overall, our proposed modeloutperforms state-of-the-art on the VisDial dataset by a significant margin(2.67% on recall@10). The source code can be downloaded fromhttps://github.com/jiasenlu/visDial.pytorch.',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.CL'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1706.01554v2'},\n",
       " 565: {'ID': 565,\n",
       "  'title': 'You Only Query Once: Effective Black Box Adversarial Attacks with  Minimal Repeated Queries',\n",
       "  'authors': ['Filipe Condessa',\n",
       "   'Zico Kolter',\n",
       "   'Devin Willmott',\n",
       "   'Fatemeh Sheikholeslami',\n",
       "   'Anit Kumar Sahu'],\n",
       "  'published': '2021-01-29T19:16:51Z',\n",
       "  'updated': '2021-01-29T19:16:51Z',\n",
       "  'abstract': 'Researchers have repeatedly shown that it is possible to craft adversarialattacks on deep classifiers (small perturbations that significantly change theclass label), even in the \"black-box\" setting where one only has query accessto the classifier. However, all prior work in the black-box setting attacks theclassifier by repeatedly querying the same image with minor modifications,usually thousands of times or more, making it easy for defenders to detect anensuing attack. In this work, we instead show that it is possible to craft(universal) adversarial perturbations in the black-box setting by querying asequence of different images only once. This attack prevents detection fromhigh number of similar queries and produces a perturbation that causesmisclassification when applied to any input to the classifier. In experiments,we show that attacks that adhere to this restriction can produce untargetedadversarial perturbations that fool the vast majority of MNIST and CIFAR-10classifier inputs, as well as in excess of $60-70\\\\%$ of inputs on ImageNetclassifiers. In the targeted setting, we exhibit targeted black-box universalattacks on ImageNet classifiers with success rates above $20\\\\%$ when onlyallowed one query per image, and $66\\\\%$ when allowed two queries per image.',\n",
       "  'categories': ['cs.LG', 'cs.CR'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2102.00029v1'},\n",
       " 566: {'ID': 566,\n",
       "  'title': 'Locally optimal detection of stochastic targeted universal adversarial  perturbations',\n",
       "  'authors': ['Amish Goel', 'Pierre Moulin'],\n",
       "  'published': '2020-12-08T19:27:39Z',\n",
       "  'updated': '2020-12-08T19:27:39Z',\n",
       "  'abstract': \"Deep learning image classifiers are known to be vulnerable to smalladversarial perturbations of input images. In this paper, we derive the locallyoptimal generalized likelihood ratio test (LO-GLRT) based detector fordetecting stochastic targeted universal adversarial perturbations (UAPs) of theclassifier inputs. We also describe a supervised training method to learn thedetector's parameters, and demonstrate better performance of the detectorcompared to other detection methods on several popular image classificationdatasets.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2012.04692v1'},\n",
       " 567: {'ID': 567,\n",
       "  'title': 'Structure Learning for Neural Module Networks',\n",
       "  'authors': ['Sarath Chandar',\n",
       "   'Vardaan Pahuja',\n",
       "   'Jie Fu',\n",
       "   'Christopher J. Pal'],\n",
       "  'published': '2019-05-27T22:53:29Z',\n",
       "  'updated': '2019-05-27T22:53:29Z',\n",
       "  'abstract': 'Neural Module Networks, originally proposed for the task of visual questionanswering, are a class of neural network architectures that involvehuman-specified neural modules, each designed for a specific form of reasoning.In current formulations of such networks only the parameters of the neuralmodules and/or the order of their execution is learned. In this work, wefurther expand this approach and also learn the underlying internal structureof modules in terms of the ordering and combination of simple and elementaryarithmetic operators. Our results show that one is indeed able tosimultaneously learn both internal module structure and module sequencingwithout extra supervisory signals for module execution sequencing. With thisapproach, we report performance comparable to models using hand-designedmodules.',\n",
       "  'categories': ['cs.LG', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1905.11532v1'},\n",
       " 568: {'ID': 568,\n",
       "  'title': 'Relational Modeling for Robust and Efficient Pulmonary Lobe Segmentation  in CT Scans',\n",
       "  'authors': ['Jean-Paul Charbonnier',\n",
       "   'Weiyi Xie',\n",
       "   'Bram van Ginneken',\n",
       "   'Colin Jacobs'],\n",
       "  'published': '2020-04-16T03:54:12Z',\n",
       "  'updated': '2020-05-12T16:20:51Z',\n",
       "  'abstract': 'Pulmonary lobe segmentation in computed tomography scans is essential forregional assessment of pulmonary diseases. Recent works based on convolutionneural networks have achieved good performance for this task. However, they arestill limited in capturing structured relationships due to the nature ofconvolution. The shape of the pulmonary lobes affect each other and theirborders relate to the appearance of other structures, such as vessels, airways,and the pleural wall. We argue that such structural relationships play acritical role in the accurate delineation of pulmonary lobes when the lungs areaffected by diseases such as COVID-19 or COPD.  In this paper, we propose a relational approach (RTSU-Net) that leveragesstructured relationships by introducing a novel non-local neural networkmodule. The proposed module learns both visual and geometric relationshipsamong all convolution features to produce self-attention weights.  With a limited amount of training data available from COVID-19 subjects, weinitially train and validate RTSU-Net on a cohort of 5000 subjects from theCOPDGene study (4000 for training and 1000 for evaluation). Using modelspre-trained on COPDGene, we apply transfer learning to retrain and evaluateRTSU-Net on 470 COVID-19 suspects (370 for retraining and 100 for evaluation).Experimental results show that RTSU-Net outperforms three baselines andperforms robustly on cases with severe lung infection due to COVID-19.',\n",
       "  'categories': ['eess.IV', 'cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2004.07443v4'},\n",
       " 569: {'ID': 569,\n",
       "  'title': 'Using M32 to Study Rapid Phases of Stellar Evolution',\n",
       "  'authors': ['Henry C. Ferguson',\n",
       "   'Thomas M. Brown',\n",
       "   'Randy A. Kimble',\n",
       "   'Charles W. Bowers',\n",
       "   'Allen V. Sweigart'],\n",
       "  'published': '2002-07-10T12:16:36Z',\n",
       "  'updated': '2002-07-10T12:16:36Z',\n",
       "  'abstract': 'The compact elliptical galaxy M32 offers a unique testing ground for theoriesof stellar evolution. Because of its proximity, solar-blind UV observations canresolve the hot evolved stars in its center. Some of these late evolutionaryphases are too rapid to study adequately in globular clusters, and their studyin the Galactic field is often complicated by uncertainties in distance andreddening. Using the UV cameras on the Space Telescope Imaging Spectrograph, wehave obtained a deep color-magnitude diagram (CMD) of the M32 center. Althoughthe hot horizontal branch is well-detected, our CMD shows a striking scarcityof the brighter post-asymptotic giant branch (PAGB) and post-early AGB starsexpected for a population of this size. This dearth suggests that the evolutionto the white dwarf phase may be much more rapid than that predicted bycanonical evolutionary tracks for low-mass stars.',\n",
       "  'categories': ['astro-ph'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/astro-ph/0207211v1'},\n",
       " 570: {'ID': 570,\n",
       "  'title': 'EDVR: Video Restoration with Enhanced Deformable Convolutional Networks',\n",
       "  'authors': ['Chao Dong',\n",
       "   'Kelvin C. K. Chan',\n",
       "   'Ke Yu',\n",
       "   'Chen Change Loy',\n",
       "   'Xintao Wang'],\n",
       "  'published': '2019-05-07T17:58:14Z',\n",
       "  'updated': '2019-05-07T17:58:14Z',\n",
       "  'abstract': 'Video restoration tasks, including super-resolution, deblurring, etc, aredrawing increasing attention in the computer vision community. A challengingbenchmark named REDS is released in the NTIRE19 Challenge. This new benchmarkchallenges existing methods from two aspects: (1) how to align multiple framesgiven large motions, and (2) how to effectively fuse different frames withdiverse motion and blur. In this work, we propose a novel Video Restorationframework with Enhanced Deformable networks, termed EDVR, to address thesechallenges. First, to handle large motions, we devise a Pyramid, Cascading andDeformable (PCD) alignment module, in which frame alignment is done at thefeature level using deformable convolutions in a coarse-to-fine manner. Second,we propose a Temporal and Spatial Attention (TSA) fusion module, in whichattention is applied both temporally and spatially, so as to emphasizeimportant features for subsequent restoration. Thanks to these modules, ourEDVR wins the champions and outperforms the second place by a large margin inall four tracks in the NTIRE19 video restoration and enhancement challenges.EDVR also demonstrates superior performance to state-of-the-art publishedmethods on video super-resolution and deblurring. The code is available athttps://github.com/xinntao/EDVR.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1905.02716v1'},\n",
       " 571: {'ID': 571,\n",
       "  'title': 'Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog',\n",
       "  'authors': ['Jianfeng Gao',\n",
       "   'Linjie Li',\n",
       "   'Ahmed El Kholy',\n",
       "   'Yu Cheng',\n",
       "   'Jingjing Liu',\n",
       "   'Zhe Gan'],\n",
       "  'published': '2019-02-01T22:48:26Z',\n",
       "  'updated': '2019-06-04T05:54:02Z',\n",
       "  'abstract': 'This paper presents a new model for visual dialog, Recurrent Dual AttentionNetwork (ReDAN), using multi-step reasoning to answer a series of questionsabout an image. In each question-answering turn of a dialog, ReDAN infers theanswer progressively through multiple reasoning steps. In each step of thereasoning process, the semantic representation of the question is updated basedon the image and the previous dialog history, and the recurrently-refinedrepresentation is used for further reasoning in the subsequent step. On theVisDial v1.0 dataset, the proposed ReDAN model achieves a new state-of-the-artof 64.47% NDCG score. Visualization on the reasoning process furtherdemonstrates that ReDAN can locate context-relevant visual and textual cluesvia iterative refinement, which can lead to the correct answer step-by-step.',\n",
       "  'categories': ['cs.CV', 'cs.CL'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1902.00579v2'},\n",
       " 572: {'ID': 572,\n",
       "  'title': 'Using Deep Image Priors to Generate Counterfactual Explanations',\n",
       "  'authors': ['Vivek Narayanaswamy',\n",
       "   'Andreas Spanias',\n",
       "   'Jayaraman J. Thiagarajan'],\n",
       "  'published': '2020-10-22T20:40:44Z',\n",
       "  'updated': '2020-10-22T20:40:44Z',\n",
       "  'abstract': 'Through the use of carefully tailored convolutional neural networkarchitectures, a deep image prior (DIP) can be used to obtain pre-images fromlatent representation encodings. Though DIP inversion has been known to besuperior to conventional regularized inversion strategies such as totalvariation, such an over-parameterized generator is able to effectivelyreconstruct even images that are not in the original data distribution. Thislimitation makes it challenging to utilize such priors for tasks such ascounterfactual reasoning, wherein the goal is to generate small, interpretablechanges to an image that systematically leads to changes in the modelprediction. To this end, we propose a novel regularization strategy based on anauxiliary loss estimator jointly trained with the predictor, which efficientlyguides the prior to recover natural pre-images. Our empirical studies with areal-world ISIC skin lesion detection problem clearly evidence theeffectiveness of the proposed approach in synthesizing meaningfulcounterfactuals. In comparison, we find that the standard DIP inversion oftenproposes visually imperceptible perturbations to irrelevant parts of the image,thus providing no additional insights into the model behavior.',\n",
       "  'categories': ['cs.LG', 'cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2010.12046v1'},\n",
       " 573: {'ID': 573,\n",
       "  'title': 'Simple iterative method for generating targeted universal adversarial  perturbations',\n",
       "  'authors': ['Hokuto Hirano', 'Kazuhiro Takemoto'],\n",
       "  'published': '2019-11-15T08:02:20Z',\n",
       "  'updated': '2019-11-18T05:53:03Z',\n",
       "  'abstract': 'Deep neural networks (DNNs) are vulnerable to adversarial attacks. Inparticular, a single perturbation known as the universal adversarialperturbation (UAP) can foil most classification tasks conducted by DNNs. Thus,different methods for generating UAPs are required to fully evaluate thevulnerability of DNNs. A realistic evaluation would be with cases that considertargeted attacks; wherein the generated UAP causes DNN to classify an inputinto a specific class. However, the development of UAPs for targeted attackshas largely fallen behind that of UAPs for non-targeted attacks. Therefore, wepropose a simple iterative method to generate UAPs for targeted attacks. Ourmethod combines the simple iterative method for generating non-targeted UAPsand the fast gradient sign method for generating a targeted adversarialperturbation for an input. We applied the proposed method to state-of-the-artDNN models for image classification and proved the existence of almostimperceptible UAPs for targeted attacks; further, we demonstrated that suchUAPs are easily generatable.',\n",
       "  'categories': ['cs.CV', 'cs.CR', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1911.06502v2'},\n",
       " 574: {'ID': 574,\n",
       "  'title': 'Measurement error models: from nonparametric methods to deep neural  networks',\n",
       "  'authors': ['Zhirui Hu', 'Jun S Liu', 'Zheng Tracy Ke'],\n",
       "  'published': '2020-07-15T06:05:37Z',\n",
       "  'updated': '2020-07-15T06:05:37Z',\n",
       "  'abstract': 'The success of deep learning has inspired recent interests in applying neuralnetworks in statistical inference. In this paper, we investigate the use ofdeep neural networks for nonparametric regression with measurement errors. Wepropose an efficient neural network design for estimating measurement errormodels, in which we use a fully connected feed-forward neural network (FNN) toapproximate the regression function $f(x)$, a normalizing flow to approximatethe prior distribution of $X$, and an inference network to approximate theposterior distribution of $X$. Our method utilizes recent advances invariational inference for deep neural networks, such as the importance weightautoencoder, doubly reparametrized gradient estimator, and non-linearindependent components estimation. We conduct an extensive numerical study tocompare the neural network approach with classical nonparametric methods andobserve that the neural network approach is more flexible in accommodatingdifferent classes of regression functions and performs superior or comparableto the best available method in nearly all settings.',\n",
       "  'categories': ['stat.ML', 'cs.LG', 'stat.ME'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2007.07498v1'},\n",
       " 575: {'ID': 575,\n",
       "  'title': 'The Devil is in Classification: A Simple Framework for Long-tail Object  Detection and Instance Segmentation',\n",
       "  'authors': ['Bingyi Kang',\n",
       "   'Junhao Liew',\n",
       "   'Yu Li',\n",
       "   'Sheng Tang',\n",
       "   'Junnan Li',\n",
       "   'Tao Wang',\n",
       "   'Jiashi Feng',\n",
       "   'Steven Hoi'],\n",
       "  'published': '2020-07-23T12:49:07Z',\n",
       "  'updated': '2020-11-03T04:11:23Z',\n",
       "  'abstract': 'Most existing object instance detection and segmentation models only workwell on fairly balanced benchmarks where per-category training sample numbersare comparable, such as COCO. They tend to suffer performance drop on realisticdatasets that are usually long-tailed. This work aims to study and address suchopen challenges. Specifically, we systematically investigate performance dropof the state-of-the-art two-stage instance segmentation model Mask R-CNN on therecent long-tail LVIS dataset, and unveil that a major cause is the inaccurateclassification of object proposals. Based on such an observation, we firstconsider various techniques for improving long-tail classification performancewhich indeed enhance instance segmentation results. We then propose a simplecalibration framework to more effectively alleviate classification head biaswith a bi-level class balanced sampling approach. Without bells and whistles,it significantly boosts the performance of instance segmentation for tailclasses on the recent LVIS dataset and our sampled COCO-LT dataset. Ouranalysis provides useful insights for solving long-tail instance detection andsegmentation problems, and the straightforward \\\\emph{SimCal} method can serveas a simple but strong baseline. With the method we have won the 2019 LVISchallenge. Codes and models are available at https://github.com/twangnh/SimCal.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2007.11978v5'},\n",
       " 576: {'ID': 576,\n",
       "  'title': 'TreeGrad: Transferring Tree Ensembles to Neural Networks',\n",
       "  'authors': ['Chapman Siu'],\n",
       "  'published': '2019-04-25T02:54:32Z',\n",
       "  'updated': '2019-12-09T12:28:48Z',\n",
       "  'abstract': 'Gradient Boosting Decision Tree (GBDT) are popular machine learningalgorithms with implementations such as LightGBM and in popular machinelearning toolkits like Scikit-Learn. Many implementations can only producetrees in an offline manner and in a greedy manner. We explore ways to convertexisting GBDT implementations to known neural network architectures withminimal performance loss in order to allow decision splits to be updated in anonline manner and provide extensions to allow splits points to be altered as aneural architecture search problem. We provide learning bounds for our neuralnetwork.',\n",
       "  'categories': ['stat.ML', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1904.11132v3'},\n",
       " 577: {'ID': 577,\n",
       "  'title': 'Knowledge Distillation for BERT Unsupervised Domain Adaptation',\n",
       "  'authors': ['Kichun Lee', 'Minho Ryu'],\n",
       "  'published': '2020-10-22T06:51:24Z',\n",
       "  'updated': '2020-10-23T02:12:06Z',\n",
       "  'abstract': 'A pre-trained language model, BERT, has brought significant performanceimprovements across a range of natural language processing tasks. Since themodel is trained on a large corpus of diverse topics, it shows robustperformance for domain shift problems in which data distributions at training(source data) and testing (target data) differ while sharing similarities.Despite its great improvements compared to previous models, it still suffersfrom performance degradation due to domain shifts. To mitigate such problems,we propose a simple but effective unsupervised domain adaptation method,adversarial adaptation with distillation (AAD), which combines the adversarialdiscriminative domain adaptation (ADDA) framework with knowledge distillation.We evaluate our approach in the task of cross-domain sentiment classificationon 30 domain pairs, advancing the state-of-the-art performance for unsuperviseddomain adaptation in text sentiment classification.',\n",
       "  'categories': ['cs.CL'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2010.11478v2'},\n",
       " 578: {'ID': 578,\n",
       "  'title': 'Negative Binomial Process Count and Mixture Modeling',\n",
       "  'authors': ['Mingyuan Zhou', 'Lawrence Carin'],\n",
       "  'published': '2012-09-15T21:55:36Z',\n",
       "  'updated': '2013-10-13T01:01:39Z',\n",
       "  'abstract': 'The seemingly disjoint problems of count and mixture modeling are unitedunder the negative binomial (NB) process. A gamma process is employed to modelthe rate measure of a Poisson process, whose normalization provides a randomprobability measure for mixture modeling and whose marginalization leads to anNB process for count modeling. A draw from the NB process consists of a Poissondistributed finite number of distinct atoms, each of which is associated with alogarithmic distributed number of data samples. We reveal relationships betweenvarious count- and mixture-modeling distributions and construct aPoisson-logarithmic bivariate distribution that connects the NB and Chineserestaurant table distributions. Fundamental properties of the models aredeveloped, and we derive efficient Bayesian inference. It is shown that withaugmentation and normalization, the NB process and gamma-NB process can bereduced to the Dirichlet process and hierarchical Dirichlet process,respectively. These relationships highlight theoretical, structural andcomputational advantages of the NB process. A variety of NB processes,including the beta-geometric, beta-NB, marked-beta-NB, marked-gamma-NB andzero-inflated-NB processes, with distinct sharing mechanisms, are alsoconstructed. These models are applied to topic modeling, with connections madeto existing algorithms under Poisson factor analysis. Example results show theimportance of inferring both the NB dispersion and probability parameters.',\n",
       "  'categories': ['stat.ME', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1209.3442v3'},\n",
       " 579: {'ID': 579,\n",
       "  'title': 'Unsupervised Hyperspectral Mixed Noise Removal Via Spatial-Spectral  Constrained Deep Image Prior',\n",
       "  'authors': ['Yu-Bang Zheng',\n",
       "   'Yi Chang',\n",
       "   'Tai-Xiang Jiang',\n",
       "   'Yi-Si Luo',\n",
       "   'Xi-Le Zhao'],\n",
       "  'published': '2020-08-22T04:25:08Z',\n",
       "  'updated': '2020-08-22T04:25:08Z',\n",
       "  'abstract': 'Hyperspectral images (HSIs) are unavoidably corrupted by mixed noise whichhinders the subsequent applications. Traditional methods exploit the structureof the HSI via optimization-based models for denoising, while their capacity isinferior to the convolutional neural network (CNN)-based methods, whichsupervisedly learn the noisy-to-denoised mapping from a large amount of data.However, as the clean-noisy pairs of hyperspectral data are always unavailablein many applications, it is eager to build an unsupervised HSI denoising methodwith high model capability. To remove the mixed noise in HSIs, we suggest thespatial-spectral constrained deep image prior (S2DIP), which simultaneouslycapitalize the high model representation ability brought by the CNN in anunsupervised manner and does not need any extra training data. Specifically, weemploy the separable 3D convolution blocks to faithfully encode the HSI in theframework of DIP, and a spatial-spectral total variation (SSTV) term istailored to explore the spatial-spectral smoothness of HSIs. Moreover, ourmethod favorably addresses the semi-convergence behavior of prevailingunsupervised methods, e.g., DIP 2D, and DIP 3D. Extensive experimentsdemonstrate that the proposed method outperforms state-of-the-artoptimization-based HSI denoising methods in terms of effectiveness androbustness.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2008.09753v1'},\n",
       " 580: {'ID': 580,\n",
       "  'title': 'ChaLearn Looking at People: IsoGD and ConGD Large-scale RGB-D Gesture  Recognition',\n",
       "  'authors': ['Qiguang Miao',\n",
       "   'Isabelle Guyon',\n",
       "   'Gholamreza Anbarjafari',\n",
       "   'Chi Lin',\n",
       "   'Guodong Guo',\n",
       "   'Stan Z. Li',\n",
       "   'Jun Wan',\n",
       "   'Yunan Li',\n",
       "   'Longyin Wen',\n",
       "   'Sergio Escalera'],\n",
       "  'published': '2019-07-29T03:09:40Z',\n",
       "  'updated': '2019-07-29T03:09:40Z',\n",
       "  'abstract': 'The ChaLearn large-scale gesture recognition challenge has been run twice intwo workshops in conjunction with the International Conference on PatternRecognition (ICPR) 2016 and International Conference on Computer Vision (ICCV)2017, attracting more than $200$ teams round the world. This challenge has twotracks, focusing on isolated and continuous gesture recognition, respectively.This paper describes the creation of both benchmark datasets and analyzes theadvances in large-scale gesture recognition based on these two datasets. Wediscuss the challenges of collecting large-scale ground-truth annotations ofgesture recognition, and provide a detailed analysis of the currentstate-of-the-art methods for large-scale isolated and continuous gesturerecognition based on RGB-D video sequences. In addition to recognition rate andmean jaccard index (MJI) as evaluation metrics used in our previous challenges,we also introduce the corrected segmentation rate (CSR) metric to evaluate theperformance of temporal segmentation for continuous gesture recognition.Furthermore, we propose a bidirectional long short-term memory (Bi-LSTM)baseline method, determining the video division points based on the skeletonpoints extracted by convolutional pose machine (CPM). Experiments demonstratethat the proposed Bi-LSTM outperforms the state-of-the-art methods with anabsolute improvement of $8.1\\\\%$ (from $0.8917$ to $0.9639$) of CSR.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1907.12193v1'},\n",
       " 581: {'ID': 581,\n",
       "  'title': 'Dynamic Connected Neural Decision Classifier and Regressor with Dynamic  Softing Pruning',\n",
       "  'authors': ['Pengcheng Zhou',\n",
       "   'Yujian He',\n",
       "   'Xinyu Fan',\n",
       "   'Junlong Liu',\n",
       "   'Hui Xu',\n",
       "   'Faen Zhang'],\n",
       "  'published': '2019-11-13T13:21:10Z',\n",
       "  'updated': '2019-11-20T15:12:09Z',\n",
       "  'abstract': 'To deal with datasets of different complexity, this paper presents anefficient learning model that combines the proposed Dynamic Connected NeuralDecision Networks (DNDN) and a new pruning method--Dynamic Soft Pruning (DSP).DNDN is a combination of random forests and deep neural networks thereby itenjoys both the properties of powerful classification capability andrepresentation learning functionality. Different from Deep Neural DecisionForests (DNDF), this paper adopts an end-to-end training approach byrepresenting the classification distribution with multiple randomly initializedsoftmax layers, which enables the placement of the forest trees after eachlayer in the neural network and greatly improves the training speed andstability. Furthermore, DSP is proposed to reduce the redundant connections ofthe network in a soft fashion which has high flexibility but demonstrates noperformance loss compared with previous approaches. Extensive experiments ondifferent datasets demonstrate the superiority of the proposed model over otherpopular algorithms in solving classification tasks.',\n",
       "  'categories': ['cs.LG', 'cs.AI', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1911.05443v2'},\n",
       " 582: {'ID': 582,\n",
       "  'title': 'DUNet: A deformable network for retinal vessel segmentation',\n",
       "  'authors': ['Leyi Wei',\n",
       "   'Qiangguo Jin',\n",
       "   'Zhaopeng Meng',\n",
       "   'Ran Su',\n",
       "   'Qi Chen',\n",
       "   'Tuan D. Pham'],\n",
       "  'published': '2018-11-03T13:05:06Z',\n",
       "  'updated': '2018-11-03T13:05:06Z',\n",
       "  'abstract': \"Automatic segmentation of retinal vessels in fundus images plays an importantrole in the diagnosis of some diseases such as diabetes and hypertension. Inthis paper, we propose Deformable U-Net (DUNet), which exploits the retinalvessels' local features with a U-shape architecture, in an end to end mannerfor retinal vessel segmentation. Inspired by the recently introduced deformableconvolutional networks, we integrate the deformable convolution into theproposed network. The DUNet, with upsampling operators to increase the outputresolution, is designed to extract context information and enable preciselocalization by combining low-level feature maps with high-level ones.Furthermore, DUNet captures the retinal vessels at various shapes and scales byadaptively adjusting the receptive fields according to vessels' scales andshapes. Three public datasets DRIVE, STARE and CHASE_DB1 are used to train andtest our model. Detailed comparisons between the proposed network and thedeformable neural network, U-Net are provided in our study. Results show thatmore detailed vessels are extracted by DUNet and it exhibits state-of-the-artperformance for retinal vessel segmentation with a global accuracy of0.9697/0.9722/0.9724 and AUC of 0.9856/0.9868/0.9863 on DRIVE, STARE andCHASE_DB1 respectively. Moreover, to show the generalization ability of theDUNet, we used another two retinal vessel data sets, one is named WIDE and theother is a synthetic data set with diverse styles, named SYNTHE, toqualitatively and quantitatively analyzed and compared with other methods.Results indicates that DUNet outperforms other state-of-the-arts.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1811.01206v1'},\n",
       " 583: {'ID': 583,\n",
       "  'title': 'Stochastic Proximal Algorithms with SON Regularization: Towards  Efficient Optimal Transport for Domain Adaptation',\n",
       "  'authors': ['Devdatt Dubhashi',\n",
       "   'Morteza Haghir Chehreghani',\n",
       "   'Arman Rahbar',\n",
       "   'Ashkan Panahi'],\n",
       "  'published': '2019-03-09T18:54:21Z',\n",
       "  'updated': '2020-03-03T11:34:03Z',\n",
       "  'abstract': 'We propose a new regularizer for optimal transport (OT) which is tailored tobetter preserve the class structure of the subjected process. Accordingly, weprovide the first theoretical guarantees for an OT scheme that respects classstructure. We derive an accelerated proximal algorithm with a closed formprojection and proximal operator scheme thereby affording a highly scalablealgorithm for computing optimal transport plans. We provide a novel argumentfor the uniqueness of the optimum even in the absence of strong convexity.Ourexperiments show that the new regularizer does not only result in a betterpreservation of the class structure but also in additional robustness relativeto previous regularizers.',\n",
       "  'categories': ['cs.LG', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1903.03850v2'},\n",
       " 584: {'ID': 584,\n",
       "  'title': 'Rethinking Normalization and Elimination Singularity in Neural Networks',\n",
       "  'authors': ['Siyuan Qiao',\n",
       "   'Chenxi Liu',\n",
       "   'Wei Shen',\n",
       "   'Huiyu Wang',\n",
       "   'Alan Yuille'],\n",
       "  'published': '2019-11-21T20:36:04Z',\n",
       "  'updated': '2019-11-21T20:36:04Z',\n",
       "  'abstract': 'In this paper, we study normalization methods for neural networks from theperspective of elimination singularity. Elimination singularities correspond tothe points on the training trajectory where neurons become consistentlydeactivated. They cause degenerate manifolds in the loss landscape which willslow down training and harm model performances. We show that channel-basednormalizations (e.g. Layer Normalization and Group Normalization) are unable toguarantee a far distance from elimination singularities, in contrast with BatchNormalization which by design avoids models from getting too close to them. Toaddress this issue, we propose BatchChannel Normalization (BCN), which usesbatch knowledge to avoid the elimination singularities in the training ofchannel-normalized models. Unlike Batch Normalization, BCN is able to run inboth large-batch and micro-batch training settings. The effectiveness of BCN isverified on many tasks, including image classification, object detection,instance segmentation, and semantic segmentation. The code is here:https://github.com/joe-siyuan-qiao/Batch-Channel-Normalization.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1911.09738v1'},\n",
       " 585: {'ID': 585,\n",
       "  'title': 'Fast Supervised Discrete Hashing and its Analysis',\n",
       "  'authors': ['Mitsuru Ambai', 'Keiichiro Shirai', 'Gou Koutaki'],\n",
       "  'published': '2016-11-30T06:35:39Z',\n",
       "  'updated': '2016-11-30T06:35:39Z',\n",
       "  'abstract': 'In this paper, we propose a learning-based supervised discrete hashingmethod. Binary hashing is widely used for large-scale image retrieval as wellas video and document searches because the compact representation of binarycode is essential for data storage and reasonable for query searches usingbit-operations. The recently proposed Supervised Discrete Hashing (SDH)efficiently solves mixed-integer programming problems by alternatingoptimization and the Discrete Cyclic Coordinate descent (DCC) method. We showthat the SDH model can be simplified without performance degradation based onsome preliminary experiments; we call the approximate model for this the \"FastSDH\" (FSDH) model. We analyze the FSDH model and provide a mathematically exactsolution for it. In contrast to SDH, our model does not require an alternatingoptimization algorithm and does not depend on initial values. FSDH is alsoeasier to implement than Iterative Quantization (ITQ). Experimental resultsinvolving a large-scale database showed that FSDH outperforms conventional SDHin terms of precision, recall, and computation time.',\n",
       "  'categories': ['cs.CV', 'cs.LG', 'cs.MM'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1611.10017v1'},\n",
       " 586: {'ID': 586,\n",
       "  'title': 'On the Effectiveness of Least Squares Generative Adversarial Networks',\n",
       "  'authors': ['Stephen Paul Smolley',\n",
       "   'Haoran Xie',\n",
       "   'Xudong Mao',\n",
       "   'Raymond Y. K. Lau',\n",
       "   'Qing Li',\n",
       "   'Zhen Wang'],\n",
       "  'published': '2017-12-18T13:36:09Z',\n",
       "  'updated': '2018-09-21T07:48:53Z',\n",
       "  'abstract': 'Unsupervised learning with generative adversarial networks (GANs) has provento be hugely successful. Regular GANs hypothesize the discriminator as aclassifier with the sigmoid cross entropy loss function. However, we found thatthis loss function may lead to the vanishing gradients problem during thelearning process. To overcome such a problem, we propose in this paper theLeast Squares Generative Adversarial Networks (LSGANs) which adopt the leastsquares loss for both the discriminator and the generator. We show thatminimizing the objective function of LSGAN yields minimizing the Pearson$\\\\chi^2$ divergence. We also show that the derived objective function thatyields minimizing the Pearson $\\\\chi^2$ divergence performs better than theclassical one of using least squares for classification. There are two benefitsof LSGANs over regular GANs. First, LSGANs are able to generate higher qualityimages than regular GANs. Second, LSGANs perform more stably during thelearning process. For evaluating the image quality, we conduct both qualitativeand quantitative experiments, and the experimental results show that LSGANs cangenerate higher quality images than regular GANs. Furthermore, we evaluate thestability of LSGANs in two groups. One is to compare between LSGANs and regularGANs without gradient penalty. We conduct three experiments, including Gaussianmixture distribution, difficult architectures, and a newly proposed method ---datasets with small variability, to illustrate the stability of LSGANs. Theother one is to compare between LSGANs with gradient penalty (LSGANs-GP) andWGANs with gradient penalty (WGANs-GP). The experimental results show thatLSGANs-GP succeed in training for all the difficult architectures used inWGANs-GP, including 101-layer ResNet.',\n",
       "  'categories': ['cs.CV', 'cs.LG', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1712.06391v2'},\n",
       " 587: {'ID': 587,\n",
       "  'title': 'Learning to Collocate Neural Modules for Image Captioning',\n",
       "  'authors': ['Hanwang Zhang', 'Xu Yang', 'Jianfei Cai'],\n",
       "  'published': '2019-04-18T07:03:19Z',\n",
       "  'updated': '2019-04-18T07:03:19Z',\n",
       "  'abstract': \"We do not speak word by word from scratch; our brain quickly structures apattern like \\\\textsc{sth do sth at someplace} and then fill in the detaileddescriptions. To render existing encoder-decoder image captioners suchhuman-like reasoning, we propose a novel framework: learning to CollocateNeural Modules (CNM), to generate the `inner pattern' connecting visual encoderand language decoder. Unlike the widely-used neural module networks in visualQ\\\\&amp;A, where the language (ie, question) is fully observable, CNM for captioningis more challenging as the language is being generated and thus is partiallyobservable. To this end, we make the following technical contributions for CNMtraining: 1) compact module design --- one for function words and three forvisual content words (eg, noun, adjective, and verb), 2) soft module fusion andmulti-step module execution, robustifying the visual reasoning in partialobservation, 3) a linguistic loss for module controller being faithful topart-of-speech collocations (eg, adjective is before noun). Extensiveexperiments on the challenging MS-COCO image captioning benchmark validate theeffectiveness of our CNM image captioner. In particular, CNM achieves a newstate-of-the-art 127.9 CIDEr-D on Karpathy split and a single-model 126.0 c40on the official server. CNM is also robust to few training samples, eg, bytraining only one sentence per image, CNM can halve the performance losscompared to a strong baseline.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1904.08608v1'},\n",
       " 588: {'ID': 588,\n",
       "  'title': 'Overgroups of exterior powers of an elementary group. I. Levels and  normalizers',\n",
       "  'authors': ['Roman Lubkov', 'Ilia Nekrasov'],\n",
       "  'published': '2018-01-24T10:58:48Z',\n",
       "  'updated': '2018-07-18T13:56:48Z',\n",
       "  'abstract': 'In the present paper, we prove the first part in the standard description ofgroups $H$ lying between $m$-th exterior power of elementary group $E(n,R)$ andthe general linear group $GL_{\\\\binom{n}{m}}(R)$. We study structure of theexterior power of elementary group and its relative analog$E\\\\left(\\\\binom{n}{m},R,A\\\\right)$. In the considering case $n \\\\geq 3m$, thedescription is explained by the classical notion of level: for every such $H$we find unique ideal $A$ of the ring $R$. Motivated by the problem, we provethe coincidence of the following groups: normalizer of the exterior power ofelementary group, normalizer of the exterior power of special linear group,transporter of the exterior power of elementary group into the exterior powerof special linear group, and an exterior power of general linear group. Thisresult mainly follows from the found explicit equations for the exterior powerof algebraic group scheme $GL_n(\\\\_)$.',\n",
       "  'categories': ['math.GR', 'math.RT'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1801.07918v2'},\n",
       " 589: {'ID': 589,\n",
       "  'title': 'Modality-Balanced Models for Visual Dialogue',\n",
       "  'authors': ['Mohit Bansal', 'Hao Tan', 'Hyounghun Kim'],\n",
       "  'published': '2020-01-17T14:57:12Z',\n",
       "  'updated': '2020-01-17T14:57:12Z',\n",
       "  'abstract': 'The Visual Dialog task requires a model to exploit both image andconversational context information to generate the next response to thedialogue. However, via manual analysis, we find that a large number ofconversational questions can be answered by only looking at the image withoutany access to the context history, while others still need the conversationcontext to predict the correct answers. We demonstrate that due to this reason,previous joint-modality (history and image) models over-rely on and are moreprone to memorizing the dialogue history (e.g., by extracting certain keywordsor patterns in the context information), whereas image-only models are moregeneralizable (because they cannot memorize or extract keywords from history)and perform substantially better at the primary normalized discountedcumulative gain (NDCG) task metric which allows multiple correct answers.Hence, this observation encourages us to explicitly maintain two models, i.e.,an image-only model and an image-history joint model, and combine theircomplementary abilities for a more balanced multimodal model. We presentmultiple methods for this integration of the two models, via ensemble andconsensus dropout fusion with shared parameters. Empirically, our modelsachieve strong results on the Visual Dialog challenge 2019 (rank 3 on NDCG andhigh balance across metrics), and substantially outperform the winner of theVisual Dialog challenge 2018 on most metrics.',\n",
       "  'categories': ['cs.CL', 'cs.AI', 'cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2001.06354v1'},\n",
       " 590: {'ID': 590,\n",
       "  'title': 'Rehearsal-Free Continual Learning over Small Non-I.I.D. Batches',\n",
       "  'authors': ['Lorenzo Pellegrini', 'Davide Maltoni', 'Vincenzo Lomonaco'],\n",
       "  'published': '2019-07-08T18:32:25Z',\n",
       "  'updated': '2020-04-21T16:13:12Z',\n",
       "  'abstract': 'Robotic vision is a field where continual learning can play a significantrole. An embodied agent operating in a complex environment subject to frequentand unpredictable changes is required to learn and adapt continuously. In thecontext of object recognition, for example, a robot should be able to learn(without forgetting) objects of never before seen classes as well as improvingits recognition capabilities as new instances of already known classes arediscovered. Ideally, continual learning should be triggered by the availabilityof short videos of single objects and performed on-line on on-board hardwarewith fine-grained updates. In this paper, we introduce a novel continuallearning protocol based on the CORe50 benchmark and propose two rehearsal-freecontinual learning techniques, CWR* and AR1*, that can learn effectively evenin the challenging case of nearly 400 small non-i.i.d. incremental batches. Inparticular, our experiments show that AR1* can outperform otherstate-of-the-art rehearsal-free techniques by more than 15% accuracy in somecases, with a very light and constant computational and memory overhead acrosstraining batches.',\n",
       "  'categories': ['cs.LG', 'cs.CV', 'cs.NE', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1907.03799v3'},\n",
       " 591: {'ID': 591,\n",
       "  'title': 'A Conceptual Framework for Lifelong Learning',\n",
       "  'authors': ['Tanner Bohn', 'Charles X. Ling'],\n",
       "  'published': '2019-11-21T19:08:18Z',\n",
       "  'updated': '2020-06-15T18:34:47Z',\n",
       "  'abstract': 'Humans can learn a variety of concepts and skills incrementally over thecourse of their lives while exhibiting many desirable properties, such ascontinual learning without forgetting, forward transfer and backward transferof knowledge, and learning a new concept or task with only a few examples.Several lines of machine learning research, such as lifelong learning, few-shotlearning, and transfer learning, attempt to capture these properties. However,most previous approaches can only demonstrate subsets of these properties,often by different complex mechanisms. In this work, we propose a simple yetpowerful unified framework that supports almost all of these properties andapproaches through one central mechanism. We also draw connections between manypeculiarities of human learning (such as memory loss and \"rain man\") and ourframework. While we do not present any state-of-the-art results, we hope thatthis conceptual framework provides a novel perspective on existing work andproposes many new research directions.',\n",
       "  'categories': ['cs.LG', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1911.09704v4'},\n",
       " 592: {'ID': 592,\n",
       "  'title': 'Universal Adversarial Perturbation for Text Classification',\n",
       "  'authors': ['Hang Gao', 'Tim Oates'],\n",
       "  'published': '2019-10-10T14:48:22Z',\n",
       "  'updated': '2019-10-10T14:48:22Z',\n",
       "  'abstract': 'Given a state-of-the-art deep neural network text classifier, we show theexistence of a universal and very small perturbation vector (in the embeddingspace) that causes natural text to be misclassified with high probability.Unlike images on which a single fixed-size adversarial perturbation can befound, text is of variable length, so we define the \"universality\" as\"token-agnostic\", where a single perturbation is applied to each token,resulting in different perturbations of flexible sizes at the sequence level.We propose an algorithm to compute universal adversarial perturbations, andshow that the state-of-the-art deep neural networks are highly vulnerable tothem, even though they keep the neighborhood of tokens mostly preserved. Wealso show how to use these adversarial perturbations to generate adversarialtext samples. The surprising existence of universal \"token-agnostic\"adversarial perturbations may reveal important properties of a text classifier.',\n",
       "  'categories': ['cs.CL', 'cs.LG', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1910.04618v1'},\n",
       " 593: {'ID': 593,\n",
       "  'title': 'Seed Phenotyping on Neural Networks using Domain Randomization and  Transfer Learning',\n",
       "  'authors': ['Venkat Margapuri', 'Mitchell Neilsen'],\n",
       "  'published': '2020-12-24T14:04:28Z',\n",
       "  'updated': '2020-12-24T14:04:28Z',\n",
       "  'abstract': 'Seed phenotyping is the idea of analyzing the morphometric characteristics ofa seed to predict the behavior of the seed in terms of development, toleranceand yield in various environmental conditions. The focus of the work is theapplication and feasibility analysis of the state-of-the-art object detectionand localization neural networks, Mask R-CNN and YOLO (You Only Look Once), forseed phenotyping using Tensorflow. One of the major bottlenecks of such anendeavor is the need for large amounts of training data. While the capture of amultitude of seed images is taunting, the images are also required to beannotated to indicate the boundaries of the seeds on the image and converted todata formats that the neural networks are able to consume. Although tools tomanually perform the task of annotation are available for free, the amount oftime required is enormous. In order to tackle such a scenario, the idea ofdomain randomization i.e. the technique of applying models trained on imagescontaining simulated objects to real-world objects, is considered. In addition,transfer learning i.e. the idea of applying the knowledge obtained whilesolving a problem to a different problem, is used. The networks are trained onpre-trained weights from the popular ImageNet and COCO data sets. As part ofthe work, experiments with different parameters are conducted on five differentseed types namely, canola, rough rice, sorghum, soy, and wheat.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2012.13259v1'},\n",
       " 594: {'ID': 594,\n",
       "  'title': 'The Devil is in the Boundary: Exploiting Boundary Representation for  Basis-based Instance Segmentation',\n",
       "  'authors': ['Myungchul Kim', 'In So Kweon', 'Dahun Kim', 'Sanghyun Woo'],\n",
       "  'published': '2020-11-26T11:26:06Z',\n",
       "  'updated': '2020-11-26T11:26:06Z',\n",
       "  'abstract': 'Pursuing a more coherent scene understanding towards real-time visionapplications, single-stage instance segmentation has recently gainedpopularity, achieving a simpler and more efficient design than its two-stagecounterparts. Besides, its global mask representation often leads to superioraccuracy to the two-stage Mask R-CNN which has been dominant thus far. Despitethe promising advances in single-stage methods, finer delineation of instanceboundaries still remains unexcavated. Indeed, boundary information provides astrong shape representation that can operate in synergy with thefully-convolutional mask features of the single-stage segmenter. In this work,we propose Boundary Basis based Instance Segmentation(B2Inst) to learn a globalboundary representation that can complement existing global-mask-based methodsthat are often lacking high-frequency details. Besides, we devise a unifiedquality measure of both mask and boundary and introduce a network block thatlearns to score the per-instance predictions of itself. When applied to thestrongest baselines in single-stage instance segmentation, our B2Inst leads toconsistent improvements and accurately parse out the instance boundaries in ascene. Regardless of being single-stage or two-stage frameworks, we outperformthe existing state-of-the-art methods on the COCO dataset with the sameResNet-50 and ResNet-101 backbones.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2011.13241v1'},\n",
       " 595: {'ID': 595,\n",
       "  'title': 'Multi-View Attention Network for Visual Dialog',\n",
       "  'authors': ['Taesun Whang', 'Yeochan Yoon', 'Heuiseok Lim', 'Sungjin Park'],\n",
       "  'published': '2020-04-29T08:46:38Z',\n",
       "  'updated': '2020-10-07T00:51:40Z',\n",
       "  'abstract': 'Visual dialog is a challenging vision-language task in which a series ofquestions visually grounded by a given image are answered. To resolve thevisual dialog task, a high-level understanding of various multimodal inputs(e.g., question, dialog history, and image) is required. Specifically, it isnecessary for an agent to 1) determine the semantic intent of question and 2)align question-relevant textual and visual contents among heterogeneousmodality inputs. In this paper, we propose Multi-View Attention Network (MVAN),which leverages multiple views about heterogeneous inputs based on attentionmechanisms. MVAN effectively captures the question-relevant information fromthe dialog history with two complementary modules (i.e., Topic Aggregation andContext Matching), and builds multimodal representations through sequentialalignment processes (i.e., Modality Alignment). Experimental results on VisDialv1.0 dataset show the effectiveness of our proposed model, which outperformsthe previous state-of-the-art methods with respect to all evaluation metrics.',\n",
       "  'categories': ['cs.AI', 'cs.CL'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2004.14025v3'},\n",
       " 596: {'ID': 596,\n",
       "  'title': 'SCNet: Training Inference Sample Consistency for Instance Segmentation',\n",
       "  'authors': ['Thang Vu', 'Haeyong Kang', 'Chang D. Yoo'],\n",
       "  'published': '2020-12-18T10:26:54Z',\n",
       "  'updated': '2020-12-18T10:26:54Z',\n",
       "  'abstract': 'Cascaded architectures have brought significant performance improvement inobject detection and instance segmentation. However, there are lingering issuesregarding the disparity in the Intersection-over-Union (IoU) distribution ofthe samples between training and inference. This disparity can potentiallyexacerbate detection accuracy. This paper proposes an architecture referred toas Sample Consistency Network (SCNet) to ensure that the IoU distribution ofthe samples at training time is close to that at inference time. Furthermore,SCNet incorporates feature relay and utilizes global contextual information tofurther reinforce the reciprocal relationships among classifying, detecting,and segmenting sub-tasks. Extensive experiments on the standard COCO datasetreveal the effectiveness of the proposed method over multiple evaluationmetrics, including box AP, mask AP, and inference speed. In particular, whilerunning 38\\\\% faster, the proposed SCNet improves the AP of the box and maskpredictions by respectively 1.3 and 2.3 points compared to the strong CascadeMask R-CNN baseline. Code is available at\\\\url{https://github.com/thangvubk/SCNet}.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2012.10150v1'},\n",
       " 597: {'ID': 597,\n",
       "  'title': 'An Empirical Study of Batch Normalization and Group Normalization in  Conditional Computation',\n",
       "  'authors': ['Chris Pal',\n",
       "   'Vincent Michalski',\n",
       "   'Vikram Voleti',\n",
       "   'Anthony Ortiz',\n",
       "   'Samira Ebrahimi Kahou',\n",
       "   'Doina Precup',\n",
       "   'Pascal Vincent'],\n",
       "  'published': '2019-07-31T19:37:16Z',\n",
       "  'updated': '2019-07-31T19:37:16Z',\n",
       "  'abstract': 'Batch normalization has been widely used to improve optimization in deepneural networks. While the uncertainty in batch statistics can act as aregularizer, using these dataset statistics specific to the training setimpairs generalization in certain tasks. Recently, alternative methods fornormalizing feature activations in neural networks have been proposed. Amongthem, group normalization has been shown to yield similar, in some domains evensuperior performance to batch normalization. All these methods utilize alearned affine transformation after the normalization operation to increaserepresentational power. Methods used in conditional computation define theparameters of these transformations as learnable functions of conditioninginformation. In this work, we study whether and where the conditionalformulation of group normalization can improve generalization compared toconditional batch normalization. We evaluate performances on the tasks ofvisual question answering, few-shot learning, and conditional image generation.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1908.00061v1'},\n",
       " 598: {'ID': 598,\n",
       "  'title': 'Multimodal Hierarchical Reinforcement Learning Policy for Task-Oriented  Visual Dialog',\n",
       "  'authors': ['Zhou Yu', 'Tiancheng Zhao', 'Jiaping Zhang'],\n",
       "  'published': '2018-05-08T19:54:47Z',\n",
       "  'updated': '2018-05-08T19:54:47Z',\n",
       "  'abstract': 'Creating an intelligent conversational system that understands vision andlanguage is one of the ultimate goals in Artificial Intelligence(AI)~\\\\cite{winograd1972understanding}. Extensive research has focused onvision-to-language generation, however, limited research has touched oncombining these two modalities in a goal-driven dialog context. We propose amultimodal hierarchical reinforcement learning framework that dynamicallyintegrates vision and language for task-oriented visual dialog. The frameworkjointly learns the multimodal dialog state representation and the hierarchicaldialog policy to improve both dialog task success and efficiency. We alsopropose a new technique, state adaptation, to integrate context awareness inthe dialog state representation. We evaluate the proposed framework and thestate adaptation technique in an image guessing game and achieve promisingresults.',\n",
       "  'categories': ['cs.CL'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1805.03257v1'},\n",
       " 599: {'ID': 599,\n",
       "  'title': 'End-to-end Deep Learning Methods for Automated Damage Detection in  Extreme Events at Various Scales',\n",
       "  'authors': ['Yongsheng Bai', 'Halil Sezen', 'Alper Yilmaz'],\n",
       "  'published': '2020-11-05T21:21:19Z',\n",
       "  'updated': '2020-11-05T21:21:19Z',\n",
       "  'abstract': 'Robust Mask R-CNN (Mask Regional Convolu-tional Neural Network) methods areproposed and tested for automatic detection of cracks on structures or theircomponents that may be damaged during extreme events, such as earth-quakes. Wecurated a new dataset with 2,021 labeled images for training and validation andaimed to find end-to-end deep neural networks for crack detection in the field.With data augmentation and parameters fine-tuning, Path Aggregation Network(PANet) with spatial attention mechanisms and High-resolution Network (HRNet)are introduced into Mask R-CNNs. The tests on three public datasets with low-or high-resolution images demonstrate that the proposed methods can achieve abig improvement over alternative networks, so the proposed method may besufficient for crack detection for a variety of scales in real applications.',\n",
       "  'categories': ['cs.CV', 'cs.LG', 'eess.IV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2011.03098v1'},\n",
       " 600: {'ID': 600,\n",
       "  'title': 'Fast and Accurate Reconstruction of Pan-Tilt RGB-D Scans via Axis Bound  Registration',\n",
       "  'authors': ['Tack-Don Han', 'Jung-Hyun Byun'],\n",
       "  'published': '2018-12-01T18:56:02Z',\n",
       "  'updated': '2019-06-10T05:46:03Z',\n",
       "  'abstract': 'A fast and accurate algorithm is presented for registering scans from anRGB-D camera on a pan-tilt platform. The pan-tilt RGB-D camera rotates andscans the entire scene in an automated fashion. The proposed algorithm exploitsthe movement of the camera that is bound by the two rotation axes of the servomotors so as to realize fast and accurate registration of acquired pointclouds. The rotation parameters, including the rotation axes, pan-tilttransformations and the servo control mechanism, are calibrated beforehand.Subsequently, fast global registration can be performed during online operationwith transformation matrices formed by the calibrated rotation axes and angles.In local registration, features are extracted and matched between two scenes.False-positive correspondences, whose distances to the rotation trajectoriesexceed a threshold, are rejected. Then, a more accurate registration can beachieved by minimizing the residual distances between corresponding points,while transformations are bound to the rotation axes. Finally, the preliminaryalignment result is input to the iterative closed point algorithm to computethe final transformation. Results of comparative experiments validate that theproposed method outperforms state-of-the-art algorithms of various approachesbased on camera calibration, global registration, andsimultaneous-localization-and-mapping in terms of root-mean-square error andcomputation time.',\n",
       "  'categories': ['cs.GR'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1812.00240v3'},\n",
       " 601: {'ID': 601,\n",
       "  'title': 'Analyzing Federated Learning through an Adversarial Lens',\n",
       "  'authors': ['Prateek Mittal',\n",
       "   'Arjun Nitin Bhagoji',\n",
       "   'Supriyo Chakraborty',\n",
       "   'Seraphin Calo'],\n",
       "  'published': '2018-11-29T20:27:14Z',\n",
       "  'updated': '2019-11-25T00:34:14Z',\n",
       "  'abstract': \"Federated learning distributes model training among a multitude of agents,who, guided by privacy concerns, perform training using their local data butshare only model parameter updates, for iterative aggregation at the server. Inthis work, we explore the threat of model poisoning attacks on federatedlearning initiated by a single, non-colluding malicious agent where theadversarial objective is to cause the model to misclassify a set of choseninputs with high confidence. We explore a number of strategies to carry outthis attack, starting with simple boosting of the malicious agent's update toovercome the effects of other agents' updates. To increase attack stealth, wepropose an alternating minimization strategy, which alternately optimizes forthe training loss and the adversarial objective. We follow up by usingparameter estimation for the benign agents' updates to improve on attacksuccess. Finally, we use a suite of interpretability techniques to generatevisual explanations of model decisions for both benign and malicious models andshow that the explanations are nearly visually indistinguishable. Our resultsindicate that even a highly constrained adversary can carry out model poisoningattacks while simultaneously maintaining stealth, thus highlighting thevulnerability of the federated learning setting and the need to developeffective defense strategies.\",\n",
       "  'categories': ['cs.LG', 'cs.AI', 'cs.CR', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1811.12470v4'},\n",
       " 602: {'ID': 602,\n",
       "  'title': 'Asymmetric Non-local Neural Networks for Semantic Segmentation',\n",
       "  'authors': ['Zhen Zhu',\n",
       "   'Mengde Xu',\n",
       "   'Song Bai',\n",
       "   'Xiang Bai',\n",
       "   'Tengteng Huang'],\n",
       "  'published': '2019-08-21T02:26:44Z',\n",
       "  'updated': '2019-08-29T13:31:38Z',\n",
       "  'abstract': 'The non-local module works as a particularly useful technique for semanticsegmentation while criticized for its prohibitive computation and GPU memoryoccupation. In this paper, we present Asymmetric Non-local Neural Network tosemantic segmentation, which has two prominent components: Asymmetric PyramidNon-local Block (APNB) and Asymmetric Fusion Non-local Block (AFNB). APNBleverages a pyramid sampling module into the non-local block to largely reducethe computation and memory consumption without sacrificing the performance.AFNB is adapted from APNB to fuse the features of different levels under asufficient consideration of long range dependencies and thus considerablyimproves the performance. Extensive experiments on semantic segmentationbenchmarks demonstrate the effectiveness and efficiency of our work. Inparticular, we report the state-of-the-art performance of 81.3 mIoU on theCityscapes test set. For a 256x128 input, APNB is around 6 times faster than anon-local block on GPU while 28 times smaller in GPU running memory occupation.Code is available at: https://github.com/MendelXu/ANN.git.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1908.07678v5'},\n",
       " 603: {'ID': 603,\n",
       "  'title': 'Chitrakar: Robotic System for Drawing Jordan Curve of Facial Portrait',\n",
       "  'authors': ['Deepak Raina',\n",
       "   'Ayush Kumar',\n",
       "   'Shivam Thukral',\n",
       "   'Swagat Kumar',\n",
       "   'Aniruddha Singhal'],\n",
       "  'published': '2020-11-21T12:44:42Z',\n",
       "  'updated': '2020-11-21T12:44:42Z',\n",
       "  'abstract': \"This paper presents a robotic system (\\\\textit{Chitrakar}) which autonomouslyconverts any image of a human face to a recognizable non-self-intersecting loop(Jordan Curve) and draws it on any planar surface. The image is processed usingMask R-CNN for instance segmentation, Laplacian of Gaussian (LoG) for featureenhancement and intensity-based probabilistic stippling for the image to pointsconversion. These points are treated as a destination for a travelling salesmanand are connected with an optimal path which is calculated heuristically byminimizing the total distance to be travelled. This path is converted to aJordan Curve in feasible time by removing intersections using a combination ofimage processing, 2-opt, and Bresenham's Algorithm. The robotic systemgenerates $n$ instances of each image for human aesthetic judgement, out ofwhich the most appealing instance is selected for the final drawing. Thedrawing is executed carefully by the robot's arm using trapezoidal velocityprofiles for jerk-free and fast motion. The drawing, with a decent resolution,can be completed in less than 30 minutes which is impossible to do by hand.This work demonstrates the use of robotics to augment humans in executingdifficult craft-work instead of replacing them altogether.\",\n",
       "  'categories': ['cs.RO'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2011.10781v1'},\n",
       " 604: {'ID': 604,\n",
       "  'title': 'Global Context for Convolutional Pose Machines',\n",
       "  'authors': ['Daniil Osokin'],\n",
       "  'published': '2019-06-10T16:24:04Z',\n",
       "  'updated': '2019-06-10T16:24:04Z',\n",
       "  'abstract': 'Convolutional Pose Machine is a popular neural network architecture forarticulated pose estimation. In this work we explore its empirical receptivefield and realize, that it can be enhanced with integration of a globalcontext. To do so U-shaped context module is proposed and compared with thepyramid pooling and atrous spatial pyramid pooling modules, which are oftenused in semantic segmentation domain. The proposed neural network achievesstate-of-the-art accuracy with 87.9% PCKh for single-person pose estimation onthe Look Into Person dataset. A smaller version of this network runs more than160 frames per second while being just 2.9% less accurate. Generalization ofthe proposed approach is tested on the MPII benchmark and shown, that it fasterthan hourglass-based networks, while provides similar accuracy. The code isavailable athttps://github.com/opencv/openvino_training_extensions/tree/develop/pytorch_toolkit/human_pose_estimation .',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1906.04104v1'},\n",
       " 605: {'ID': 605,\n",
       "  'title': 'Effective and Efficient Dropout for Deep Convolutional Neural Networks',\n",
       "  'authors': ['Yao Shu',\n",
       "   'Meihui Zhang',\n",
       "   'Gang Chen',\n",
       "   'Wei Wang',\n",
       "   'Beng Chin Ooi',\n",
       "   'Shaofeng Cai'],\n",
       "  'published': '2019-04-06T09:17:51Z',\n",
       "  'updated': '2020-07-28T17:30:11Z',\n",
       "  'abstract': 'Convolutional Neural networks (CNNs) based applications have becomeubiquitous, where proper regularization is greatly needed. To prevent largeneural network models from overfitting, dropout has been widely used as anefficient regularization technique in practice. However, many recent works showthat the standard dropout is ineffective or even detrimental to the training ofCNNs. In this paper, we revisit this issue and examine various dropout variantsin an attempt to improve existing dropout-based regularization techniques forCNNs. We attribute the failure of standard dropout to the conflict between thestochasticity of dropout and its following Batch Normalization (BN), andpropose to reduce the conflict by placing dropout operations right before theconvolutional operation instead of BN, or totally address this issue byreplacing BN with Group Normalization (GN). We further introduce a structurallymore suited dropout variant Drop-Conv2d, which provides more efficient andeffective regularization for deep CNNs. These dropout variants can be readilyintegrated into the building blocks of CNNs and implemented in existing deeplearning platforms. Extensive experiments on benchmark datasets includingCIFAR, SVHN and ImageNet are conducted to compare the existing building blocksand the proposed ones with dropout training. Results show that our buildingblocks improve over state-of-the-art CNNs significantly, which is mainly due tothe better regularization and implicit model ensemble effect.',\n",
       "  'categories': ['cs.LG', 'cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1904.03392v5'},\n",
       " 606: {'ID': 606,\n",
       "  'title': 'Revisiting newly Large Magellanic Cloud age gap star clusters',\n",
       "  'authors': ['Andrés E. Piatti'],\n",
       "  'published': '2021-02-06T16:01:07Z',\n",
       "  'updated': '2021-02-06T16:01:07Z',\n",
       "  'abstract': 'Recently, a noticeable number of new star clusters was identified in theoutskirts of the Large Magellanic Cloud (LMC) populating the so-called starcluster age gap, a space of time (~ 4-12 Gyr) where the only known star clusteris up-to-date ESO121-SC\\\\,03. We used Survey of the Magellanic Stellar History(SMASH) DR2 data sets, as well as those employed to identify these star clustercandidates, to produce relatively deep color-magnitude diagrams (CMDs) of 17out of 20 discovered age gap star clusters with the aim of investigating themin detail. Our analysis relies on a thorough CMD cleaning procedure of thefield star contamination, which presents variations in its stellar density andastrophysical properties, such as luminosity and effective temperature, aroundthe star cluster fields. We built star cluster CMDs from stars with membershipprobabilities assigned from the cleaning procedure. These CMDs and theirrespective spatial distribution maps favor the existence of LMC star fielddensity fluctuations rather than age gap star clusters, although a definitiveassessment on them will be possible from further deeper photometry.',\n",
       "  'categories': ['astro-ph.GA'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2102.03601v1'},\n",
       " 607: {'ID': 607,\n",
       "  'title': 'Explaining the Black-box Smoothly- A Counterfactual Approach',\n",
       "  'authors': ['Sumedha Singla',\n",
       "   'Brian Pollack',\n",
       "   'Stephen Wallace',\n",
       "   'Kayhan Batmanghelich'],\n",
       "  'published': '2021-01-11T23:12:11Z',\n",
       "  'updated': '2021-01-11T23:12:11Z',\n",
       "  'abstract': \"We propose a BlackBox \\\\emph{Counterfactual Explainer} that is explicitlydeveloped for medical imaging applications. Classical approaches (e.g. saliencymaps) assessing feature importance do not explain \\\\emph{how} and \\\\emph{why}variations in a particular anatomical region is relevant to the outcome, whichis crucial for transparent decision making in healthcare application. Ourframework explains the outcome by gradually \\\\emph{exaggerating} the semanticeffect of the given outcome label. Given a query input to a classifier,Generative Adversarial Networks produce a progressive set of perturbations tothe query image that gradually changes the posterior probability from itsoriginal class to its negation. We design the loss function to ensure thatessential and potentially relevant details, such as support devices, arepreserved in the counterfactually generated images. We provide an extensiveevaluation of different classification tasks on the chest X-Ray images. Ourexperiments show that a counterfactually generated visual explanation isconsistent with the disease's clinical relevant measurements, bothquantitatively and qualitatively.\",\n",
       "  'categories': ['cs.CV', 'eess.IV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2101.04230v1'},\n",
       " 608: {'ID': 608,\n",
       "  'title': 'A Comprehensive Comparison of Multi-Dimensional Image Denoising Methods',\n",
       "  'authors': ['Xiaowei Yang', 'Zhaoming Kong', 'Lifang He'],\n",
       "  'published': '2020-11-06T16:28:17Z',\n",
       "  'updated': '2020-11-06T16:28:17Z',\n",
       "  'abstract': 'Filtering multi-dimensional images such as color images, color videos,multispectral images and magnetic resonance images is challenging in terms ofboth effectiveness and efficiency. Leveraging the nonlocal self-similarity(NLSS) characteristic of images and sparse representation in the transformdomain, the block-matching and 3D filtering (BM3D) based methods show powerfuldenoising performance. Recently, numerous new approaches with differentregularization terms, transforms and advanced deep neural network (DNN)architectures are proposed to improve denoising quality. In this paper, weextensively compare over 60 methods on both synthetic and real-world datasets.We also introduce a new color image and video dataset for benchmarking, and ourevaluations are performed from four different perspectives includingquantitative metrics, visual effects, human ratings and computational cost.Comprehensive experiments demonstrate: (i) the effectiveness and efficiency ofthe BM3D family for various denoising tasks, (ii) a simple matrix-basedalgorithm could produce similar results compared with its tensor counterparts,and (iii) several DNN models trained with synthetic Gaussian noise showstate-of-the-art performance on real-world color image and video datasets.Despite the progress in recent years, we discuss shortcomings and possibleextensions of existing techniques. Datasets and codes for evaluation are madepublicly available at https://github.com/ZhaomingKong/Denoising-Comparison.',\n",
       "  'categories': ['eess.IV', 'cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2011.03462v1'},\n",
       " 609: {'ID': 609,\n",
       "  'title': 'Graph-based Heuristic Search for Module Selection Procedure in Neural  Module Network',\n",
       "  'authors': ['Hideki Nakayama', 'Yuxuan Wu'],\n",
       "  'published': '2020-09-30T15:55:44Z',\n",
       "  'updated': '2020-09-30T15:55:44Z',\n",
       "  'abstract': \"Neural Module Network (NMN) is a machine learning model for solving thevisual question answering tasks. NMN uses programs to encode modules'structures, and its modularized architecture enables it to solve logicalproblems more reasonably. However, because of the non-differentiable procedureof module selection, NMN is hard to be trained end-to-end. To overcome thisproblem, existing work either included ground-truth program into training dataor applied reinforcement learning to explore the program. However, both ofthese methods still have weaknesses. In consideration of this, we proposed anew learning framework for NMN. Graph-based Heuristic Search is the algorithmwe proposed to discover the optimal program through a heuristic search on thedata structure named Program Graph. Our experiments on FigureQA and CLEVRdataset show that our methods can realize the training of NMN withoutground-truth programs and achieve superior efficiency over existingreinforcement learning methods in program exploration.\",\n",
       "  'categories': ['cs.AI'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2009.14759v1'},\n",
       " 610: {'ID': 610,\n",
       "  'title': 'Comicolorization: Semi-Automatic Manga Colorization',\n",
       "  'authors': ['Chie Furusawa',\n",
       "   'Yuri Odagiri',\n",
       "   'Kazuyuki Hiroshiba',\n",
       "   'Keisuke Ogaki'],\n",
       "  'published': '2017-06-21T06:52:09Z',\n",
       "  'updated': '2017-09-28T12:58:52Z',\n",
       "  'abstract': 'We developed \"Comicolorization\", a semi-automatic colorization system formanga images. Given a monochrome manga and reference images as inputs, oursystem generates a plausible color version of the manga. This is the first workto address the colorization of an entire manga title (a set of manga pages).Our method colorizes a whole page (not a single panel) semi-automatically, withthe same color for the same character across multiple panels. To colorize thetarget character by the color from the reference image, we extract a colorfeature from the reference and feed it to the colorization network to help thecolorization. Our approach employs adversarial loss to encourage the effect ofthe color features. Optionally, our tool allows users to revise thecolorization result interactively. By feeding the color features to our deepcolorization network, we accomplish colorization of the entire manga using thedesired colors for each panel.',\n",
       "  'categories': ['cs.CV', 'cs.GR'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1706.06759v4'},\n",
       " 611: {'ID': 611,\n",
       "  'title': 'Frequency-Tuned Universal Adversarial Attacks',\n",
       "  'authors': ['Yingpeng Deng', 'Lina J. Karam'],\n",
       "  'published': '2020-03-11T22:52:19Z',\n",
       "  'updated': '2020-06-09T18:37:09Z',\n",
       "  'abstract': 'Researchers have shown that the predictions of a convolutional neural network(CNN) for an image set can be severely distorted by one single image-agnosticperturbation, or universal perturbation, usually with an empirically fixedthreshold in the spatial domain to restrict its perceivability. However, byconsidering the human perception, we propose to adopt JND thresholds to guidethe perceivability of universal adversarial perturbations. Based on this, wepropose a frequency-tuned universal attack method to compute universalperturbations and show that our method can realize a good balance betweenperceivability and effectiveness in terms of fooling rate by adapting theperturbations to the local frequency content. Compared with existing universaladversarial attack techniques, our frequency-tuned attack method can achievecutting-edge quantitative results. We demonstrate that our approach cansignificantly improve the performance of the baseline on both white-box andblack-box attacks.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2003.05549v2'},\n",
       " 612: {'ID': 612,\n",
       "  'title': \"Answerer in Questioner's Mind: Information Theoretic Approach to  Goal-Oriented Visual Dialog\",\n",
       "  'authors': ['Byoung-Tak Zhang', 'Sang-Woo Lee', 'Yu-Jung Heo'],\n",
       "  'published': '2018-02-12T04:08:06Z',\n",
       "  'updated': '2018-11-28T05:07:23Z',\n",
       "  'abstract': 'Goal-oriented dialog has been given attention due to its numerousapplications in artificial intelligence. Goal-oriented dialogue tasks occurwhen a questioner asks an action-oriented question and an answerer respondswith the intent of letting the questioner know a correct action to take. To askthe adequate question, deep learning and reinforcement learning have beenrecently applied. However, these approaches struggle to find a competentrecurrent neural questioner, owing to the complexity of learning a series ofsentences. Motivated by theory of mind, we propose \"Answerer in Questioner\\'sMind\" (AQM), a novel information theoretic algorithm for goal-oriented dialog.With AQM, a questioner asks and infers based on an approximated probabilisticmodel of the answerer. The questioner figures out the answerer\\'s intention viaselecting a plausible question by explicitly calculating the information gainof the candidate intentions and possible answers to each question. We test ourframework on two goal-oriented visual dialog tasks: \"MNIST Counting Dialog\" and\"GuessWhat?!\". In our experiments, AQM outperforms comparative algorithms by alarge margin.',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1802.03881v3'},\n",
       " 613: {'ID': 613,\n",
       "  'title': 'Neural Models for Documents with Metadata',\n",
       "  'authors': ['Noah A. Smith', 'Chenhao Tan', 'Dallas Card'],\n",
       "  'published': '2017-05-25T18:00:03Z',\n",
       "  'updated': '2018-10-23T20:26:37Z',\n",
       "  'abstract': 'Most real-world document collections involve various types of metadata, suchas author, source, and date, and yet the most commonly-used approaches tomodeling text corpora ignore this information. While specialized models havebeen developed for particular applications, few are widely used in practice, ascustomization typically requires derivation of a custom inference algorithm. Inthis paper, we build on recent advances in variational inference methods andpropose a general neural framework, based on topic models, to enable flexibleincorporation of metadata and allow for rapid exploration of alternativemodels. Our approach achieves strong performance, with a manageable tradeoffbetween perplexity, coherence, and sparsity. Finally, we demonstrate thepotential of our framework through an exploration of a corpus of articles aboutUS immigration.',\n",
       "  'categories': ['stat.ML', 'cs.CL'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1705.09296v2'},\n",
       " 614: {'ID': 614,\n",
       "  'title': 'Universal Adversarial Perturbations: A Survey',\n",
       "  'authors': ['Pramod Mehta',\n",
       "   'Ashutosh Chaubey',\n",
       "   'Kavya Barnwal',\n",
       "   'Keerat K. Guliani',\n",
       "   'Nikhil Agrawal'],\n",
       "  'published': '2020-05-16T20:18:26Z',\n",
       "  'updated': '2020-05-16T20:18:26Z',\n",
       "  'abstract': \"Over the past decade, Deep Learning has emerged as a useful and efficienttool to solve a wide variety of complex learning problems ranging from imageclassification to human pose estimation, which is challenging to solve usingstatistical machine learning algorithms. However, despite their superiorperformance, deep neural networks are susceptible to adversarial perturbations,which can cause the network's prediction to change without making perceptiblechanges to the input image, thus creating severe security issues at the time ofdeployment of such systems. Recent works have shown the existence of UniversalAdversarial Perturbations, which, when added to any image in a dataset,misclassifies it when passed through a target model. Such perturbations aremore practical to deploy since there is minimal computation done during theactual attack. Several techniques have also been proposed to defend the neuralnetworks against these perturbations. In this paper, we attempt to provide adetailed discussion on the various data-driven and data-independent methods forgenerating universal perturbations, along with measures to defend against suchperturbations. We also cover the applications of such universal perturbationsin various deep learning tasks.\",\n",
       "  'categories': ['cs.CV', 'cs.CR', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2005.08087v1'},\n",
       " 615: {'ID': 615,\n",
       "  'title': 'Color Sails: Discrete-Continuous Palettes for Deep Color Exploration',\n",
       "  'authors': ['Maria Shugrina', 'Karan Singh', 'Amlan Kar', 'Sanja Fidler'],\n",
       "  'published': '2018-06-07T22:42:00Z',\n",
       "  'updated': '2018-06-07T22:42:00Z',\n",
       "  'abstract': 'We present color sails, a discrete-continuous color gamut representation thatextends the color gradient analogy to three dimensions and allows interactivecontrol of the color blending behavior. Our representation models a widevariety of color distributions in a compact manner, and lends itself toapplications such as color exploration for graphic design, illustration andsimilar fields. We propose a Neural Network that can fit a color sail to anyimage. Then, the user can adjust color sail parameters to change the basecolors, their blending behavior and the number of colors, exploring a widerange of options for the original design. In addition, we propose a DeepLearning model that learns to automatically segment an image intocolor-compatible alpha masks, each equipped with its own color sail. Thisallows targeted color exploration by either editing their corresponding colorsails or using standard software packages. Our model is trained on a customdiverse dataset of art and design. We provide both quantitative evaluations,and a user study, demonstrating the effectiveness of color sail interaction.Interactive demos are available at www.colorsails.com.',\n",
       "  'categories': ['cs.GR', 'cs.AI', 'cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1806.02918v1'},\n",
       " 616: {'ID': 616,\n",
       "  'title': 'Short-Term Temporal Convolutional Networks for Dynamic Hand Gesture  Recognition',\n",
       "  'authors': ['Jieyu Zhao',\n",
       "   'Yi Zhang',\n",
       "   'Xijiong Xie',\n",
       "   'Ye Zheng',\n",
       "   'Yuqi Li',\n",
       "   'Chong Wang'],\n",
       "  'published': '2019-12-31T23:30:27Z',\n",
       "  'updated': '2019-12-31T23:30:27Z',\n",
       "  'abstract': \"The purpose of gesture recognition is to recognize meaningful movements ofhuman bodies, and gesture recognition is an important issue in computer vision.In this paper, we present a multimodal gesture recognition method based on 3Ddensely convolutional networks (3D-DenseNets) and improved temporalconvolutional networks (TCNs). The key idea of our approach is to find acompact and effective representation of spatial and temporal features, whichorderly and separately divide task of gesture video analysis into two parts:spatial analysis and temporal analysis. In spatial analysis, we adopt3D-DenseNets to learn short-term spatio-temporal features effectively.Subsequently, in temporal analysis, we use TCNs to extract temporal featuresand employ improved Squeeze-and-Excitation Networks (SENets) to strengthen therepresentational power of temporal features from each TCNs' layers. The methodhas been evaluated on the VIVA and the NVIDIA Gesture Dynamic Hand GestureDatasets. Our approach obtains very competitive performance on VIVA benchmarkswith the classification accuracies of 91.54%, and achieve state-of-the artperformance with 86.37% accuracy on NVIDIA benchmark.\",\n",
       "  'categories': ['cs.CV', 'eess.IV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2001.05833v1'},\n",
       " 617: {'ID': 617,\n",
       "  'title': 'Towards Design Space Exploration and Optimization of Fast Algorithms for  Convolutional Neural Networks (CNNs) on FPGAs',\n",
       "  'authors': ['Afzal Ahmad', 'Muhammad Adeel Pasha'],\n",
       "  'published': '2019-03-05T13:28:07Z',\n",
       "  'updated': '2019-03-05T13:28:07Z',\n",
       "  'abstract': 'Convolutional Neural Networks (CNNs) have gained widespread popularity in thefield of computer vision and image processing. Due to huge computationalrequirements of CNNs, dedicated hardware-based implementations are beingexplored to improve their performance. Hardware platforms such as FieldProgrammable Gate Arrays (FPGAs) are widely being used to design parallelarchitectures for this purpose. In this paper, we analyze Winograd minimalfiltering or fast convolution algorithms to reduce the arithmetic complexity ofconvolutional layers of CNNs. We explore a complex design space to find thesets of parameters that result in improved throughput and power-efficiency. Wealso design a pipelined and parallel Winograd convolution engine that improvesthe throughput and power-efficiency while reducing the computational complexityof the overall system. Our proposed designs show up to 4.75$\\\\times$ and1.44$\\\\times$ improvements in throughput and power-efficiency, respectively, incomparison to the state-of-the-art design while using approximately2.67$\\\\times$ more multipliers. Furthermore, we obtain savings of up to 53.6\\\\%in logic resources compared with the state-of-the-art implementation.',\n",
       "  'categories': ['eess.SP', 'cs.CV', 'eess.IV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1903.01811v1'},\n",
       " 618: {'ID': 618,\n",
       "  'title': 'Efficient Attention Mechanism for Visual Dialog that can Handle All the  Interactions between Multiple Inputs',\n",
       "  'authors': ['Masanori Suganuma', 'Van-Quang Nguyen', 'Takayuki Okatani'],\n",
       "  'published': '2019-11-26T08:10:02Z',\n",
       "  'updated': '2020-07-17T14:10:12Z',\n",
       "  'abstract': 'It has been a primary concern in recent studies of vision and language tasksto design an effective attention mechanism dealing with interactions betweenthe two modalities. The Transformer has recently been extended and applied toseveral bi-modal tasks, yielding promising results. For visual dialog, itbecomes necessary to consider interactions between three or more inputs, i.e.,an image, a question, and a dialog history, or even its individual dialogcomponents. In this paper, we present a neural architecture named Light-weightTransformer for Many Inputs (LTMI) that can efficiently deal with all theinteractions between multiple such inputs in visual dialog. It has a blockstructure similar to the Transformer and employs the same design of attentioncomputation, whereas it has only a small number of parameters, yet hassufficient representational power for the purpose. Assuming a standard settingof visual dialog, a layer built upon the proposed attention block has less thanone-tenth of parameters as compared with its counterpart, a natural Transformerextension. The experimental results on the VisDial datasets validate theeffectiveness of the proposed approach, showing improvements of the best NDCGscore on the VisDial v1.0 dataset from 57.59 to 60.92 with a single model, from64.47 to 66.53 with ensemble models, and even to 74.88 with additionalfinetuning. Our implementation code is available athttps://github.com/davidnvq/visdial.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1911.11390v2'},\n",
       " 619: {'ID': 619,\n",
       "  'title': 'Skip Connections Matter: On the Transferability of Adversarial Examples  Generated with ResNets',\n",
       "  'authors': ['Dongxian Wu',\n",
       "   'Xingjun Ma',\n",
       "   'Yisen Wang',\n",
       "   'Shu-Tao Xia',\n",
       "   'James Bailey'],\n",
       "  'published': '2020-02-14T12:09:21Z',\n",
       "  'updated': '2020-02-14T12:09:21Z',\n",
       "  'abstract': 'Skip connections are an essential component of current state-of-the-art deepneural networks (DNNs) such as ResNet, WideResNet, DenseNet, and ResNeXt.Despite their huge success in building deeper and more powerful DNNs, weidentify a surprising security weakness of skip connections in this paper. Useof skip connections allows easier generation of highly transferable adversarialexamples. Specifically, in ResNet-like (with skip connections) neural networks,gradients can backpropagate through either skip connections or residualmodules. We find that using more gradients from the skip connections ratherthan the residual modules according to a decay factor, allows one to craftadversarial examples with high transferability. Our method is termed SkipGradient Method(SGM). We conduct comprehensive transfer attacks againststate-of-the-art DNNs including ResNets, DenseNets, Inceptions,Inception-ResNet, Squeeze-and-Excitation Network (SENet) and robustly trainedDNNs. We show that employing SGM on the gradient flow can greatly improve thetransferability of crafted attacks in almost all cases. Furthermore, SGM can beeasily combined with existing black-box attack techniques, and obtain highimprovements over state-of-the-art transferability methods. Our findings notonly motivate new research into the architectural vulnerability of DNNs, butalso open up further challenges for the design of secure DNN architectures.',\n",
       "  'categories': ['cs.LG', 'cs.CR', 'cs.CV', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2002.05990v1'},\n",
       " 620: {'ID': 620,\n",
       "  'title': 'Mutual Linear Regression-based Discrete Hashing',\n",
       "  'authors': ['Xingbo Liu', 'Yilong Yin', 'Xiushan Nie'],\n",
       "  'published': '2019-03-15T01:13:34Z',\n",
       "  'updated': '2019-03-15T01:13:34Z',\n",
       "  'abstract': 'Label information is widely used in hashing methods because of itseffectiveness of improving the precision. The existing hashing methods alwaysuse two different projections to represent the mutual regression between hashcodes and class labels. In contrast to the existing methods, we propose a novellearning-based hashing method termed stable supervised discrete hashing withmutual linear regression (S2DHMLR) in this study, where only one stableprojection is used to describe the linear correlation between hash codes andcorresponding labels. To the best of our knowledge, this strategy has not beenused for hashing previously. In addition, we further use a boosting strategy toimprove the final performance of the proposed method without adding extraconstraints and with little extra expenditure in terms of time and space.Extensive experiments conducted on three image benchmarks demonstrate thesuperior performance of the proposed method.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1904.00744v1'},\n",
       " 621: {'ID': 621,\n",
       "  'title': 'Markerless Visual Robot Programming by Demonstration',\n",
       "  'authors': ['Dietrich Paulus',\n",
       "   'Nick Theisen',\n",
       "   'Ivanna Mykhalchyshyna',\n",
       "   'Raphael Memmesheimer',\n",
       "   'Viktor Seib'],\n",
       "  'published': '2018-07-30T19:33:00Z',\n",
       "  'updated': '2018-07-30T19:33:00Z',\n",
       "  'abstract': 'In this paper we present an approach for learning to imitate human behavioron a semantic level by markerless visual observation. We analyze a set ofspatial constraints on human pose data extracted using convolutional posemachines and object informations extracted from 2D image sequences. A sceneanalysis, based on an ontology of objects and affordances, is combined withcontinuous human pose estimation and spatial object relations. Using a set ofconstraints we associate the observed human actions with a set of executablerobot commands. We demonstrate our approach in a kitchen task, where the robotlearns to prepare a meal.',\n",
       "  'categories': ['cs.CV', 'cs.RO'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1807.11541v1'},\n",
       " 622: {'ID': 622,\n",
       "  'title': 'Object Tracking via Dynamic Feature Selection Processes',\n",
       "  'authors': ['Simone Melzi', 'Giorgio Roffo'],\n",
       "  'published': '2016-09-07T12:27:11Z',\n",
       "  'updated': '2016-09-07T12:27:11Z',\n",
       "  'abstract': 'DFST proposes an optimized visual tracking algorithm based on the real-timeselection of locally and temporally discriminative features. A featureselection mechanism is embedded in the Adaptive colour Names (CN) trackingsystem that adaptively selects the top-ranked discriminative features fortracking. DFST provides a significant gain in accuracy and precision allowingthe use of a dynamic set of features that results in an increased systemflexibility. DFST is based on the unsupervised method \"Infinite FeatureSelection\" (Inf-FS), which ranks features according with their \"redundancy\"without using class labels. By using a fast online algorithm for learningdictionaries the size of the box is adapted during the processing. At eachupdate, we use multiple examples around the target (at different positions andscales). DFST also improved the CN by adding micro-shift at the predictedposition and bounding box adaptation.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1609.01958v1'},\n",
       " 623: {'ID': 623,\n",
       "  'title': 'Batch Group Normalization',\n",
       "  'authors': ['Xu Lan',\n",
       "   'Xiao-Yun Zhou',\n",
       "   'Nanyang Ye',\n",
       "   'Zhenguo Li',\n",
       "   'Bo-Lin Lai',\n",
       "   'Guang-Zhong Yang',\n",
       "   'Jiacheng Sun',\n",
       "   'Pedro Esperanca',\n",
       "   'Qijun Luo'],\n",
       "  'published': '2020-12-04T18:57:52Z',\n",
       "  'updated': '2020-12-09T01:26:51Z',\n",
       "  'abstract': 'Deep Convolutional Neural Networks (DCNNs) are hard and time-consuming totrain. Normalization is one of the effective solutions. Among previousnormalization methods, Batch Normalization (BN) performs well at medium andlarge batch sizes and is with good generalizability to multiple vision tasks,while its performance degrades significantly at small batch sizes. In thispaper, we find that BN saturates at extreme large batch sizes, i.e., 128 imagesper worker, i.e., GPU, as well and propose that the degradation/saturation ofBN at small/extreme large batch sizes is caused by noisy/confused statisticcalculation. Hence without adding new trainable parameters, usingmultiple-layer or multi-iteration information, or introducing extracomputation, Batch Group Normalization (BGN) is proposed to solve thenoisy/confused statistic calculation of BN at small/extreme large batch sizeswith introducing the channel, height and width dimension to compensate. Thegroup technique in Group Normalization (GN) is used and a hyper-parameter G isused to control the number of feature instances used for statistic calculation,hence to offer neither noisy nor confused statistic for different batch sizes.We empirically demonstrate that BGN consistently outperforms BN, InstanceNormalization (IN), Layer Normalization (LN), GN, and Positional Normalization(PN), across a wide spectrum of vision tasks, including image classification,Neural Architecture Search (NAS), adversarial learning, Few Shot Learning (FSL)and Unsupervised Domain Adaptation (UDA), indicating its good performance,robust stability to batch size and wide generalizability. For example, fortraining ResNet-50 on ImageNet with a batch size of 2, BN achieves Top1accuracy of 66.512% while BGN achieves 76.096% with notable improvement.',\n",
       "  'categories': ['cs.LG', 'cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2012.02782v2'},\n",
       " 624: {'ID': 624,\n",
       "  'title': 'Stabilising priors for robust Bayesian deep learning',\n",
       "  'authors': ['Steve Kroon',\n",
       "   'Felix McGregor',\n",
       "   'Arnu Pretorius',\n",
       "   'Johan du Preez'],\n",
       "  'published': '2019-10-23T07:01:17Z',\n",
       "  'updated': '2019-10-23T07:01:17Z',\n",
       "  'abstract': 'Bayesian neural networks (BNNs) have developed into useful tools forprobabilistic modelling due to recent advances in variational inferenceenabling large scale BNNs. However, BNNs remain brittle and hard to train,especially: (1) when using deep architectures consisting of many hidden layersand (2) in situations with large weight variances. We use signal propagationtheory to quantify these challenges and propose self-stabilising priors. Thisis achieved by a reformulation of the ELBO to allow the prior to influencenetwork signal propagation. Then, we develop a stabilising prior, where thedistributional parameters of the prior are adjusted before each forward pass toensure stability of the propagating signal. This stabilised signal propagationleads to improved convergence and robustness making it possible to train deepernetworks and in more noisy settings.',\n",
       "  'categories': ['cs.LG', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1910.10386v1'},\n",
       " 625: {'ID': 625,\n",
       "  'title': 'VD-BERT: A Unified Vision and Dialog Transformer with BERT',\n",
       "  'authors': ['Shafiq Joty',\n",
       "   'Yue Wang',\n",
       "   'Caiming Xiong',\n",
       "   'Michael R. Lyu',\n",
       "   'Steven C. H. Hoi',\n",
       "   'Irwin King'],\n",
       "  'published': '2020-04-28T04:08:46Z',\n",
       "  'updated': '2020-11-02T09:07:41Z',\n",
       "  'abstract': 'Visual dialog is a challenging vision-language task, where a dialog agentneeds to answer a series of questions through reasoning on the image contentand dialog history. Prior work has mostly focused on various attentionmechanisms to model such intricate interactions. By contrast, in this work, wepropose VD-BERT, a simple yet effective framework of unified vision-dialogTransformer that leverages the pretrained BERT language models for VisualDialog tasks. The model is unified in that (1) it captures all the interactionsbetween the image and the multi-turn dialog using a single-stream Transformerencoder, and (2) it supports both answer ranking and answer generationseamlessly through the same architecture. More crucially, we adapt BERT for theeffective fusion of vision and dialog contents via visually grounded training.Without the need of pretraining on external vision-language data, our modelyields new state of the art, achieving the top position in both single-modeland ensemble settings (74.54 and 75.35 NDCG scores) on the visual dialogleaderboard. Our code and pretrained models are released athttps://github.com/salesforce/VD-BERT.',\n",
       "  'categories': ['cs.CV', 'cs.CL'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2004.13278v3'},\n",
       " 626: {'ID': 626,\n",
       "  'title': 'Over-the-Air Adversarial Attacks on Deep Learning Based Modulation  Classifier over Wireless Channels',\n",
       "  'authors': ['Brian Kim',\n",
       "   'Tugba Erpek',\n",
       "   'Kemal Davaslioglu',\n",
       "   'Yalin E. Sagduyu',\n",
       "   'Sennur Ulukus'],\n",
       "  'published': '2020-02-05T18:45:43Z',\n",
       "  'updated': '2020-02-13T17:35:34Z',\n",
       "  'abstract': \"We consider a wireless communication system that consists of a transmitter, areceiver, and an adversary. The transmitter transmits signals with differentmodulation types, while the receiver classifies its received signals tomodulation types using a deep learning-based classifier. In the meantime, theadversary makes over-the-air transmissions that are received as superimposedwith the transmitter's signals to fool the classifier at the receiver intomaking errors. While this evasion attack has received growing interestrecently, the channel effects from the adversary to the receiver have beenignored so far such that the previous attack mechanisms cannot be applied underrealistic channel effects. In this paper, we present how to launch a realisticevasion attack by considering channels from the adversary to the receiver. Ourresults show that modulation classification is vulnerable to an adversarialattack over a wireless channel that is modeled as Rayleigh fading with pathloss and shadowing. We present various adversarial attacks with respect toavailability of information about channel, transmitter input, and classifierarchitecture. First, we present two types of adversarial attacks, namely atargeted attack (with minimum power) and non-targeted attack that aims tochange the classification to a target label or to any other label other thanthe true label, respectively. Both are white-box attacks that are transmitterinput-specific and use channel information. Then we introduce an algorithm togenerate adversarial attacks using limited channel information where theadversary only knows the channel distribution. Finally, we present a black-boxuniversal adversarial perturbation (UAP) attack where the adversary has limitedknowledge about both channel and transmitter input.\",\n",
       "  'categories': ['eess.SP', 'cs.LG', 'cs.NI', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2002.02400v2'},\n",
       " 627: {'ID': 627,\n",
       "  'title': 'Forest R-CNN: Large-Vocabulary Long-Tailed Object Detection and Instance  Segmentation',\n",
       "  'authors': ['Qian Zhang',\n",
       "   'Junsong Yuan',\n",
       "   'Liangchen Song',\n",
       "   'Tiancai Wang',\n",
       "   'Jialian Wu'],\n",
       "  'published': '2020-08-13T03:52:37Z',\n",
       "  'updated': '2020-08-13T03:52:37Z',\n",
       "  'abstract': 'Despite the previous success of object analysis, detecting and segmenting alarge number of object categories with a long-tailed data distribution remainsa challenging problem and is less investigated. For a large-vocabularyclassifier, the chance of obtaining noisy logits is much higher, which caneasily lead to a wrong recognition. In this paper, we exploit prior knowledgeof the relations among object categories to cluster fine-grained classes intocoarser parent classes, and construct a classification tree that is responsiblefor parsing an object instance into a fine-grained category via its parentclass. In the classification tree, as the number of parent class nodes aresignificantly less, their logits are less noisy and can be utilized to suppressthe wrong/noisy logits existed in the fine-grained class nodes. As the way toconstruct the parent class is not unique, we further build multiple trees toform a classification forest where each tree contributes its vote to thefine-grained classification. To alleviate the imbalanced learning caused by thelong-tail phenomena, we propose a simple yet effective resampling method, NMSResampling, to re-balance the data distribution. Our method, termed as ForestR-CNN, can serve as a plug-and-play module being applied to most objectrecognition models for recognizing more than 1000 categories. Extensiveexperiments are performed on the large vocabulary dataset LVIS. Compared withthe Mask R-CNN baseline, the Forest R-CNN significantly boosts the performancewith 11.5% and 3.9% AP improvements on the rare categories and overallcategories, respectively. Moreover, we achieve state-of-the-art results on theLVIS dataset. Code is available at https://github.com/JialianW/Forest_RCNN.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2008.05676v1'},\n",
       " 628: {'ID': 628,\n",
       "  'title': 'Continual Class Incremental Learning for CT Thoracic Segmentation',\n",
       "  'authors': ['Nassir Navab',\n",
       "   'Matthias Keicher',\n",
       "   'Josep Henry',\n",
       "   'Paul Thomson',\n",
       "   'Abdelrahman Elskhawy',\n",
       "   'Aneta Lisowska'],\n",
       "  'published': '2020-08-12T20:08:39Z',\n",
       "  'updated': '2020-08-12T20:08:39Z',\n",
       "  'abstract': 'Deep learning organ segmentation approaches require large amounts ofannotated training data, which is limited in supply due to reasons ofconfidentiality and the time required for expert manual annotation. Therefore,being able to train models incrementally without having access to previouslyused data is desirable. A common form of sequential training is fine tuning(FT). In this setting, a model learns a new task effectively, but losesperformance on previously learned tasks. The Learning without Forgetting (LwF)approach addresses this issue via replaying its own prediction for past tasksduring model training. In this work, we evaluate FT and LwF for classincremental learning in multi-organ segmentation using the publicly availableAAPM dataset. We show that LwF can successfully retain knowledge on previoussegmentations, however, its ability to learn a new class decreases with theaddition of each class. To address this problem we propose an adversarialcontinual learning segmentation approach (ACLSeg), which disentangles featurespace into task-specific and task-invariant features. This enables preservationof performance on past tasks and effective acquisition of new knowledge.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2008.05557v1'},\n",
       " 629: {'ID': 629,\n",
       "  'title': 'Recursive Visual Attention in Visual Dialog',\n",
       "  'authors': ['Hanwang Zhang',\n",
       "   'Yulei Niu',\n",
       "   'Manli Zhang',\n",
       "   'Jianhong Zhang',\n",
       "   'Zhiwu Lu',\n",
       "   'Ji-Rong Wen'],\n",
       "  'published': '2018-12-06T17:00:16Z',\n",
       "  'updated': '2019-04-06T15:02:24Z',\n",
       "  'abstract': \"Visual dialog is a challenging vision-language task, which requires the agentto answer multi-round questions about an image. It typically needs to addresstwo major problems: (1) How to answer visually-grounded questions, which is thecore challenge in visual question answering (VQA); (2) How to infer theco-reference between questions and the dialog history. An example of visualco-reference is: pronouns (\\\\eg, ``they'') in the question (\\\\eg, ``Are they onor off?'') are linked with nouns (\\\\eg, ``lamps'') appearing in the dialoghistory (\\\\eg, ``How many lamps are there?'') and the object grounded in theimage. In this work, to resolve the visual co-reference for visual dialog, wepropose a novel attention mechanism called Recursive Visual Attention (RvA).Specifically, our dialog agent browses the dialog history until the agent hassufficient confidence in the visual co-reference resolution, and refines thevisual attention recursively. The quantitative and qualitative experimentalresults on the large-scale VisDial v0.9 and v1.0 datasets demonstrate that theproposed RvA not only outperforms the state-of-the-art methods, but alsoachieves reasonable recursion and interpretable attention maps withoutadditional annotations. The code is available at\\\\url{https://github.com/yuleiniu/rva}.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1812.02664v2'},\n",
       " 630: {'ID': 630,\n",
       "  'title': 'Bridging the gap between Natural and Medical Images through Deep  Colorization',\n",
       "  'authors': ['Tatiana Tommasi',\n",
       "   'Lia Morra',\n",
       "   'Fabrizio Lamberti',\n",
       "   'Luca Piano'],\n",
       "  'published': '2020-05-21T12:03:14Z',\n",
       "  'updated': '2020-10-19T21:47:58Z',\n",
       "  'abstract': 'Deep learning has thrived by training on large-scale datasets. However, inmany applications, as for medical image diagnosis, getting massive amount ofdata is still prohibitive due to privacy, lack of acquisition homogeneity andannotation cost. In this scenario, transfer learning from natural imagecollections is a standard practice that attempts to tackle shape, texture andcolor discrepancies all at once through pretrained model fine-tuning. In thiswork, we propose to disentangle those challenges and design a dedicated networkmodule that focuses on color adaptation. We combine learning from scratch ofthe color module with transfer learning of different classification backbones,obtaining an end-to-end, easy-to-train architecture for diagnostic imagerecognition on X-ray images. Extensive experiments showed how our approach isparticularly efficient in case of data scarcity and provides a new path forfurther transferring the learned color information across multiple medicaldatasets.',\n",
       "  'categories': ['cs.CV', 'physics.med-ph'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2005.10589v2'},\n",
       " 631: {'ID': 631,\n",
       "  'title': 'Mapping the stellar age of the Milky Way bulge with the VVV. I. The  method',\n",
       "  'authors': ['O. A. Gonzalez',\n",
       "   'S. Cassisi',\n",
       "   'D. Minniti',\n",
       "   'A. Renzini',\n",
       "   'M. Rejkuba',\n",
       "   'A. Weiss',\n",
       "   'S. L. Hidalgo',\n",
       "   'E. Valenti',\n",
       "   'F. Surot',\n",
       "   'E. Sökmen',\n",
       "   'M. Zoccali'],\n",
       "  'published': '2019-02-05T14:13:15Z',\n",
       "  'updated': '2019-02-05T14:13:15Z',\n",
       "  'abstract': \"Recent observational programmes are providing a global view of the Milky Waybulge that serves as template for detailed comparison with models andextragalactic bulges. A number of surveys (i.e. VVV, GIBS, GES, ARGOS, BRAVA,APOGEE) are producing comprehensive and detailed extinction, metallicity,kinematics and stellar density maps of the Galactic bulge with unprecedentedaccuracy. However, the still missing key ingredient is the distribution ofstellar ages across the bulge. To overcome this limitation, we aim to age-datethe stellar population in several bulge fields with the ultimate goal ofderiving an age map of the Bulge. This paper presents the methodology and thefirst results obtained for a field along the Bulge minor axis, at $b=-6^\\\\circ$.We use a new PSF-fitting photometry of the VISTA Variables in the V\\\\'{i}aL\\\\'{a}ctea (VVV) survey data to construct deep color-magnitude diagrams of thebulge stellar population down to $\\\\sim$ 2 mag below the Main Sequence turnoff.We find the bulk of the bulge stellar population in the observed field alongthe minor axis to be at least older than $\\\\sim$ 7.5 Gyr. In particular, whenthe metallicity distribution function spectroscopically derived by GIBS isused, the best fit to the data is obtained with a combination of syntheticpopulations with ages in between $\\\\sim$ 7.5 Gyr and 11 Gyr. However, thefraction of stars younger than $\\\\sim$ 10 Gyr strongly depends upon the numberof Blue Straggler Stars present in the bulge. Simulations show that theobserved color-magnitude diagram of the bulge in the field along the minor axisis incompatible with the presence of a conspicuous population ofintermediate-age/young (i.e. $\\\\lesssim 5$ Gyr) stars.\",\n",
       "  'categories': ['astro-ph.GA', 'astro-ph.IM', 'astro-ph.SR'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1902.01695v1'},\n",
       " 632: {'ID': 632,\n",
       "  'title': 'A Novel and Reliable Deep Learning Web-Based Tool to Detect COVID-19  Infection from Chest CT-Scan',\n",
       "  'authors': ['Maryam Saeedi', 'Abdolkarim Saeedi', 'Arash Maghsoudi'],\n",
       "  'published': '2020-06-24T13:47:54Z',\n",
       "  'updated': '2020-06-26T13:26:14Z',\n",
       "  'abstract': 'The corona virus is already spread around the world in many countries, and ithas taken many lives. Furthermore, the world health organization (WHO) hasannounced that COVID-19 has reached the global epidemic stage. Early andreliable diagnosis using chest CT-scan can assist medical specialists in vitalcircumstances. In this work, we introduce a computer aided diagnosis (CAD) webservice to detect COVID- 19 online. One of the largest public chest CT-scandatabases, containing 746 participants was used in this experiment. A number ofwell-known deep neural network architectures consisting of ResNet, Inceptionand MobileNet were inspected to find the most efficient model for the hybridsystem. A combination of the Densely connected convolutional network (DenseNet)in order to reduce image dimensions and Nu-SVM as an anti-overfittingbottleneck was chosen to distinguish between COVID-19 and healthy controls. Theproposed methodology achieved 90.80% recall, 89.76% precision and 90.61%accuracy. The method also yields an AUC of 95.05%. Ultimately a flask webservice is made public through ngrok using the trained models to provide aRESTful COVID-19 detector, which takes only 39 milliseconds to process oneimage. The source code is also available athttps://github.com/KiLJ4EdeN/COVID_WEB. Based on the findings, it can beinferred that it is feasible to use the proposed technique as an automated toolfor diagnosis of COVID-19.',\n",
       "  'categories': ['eess.IV', 'cs.CV', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2006.14419v2'},\n",
       " 633: {'ID': 633,\n",
       "  'title': \"Visual Explanations From Deep 3D Convolutional Neural Networks for  Alzheimer's Disease Classification\",\n",
       "  'authors': ['Sanjay Ranka', 'Anand Rangarajan', 'Chengliang Yang'],\n",
       "  'published': '2018-03-07T07:07:39Z',\n",
       "  'updated': '2018-07-06T00:28:49Z',\n",
       "  'abstract': \"We develop three efficient approaches for generating visual explanations from3D convolutional neural networks (3D-CNNs) for Alzheimer's diseaseclassification. One approach conducts sensitivity analysis on hierarchical 3Dimage segmentation, and the other two visualize network activations on aspatial map. Visual checks and a quantitative localization benchmark indicatethat all approaches identify important brain parts for Alzheimer's diseasediagnosis. Comparative analysis show that the sensitivity analysis basedapproach has difficulty handling loosely distributed cerebral cortex, andapproaches based on visualization of activations are constrained by theresolution of the convolutional layer. The complementarity of these methodsimproves the understanding of 3D-CNNs in Alzheimer's disease classificationfrom different perspectives.\",\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.LG', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1803.02544v3'},\n",
       " 634: {'ID': 634,\n",
       "  'title': 'Empirical study of PROXTONE and PROXTONE$^+$ for Fast Learning of Large  Scale Sparse Models',\n",
       "  'authors': ['Ziqiang Shi', 'Rujie Liu'],\n",
       "  'published': '2016-04-18T08:01:02Z',\n",
       "  'updated': '2016-04-18T08:01:02Z',\n",
       "  'abstract': 'PROXTONE is a novel and fast method for optimization of large scalenon-smooth convex problem \\\\cite{shi2015large}. In this work, we try to usePROXTONE method in solving large scale \\\\emph{non-smooth non-convex} problems,for example training of sparse deep neural network (sparse DNN) or sparseconvolutional neural network (sparse CNN) for embedded or mobile device.PROXTONE converges much faster than first order methods, while first ordermethod is easy in deriving and controlling the sparseness of the solutions.Thus in some applications, in order to train sparse models fast, we propose tocombine the merits of both methods, that is we use PROXTONE in the firstseveral epochs to reach the neighborhood of an optimal solution, and then usethe first order method to explore the possibility of sparsity in the followingtraining. We call such method PROXTONE plus (PROXTONE$^+$). Both PROXTONE andPROXTONE$^+$ are tested in our experiments, and which demonstrate both methodsimproved convergence speed twice as fast at least on diverse sparse modellearning problems, and at the same time reduce the size to 0.5\\\\% for DNNmodels. The source of all the algorithms is available upon request.',\n",
       "  'categories': ['cs.LG', 'cs.AI'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1604.05024v1'},\n",
       " 635: {'ID': 635,\n",
       "  'title': 'History for Visual Dialog: Do we really need it?',\n",
       "  'authors': ['Verena Rieser',\n",
       "   'Trung Bui',\n",
       "   'Joon-Young Lee',\n",
       "   'Shubham Agarwal',\n",
       "   'Ioannis Konstas'],\n",
       "  'published': '2020-05-08T14:58:09Z',\n",
       "  'updated': '2020-05-08T14:58:09Z',\n",
       "  'abstract': 'Visual Dialog involves \"understanding\" the dialog history (what has beendiscussed previously) and the current question (what is asked), in addition togrounding information in the image, to generate the correct response. In thispaper, we show that co-attention models which explicitly encode dialog historyoutperform models that don\\'t, achieving state-of-the-art performance (72 % NDCGon val set). However, we also expose shortcomings of the crowd-sourcing datasetcollection procedure by showing that history is indeed only required for asmall amount of the data and that the current evaluation metric encouragesgeneric replies. To that end, we propose a challenging subset (VisDialConv) ofthe VisDial val set and provide a benchmark of 63% NDCG.',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2005.07493v1'},\n",
       " 636: {'ID': 636,\n",
       "  'title': 'An Adaptive Sampling Scheme to Efficiently Train Fully Convolutional  Networks for Semantic Segmentation',\n",
       "  'authors': ['Eoin Hyde',\n",
       "   'M. Jorge Cardoso',\n",
       "   'Lorenz Berger',\n",
       "   'Sebastien Ourselin'],\n",
       "  'published': '2017-09-08T16:17:55Z',\n",
       "  'updated': '2017-12-22T14:16:07Z',\n",
       "  'abstract': 'Deep convolutional neural networks (CNNs) have shown excellent performance inobject recognition tasks and dense classification problems such as semanticsegmentation. However, training deep neural networks on large and sparsedatasets is still challenging and can require large amounts of computation andmemory. In this work, we address the task of performing semantic segmentationon large data sets, such as three-dimensional medical images. We propose anadaptive sampling scheme that uses a-posterior error maps, generated throughouttraining, to focus sampling on difficult regions, resulting in improvedlearning. Our contribution is threefold: 1) We give a detailed description ofthe proposed sampling algorithm to speed up and improve learning performance onlarge images. We propose a deep dual path CNN that captures information at fineand coarse scales, resulting in a network with a large field of view and highresolution outputs. We show that our method is able to attain newstate-of-the-art results on the VISCERAL Anatomy benchmark.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1709.02764v4'},\n",
       " 637: {'ID': 637,\n",
       "  'title': 'Multimodal Compact Bilinear Pooling for Visual Question Answering and  Visual Grounding',\n",
       "  'authors': ['Daylen Yang',\n",
       "   'Dong Huk Park',\n",
       "   'Akira Fukui',\n",
       "   'Marcus Rohrbach',\n",
       "   'Trevor Darrell',\n",
       "   'Anna Rohrbach'],\n",
       "  'published': '2016-06-06T17:59:56Z',\n",
       "  'updated': '2016-09-24T01:58:59Z',\n",
       "  'abstract': 'Modeling textual or visual information with vector representations trainedfrom large language or visual datasets has been successfully explored in recentyears. However, tasks such as visual question answering require combining thesevector representations with each other. Approaches to multimodal poolinginclude element-wise product or sum, as well as concatenation of the visual andtextual representations. We hypothesize that these methods are not asexpressive as an outer product of the visual and textual vectors. As the outerproduct is typically infeasible due to its high dimensionality, we insteadpropose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently andexpressively combine multimodal features. We extensively evaluate MCB on thevisual question answering and grounding tasks. We consistently show the benefitof MCB over ablations without MCB. For visual question answering, we present anarchitecture which uses MCB twice, once for predicting attention over spatialfeatures and again to combine the attended representation with the questionrepresentation. This model outperforms the state-of-the-art on the Visual7Wdataset and the VQA challenge.',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.CL'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1606.01847v3'},\n",
       " 638: {'ID': 638,\n",
       "  'title': 'Structured learning of metric ensembles with application to person  re-identification',\n",
       "  'authors': ['Anton van den Hengel',\n",
       "   'Lin Wu',\n",
       "   'Chunhua Shen',\n",
       "   'Sakrapee Paisitkriangkrai'],\n",
       "  'published': '2015-11-27T00:10:59Z',\n",
       "  'updated': '2016-05-24T04:48:59Z',\n",
       "  'abstract': 'Matching individuals across non-overlapping camera networks, known as personre-identification, is a fundamentally challenging problem due to the largevisual appearance changes caused by variations of viewpoints, lighting, andocclusion. Approaches in literature can be categoried into two streams: Thefirst stream is to develop reliable features against realistic conditions bycombining several visual features in a pre-defined way; the second stream is tolearn a metric from training data to ensure strong inter-class differences andintra-class similarities. However, seeking an optimal combination of visualfeatures which is generic yet adaptive to different benchmarks is a unsovedproblem, and metric learning models easily get over-fitted due to the scarcityof training data in person re-identification. In this paper, we propose twoeffective structured learning based approaches which explore the adaptiveeffects of visual features in recognizing persons in different benchmark datasets. Our framework is built on the basis of multiple low-level visual featureswith an optimal ensemble of their metrics. We formulate two optimizationalgorithms, CMCtriplet and CMCstruct, which directly optimize evaluationmeasures commonly used in person re-identification, also known as theCumulative Matching Characteristic (CMC) curve.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1511.08531v2'},\n",
       " 639: {'ID': 639,\n",
       "  'title': 'Rotate your Networks: Better Weight Consolidation and Less Catastrophic  Forgetting',\n",
       "  'authors': ['Andrew D. Bagdanov',\n",
       "   'Luis Herranz',\n",
       "   'Marc Masana',\n",
       "   'Xialei Liu',\n",
       "   'Antonio M. Lopez',\n",
       "   'Joost Van de Weijer'],\n",
       "  'published': '2018-02-08T16:25:29Z',\n",
       "  'updated': '2018-12-12T16:17:25Z',\n",
       "  'abstract': 'In this paper we propose an approach to avoiding catastrophic forgetting insequential task learning scenarios. Our technique is based on a networkreparameterization that approximately diagonalizes the Fisher InformationMatrix of the network parameters. This reparameterization takes the form of afactorized rotation of parameter space which, when used in conjunction withElastic Weight Consolidation (which assumes a diagonal Fisher InformationMatrix), leads to significantly better performance on lifelong learning ofsequential tasks. Experimental results on the MNIST, CIFAR-100, CUB-200 andStanford-40 datasets demonstrate that we significantly improve the results ofstandard elastic weight consolidation, and that we obtain competitive resultswhen compared to other state-of-the-art in lifelong learning withoutforgetting.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1802.02950v4'},\n",
       " 640: {'ID': 640,\n",
       "  'title': 'MoGA: Searching Beyond MobileNetV3',\n",
       "  'authors': ['Bo Zhang', 'Xiangxiang Chu', 'Ruijun Xu'],\n",
       "  'published': '2019-08-04T10:40:04Z',\n",
       "  'updated': '2020-03-03T04:11:41Z',\n",
       "  'abstract': 'The evolution of MobileNets has laid a solid foundation for neural networkapplications on mobile end. With the latest MobileNetV3, neural architecturesearch again claimed its supremacy in network design. Unfortunately, till todayall mobile methods mainly focus on CPU latencies instead of GPU, the latter,however, is much preferred in practice for it has faster speed, lower overheadand less interference. Bearing the target hardware in mind, we propose thefirst Mobile GPU-Aware (MoGA) neural architecture search in order to beprecisely tailored for real-world applications. Further, the ultimate objectiveto devise a mobile network lies in achieving better performance by maximizingthe utilization of bounded resources. Urging higher capability whilerestraining time consumption is not reconcilable. We alleviate the tension byweighted evolution techniques. Moreover, we encourage increasing the number ofparameters for higher representational power. With 200x fewer GPU days thanMnasNet, we obtain a series of models that outperform MobileNetV3 under thesimilar latency constraints, i.e., MoGA-A achieves 75.9% top-1 accuracy onImageNet, MoGA-B meets 75.5% which costs only 0.5 ms more on mobile GPU. MoGA-Cbest attests GPU-awareness by reaching 75.3% and being slower on CPU but fasteron GPU.The models and test code is made available herehttps://github.com/xiaomi-automl/MoGA.',\n",
       "  'categories': ['cs.LG', 'cs.CV', 'cs.NE', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1908.01314v4'},\n",
       " 641: {'ID': 641,\n",
       "  'title': \"Deep HST WFPC2 Photometry of M31's Thick Disk(?)\",\n",
       "  'authors': ['Jeffrey Van Duyne', 'Ata Sarajedini'],\n",
       "  'published': '2001-07-18T16:19:37Z',\n",
       "  'updated': '2001-07-18T16:19:37Z',\n",
       "  'abstract': \"We present deep color-magnitude diagrams (CMDs) for a field along the outerdisk of M31 based on archival Hubble Space Telescope Wide Field PlanetaryCamera 2 observations in the F555W (~V) and F814W (~I) filters. The CMDs, whichcontain a total of about 50,000 stars, feature a prominent red giant branch(RGB) along with a significant population of helium burning red clump stars. Inaddition, they exhibit the rarely seen asymptotic giant branch clump as well asa weak `Pop II' horizontal branch. There is also the hint of a ~2 Gyr subgiantbranch at the faintest levels of the CMDs. After adopting an M31 distance of(m-M)o = 24.5 and a reddening of E(B-V) = 0.08, we draw the followingconclusions. 1) The I-band absolute magnitude of the helium burning red clumpstars is M(RC) = -0.29 +/- 0.05, which is in accord with the value derived fromHipparcos parallaxes of solar neighborhood clump stars by Stanek &amp; Garnavich.2) The metallicity distribution function constructed from bright RGB starsshows a characteristic shape; however, a pure halo population consisting ofmetal-poor and intermediate metallicity components (as advocated in theliterature) are not sufficient to account for this shape. Instead, anadditional Gaussian component with &lt;[Fe/H]&gt; = -0.22 +/- 0.26, comprising 70% ofthe total number of stars, is required. 3) A comparison of our CMD withtheoretical isochrones indicates that the majority of stars in our M31 fieldhave ages that are &gt;~1.5 Gyr. 4) These points, along with the physical locationof our field in M31, suggest that we are observing the thick disk population ofthis galaxy.\",\n",
       "  'categories': ['astro-ph'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/astro-ph/0107344v1'},\n",
       " 642: {'ID': 642,\n",
       "  'title': 'Nonparametric Structure Regularization Machine for 2D Hand Pose  Estimation',\n",
       "  'authors': ['Yifei Chen',\n",
       "   'Haoyu Ma',\n",
       "   'Wei Fan',\n",
       "   'Jianbao Wu',\n",
       "   'Deying Kong',\n",
       "   'Xiangyi Yan',\n",
       "   'Xiaohui Xie'],\n",
       "  'published': '2020-01-24T03:27:32Z',\n",
       "  'updated': '2020-01-24T03:27:32Z',\n",
       "  'abstract': 'Hand pose estimation is more challenging than body pose estimation due tosevere articulation, self-occlusion and high dexterity of the hand. Currentapproaches often rely on a popular body pose algorithm, such as theConvolutional Pose Machine (CPM), to learn 2D keypoint features. Thesealgorithms cannot adequately address the unique challenges of hand poseestimation, because they are trained solely based on keypoint positions withoutseeking to explicitly model structural relationship between them. We propose anovel Nonparametric Structure Regularization Machine (NSRM) for 2D hand poseestimation, adopting a cascade multi-task architecture to learn hand structureand keypoint representations jointly. The structure learning is guided bysynthetic hand mask representations, which are directly computed from keypointpositions, and is further strengthened by a novel probabilistic representationof hand limbs and an anatomically inspired composition strategy of masksynthesis. We conduct extensive studies on two public datasets - OneHand 10kand CMU Panoptic Hand. Experimental results demonstrate that explicitlyenforcing structure learning consistently improves pose estimation accuracy ofCPM baseline models, by 1.17% on the first dataset and 4.01% on the second one.The implementation and experiment code is freely available online. Our proposalof incorporating structural learning to hand pose estimation requires noadditional training information, and can be a generic add-on module to otherpose estimation models.',\n",
       "  'categories': ['cs.CV', 'cs.LG', 'eess.IV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2001.08869v1'},\n",
       " 643: {'ID': 643,\n",
       "  'title': 'A Projectional Ansatz to Reconstruction',\n",
       "  'authors': ['Sören Dittmer', 'Peter Maass'],\n",
       "  'published': '2019-07-10T12:49:07Z',\n",
       "  'updated': '2019-08-06T11:36:52Z',\n",
       "  'abstract': 'Recently the field of inverse problems has seen a growing usage ofmathematically only partially understood learned and non-learned priors. Basedon first principles, we develop a projectional approach to inverse problemsthat addresses the incorporation of these priors, while still guaranteeing dataconsistency. We implement this projectional method (PM) on the one hand viavery general Plug-and-Play priors and on the other hand, via an end-to-endtraining approach. To this end, we introduce a novel alternating neuralarchitecture, allowing for the incorporation of highly customized priors fromdata in a principled manner. We also show how the recent success ofRegularization by Denoising (RED) can, at least to some extent, be explained asan approximation of the PM. Furthermore, we demonstrate how the idea can beapplied to stop the degradation of Deep Image Prior (DIP) reconstructions overtime.',\n",
       "  'categories': ['cs.LG', 'cs.CV', 'math.FA', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1907.04675v2'},\n",
       " 644: {'ID': 644,\n",
       "  'title': 'Benchmarking Semi-supervised Federated Learning',\n",
       "  'authors': ['Michael W. Mahoney',\n",
       "   'Zhewei Yao',\n",
       "   'Yaoqing Yang',\n",
       "   'Yujun Yan',\n",
       "   'Zhengming Zhang',\n",
       "   'Joseph E. Gonzalez'],\n",
       "  'published': '2020-08-26T03:36:07Z',\n",
       "  'updated': '2020-08-26T03:36:07Z',\n",
       "  'abstract': 'Federated learning promises to use the computational power of edge deviceswhile maintaining user data privacy. Current frameworks, however, typicallymake the unrealistic assumption that the data stored on user devices come withground truth labels, while the server has no data. In this work, we considerthe more realistic scenario where the users have only unlabeled data and theserver has a limited amount of labeled data. In this semi-supervised federatedlearning (ssfl) setting, the data distribution can be non-iid, in the sense ofdifferent distributions of classes at different users. We define a metric, $R$,to measure this non-iidness in class distributions. In this setting, we providea thorough study on different factors that can affect the final test accuracy,including algorithm design (such as training objective), the non-iidness $R$,the communication period $T$, the number of users $K$, the amount of labeleddata in the server $N_s$, and the number of users $C_k\\\\leq K$ that communicatewith the server in each communication round. We evaluate our ssfl framework onCifar-10, SVHN, and EMNIST. Overall, we find that a simple consistencyloss-based method, along with group normalization, achieves bettergeneralization performance, even compared to previous supervised federatedlearning settings. Furthermore, we propose a novel grouping-based model averagemethod to improve convergence efficiency, and we show that this can boostperformance by up to 10.79% on EMNIST, compared to the non-grouping basedmethod.',\n",
       "  'categories': ['cs.LG', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2008.11364v1'},\n",
       " 645: {'ID': 645,\n",
       "  'title': 'Proxy Templates for Inverse Compositional Photometric Bundle Adjustment',\n",
       "  'authors': ['Surya Singh', 'Christopher Ham', 'Simon Lucey'],\n",
       "  'published': '2017-04-23T19:42:48Z',\n",
       "  'updated': '2017-04-23T19:42:48Z',\n",
       "  'abstract': 'Recent advances in 3D vision have demonstrated the strengths of photometricbundle adjustment. By directly minimizing reprojected pixel errors, instead ofgeometric reprojection errors, such methods can achieve sub-pixel alignmentaccuracy in both high and low textured regions. Typically, these problems aresolved using a forwards compositional Lucas-Kanade formulation parameterized by6-DoF rigid camera poses and a depth per point in the structure. For largeproblems the most CPU-intensive component of the pipeline is the creation andfactorization of the Hessian matrix at each iteration. For many warps, theinverse compositional formulation can offer significant speed-ups since theHessian need only be inverted once. In this paper, we show that an ordinaryinverse compositional formulation does not work for warps of this type ofparameterization due to ill-conditioning of its partial derivatives. However,we show that it is possible to overcome this limitation by introducing theconcept of a proxy template image. We show an order of magnitude improvement inspeed, with little effect on quality, going from forwards to inversecompositional in our own photometric bundle adjustment method designed forobject-centric structure from motion. This means less processing time for largesystems or denser reconstructions under the same real-time constraints. Weadditionally show that this theory can be readily applied to existing methodsby integrating it with the recently released Direct Sparse Odometry SLAMalgorithm.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1704.06967v1'},\n",
       " 646: {'ID': 646,\n",
       "  'title': 'Reconstructing boosted Higgs jets from event image segmentation',\n",
       "  'authors': ['Tianjun Li', 'Fang-Zhou Xu', 'Jinmian Li'],\n",
       "  'published': '2020-08-31T12:25:50Z',\n",
       "  'updated': '2020-08-31T12:25:50Z',\n",
       "  'abstract': 'The Mask R-CNN framework is adopted to reconstruct Higgs jets in Higgs bosonproduction events, with the effects of pileup contamination taken into account.This automatic reconstruction method achieves higher efficiency of Higgs jetdetection and higher accuracy of Higgs boson four-momentum reconstruction thantraditional jet clustering and tagging methods using substructure information.Moreover, the Mask R-CNN trained on events containing a single Higgs jet iscapable of detecting one or more Higgs jets in events of several differentprocesses, without apparent degradation in reconstruction efficiency andaccuracy. Taking the outputs of the network as new features to complementtraditional jet substructure variables, the signal events can be discriminatedfrom background events even further.',\n",
       "  'categories': ['hep-ph', 'hep-ex'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2008.13529v1'},\n",
       " 647: {'ID': 647,\n",
       "  'title': 'Fast Supervised Discrete Hashing',\n",
       "  'authors': ['Tongliang Liu',\n",
       "   'Jie Gui',\n",
       "   'Zhenan Sun',\n",
       "   'Dacheng Tao',\n",
       "   'Tieniu Tan'],\n",
       "  'published': '2019-04-07T00:35:54Z',\n",
       "  'updated': '2019-04-07T00:35:54Z',\n",
       "  'abstract': 'Learning-based hashing algorithms are ``hot topics\" because they can greatlyincrease the scale at which existing methods operate. In this paper, we proposea new learning-based hashing method called ``fast supervised discrete hashing\"(FSDH) based on ``supervised discrete hashing\" (SDH). Regressing the trainingexamples (or hash code) to the corresponding class labels is widely used inordinary least squares regression. Rather than adopting this method, FSDH usesa very simple yet effective regression of the class labels of training examplesto the corresponding hash code to accelerate the algorithm. To the best of ourknowledge, this strategy has not previously been used for hashing. TraditionalSDH decomposes the optimization into three sub-problems, with the most criticalsub-problem - discrete optimization for binary hash codes - solved usingiterative discrete cyclic coordinate descent (DCC), which is time-consuming.However, FSDH has a closed-form solution and only requires a single rather thaniterative hash code-solving step, which is highly efficient. Furthermore, FSDHis usually faster than SDH for solving the projection matrix for least squaresregression, making FSDH generally faster than SDH. For example, our resultsshow that FSDH is about 12-times faster than SDH when the number of hashingbits is 128 on the CIFAR-10 data base, and FSDH is about 151-times faster thanFastHash when the number of hashing bits is 64 on the MNIST data-base. Ourexperimental results show that FSDH is not only fast, but also outperformsother comparative methods.',\n",
       "  'categories': ['cs.LG', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1904.03556v1'},\n",
       " 648: {'ID': 648,\n",
       "  'title': 'DeepRED: Deep Image Prior Powered by RED',\n",
       "  'authors': ['Gary Mataev', 'Michael Elad', 'Peyman Milanfar'],\n",
       "  'published': '2019-03-25T08:39:53Z',\n",
       "  'updated': '2019-10-24T10:50:39Z',\n",
       "  'abstract': 'Inverse problems in imaging are extensively studied, with a variety ofstrategies, tools, and theory that have been accumulated over the years.Recently, this field has been immensely influenced by the emergence ofdeep-learning techniques. One such contribution, which is the focus of thispaper, is the Deep Image Prior (DIP) work by Ulyanov, Vedaldi, and Lempitsky(2018). DIP offers a new approach towards the regularization of inverseproblems, obtained by forcing the recovered image to be synthesized from agiven deep architecture. While DIP has been shown to be quite an effectiveunsupervised approach, its results still fall short when compared tostate-of-the-art alternatives.  In this work, we aim to boost DIP by adding an explicit prior, which enrichesthe overall regularization effect in order to lead to better-recovered images.More specifically, we propose to bring-in the concept of Regularization byDenoising (RED), which leverages existing denoisers for regularizing inverseproblems. Our work shows how the two (DIP and RED) can be merged into a highlyeffective unsupervised recovery process while avoiding the need todifferentiate the chosen denoiser, and leading to very effective results,demonstrated for several tested problems.',\n",
       "  'categories': ['cs.CV', 'eess.IV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1903.10176v3'},\n",
       " 649: {'ID': 649,\n",
       "  'title': 'A Method for Computing Class-wise Universal Adversarial Perturbations',\n",
       "  'authors': ['Mayank Singh',\n",
       "   'Abhishek Sinha',\n",
       "   'Balaji Krishnamurthy',\n",
       "   'Nupur Kumari',\n",
       "   'Tejus Gupta'],\n",
       "  'published': '2019-12-01T18:22:14Z',\n",
       "  'updated': '2019-12-01T18:22:14Z',\n",
       "  'abstract': 'We present an algorithm for computing class-specific universal adversarialperturbations for deep neural networks. Such perturbations can inducemisclassification in a large fraction of images of a specific class. Unlikeprevious methods that use iterative optimization for computing a universalperturbation, the proposed method employs a perturbation that is a linearfunction of weights of the neural network and hence can be computed muchfaster. The method does not require any training data and has nohyper-parameters. The attack obtains 34% to 51% fooling rate onstate-of-the-art deep neural networks on ImageNet and transfers across models.We also study the characteristics of the decision boundaries learned bystandard and adversarially trained models to understand the universaladversarial perturbations.',\n",
       "  'categories': ['cs.LG', 'cs.CR', 'cs.CV', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1912.00466v1'},\n",
       " 650: {'ID': 650,\n",
       "  'title': 'Audio Visual Scene-Aware Dialog (AVSD) Challenge at DSTC7',\n",
       "  'authors': ['Irfan Essa',\n",
       "   'Raphael Gontijo Lopes',\n",
       "   'Vincent Cartillier',\n",
       "   'Chiori Hori',\n",
       "   'Abhishek Das',\n",
       "   'Jue Wang',\n",
       "   'Huda Alamri',\n",
       "   'Anoop Cherian',\n",
       "   'Dhruv Batra',\n",
       "   'Tim K. Marks',\n",
       "   'Devi Parikh'],\n",
       "  'published': '2018-06-01T19:51:58Z',\n",
       "  'updated': '2018-06-01T19:51:58Z',\n",
       "  'abstract': 'Scene-aware dialog systems will be able to have conversations with usersabout the objects and events around them. Progress on such systems can be madeby integrating state-of-the-art technologies from multiple research areasincluding end-to-end dialog systems visual dialog, and video description. Weintroduce the Audio Visual Scene Aware Dialog (AVSD) challenge and dataset. Inthis challenge, which is one track of the 7th Dialog System TechnologyChallenges (DSTC7) workshop1, the task is to build a system that generatesresponses in a dialog about an input video',\n",
       "  'categories': ['cs.CL', 'cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1806.00525v1'},\n",
       " 651: {'ID': 651,\n",
       "  'title': 'GAN Mask R-CNN:Instance semantic segmentation benefits from  generativeadversarial networks',\n",
       "  'authors': ['Kamal Youcef-Toumi',\n",
       "   'Dzmitry Tsetserukou',\n",
       "   'Quang H. Le',\n",
       "   'Ali Jahanian'],\n",
       "  'published': '2020-10-26T17:47:30Z',\n",
       "  'updated': '2020-10-26T17:47:30Z',\n",
       "  'abstract': 'In designing instance segmentation ConvNets that reconstruct masks,segmentation is often taken as its literal definition -- assigning label toevery pixel -- for defining the loss functions. That is, using losses thatcompute the difference between pixels in the predicted (reconstructed) mask andthe ground truth mask -- a template matching mechanism. However, any suchinstance segmentation ConvNet is a generator, so we can lay the problem ofpredicting masks as a GANs game framework: We can think the ground truth maskis drawn from the true distribution, and a ConvNet like Mask R-CNN is animplicit model that infers the true distribution. Then, designing adiscriminator in front of this generator will close the loop of GANs conceptand more importantly obtains a loss that is trained not hand-designed. We showthis design outperforms the baseline when trying on, without extra settings,several different domains: cellphone recycling, autonomous driving, large-scaleobject detection, and medical glands. Further, we observe in general GANs yieldmasks that account for better boundaries, clutter, and small details.',\n",
       "  'categories': ['cs.CV', 'I.4.6'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2010.13757v1'},\n",
       " 652: {'ID': 652,\n",
       "  'title': 'Adversarial Threats to DeepFake Detection: A Practical Perspective',\n",
       "  'authors': ['Cristian Canton Ferrer',\n",
       "   'Joanna Bitton',\n",
       "   'Brian Dolhansky',\n",
       "   'Paarth Neekhara'],\n",
       "  'published': '2020-11-19T16:53:38Z',\n",
       "  'updated': '2020-11-19T16:53:38Z',\n",
       "  'abstract': 'Facially manipulated images and videos or DeepFakes can be used maliciouslyto fuel misinformation or defame individuals. Therefore, detecting DeepFakes iscrucial to increase the credibility of social media platforms and other mediasharing web sites. State-of-the art DeepFake detection techniques rely onneural network based classification models which are known to be vulnerable toadversarial examples. In this work, we study the vulnerabilities ofstate-of-the-art DeepFake detection methods from a practical stand point. Weperform adversarial attacks on DeepFake detectors in a black box setting wherethe adversary does not have complete knowledge of the classification models. Westudy the extent to which adversarial perturbations transfer across differentmodels and propose techniques to improve the transferability of adversarialexamples. We also create more accessible attacks using Universal AdversarialPerturbations which pose a very feasible attack scenario since they can beeasily shared amongst attackers. We perform our evaluations on the winningentries of the DeepFake Detection Challenge (DFDC) and demonstrate that theycan be easily bypassed in a practical attack scenario by designing transferableand accessible adversarial attacks.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2011.09957v1'},\n",
       " 653: {'ID': 653,\n",
       "  'title': 'Parallax Motion Effect Generation Through Instance Segmentation And  Depth Estimation',\n",
       "  'authors': ['Jose L. Flores-Campana',\n",
       "   'Allan Pinto',\n",
       "   'Diogo C. Luvizon',\n",
       "   'Helio Pedrini',\n",
       "   'Henrique F. Gagliardi',\n",
       "   'Andreza A. dos Santos',\n",
       "   'Jhonatas S. Conceição',\n",
       "   'Ricardo da S. Torres',\n",
       "   'Marcos R. Souza',\n",
       "   'Luis G. L. Decker',\n",
       "   'Manuel A. Córdova'],\n",
       "  'published': '2020-10-06T12:56:59Z',\n",
       "  'updated': '2020-10-06T12:56:59Z',\n",
       "  'abstract': \"Stereo vision is a growing topic in computer vision due to the innumerableopportunities and applications this technology offers for the development ofmodern solutions, such as virtual and augmented reality applications. Toenhance the user's experience in three-dimensional virtual environments, themotion parallax estimation is a promising technique to achieve this objective.In this paper, we propose an algorithm for generating parallax motion effectsfrom a single image, taking advantage of state-of-the-art instance segmentationand depth estimation approaches. This work also presents a comparison againstsuch algorithms to investigate the trade-off between efficiency and quality ofthe parallax motion effects, taking into consideration a multi-task learningnetwork capable of estimating instance segmentation and depth estimation atonce. Experimental results and visual quality assessment indicate that thePyD-Net network (depth estimation) combined with Mask R-CNN or FBNet networks(instance segmentation) can produce parallax motion effects with good visualquality.\",\n",
       "  'categories': ['cs.CV', 'cs.LG', 'eess.IV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2010.02680v1'},\n",
       " 654: {'ID': 654,\n",
       "  'title': 'SD-Measure: A Social Distancing Detector',\n",
       "  'authors': ['Shreyas Srinivas Joshi',\n",
       "   'Aniruddha Srinivas Joshi',\n",
       "   'Rudraksh Kapil',\n",
       "   'Goutham Kanahasabai',\n",
       "   'Savyasachi Gupta'],\n",
       "  'published': '2020-11-04T15:47:14Z',\n",
       "  'updated': '2020-11-04T15:47:14Z',\n",
       "  'abstract': 'The practice of social distancing is imperative to curbing the spread ofcontagious diseases and has been globally adopted as a non-pharmaceuticalprevention measure during the COVID-19 pandemic. This work proposes a novelframework named SD-Measure for detecting social distancing from video footages.The proposed framework leverages the Mask R-CNN deep neural network to detectpeople in a video frame. To consistently identify whether social distancing ispracticed during the interaction between people, a centroid tracking algorithmis utilised to track the subjects over the course of the footage. With the aidof authentic algorithms for approximating the distance of people from thecamera and between themselves, we determine whether the social distancingguidelines are being adhered to. The framework attained a high accuracy valuein conjunction with a low false alarm rate when tested on Custom Video FootageDataset (CVFD) and Custom Personal Images Dataset (CPID), where it manifestedits effectiveness in determining whether social distancing guidelines werepracticed.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2011.02365v1'},\n",
       " 655: {'ID': 655,\n",
       "  'title': 'Instance Segmentation of Fibers from Low Resolution CT Scans via 3D Deep  Embedding Learning',\n",
       "  'authors': ['Thorben Kröger',\n",
       "   'Jürgen Hesser',\n",
       "   'Tomasz Konopczyński',\n",
       "   'Lei Zheng'],\n",
       "  'published': '2019-01-04T09:53:07Z',\n",
       "  'updated': '2019-01-04T09:53:07Z',\n",
       "  'abstract': 'We propose a novel approach for automatic extraction (instance segmentation)of fibers from low resolution 3D X-ray computed tomography scans of short glassfiber reinforced polymers. We have designed a 3D instance segmentationarchitecture built upon a deep fully convolutional network for semanticsegmentation with an extra output for embedding learning. We show that theembedding learning is capable of learning a mapping of voxels to an embeddedspace in which a standard clustering algorithm can be used to distinguishbetween different instances of an object in a volume. In addition, we discuss amerging post-processing method which makes it possible to process volumes ofany size. The proposed 3D instance segmentation network together with ourmerging algorithm is the first known to authors knowledge procedure thatproduces results good enough, that they can be used for further analysis of lowresolution fiber composites CT scans.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1901.01034v1'},\n",
       " 656: {'ID': 656,\n",
       "  'title': 'An Exponential Learning Rate Schedule for Deep Learning',\n",
       "  'authors': ['Sanjeev Arora', 'Zhiyuan Li'],\n",
       "  'published': '2019-10-16T16:22:58Z',\n",
       "  'updated': '2019-11-21T11:01:34Z',\n",
       "  'abstract': 'Intriguing empirical evidence exists that deep learning can work well withexoticschedules for varying the learning rate. This paper suggests that thephenomenon may be due to Batch Normalization or BN, which is ubiquitous andprovides benefits in optimization and generalization across all standardarchitectures. The following new results are shown about BN with weight decayand momentum (in other words, the typical use case which was not considered inearlier theoretical analyses of stand-alone BN.  1. Training can be done using SGD with momentum and an exponentiallyincreasing learning rate schedule, i.e., learning rate increases by some $(1+\\\\alpha)$ factor in every epoch for some $\\\\alpha &gt;0$. (Precise statement in thepaper.) To the best of our knowledge this is the first time such a rateschedule has been successfully used, let alone for highly successfularchitectures. As expected, such training rapidly blows up network weights, butthe net stays well-behaved due to normalization.  2. Mathematical explanation of the success of the above rate schedule: arigorous proof that it is equivalent to the standard setting of BN + SGD +StandardRate Tuning + Weight Decay + Momentum. This equivalence holds for othernormalization layers as well, Group Normalization, LayerNormalization, InstanceNorm, etc.  3. A worked-out toy example illustrating the above linkage ofhyper-parameters. Using either weight decay or BN alone reaches global minimum,but convergence fails when both are used.',\n",
       "  'categories': ['cs.LG', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1910.07454v3'},\n",
       " 657: {'ID': 657,\n",
       "  'title': 'Regularization of Building Boundaries in Satellite Images using  Adversarial and Regularized Losses',\n",
       "  'authors': ['Stefano Zorzi', 'Friedrich Fraundorfer'],\n",
       "  'published': '2020-07-23T08:07:55Z',\n",
       "  'updated': '2020-07-23T08:07:55Z',\n",
       "  'abstract': 'In this paper we present a method for building boundary refinement andregularization in satellite images using a fully convolutional neural networktrained with a combination of adversarial and regularized losses. Compared to apure Mask R-CNN model, the overall algorithm can achieve equivalent performancein terms of accuracy and completeness. However, unlike Mask R-CNN that producesirregular footprints, our framework generates regularized and visually pleasingbuilding boundaries which are beneficial in many applications.',\n",
       "  'categories': ['eess.IV', 'cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2007.11840v1'},\n",
       " 658: {'ID': 658,\n",
       "  'title': 'Disentangled Non-Local Neural Networks',\n",
       "  'authors': ['Han Hu',\n",
       "   'Zhuliang Yao',\n",
       "   'Minghao Yin',\n",
       "   'Yue Cao',\n",
       "   'Zheng Zhang',\n",
       "   'Xiu Li',\n",
       "   'Stephen Lin'],\n",
       "  'published': '2020-06-11T17:59:22Z',\n",
       "  'updated': '2020-09-08T14:12:09Z',\n",
       "  'abstract': 'The non-local block is a popular module for strengthening the contextmodeling ability of a regular convolutional neural network. This paper firststudies the non-local block in depth, where we find that its attentioncomputation can be split into two terms, a whitened pairwise term accountingfor the relationship between two pixels and a unary term representing thesaliency of every pixel. We also observe that the two terms trained alone tendto model different visual clues, e.g. the whitened pairwise term learnswithin-region relationships while the unary term learns salient boundaries.However, the two terms are tightly coupled in the non-local block, whichhinders the learning of each. Based on these findings, we present thedisentangled non-local block, where the two terms are decoupled to facilitatelearning for both terms. We demonstrate the effectiveness of the decoupleddesign on various tasks, such as semantic segmentation on Cityscapes, ADE20Kand PASCAL Context, object detection on COCO, and action recognition onKinetics.',\n",
       "  'categories': ['cs.CV', 'cs.CL', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2006.06668v2'},\n",
       " 659: {'ID': 659,\n",
       "  'title': 'Assistive Diagnostic Tool for Brain Tumor Detection using Computer  Vision',\n",
       "  'authors': ['Sahithi Ankireddy'],\n",
       "  'published': '2020-11-17T04:58:33Z',\n",
       "  'updated': '2020-11-17T04:58:33Z',\n",
       "  'abstract': 'Today, over 700,000 people are living with brain tumors in the United States.Brain tumors can spread very quickly to other parts of the brain and the spinalcord unless necessary preventive action is taken. Thus, the survival rate forthis disease is less than 40% for both men and women. A conclusive and earlydiagnosis of a brain tumor could be the difference between life and death forsome. However, brain tumor detection and segmentation are tedious andtime-consuming processes as it can only be done by radiologists and clinicalexperts. The use of computer vision techniques, such as Mask R ConvolutionalNeural Network (Mask R CNN), to detect and segment brain tumors can mitigatethe possibility of human error while increasing prediction accuracy rates. Thegoal of this project is to create an assistive diagnostics tool for brain tumordetection and segmentation. Transfer learning was used with the Mask R CNN, andnecessary parameters were accordingly altered, as a starting point. The modelwas trained with 20 epochs and later tested. The prediction segmentationmatched 90% with the ground truth. This suggests that the model was able toperform at a high level. Once the model was finalized, the application runningon Flask was created. The application will serve as a tool for medicalprofessionals. It allows doctors to upload patient brain tumor MRI images inorder to receive immediate results on the diagnosis and segmentation for eachpatient.',\n",
       "  'categories': ['cs.CV', 'eess.IV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2011.08185v1'},\n",
       " 660: {'ID': 660,\n",
       "  'title': 'Activity-conditioned continuous human pose estimation for performance  analysis of athletes using the example of swimming',\n",
       "  'authors': ['Rainer Lienhart', 'Moritz Einfalt', 'Dan Zecha'],\n",
       "  'published': '2018-02-02T10:56:41Z',\n",
       "  'updated': '2018-02-02T10:56:41Z',\n",
       "  'abstract': \"In this paper we consider the problem of human pose estimation in real-worldvideos of swimmers. Swimming channels allow filming swimmers simultaneouslyabove and below the water surface with a single stationary camera. Theserecordings can be used to quantitatively assess the athletes' performance. Thequantitative evaluation, so far, requires manual annotations of body parts ineach video frame. We therefore apply the concept of CNNs in order toautomatically infer the required pose information. Starting with anoff-the-shelf architecture, we develop extensions to leverage activityinformation - in our case the swimming style of an athlete - and the continuousnature of the video recordings. Our main contributions are threefold: (a) Weapply and evaluate a fine-tuned Convolutional Pose Machine architecture as abaseline in our very challenging aquatic environment and discuss its errormodes, (b) we propose an extension to input swimming style information into thefully convolutional architecture and (c) modify the architecture for continuouspose estimation in videos. With these additions we achieve reliable poseestimates with up to +16% more correct body joint detections compared to thebaseline architecture.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1802.00634v1'},\n",
       " 661: {'ID': 661,\n",
       "  'title': 'Robust Adversarial Attacks Against DNN-Based Wireless Communication  Systems',\n",
       "  'authors': ['Alireza Bahramali',\n",
       "   'Milad Nasr',\n",
       "   'Don Towsley',\n",
       "   'Dennis Goeckel',\n",
       "   'Amir Houmansadr'],\n",
       "  'published': '2021-02-01T15:36:40Z',\n",
       "  'updated': '2021-02-01T15:36:40Z',\n",
       "  'abstract': 'Deep Neural Networks (DNNs) have become prevalent in wireless communicationsystems due to their promising performance. However, similar to other DNN-basedapplications, they are vulnerable to adversarial examples. In this work, wepropose an input-agnostic, undetectable, and robust adversarial attack againstDNN-based wireless communication systems in both white-box and black-boxscenarios. We design tailored Universal Adversarial Perturbations (UAPs) toperform the attack. We also use a Generative Adversarial Network (GAN) toenforce an undetectability constraint for our attack. Furthermore, weinvestigate the robustness of our attack against countermeasures. We show thatin the presence of defense mechanisms deployed by the communicating parties,our attack performs significantly better compared to existing attacks againstDNN-based wireless systems. In particular, the results demonstrate that evenwhen employing well-considered defenses, DNN-based wireless communications arevulnerable to adversarial attacks.',\n",
       "  'categories': ['cs.CR'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2102.00918v1'},\n",
       " 662: {'ID': 662,\n",
       "  'title': 'Synthetically Trained Neural Networks for Learning Human-Readable Plans  from Real-World Demonstrations',\n",
       "  'authors': ['Stephen Tyree',\n",
       "   'Jonathan Tremblay',\n",
       "   'Stan Birchfield',\n",
       "   'Thang To',\n",
       "   'Artem Molchanov',\n",
       "   'Jan Kautz'],\n",
       "  'published': '2018-05-18T05:30:29Z',\n",
       "  'updated': '2018-07-10T22:28:18Z',\n",
       "  'abstract': 'We present a system to infer and execute a human-readable program from areal-world demonstration. The system consists of a series of neural networks toperform perception, program generation, and program execution. Leveragingconvolutional pose machines, the perception network reliably detects thebounding cuboids of objects in real images even when severely occluded, aftertraining only on synthetic images using domain randomization. To increase theapplicability of the perception network to new scenarios, the network isformulated to predict in image space rather than in world space. Additionalnetworks detect relationships between objects, generate plans, and determineactions to reproduce a real-world demonstration. The networks are trainedentirely in simulation, and the system is tested in the real world on thepick-and-place problem of stacking colored cubes using a Baxter robot.',\n",
       "  'categories': ['cs.RO'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1805.07054v3'},\n",
       " 663: {'ID': 663,\n",
       "  'title': 'Neuro-Symbolic Visual Reasoning: Disentangling \"Visual\" from \"Reasoning\"',\n",
       "  'authors': ['Oleksandr Polozov',\n",
       "   'Kazuhito Koishida',\n",
       "   'Hamid Palangi',\n",
       "   'Saeed Amizadeh',\n",
       "   'Yichen Huang'],\n",
       "  'published': '2020-06-20T08:48:29Z',\n",
       "  'updated': '2020-08-25T23:30:57Z',\n",
       "  'abstract': 'Visual reasoning tasks such as visual question answering (VQA) require aninterplay of visual perception with reasoning about the question semanticsgrounded in perception. However, recent advances in this area are stillprimarily driven by perception improvements (e.g. scene graph generation)rather than reasoning. Neuro-symbolic models such as Neural Module Networksbring the benefits of compositional reasoning to VQA, but they are stillentangled with visual representation learning, and thus neural reasoning ishard to improve and assess on its own. To address this, we propose (1) aframework to isolate and evaluate the reasoning aspect of VQA separately fromits perception, and (2) a novel top-down calibration technique that allows themodel to answer reasoning questions even with imperfect perception. To thisend, we introduce a differentiable first-order logic formalism for VQA thatexplicitly decouples question answering from visual perception. On thechallenging GQA dataset, this framework is used to perform in-depth,disentangled comparisons between well-known VQA models leading to informativeinsights regarding the participating models as well as the task.',\n",
       "  'categories': ['cs.LG', 'cs.AI', 'cs.CV', 'cs.NE', 'cs.SC', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2006.11524v3'},\n",
       " 664: {'ID': 664,\n",
       "  'title': 'Textually Enriched Neural Module Networks for Visual Question Answering',\n",
       "  'authors': ['Mary Arpita Pyreddy',\n",
       "   'Narendra Nath Joshi',\n",
       "   'Khyathi Raghavi Chandu',\n",
       "   'Matthieu Felix'],\n",
       "  'published': '2018-09-23T23:45:54Z',\n",
       "  'updated': '2018-09-23T23:45:54Z',\n",
       "  'abstract': 'Problems at the intersection of language and vision, like visual questionanswering, have recently been gaining a lot of attention in the field ofmulti-modal machine learning as computer vision research moves beyondtraditional recognition tasks. There has been recent success in visual questionanswering using deep neural network models which use the linguistic structureof the questions to dynamically instantiate network layouts. In the process ofconverting the question to a network layout, the question is simplified, whichresults in loss of information in the model. In this paper, we enrich the imageinformation with textual data using image captions and external knowledge basesto generate more coherent answers. We achieve 57.1% overall accuracy on thetest-dev open-ended questions from the visual question answering (VQA 1.0) realimage dataset.',\n",
       "  'categories': ['cs.CL', 'cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1809.08697v1'},\n",
       " 665: {'ID': 665,\n",
       "  'title': 'Inception Recurrent Convolutional Neural Network for Object Recognition',\n",
       "  'authors': ['Md Zahangir Alom',\n",
       "   'Mahmudul Hasan',\n",
       "   'Tarek M. Taha',\n",
       "   'Chris Yakopcic'],\n",
       "  'published': '2017-04-25T14:19:26Z',\n",
       "  'updated': '2017-04-25T14:19:26Z',\n",
       "  'abstract': 'Deep convolutional neural networks (DCNNs) are an influential tool forsolving various problems in the machine learning and computer vision fields. Inthis paper, we introduce a new deep learning model called an Inception-Recurrent Convolutional Neural Network (IRCNN), which utilizes the power of aninception network combined with recurrent layers in DCNN architecture. We haveempirically evaluated the recognition performance of the proposed IRCNN modelusing different benchmark datasets such as MNIST, CIFAR-10, CIFAR- 100, andSVHN. Experimental results show similar or higher recognition accuracy whencompared to most of the popular DCNNs including the RCNN. Furthermore, we haveinvestigated IRCNN performance against equivalent Inception Networks andInception-Residual Networks using the CIFAR-100 dataset. We report about 3.5%,3.47% and 2.54% improvement in classification accuracy when compared to theRCNN, equivalent Inception Networks, and Inception- Residual Networks on theaugmented CIFAR- 100 dataset respectively.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1704.07709v1'},\n",
       " 666: {'ID': 666,\n",
       "  'title': 'Solving Inverse Problems with Hybrid Deep Image Priors: the challenge of  preventing overfitting',\n",
       "  'authors': ['Zhaodong Sun'],\n",
       "  'published': '2020-11-03T14:50:53Z',\n",
       "  'updated': '2020-11-03T14:50:53Z',\n",
       "  'abstract': 'We mainly analyze and solve the overfitting problem of deep image prior(DIP). Deep image prior can solve inverse problems such as super-resolution,inpainting and denoising. The main advantage of DIP over other deep learningapproaches is that it does not need access to a large dataset. However, due tothe large number of parameters of the neural network and noisy data, DIPoverfits to the noise in the image as the number of iterations grows. In thethesis, we use hybrid deep image priors to avoid overfitting. The hybrid priorsare to combine DIP with an explicit prior such as total variation or with animplicit prior such as a denoising algorithm. We use the alternating directionmethod-of-multipliers (ADMM) to incorporate the new prior and try differentforms of ADMM to avoid extra computation caused by the inner loop of ADMMsteps. We also study the relation between the dynamics of gradient descent, andthe overfitting phenomenon. The numerical results show the hybrid priors playan important role in preventing overfitting. Besides, we try to fit the imagealong some directions and find this method can reduce overfitting when thenoise level is large. When the noise level is small, it does not considerablyreduce the overfitting problem.',\n",
       "  'categories': ['eess.IV', 'cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2011.01748v1'},\n",
       " 667: {'ID': 667,\n",
       "  'title': 'Building Task-Oriented Visual Dialog Systems Through Alternative  Optimization Between Dialog Policy and Language Generation',\n",
       "  'authors': ['Mingyang Zhou', 'Josh Arnold', 'Zhou Yu'],\n",
       "  'published': '2019-09-06T01:28:34Z',\n",
       "  'updated': '2019-10-28T19:45:17Z',\n",
       "  'abstract': 'Reinforcement learning (RL) is an effective approach to learn an optimaldialog policy for task-oriented visual dialog systems. A common practice is toapply RL on a neural sequence-to-sequence (seq2seq) framework with the actionspace being the output vocabulary in the decoder. However, it is difficult todesign a reward function that can achieve a balance between learning aneffective policy and generating a natural dialog response. This paper proposesa novel framework that alternatively trains a RL policy for image guessing anda supervised seq2seq model to improve dialog generation quality. We evaluateour framework on the GuessWhich task and the framework achieves thestate-of-the-art performance in both task completion and dialog quality.',\n",
       "  'categories': ['cs.CL', 'cs.AI', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1909.05365v2'},\n",
       " 668: {'ID': 668,\n",
       "  'title': 'Image Captioning with Compositional Neural Module Networks',\n",
       "  'authors': ['Junjiao Tian', 'Jean Oh'],\n",
       "  'published': '2020-07-10T20:58:04Z',\n",
       "  'updated': '2020-07-10T20:58:04Z',\n",
       "  'abstract': 'In image captioning where fluency is an important factor in evaluation, e.g.,$n$-gram metrics, sequential models are commonly used; however, sequentialmodels generally result in overgeneralized expressions that lack the detailsthat may be present in an input image. Inspired by the idea of thecompositional neural module networks in the visual question answering task, weintroduce a hierarchical framework for image captioning that explores bothcompositionality and sequentiality of natural language. Our algorithm learns tocompose a detail-rich sentence by selectively attending to different modulescorresponding to unique aspects of each object detected in an input image toinclude specific descriptions such as counts and color. In a set of experimentson the MSCOCO dataset, the proposed model outperforms a state-of-the art modelacross multiple evaluation metrics, more importantly, presenting visuallyinterpretable results. Furthermore, the breakdown of subcategories $f$-scoresof the SPICE metric and human evaluation on Amazon Mechanical Turk show thatour compositional module networks effectively generate accurate and detailedcaptions.',\n",
       "  'categories': ['cs.CV', 'cs.CL', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2007.05608v1'},\n",
       " 669: {'ID': 669,\n",
       "  'title': 'View-volume Network for Semantic Scene Completion from a Single Depth  Image',\n",
       "  'authors': ['Yu-Xiao Guo', 'Xin Tong'],\n",
       "  'published': '2018-06-14T04:42:05Z',\n",
       "  'updated': '2018-06-14T04:42:05Z',\n",
       "  'abstract': 'We introduce a View-Volume convolutional neural network (VVNet) for inferringthe occupancy and semantic labels of a volumetric 3D scene from a single depthimage. The VVNet concatenates a 2D view CNN and a 3D volume CNN with adifferentiable projection layer. Given a single RGBD image, our method extractsthe detailed geometric features from the input depth image with a 2D view CNNand then projects the features into a 3D volume according to the input depthmap via a projection layer. After that, we learn the 3D context information ofthe scene with a 3D volume CNN for computing the result volumetric occupancyand semantic labels. With combined 2D and 3D representations, the VVNetefficiently reduces the computational cost, enables feature extraction frommulti-channel high resolution inputs, and thus significantly improves theresult accuracy. We validate our method and demonstrate its efficiency andeffectiveness on both synthetic SUNCG and real NYU dataset.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1806.05361v1'},\n",
       " 670: {'ID': 670,\n",
       "  'title': 'Ground-aware Monocular 3D Object Detection for Autonomous Driving',\n",
       "  'authors': ['Yuxuan Liu', 'Ming Liu', 'Yuan Yixuan'],\n",
       "  'published': '2021-02-01T08:18:24Z',\n",
       "  'updated': '2021-02-01T08:18:24Z',\n",
       "  'abstract': 'Estimating the 3D position and orientation of objects in the environment witha single RGB camera is a critical and challenging task for low-cost urbanautonomous driving and mobile robots. Most of the existing algorithms are basedon the geometric constraints in 2D-3D correspondence, which stems from generic6D object pose estimation. We first identify how the ground plane providesadditional clues in depth reasoning in 3D detection in driving scenes. Based onthis observation, we then improve the processing of 3D anchors and introduce anovel neural network module to fully utilize such application-specific priorsin the framework of deep learning. Finally, we introduce an efficient neuralnetwork embedded with the proposed module for 3D object detection. We furtherverify the power of the proposed module with a neural network designed formonocular depth prediction. The two proposed networks achieve state-of-the-artperformances on the KITTI 3D object detection and depth prediction benchmarks,respectively. The code will be published inhttps://www.github.com/Owen-Liuyuxuan/visualDet3D',\n",
       "  'categories': ['cs.CV', 'cs.RO'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2102.00690v1'},\n",
       " 671: {'ID': 671,\n",
       "  'title': 'Small, Sparse, but Substantial: Techniques for Segmenting Small  Agricultural Fields Using Sparse Ground Data',\n",
       "  'authors': ['Jagabondhu Hazra',\n",
       "   'Smit Marvaniya',\n",
       "   'Nitin Gupta',\n",
       "   'Umamaheswari Devi',\n",
       "   'Shashank Mujumdar'],\n",
       "  'published': '2020-05-05T05:26:19Z',\n",
       "  'updated': '2020-05-05T05:26:19Z',\n",
       "  'abstract': \"The recent thrust on digital agriculture (DA) has renewed significantresearch interest in the automated delineation of agricultural fields. Mostprior work addressing this problem have focused on detecting medium to largefields, while there is strong evidence that around 40\\\\% of the fieldsworld-wide and 70% of the fields in Asia and Africa are small. The lack ofadequate labeled images for small fields, huge variations in their color,texture, and shape, and faint boundary lines separating them make it difficultto develop an end-to-end learning model for detecting such fields. Hence, inthis paper, we present a multi-stage approach that uses a combination ofmachine learning and image processing techniques. In the first stage, weleverage state-of-the-art edge detection algorithms such as holistically-nestededge detection (HED) to extract first-level contours and polygons. In thesecond stage, we propose image-processing techniques to identify polygons thatare non-fields, over-segmentations, or noise and eliminate them. The next stagetackles under-segmentations using a combination of a novel ``cut-point'' basedtechnique and localized second-level edge detection to obtain individualparcels. Since a few small, non-cropped but vegetated or constructed pocketscan be interspersed in areas that are predominantly croplands, in the finalstage, we train a classifier for identifying each parcel from the previousstage as an agricultural field or not. In an evaluation using high-resolutionimagery, we show that our approach has a high F-Score of 0.84 in areas withlarge fields and reasonable accuracy with an F-Score of 0.73 in areas withsmall fields, which is encouraging.\",\n",
       "  'categories': ['cs.CV', 'eess.IV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2005.01947v1'},\n",
       " 672: {'ID': 672,\n",
       "  'title': 'Universal Adversarial Perturbations for CNN Classifiers in EEG-Based  BCIs',\n",
       "  'authors': ['Weili Fang',\n",
       "   'Zihan Liu',\n",
       "   'Hanbin Luo',\n",
       "   'Dongrui Wu',\n",
       "   'Lubin Meng',\n",
       "   'Xiao Zhang',\n",
       "   'Lieyun Ding'],\n",
       "  'published': '2019-12-03T03:00:08Z',\n",
       "  'updated': '2021-01-30T20:34:58Z',\n",
       "  'abstract': 'Multiple convolutional neural network (CNN) classifiers have been proposedfor electroencephalogram (EEG) based brain-computer interfaces (BCIs). However,CNN models have been found vulnerable to universal adversarial perturbations(UAPs), which are small and example-independent, yet powerful enough to degradethe performance of a CNN model, when added to a benign example. This paperproposes a novel total loss minimization (TLM) approach to generate UAPs forEEG-based BCIs. Experimental results demonstrated the effectiveness of TLM onthree popular CNN classifiers for both target and non-target attacks. We alsoverified the transferability of UAPs in EEG-based BCI systems. To ourknowledge, this is the first study on UAPs of CNN classifiers in EEG-basedBCIs, and also the first study on optimization based UAPs for target attacks.UAPs are easy to construct, and can attack BCIs in real-time, exposing apotentially critical security concern of BCIs.',\n",
       "  'categories': ['cs.LG', 'cs.HC', 'eess.SP'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1912.01171v3'},\n",
       " 673: {'ID': 673,\n",
       "  'title': 'Prefix Block-Interchanges on Binary and Ternary Strings',\n",
       "  'authors': ['M. Sohel Rahman', 'Md. Khaledur Rahman'],\n",
       "  'published': '2019-05-20T01:53:05Z',\n",
       "  'updated': '2019-05-20T01:53:05Z',\n",
       "  'abstract': \"The genome rearrangement problem computes the minimum number of operationsthat are required to sort all elements of a permutation. A block-interchangeoperation exchanges two blocks of a permutation which are not necessarilyadjacent and in a prefix block-interchange, one block is always the prefix ofthat permutation. In this paper, we focus on applying prefix block-interchangeson binary and ternary strings. We present upper bounds to group and sort agiven binary/ternary string. We also provide upper bounds for a differentversion of the block-interchange operation which we refer to as the `restrictedprefix block-interchange'. We observe that our obtained upper bound forrestricted prefix block-interchange operations on binary strings is better thanthat of other genome rearrangement operations to group fully normalized binarystrings. Consequently, we provide a linear-time algorithm to solve the problemof grouping binary normalized strings by restricted prefix block-interchanges.We also provide a polynomial time algorithm to group normalized ternary stringsby prefix block-interchange operations. Finally, we provide a classificationfor ternary strings based on the required number of prefix block-interchangeoperations.\",\n",
       "  'categories': ['cs.DS'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1906.04897v1'},\n",
       " 674: {'ID': 674,\n",
       "  'title': 'Video Object Segmentation Without Temporal Information',\n",
       "  'authors': ['Luc Van Gool',\n",
       "   'Daniel Cremers',\n",
       "   'Yuhua Chen',\n",
       "   'Laura Leal-Taixé',\n",
       "   'Sergi Caelles',\n",
       "   'Jordi Pont-Tuset',\n",
       "   'Kevis-Kokitsi Maninis'],\n",
       "  'published': '2017-09-18T16:28:02Z',\n",
       "  'updated': '2018-05-16T12:16:48Z',\n",
       "  'abstract': 'Video Object Segmentation, and video processing in general, has beenhistorically dominated by methods that rely on the temporal consistency andredundancy in consecutive video frames. When the temporal smoothness issuddenly broken, such as when an object is occluded, or some frames are missingin a sequence, the result of these methods can deteriorate significantly orthey may not even produce any result at all. This paper explores the orthogonalapproach of processing each frame independently, i.e disregarding the temporalinformation. In particular, it tackles the task of semi-supervised video objectsegmentation: the separation of an object from the background in a video, givenits mask in the first frame. We present Semantic One-Shot Video ObjectSegmentation (OSVOS-S), based on a fully-convolutional neural networkarchitecture that is able to successively transfer generic semanticinformation, learned on ImageNet, to the task of foreground segmentation, andfinally to learning the appearance of a single annotated object of the testsequence (hence one shot). We show that instance level semantic information,when combined effectively, can dramatically improve the results of our previousmethod, OSVOS. We perform experiments on two recent video segmentationdatabases, which show that OSVOS-S is both the fastest and most accurate methodin the state of the art.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1709.06031v2'},\n",
       " 675: {'ID': 675,\n",
       "  'title': 'Regional Homogeneity: Towards Learning Transferable Universal  Adversarial Perturbations Against Defenses',\n",
       "  'authors': ['Zhenyu Liao',\n",
       "   'Alan L. Yuille',\n",
       "   'Song Bai',\n",
       "   'Xiaohui Shen',\n",
       "   'Yingwei Li',\n",
       "   'Cihang Xie'],\n",
       "  'published': '2019-04-01T17:31:02Z',\n",
       "  'updated': '2020-07-31T01:42:37Z',\n",
       "  'abstract': 'This paper focuses on learning transferable adversarial examples specificallyagainst defense models (models to defense adversarial attacks). In particular,we show that a simple universal perturbation can fool a series ofstate-of-the-art defenses.  Adversarial examples generated by existing attacks are generally hard totransfer to defense models. We observe the property of regional homogeneity inadversarial perturbations and suggest that the defenses are less robust toregionally homogeneous perturbations. Therefore, we propose an effectivetransforming paradigm and a customized gradient transformer module to transformexisting perturbations into regionally homogeneous ones. Without explicitlyforcing the perturbations to be universal, we observe that a well-trainedgradient transformer module tends to output input-independent gradients (henceuniversal) benefiting from the under-fitting phenomenon. Thorough experimentsdemonstrate that our work significantly outperforms the prior art attackingalgorithms (either image-dependent or universal ones) by an average improvementof 14.0% when attacking 9 defenses in the transfer-based attack setting. Inaddition to the cross-model transferability, we also verify that regionallyhomogeneous perturbations can well transfer across different vision tasks(attacking with the semantic segmentation task and testing on the objectdetection task). The code is available here:https://github.com/LiYingwei/Regional-Homogeneity.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1904.00979v2'},\n",
       " 676: {'ID': 676,\n",
       "  'title': 'Universal Adversarial Perturbations for Speech Recognition Systems',\n",
       "  'authors': ['Shehzeen Hussain',\n",
       "   'Paarth Neekhara',\n",
       "   'Julian McAuley',\n",
       "   'Prakhar Pandey',\n",
       "   'Shlomo Dubnov',\n",
       "   'Farinaz Koushanfar'],\n",
       "  'published': '2019-05-09T19:35:30Z',\n",
       "  'updated': '2019-08-15T05:15:43Z',\n",
       "  'abstract': 'In this work, we demonstrate the existence of universal adversarial audioperturbations that cause mis-transcription of audio signals by automatic speechrecognition (ASR) systems. We propose an algorithm to find a singlequasi-imperceptible perturbation, which when added to any arbitrary speechsignal, will most likely fool the victim speech recognition model. Ourexperiments demonstrate the application of our proposed technique by craftingaudio-agnostic universal perturbations for the state-of-the-art ASR system --Mozilla DeepSpeech. Additionally, we show that such perturbations generalize toa significant extent across models that are not available during training, byperforming a transferability test on a WaveNet based ASR system.',\n",
       "  'categories': ['cs.LG', 'cs.SD', 'eess.AS', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1905.03828v2'},\n",
       " 677: {'ID': 677,\n",
       "  'title': 'Fully automatic detection and segmentation of abdominal aortic thrombus  in post-operative CTA images using deep convolutional neural networks',\n",
       "  'authors': ['Ainhoa García-Familiar',\n",
       "   'Luis Kabongo',\n",
       "   'Nerea Lete',\n",
       "   'Iván Macía',\n",
       "   'Karen López-Linares',\n",
       "   'Mario Ceresa',\n",
       "   'Gregory Maclair',\n",
       "   'Miguel A. González Ballester',\n",
       "   'Nerea Aranjuelo'],\n",
       "  'published': '2018-04-01T15:26:32Z',\n",
       "  'updated': '2018-04-01T15:26:32Z',\n",
       "  'abstract': 'Computerized Tomography Angiography (CTA) based follow-up of Abdominal AorticAneurysms (AAA) treated with Endovascular Aneurysm Repair (EVAR) is essentialto evaluate the progress of the patient and detect complications. In thiscontext, accurate quantification of post-operative thrombus volume is required.However, a proper evaluation is hindered by the lack of automatic, robust andreproducible thrombus segmentation algorithms. We propose a new fully automaticapproach based on Deep Convolutional Neural Networks (DCNN) for robust andreproducible thrombus region of interest detection and subsequent fine thrombussegmentation. The DetecNet detection network is adapted to perform region ofinterest extraction from a complete CTA and a new segmentation networkarchitecture, based on Fully Convolutional Networks and a Holistically-NestedEdge Detection Network, is presented. These networks are trained, validated andtested in 13 post-operative CTA volumes of different patients using a 4-foldcross-validation approach to provide more robustness to the results. Ourpipeline achieves a Dice score of more than 82% for post-operative thrombussegmentation and provides a mean relative volume difference between groundtruth and automatic segmentation that lays within the experienced humanobserver variance without the need of human intervention in most common cases.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1804.00304v1'},\n",
       " 678: {'ID': 678,\n",
       "  'title': 'Continuous Trade-off Optimization between Fast and Accurate Deep Face  Detectors',\n",
       "  'authors': ['Petru Soviany', 'Radu Tudor Ionescu'],\n",
       "  'published': '2018-11-27T12:16:22Z',\n",
       "  'updated': '2018-11-27T12:16:22Z',\n",
       "  'abstract': 'Although deep neural networks offer better face detection results thanshallow or handcrafted models, their complex architectures come with highercomputational requirements and slower inference speeds than shallow neuralnetworks. In this context, we study five straightforward approaches to achievean optimal trade-off between accuracy and speed in face detection. All theapproaches are based on separating the test images in two batches, an easybatch that is fed to a faster face detector and a difficult batch that is fedto a more accurate yet slower detector. We conduct experiments on the AFW andthe FDDB data sets, using MobileNet-SSD as the fast face detector and S3FD(Single Shot Scale-invariant Face Detector) as the accurate face detector, bothmodels being pre-trained on the WIDER FACE data set. Our experiments show thatthe proposed difficulty metrics compare favorably to a random split of theimages.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1811.11582v1'},\n",
       " 679: {'ID': 679,\n",
       "  'title': 'Image-to-Image Retrieval by Learning Similarity between Scene Graphs',\n",
       "  'authors': ['Jonghun Park',\n",
       "   'Eun-Sol Kim',\n",
       "   'Woo Young Kang',\n",
       "   'SeongEun Lee',\n",
       "   'Sangwoong Yoon',\n",
       "   'Changjin Han',\n",
       "   'Sungwook Jeon'],\n",
       "  'published': '2020-12-29T10:45:20Z',\n",
       "  'updated': '2020-12-29T10:45:20Z',\n",
       "  'abstract': 'As a scene graph compactly summarizes the high-level content of an image in astructured and symbolic manner, the similarity between scene graphs of twoimages reflects the relevance of their contents. Based on this idea, we proposea novel approach for image-to-image retrieval using scene graph similaritymeasured by graph neural networks. In our approach, graph neural networks aretrained to predict the proxy image relevance measure, computed fromhuman-annotated captions using a pre-trained sentence similarity model. Wecollect and publish the dataset for image relevance measured by humanannotators to evaluate retrieval algorithms. The collected dataset shows thatour method agrees well with the human perception of image similarity than othercompetitive baselines.',\n",
       "  'categories': ['cs.CV', 'cs.IR', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2012.14700v1'},\n",
       " 680: {'ID': 680,\n",
       "  'title': 'Introduction to Lie groups, isometric and adjoint actions and some  generalizations',\n",
       "  'authors': ['Marcos M. Alexandrino', 'Renato G. Bettiol'],\n",
       "  'published': '2009-01-16T00:40:38Z',\n",
       "  'updated': '2010-08-27T20:43:41Z',\n",
       "  'abstract': 'The main purpose of these lecture notes is to provide a concise introductionto Lie groups, Lie algebras, and isometric and adjoint actions, aiming mostlyat advanced undergraduate and graduate students. In addition, the connectionbetween such classic theories and the research area of the first author isexplored. Namely, generalizations to isoparametric submanifolds, polar actionsand singular Riemannian foliations with sections (s.r.f.s.) are mentioned. Thefirst chapters cover basic concepts, giving results on adjoint representation,closed subgroups, bi-invariant metrics, Killing forms and splitting in simpleideals. In the following chapters, proper and isometric actions are recalledtogether with adjoint action and foliations, mostly concerning the Weyl group,normal slices and Dynkin diagrams. A special focus is given to maximal tori androots of compact Lie groups, exploring its connection with isoparametricsubmanifolds and polar actions. Furthermore, in the last chapter, a survey onrecent research results on s.r.f.s. is given.  In this revised version, more details about fiber bundles, proper andisometric actions are explored, and further exercises and examples were added.It also features new sections with examples of singular Riemannian foliationsconstructed with surgery and suspension of homomorphisms. This is still apreliminary version and we expect to improve it in the future. We would begrateful for any kind of suggestions.',\n",
       "  'categories': ['math.DG', '17Bxx, 22C05, 22Exx, 22F05, 53C12, 57R30'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/0901.2374v3'},\n",
       " 681: {'ID': 681,\n",
       "  'title': 'Robust Vision Challenge 2020 -- 1st Place Report for Panoptic  Segmentation',\n",
       "  'authors': ['Abhinav Valada', 'Rohit Mohan'],\n",
       "  'published': '2020-08-23T21:41:43Z',\n",
       "  'updated': '2020-08-23T21:41:43Z',\n",
       "  'abstract': 'In this technical report, we present key details of our winning panopticsegmentation architecture EffPS_b1bs4_RVC. Our network is a lightweight versionof our state-of-the-art EfficientPS architecture that consists of our proposedshared backbone with a modified EfficientNet-B5 model as the encoder, followedby the 2-way FPN to learn semantically rich multi-scale features. It consistsof two task-specific heads, a modified Mask R-CNN instance head and our novelsemantic segmentation head that processes features of different scales withspecialized modules for coherent feature refinement. Finally, our proposedpanoptic fusion module adaptively fuses logits from each of the heads to yieldthe panoptic segmentation output. The Robust Vision Challenge 2020 benchmarkingresults show that our model is ranked #1 on Microsoft COCO, VIPER and WildDash,and is ranked #2 on Cityscapes and Mapillary Vistas, thereby achieving theoverall rank #1 for the panoptic segmentation task.',\n",
       "  'categories': ['cs.CV', 'cs.LG', 'cs.RO'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2008.10112v1'},\n",
       " 682: {'ID': 682,\n",
       "  'title': 'Deep Virtual Stereo Odometry: Leveraging Deep Depth Prediction for  Monocular Direct Sparse Odometry',\n",
       "  'authors': ['Rui Wang', 'Jörg Stückler', 'Daniel Cremers', 'Nan Yang'],\n",
       "  'published': '2018-07-06T21:14:31Z',\n",
       "  'updated': '2018-07-25T16:24:02Z',\n",
       "  'abstract': 'Monocular visual odometry approaches that purely rely on geometric cues areprone to scale drift and require sufficient motion parallax in successiveframes for motion estimation and 3D reconstruction. In this paper, we proposeto leverage deep monocular depth prediction to overcome limitations ofgeometry-based monocular visual odometry. To this end, we incorporate deepdepth predictions into Direct Sparse Odometry (DSO) as direct virtual stereomeasurements. For depth prediction, we design a novel deep network that refinespredicted depth from a single image in a two-stage process. We train ournetwork in a semi-supervised way on photoconsistency in stereo images and onconsistency with accurate sparse depth reconstructions from Stereo DSO. Ourdeep predictions excel state-of-the-art approaches for monocular depth on theKITTI benchmark. Moreover, our Deep Virtual Stereo Odometry clearly exceedsprevious monocular and deep learning based methods in accuracy. It evenachieves comparable performance to the state-of-the-art stereo methods, whileonly relying on a single camera.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1807.02570v2'},\n",
       " 683: {'ID': 683,\n",
       "  'title': 'Learning Adaptive Embedding Considering Incremental Class',\n",
       "  'authors': ['Yang Yang',\n",
       "   'Zhen-Qiang Sun',\n",
       "   'Jian Yang',\n",
       "   'HengShu Zhu',\n",
       "   'Yanjie Fu',\n",
       "   'Hui Xiong'],\n",
       "  'published': '2020-08-31T04:11:24Z',\n",
       "  'updated': '2020-08-31T04:11:24Z',\n",
       "  'abstract': 'Class-Incremental Learning (CIL) aims to train a reliable model with thestreaming data, which emerges unknown classes sequentially. Different fromtraditional closed set learning, CIL has two main challenges: 1) Novel classdetection. The initial training data only contains incomplete classes, andstreaming test data will accept unknown classes. Therefore, the model needs tonot only accurately classify known classes, but also effectively detect unknownclasses; 2) Model expansion. After the novel classes are detected, the modelneeds to be updated without re-training using entire previous data. However,traditional CIL methods have not fully considered these two challenges, first,they are always restricted to single novel class detection each phase andembedding confusion caused by unknown classes. Besides, they also ignore thecatastrophic forgetting of known categories in model update. To this end, wepropose a Class-Incremental Learning without Forgetting (CILF) framework, whichaims to learn adaptive embedding for processing novel class detection and modelupdate in a unified framework. In detail, CILF designs to regularizeclassification with decoupled prototype based loss, which can improve theintra-class and inter-class structure significantly, and acquire a compactembedding representation for novel class detection in result. Then, CILFemploys a learnable curriculum clustering operator to estimate the number ofsemantic clusters via fine-tuning the learned network, in which curriculumoperator can adaptively learn the embedding in self-taught form. Therefore,CILF can detect multiple novel classes and mitigate the embedding confusionproblem. Last, with the labeled streaming test data, CILF can update thenetwork with robust regularization to mitigate the catastrophic forgetting.Consequently, CILF is able to iteratively perform novel class detection andmodel update.',\n",
       "  'categories': ['cs.LG', 'cs.AI', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2008.13351v1'},\n",
       " 684: {'ID': 684,\n",
       "  'title': 'Stereo DSO: Large-Scale Direct Sparse Visual Odometry with Stereo  Cameras',\n",
       "  'authors': ['Rui Wang', 'Daniel Cremers', 'Martin Schwörer'],\n",
       "  'published': '2017-08-25T20:50:54Z',\n",
       "  'updated': '2017-08-25T20:50:54Z',\n",
       "  'abstract': 'We propose Stereo Direct Sparse Odometry (Stereo DSO) as a novel method forhighly accurate real-time visual odometry estimation of large-scaleenvironments from stereo cameras. It jointly optimizes for all the modelparameters within the active window, including the intrinsic/extrinsic cameraparameters of all keyframes and the depth values of all selected pixels. Inparticular, we propose a novel approach to integrate constraints from staticstereo into the bundle adjustment pipeline of temporal multi-view stereo.Real-time optimization is realized by sampling pixels uniformly from imageregions with sufficient intensity gradient. Fixed-baseline stereo resolvesscale drift. It also reduces the sensitivities to large optical flow and torolling shutter effect which are known shortcomings of direct image alignmentmethods. Quantitative evaluation demonstrates that the proposed Stereo DSOoutperforms existing state-of-the-art visual odometry methods both in terms oftracking accuracy and robustness. Moreover, our method delivers a more precisemetric 3D reconstruction than previous dense/semi-dense direct approaches whileproviding a higher reconstruction density than feature-based methods.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1708.07878v1'},\n",
       " 685: {'ID': 685,\n",
       "  'title': 'A single image deep learning approach to restoration of corrupted remote  sensing products',\n",
       "  'authors': ['Ivan V. Oseledets', 'Anna Petrovskaia', 'Raghavendra B. Jana'],\n",
       "  'published': '2020-04-08T19:11:32Z',\n",
       "  'updated': '2020-04-08T19:11:32Z',\n",
       "  'abstract': 'Remote sensing images are used for a variety of analyses, from agriculturalmonitoring, to disaster relief, to resource planning, among others. The imagescan be corrupted due to a number of reasons, including instrument errors andnatural obstacles such as clouds. We present here a novel approach forreconstruction of missing information in such cases using only the corruptedimage as the input. The Deep Image Prior methodology eliminates the need for apre-trained network or an image database. It is shown that the approach easilybeats the performance of traditional single-image methods.',\n",
       "  'categories': ['eess.IV', 'cs.CV', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2004.04209v1'},\n",
       " 686: {'ID': 686,\n",
       "  'title': 'Self-Attention Based Context-Aware 3D Object Detection',\n",
       "  'authors': ['Prarthana Bhattacharyya',\n",
       "   'Krzysztof Czarnecki',\n",
       "   'Chengjie Huang'],\n",
       "  'published': '2021-01-07T18:30:32Z',\n",
       "  'updated': '2021-01-07T18:30:32Z',\n",
       "  'abstract': 'Most existing point-cloud based 3D object detectors use convolution-likeoperators to process information in a local neighbourhood with fixed-weightkernels and aggregate global context hierarchically. However, recent work onnon-local neural networks and self-attention for 2D vision has shown thatexplicitly modeling global context and long-range interactions betweenpositions can lead to more robust and competitive models. In this paper, weexplore two variants of self-attention for contextual modeling in 3D objectdetection by augmenting convolutional features with self-attention features. Wefirst incorporate the pairwise self-attention mechanism into the currentstate-of-the-art BEV, voxel and point-based detectors and show consistentimprovement over strong baseline models while simultaneously significantlyreducing their parameter footprint and computational cost. We also propose aself-attention variant that samples a subset of the most representativefeatures by learning deformations over randomly sampled locations. This notonly allows us to scale explicit global contextual modeling to largerpoint-clouds, but also leads to more discriminative and informative featuredescriptors. Our method can be flexibly applied to most state-of-the-artdetectors with increased accuracy and parameter and compute efficiency. Weachieve new state-of-the-art detection performance on KITTI and nuScenesdatasets. Code is available at\\\\url{https://github.com/AutoVision-cloud/SA-Det3D}.',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2101.02672v1'},\n",
       " 687: {'ID': 687,\n",
       "  'title': 'Adversarial Semantic Scene Completion from a Single Depth Image',\n",
       "  'authors': ['David Joseph Tan',\n",
       "   'Nassir Navab',\n",
       "   'Yida Wang',\n",
       "   'Federico Tombari'],\n",
       "  'published': '2018-10-25T14:43:12Z',\n",
       "  'updated': '2018-10-25T14:43:12Z',\n",
       "  'abstract': 'We propose a method to reconstruct, complete and semantically label a 3Dscene from a single input depth image. We improve the accuracy of the regressedsemantic 3D maps by a novel architecture based on adversarial learning. Inparticular, we suggest using multiple adversarial loss terms that not onlyenforce realistic outputs with respect to the ground truth, but also aneffective embedding of the internal features. This is done by correlating thelatent features of the encoder working on partial 2.5D data with the latentfeatures extracted from a variational 3D auto-encoder trained to reconstructthe complete semantic scene. In addition, differently from other approachesthat operate entirely through 3D convolutions, at test time we retain theoriginal 2.5D structure of the input during downsampling to improve theeffectiveness of the internal representation of our model. We test our approachon the main benchmark datasets for semantic scene completion to qualitativelyand quantitatively assess the effectiveness of our proposal.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1810.10901v1'},\n",
       " 688: {'ID': 688,\n",
       "  'title': 'Densely Connected Convolutional Networks for Speech Recognition',\n",
       "  'authors': ['Ngoc Thang Vu', 'Chia Yu Li'],\n",
       "  'published': '2018-08-10T14:54:10Z',\n",
       "  'updated': '2018-08-10T14:54:10Z',\n",
       "  'abstract': 'This paper presents our latest investigation on Densely ConnectedConvolutional Networks (DenseNets) for acoustic modelling (AM) in automaticspeech recognition. DenseN-ets are very deep, compact convolutional neuralnetworks, which have demonstrated incredible improvements over thestate-of-the-art results on several data sets in computer vision. Ourexperimental results show that DenseNet can be used for AM significantlyoutperforming other neural-based models such as DNNs, CNNs, VGGs. Furthermore,results on Wall Street Journal revealed that with only a half of the trainingdata DenseNet was able to outperform other models trained with the full dataset by a large margin.',\n",
       "  'categories': ['cs.CL'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1808.03570v1'},\n",
       " 689: {'ID': 689,\n",
       "  'title': 'ADMM-DIPTV: combining Total Variation and Deep Image Prior for image  restoration',\n",
       "  'authors': ['Andrea Sebastiani',\n",
       "   'Maria Colomba Comes',\n",
       "   'Pasquale Cascarano'],\n",
       "  'published': '2020-09-23T21:19:55Z',\n",
       "  'updated': '2020-09-23T21:19:55Z',\n",
       "  'abstract': 'In the last decades, unsupervised deep learning based methods have caughtresearchers attention, since in many applications collecting a great amount oftraining examples is not always feasible. Moreover, the construction of a goodtraining set is time consuming and hard because the selected data have to beenough representative for the task. In this paper, we mainly focus on the DeepImage Prior (DIP) framework powered by adding the Total Variation regularizerwhich promotes gradient-sparsity of the solution. Differently from otherexisting approaches, we solve the arising minimization problem by using thewell known Alternating Direction Method of Multipliers (ADMM) framework,decoupling the contribution of the DIP $L_{2}$-norm and Total Variation terms.The promising performances of the proposed approach, in terms of PSNR and SSIMvalues, are addressed by means of experiments for different image restorationtasks on synthetic as well as on real data.',\n",
       "  'categories': ['eess.IV',\n",
       "   'cs.NA',\n",
       "   'math.NA',\n",
       "   '65F22, 65K10',\n",
       "   'G.1.6; G.1.10; I.4.3; I.4.4; I.4.5; I.4.6; I.2.6; I.2.0; I.4.3'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2009.11380v1'},\n",
       " 690: {'ID': 690,\n",
       "  'title': 'Deformable Medical Image Registration Using a Randomly-Initialized CNN  as Regularization Prior',\n",
       "  'authors': ['Tobias Ortmaier', 'Max-Heinrich Laves', 'Sontje Ihler'],\n",
       "  'published': '2019-08-02T10:19:44Z',\n",
       "  'updated': '2019-08-02T10:19:44Z',\n",
       "  'abstract': 'We present deformable unsupervised medical image registration using arandomly-initialized deep convolutional neural network (CNN) as regularizationprior. Conventional registration methods predict a transformation by minimizingdissimilarities between an image pair. The minimization is usually regularizedwith manually engineered priors, which limits the potential of theregistration. By learning transformation priors from a large dataset, CNNs haveachieved great success in deformable registration. However, learned methods arerestricted to domain-specific data and the required amounts of medical data aredifficult to obtain. Our approach uses the idea of deep image priors to combineconvolutional networks with conventional registration methods based on manuallyengineered priors. The proposed method is applied to brain MRI scans. We showthat our approach registers image pairs with state-of-the-art accuracy byproviding dense, pixel-wise correspondence maps. It does not rely on priortraining and is therefore not limited to a specific image domain.',\n",
       "  'categories': ['eess.IV', 'cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1908.00788v1'},\n",
       " 691: {'ID': 691,\n",
       "  'title': 'Towards Imperceptible Universal Attacks on Texture Recognition',\n",
       "  'authors': ['Yingpeng Deng', 'Lina J. Karam'],\n",
       "  'published': '2020-11-24T08:33:59Z',\n",
       "  'updated': '2020-11-24T08:33:59Z',\n",
       "  'abstract': \"Although deep neural networks (DNNs) have been shown to be susceptible toimage-agnostic adversarial attacks on natural image classification problems,the effects of such attacks on DNN-based texture recognition have yet to beexplored. As part of our work, we find that limiting the perturbation's $l_p$norm in the spatial domain may not be a suitable way to restrict theperceptibility of universal adversarial perturbations for texture images. Basedon the fact that human perception is affected by local visual frequencycharacteristics, we propose a frequency-tuned universal attack method tocompute universal perturbations in the frequency domain. Our experimentsindicate that our proposed method can produce less perceptible perturbationsyet with a similar or higher white-box fooling rates on various DNN textureclassifiers and texture datasets as compared to existing universal attacktechniques. We also demonstrate that our approach can improve the attackrobustness against defended models as well as the cross-dataset transferabilityfor texture recognition problems.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2011.11957v1'},\n",
       " 692: {'ID': 692,\n",
       "  'title': 'YH Technologies at ActivityNet Challenge 2018',\n",
       "  'authors': ['Ting Yao', 'Xue Li'],\n",
       "  'published': '2018-06-29T07:49:08Z',\n",
       "  'updated': '2018-06-29T07:49:08Z',\n",
       "  'abstract': 'This notebook paper presents an overview and comparative analysis of oursystems designed for the following five tasks in ActivityNet Challenge 2018:temporal action proposals, temporal action localization, dense-captioningevents in videos, trimmed action recognition, and spatio-temporal actionlocalization.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1807.00686v1'},\n",
       " 693: {'ID': 693,\n",
       "  'title': 'Auto-Meta: Automated Gradient Based Meta Learner Search',\n",
       "  'authors': ['Yongseok Choi',\n",
       "   'Dong-Yeon Cho',\n",
       "   'Sungwan Kim',\n",
       "   'Moonsu Cha',\n",
       "   'Jaehong Kim',\n",
       "   'Jiwon Kim',\n",
       "   'Sangyeul Lee',\n",
       "   'Youngduck Choi',\n",
       "   'Jung Kwon Lee'],\n",
       "  'published': '2018-06-11T04:28:02Z',\n",
       "  'updated': '2018-12-10T19:02:53Z',\n",
       "  'abstract': 'Fully automating machine learning pipelines is one of the key challenges ofcurrent artificial intelligence research, since practical machine learningoften requires costly and time-consuming human-powered processes such as modeldesign, algorithm development, and hyperparameter tuning. In this paper, weverify that automated architecture search synergizes with the effect ofgradient-based meta learning. We adopt the progressive neural architecturesearch \\\\cite{liu:pnas_google:DBLP:journals/corr/abs-1712-00559} to find optimalarchitectures for meta-learners. The gradient based meta-learner whosearchitecture was automatically found achieved state-of-the-art results on the5-shot 5-way Mini-ImageNet classification problem with $74.65\\\\%$ accuracy,which is $11.54\\\\%$ improvement over the result obtained by the firstgradient-based meta-learner called MAML\\\\cite{finn:maml:DBLP:conf/icml/FinnAL17}. To our best knowledge, this work isthe first successful neural architecture search implementation in the contextof meta learning.',\n",
       "  'categories': ['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1806.06927v2'},\n",
       " 694: {'ID': 694,\n",
       "  'title': 'Improved Distances to Type Ia Supernovae with Two Spectroscopic  Subclasses',\n",
       "  'authors': ['B. Macomber',\n",
       "   'J. M. Silverman',\n",
       "   'D. S. Wong',\n",
       "   'A. V. Filippenko',\n",
       "   'R. Chornock',\n",
       "   'Xiaofeng Wang',\n",
       "   'F. J. D. Serduke',\n",
       "   'W. Li',\n",
       "   'L. Wang',\n",
       "   'R. J. Foley',\n",
       "   'M. Ganeshalingam',\n",
       "   'E. L. Gates',\n",
       "   'T. N. Steele'],\n",
       "  'published': '2009-06-09T00:15:18Z',\n",
       "  'updated': '2009-06-09T00:15:18Z',\n",
       "  'abstract': 'We study the observables of 158 relatively normal Type Ia supernovae (SNe Ia)by dividing them into two groups in terms of the expansion velocity inferredfrom the absorption minimum of the Si II 6355 line in their spectra near B-bandmaximum brightness. One group (\"Normal\") consists of normal SNe Ia populating anarrow strip in the Si II velocity distribution, with an average expansionvelocity v=10,600+/-400 km/s near B maximum; the other group (\"HV\") consists ofobjects with higher velocities, v &gt; 11,800 km/s. Compared with the Normalgroup, the HV one shows a narrower distribution in both the peak luminosity andthe luminosity decline rate dm_{15}. In particular, their B-V colors at maximumbrightness are found to be on average redder by ~0.1, suggesting that theyeither are associated with dusty environments or have intrinsically red B-Vcolors. The HV SNe Ia are also found to prefer a lower extinction ratio Rv~1.6(versus ~2.4 for the Normal ones). Applying such an absorption-correctiondichotomy to SNe Ia of these two groups remarkably reduces the dispersion intheir peak luminosity from 0.178 mag to only 0.125 mag.',\n",
       "  'categories': ['astro-ph.CO', 'astro-ph.HE'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/0906.1616v1'},\n",
       " 695: {'ID': 695,\n",
       "  'title': 'Disentanglement of Color and Shape Representations for Continual  Learning',\n",
       "  'authors': ['Marc Masana', 'Joost Van de Weijer', 'David Berga'],\n",
       "  'published': '2020-07-13T13:05:45Z',\n",
       "  'updated': '2020-07-13T13:05:45Z',\n",
       "  'abstract': 'We hypothesize that disentangled feature representations suffer less fromcatastrophic forgetting. As a case study we perform explicit disentanglement ofcolor and shape, by adjusting the network architecture. We testedclassification accuracy and forgetting in a task-incremental setting withOxford-102 Flowers dataset. We combine our method with Elastic WeightConsolidation, Learning without Forgetting, Synaptic Intelligence and MemoryAware Synapses, and show that feature disentanglement positively impactscontinual learning performance.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2007.06356v1'},\n",
       " 696: {'ID': 696,\n",
       "  'title': '2nd Place Solution to Instance Segmentation of IJCAI 3D AI Challenge  2020',\n",
       "  'authors': ['Zheng Ju', 'Xiang Luo', 'Kai Jiang', 'Xiangyue Liu'],\n",
       "  'published': '2020-10-21T12:53:01Z',\n",
       "  'updated': '2020-10-21T12:53:01Z',\n",
       "  'abstract': 'Compared with MS-COCO, the dataset for the competition has a largerproportion of large objects which area is greater than 96x96 pixels. As gettingfine boundaries is vitally important for large object segmentation, Mask R-CNNwith PointRend is selected as the base segmentation framework to outputhigh-quality object boundaries. Besides, a better engine that integratesResNeSt, FPN and DCNv2, and a range of effective tricks that includingmulti-scale training and test time augmentation are applied to improvesegmentation performance. Our best performance is an ensemble of four models(three PointRend-based models and SOLOv2), which won the 2nd place inIJCAI-PRICAI 3D AI Challenge 2020: Instance Segmentation.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2010.10957v1'},\n",
       " 697: {'ID': 697,\n",
       "  'title': 'The Non-IID Data Quagmire of Decentralized Machine Learning',\n",
       "  'authors': ['Onur Mutlu',\n",
       "   'Kevin Hsieh',\n",
       "   'Phillip B. Gibbons',\n",
       "   'Amar Phanishayee'],\n",
       "  'published': '2019-10-01T03:52:47Z',\n",
       "  'updated': '2020-08-19T00:58:47Z',\n",
       "  'abstract': 'Many large-scale machine learning (ML) applications need to performdecentralized learning over datasets generated at different devices andlocations. Such datasets pose a significant challenge to decentralized learningbecause their different contexts result in significant data distribution skewacross devices/locations. In this paper, we take a step toward betterunderstanding this challenge by presenting a detailed experimental study ofdecentralized DNN training on a common type of data skew: skewed distributionof data labels across devices/locations. Our study shows that: (i) skewed datalabels are a fundamental and pervasive problem for decentralized learning,causing significant accuracy loss across many ML applications, DNN models,training datasets, and decentralized learning algorithms; (ii) the problem isparticularly challenging for DNN models with batch normalization; and (iii) thedegree of data skew is a key determinant of the difficulty of the problem.Based on these findings, we present SkewScout, a system-level approach thatadapts the communication frequency of decentralized learning algorithms to the(skew-induced) accuracy loss between data partitions. We also show that groupnormalization can recover much of the accuracy loss of batch normalization.',\n",
       "  'categories': ['cs.LG', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1910.00189v2'},\n",
       " 698: {'ID': 698,\n",
       "  'title': 'Efficient Progressive Neural Architecture Search',\n",
       "  'authors': ['Stephane Pateux', 'Juan-Manuel Perez-Rua', 'Moez Baccouche'],\n",
       "  'published': '2018-08-01T15:56:08Z',\n",
       "  'updated': '2018-08-01T15:56:08Z',\n",
       "  'abstract': 'This paper addresses the difficult problem of finding an optimal neuralarchitecture design for a given image classification task. We propose a methodthat aggregates two main results of the previous state-of-the-art in neuralarchitecture search. These are, appealing to the strong sampling efficiency ofa search scheme based on sequential model-based optimization (SMBO), andincreasing training efficiency by sharing weights among sampled architectures.Sequential search has previously demonstrated its capabilities to findstate-of-the-art neural architectures for image classification. However, itscomputational cost remains high, even unreachable under modest computationalsettings. Affording SMBO with weight-sharing alleviates this problem. On theother hand, progressive search with SMBO is inherently greedy, as it leveragesa learned surrogate function to predict the validation error of neuralarchitectures. This prediction is directly used to rank the sampled neuralarchitectures. We propose to attenuate the greediness of the original SMBOmethod by relaxing the role of the surrogate function so it predictsarchitecture sampling probability instead. We demonstrate with experiments onthe CIFAR-10 dataset that our method, denominated Efficient progressive neuralarchitecture search (EPNAS), leads to increased search efficiency, whileretaining competitiveness of found architectures.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1808.00391v1'},\n",
       " 699: {'ID': 699,\n",
       "  'title': 'FPGA-based Acceleration System for Visual Tracking',\n",
       "  'authors': ['Chun Yuan', 'Peng Gao', 'Yunxu Sun', 'Ke Song'],\n",
       "  'published': '2018-10-12T05:46:05Z',\n",
       "  'updated': '2018-10-15T00:31:32Z',\n",
       "  'abstract': 'Visual tracking is one of the most important application areas of computervision. At present, most algorithms are mainly implemented on PCs, and it isdifficult to ensure real-time performance when applied in the real scenario. Inorder to improve the tracking speed and reduce the overall power consumption ofvisual tracking, this paper proposes a real-time visual tracking algorithmbased on DSST(Discriminative Scale Space Tracking) approach. We implement ahardware system on Xilinx XC7K325T FPGA platform based on our proposed visualtracking algorithm. Our hardware system can run at more than 153 frames persecond. In order to reduce the resource occupation, our system adopts the batchprocessing method in the feature extraction module. In the filter processingmodule, the FFT IP core is time-division multiplexed. Therefore, our hardwaresystem utilizes LUTs and storage blocks of 33% and 40%, respectively. Testresults show that the proposed visual tracking hardware system has excellentperformance.',\n",
       "  'categories': ['cs.CV', 'eess.IV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1810.05367v2'},\n",
       " 700: {'ID': 700,\n",
       "  'title': 'Intensity-only Mode Decomposition on Multimode Fibers using a Densely  Connected Convolutional Network',\n",
       "  'authors': ['Nektarios Koukourakis',\n",
       "   'Jürgen W. Czarske',\n",
       "   'Qian Zhang',\n",
       "   'Stefan Rothe'],\n",
       "  'published': '2020-08-03T13:33:51Z',\n",
       "  'updated': '2020-08-04T13:27:06Z',\n",
       "  'abstract': 'The use of multimode fibers offers advantages in the field of communicationtechnology in terms of transferable information density and informationsecurity. For applications using physical layer security or mode divisionmultiplexing, the complex transmission matrix must be known. To measure thetransmission matrix, the individual modes of the multimode fiber are excitedsequentially at the input and a mode decomposition is performed at the output.Mode decomposition is usually performed using digital holography, whichrequires the provision of a reference wave and leads to high efforts. Toovercome these drawbacks, a neural network is proposed, which performs modedecomposition with intensity-only camera recordings of the multimode fiberfacet. Due to the high computational complexity of the problem, this approachwas usually limited to a number of 6 modes. In this work, it could be shown forthe first time that by using a DenseNet with 121 layers it is possible to breakthrough the hurdle of 6 modes. The advancement is demonstrated by a modedecomposition with 10 modes experimentally. The training process is based onsynthetic data. The proposed method is quantitatively compared to theconventional approach with digital holography. In addition, it is shown thatthe network can perform mode decomposition on a 55-mode fiber, which alsosupports modes unknown to the neural network. The smart detection using aDenseNet opens new ways for the application of multimode fibers in opticalcommunication networks for physical layer security.',\n",
       "  'categories': ['eess.IV', 'cs.SY', 'eess.SY'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2008.00864v2'},\n",
       " 701: {'ID': 701,\n",
       "  'title': 'Recurrent Instance Segmentation using Sequences of Referring Expressions',\n",
       "  'authors': ['Carina Silberer',\n",
       "   'Carles Ventura',\n",
       "   'Xavier Giro-i-Nieto',\n",
       "   'Ionut-Teodor Sorodoc',\n",
       "   'Gemma Boleda',\n",
       "   'Alba Herrera-Palacio'],\n",
       "  'published': '2019-11-05T21:49:55Z',\n",
       "  'updated': '2019-11-05T21:49:55Z',\n",
       "  'abstract': 'The goal of this work is to segment the objects in an image that are referredto by a sequence of linguistic descriptions (referring expressions). We proposea deep neural network with recurrent layers that output a sequence of binarymasks, one for each referring expression provided by the user. The recurrentlayers in the architecture allow the model to condition each predicted mask onthe previous ones, from a spatial perspective within the same image. Ourmultimodal approach uses off-the-shelf architectures to encode both the imageand the referring expressions. The visual branch provides a tensor of pixelembeddings that are concatenated with the phrase embeddings produced by alanguage encoder. Our experiments on the RefCOCO dataset for still imagesindicate how the proposed architecture successfully exploits the sequences ofreferring expressions to solve a pixel-wise task of instance segmentation.',\n",
       "  'categories': ['cs.CV', 'cs.CL', 'cs.MM'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1911.02103v1'},\n",
       " 702: {'ID': 702,\n",
       "  'title': 'Audio-Based Music Classification with DenseNet And Data Augmentation',\n",
       "  'authors': ['Jing Xiao',\n",
       "   'Jie Wang',\n",
       "   'Bojin Zhuang',\n",
       "   'Shaojun Wang',\n",
       "   'Wenhao Bian',\n",
       "   'Jiankui Yang'],\n",
       "  'published': '2019-06-15T09:16:08Z',\n",
       "  'updated': '2019-06-15T09:16:08Z',\n",
       "  'abstract': 'In recent years, deep learning technique has received intense attention owingto its great success in image recognition. A tendency of adaption of deeplearning in various information processing fields has formed, including musicinformation retrieval (MIR). In this paper, we conduct a comprehensive study onmusic audio classification with improved convolutional neural networks (CNNs).To the best of our knowledge, this the first work to apply Densely ConnectedConvolutional Networks (DenseNet) to music audio tagging, which has beendemonstrated to perform better than Residual neural network (ResNet).Additionally, two specific data augmentation approaches of time overlapping andpitch shifting have been proposed to address the deficiency of labelled data inthe MIR. Moreover, an ensemble learning of stacking is employed based on SVM.We believe that the proposed combination of strong representation of DenseNetand data augmentation can be adapted to other audio processing tasks.',\n",
       "  'categories': ['eess.AS', 'cs.MM', 'cs.SD'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1906.11620v1'},\n",
       " 703: {'ID': 703,\n",
       "  'title': 'Groups normalized by the odd unitary group',\n",
       "  'authors': ['Egor Voronetsky'],\n",
       "  'published': '2019-02-22T19:23:20Z',\n",
       "  'updated': '2019-03-01T15:21:41Z',\n",
       "  'abstract': 'We will give a definition of quadratic forms on bimodules and prove thesandwich classification theorem for subgroups of the general linear group$\\\\mathrm{GL}(P)$ normalized by the elementary unitary group $\\\\mathrm{EU}(P)$ if$P$ is a nondegenerate bimodule with large enough hyperbolic part.',\n",
       "  'categories': ['math.GR', '19G38'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1902.08644v2'},\n",
       " 704: {'ID': 704,\n",
       "  'title': 'Look, Listen and Learn - A Multimodal LSTM for Speaker Identification',\n",
       "  'authors': ['Li Xu',\n",
       "   'Yongtao Hu',\n",
       "   'Yu-Wing Tai',\n",
       "   'Jimmy Ren',\n",
       "   'Qiong Yan',\n",
       "   'Wenxiu Sun',\n",
       "   'Chuan Wang'],\n",
       "  'published': '2016-02-13T18:49:50Z',\n",
       "  'updated': '2016-02-13T18:49:50Z',\n",
       "  'abstract': 'Speaker identification refers to the task of localizing the face of a personwho has the same identity as the ongoing voice in a video. This task not onlyrequires collective perception over both visual and auditory signals, therobustness to handle severe quality degradations and unconstrained contentvariations are also indispensable. In this paper, we describe a novelmultimodal Long Short-Term Memory (LSTM) architecture which seamlessly unifiesboth visual and auditory modalities from the beginning of each sequence input.The key idea is to extend the conventional LSTM by not only sharing weightsacross time steps, but also sharing weights across modalities. We show thatmodeling the temporal dependency across face and voice can significantlyimprove the robustness to content quality degradations and variations. We alsofound that our multimodal LSTM is robustness to distractors, namely thenon-speaking identities. We applied our multimodal LSTM to The Big Bang Theorydataset and showed that our system outperforms the state-of-the-art systems inspeaker identification with lower false alarm rate and higher recognitionaccuracy.',\n",
       "  'categories': ['cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1602.04364v1'},\n",
       " 705: {'ID': 705,\n",
       "  'title': 'Properties of Intra-group Stars and Galaxies in Galaxy Groups: \"Normal\"  versus \"Fossil\" Groups',\n",
       "  'authors': ['Jesper Sommer-Larsen'],\n",
       "  'published': '2005-09-28T15:03:10Z',\n",
       "  'updated': '2006-03-27T18:48:34Z',\n",
       "  'abstract': 'Cosmological LCDM simulations of 12 M_vir~10^14 Msun galaxy groups have beenperformed, invoking star formation, chemical evolution with non-instantaneousrecycling, metallicity dependent radiative cooling, strong star-burst drivengalactic super-winds and effects of a meta-galactic UV field. At z=0,intra-group light (IGL) fractions are found to be 12-45%. Low values refer togroups with only a small difference between the R-band magnitudes of the firstand second ranked group galaxy, large are typical of \"fossil\" groups (FGs). TheIG stars in the 4 FGs are 0.3-0.5 Gyr older than in the 8 nonFGs. For the IGL,B-R=~1.4, in good agreement with observations. For FGs/nonFGs the ironabundance of the IG stars is slightly sub-solar in the central parts (r~100kpc) decreasing to about 40% solar at about 0.5 r_vir The IG stars arealpha-element enhanced with [O/Fe] increasing with r, and an overall[O/Fe]~0.45, indicating predominant SNII enrichment. The velocity distributionsof the IG stars and group galaxies are, at r&gt;~30 kpc, significantly moreradially anisotropic for FGs than for nonFGs. So a characteristic of FGformation, apart from formation time (D\\'Onghia et al.), may be the \"initial\"velocity distribution of the group galaxies. For FGs one can dynamically inferthe (dark matter dominated) mass distribution of the groups all the way tor_vir, from the kinematics of the IG stars or group galaxies. For the nonFGsthis method overestimates the group mass at r&gt;~200 kpc, by up to a factor oftwo at r_vir. This is interpreted as FGs being, in general, more relaxed thannonFGs. Finally, FGs of the above M_vir should host ~500 planetary nebulae atprojected distances between 100 and 1000 kpc from the first ranked galaxy. Allresults appear consistent with the FG formation scenario of D\\'Onghia et al.',\n",
       "  'categories': ['astro-ph'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/astro-ph/0509849v2'},\n",
       " 706: {'ID': 706,\n",
       "  'title': 'Double Targeted Universal Adversarial Perturbations',\n",
       "  'authors': ['Tooba Imtiaz', 'Chaoning Zhang', 'Philipp Benz', 'In So Kweon'],\n",
       "  'published': '2020-10-07T09:08:51Z',\n",
       "  'updated': '2020-10-07T09:08:51Z',\n",
       "  'abstract': 'Despite their impressive performance, deep neural networks (DNNs) are widelyknown to be vulnerable to adversarial attacks, which makes it challenging forthem to be deployed in security-sensitive applications, such as autonomousdriving. Image-dependent perturbations can fool a network for one specificimage, while universal adversarial perturbations are capable of fooling anetwork for samples from all classes without selection. We introduce a doubletargeted universal adversarial perturbations (DT-UAPs) to bridge the gapbetween the instance-discriminative image-dependent perturbations and thegeneric universal perturbations. This universal perturbation attacks onetargeted source class to sink class, while having a limited adversarial effecton other non-targeted source classes, for avoiding raising suspicions.Targeting the source and sink class simultaneously, we term it double targetedattack (DTA). This provides an attacker with the freedom to perform preciseattacks on a DNN model while raising little suspicion. We show theeffectiveness of the proposed DTA algorithm on a wide range of datasets andalso demonstrate its potential as a physical attack.',\n",
       "  'categories': ['cs.LG', 'cs.CR', 'cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2010.03288v1'},\n",
       " 707: {'ID': 707,\n",
       "  'title': 'Near-Infrared Photometry of Four Stellar Clusters in the Small  Magellanic Cloud',\n",
       "  'authors': ['Alessio Mucciarelli',\n",
       "   'Livia Origlia',\n",
       "   'Francesco R. Ferraro',\n",
       "   'Claudia Maraston'],\n",
       "  'published': '2008-09-02T18:42:14Z',\n",
       "  'updated': '2008-09-02T18:42:14Z',\n",
       "  'abstract': 'We present high-quality J, H and K photometry of four Small Magellanic Cloudstellar clusters with intermediate ages in the 1-7 Gyr range (namely NGC 339,361, 416 and 419) . We obtained deep Color-Magnitude Diagrams to study theevolved sequences and providing a detailed census of the Red Giant Branch(RGB), Asymptotic Giant Branch (AGB) and Carbon star populations in eachcluster and their contribution to the total cluster light. We find that in the5-7 Gyr old clusters AGB stars account for ~6 % of the total light in K-band,Carbon stars are lacking and RGB stars account for ~45 % of the totalbolometric luminosity. These empirical findings are in good agreement with thetheoretical predictions. Finally, we derived photometric metallicities computedby using the properties of the RGB and finding an iron content of [Fe/H]=-1.18,-1.08, -0.99 and -0.96 dex for NGC 339, 361, 416 and 419 respectively.',\n",
       "  'categories': ['astro-ph'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/0809.0495v1'},\n",
       " 708: {'ID': 708,\n",
       "  'title': 'Flow Fields: Dense Correspondence Fields for Highly Accurate Large  Displacement Optical Flow Estimation',\n",
       "  'authors': ['Bertram Taetz', 'Christian Bailer', 'Didier Stricker'],\n",
       "  'published': '2017-03-07T19:28:45Z',\n",
       "  'updated': '2018-10-28T08:00:11Z',\n",
       "  'abstract': 'Modern large displacement optical flow algorithms usually use aninitialization by either sparse descriptor matching techniques or denseapproximate nearest neighbor fields. While the latter have the advantage ofbeing dense, they have the major disadvantage of being very outlier-prone asthey are not designed to find the optical flow, but the visually most similarcorrespondence. In this article we present a dense correspondence fieldapproach that is much less outlier-prone and thus much better suited foroptical flow estimation than approximate nearest neighbor fields. Our approachdoes not require explicit regularization, smoothing (like median filtering) ora new data term. Instead we solely rely on patch matching techniques and anovel multi-scale matching strategy. We also present enhancements for outlierfiltering. We show that our approach is better suited for large displacementoptical flow estimation than modern descriptor matching techniques. We do so byinitializing EpicFlow with our approach instead of their originally usedstate-of-the-art descriptor matching technique. We significantly outperform theoriginal EpicFlow on MPI-Sintel, KITTI 2012, KITTI 2015 and Middlebury. In thisextended article of our former conference publication we further improve ourapproach in matching accuracy as well as runtime and present more experimentsand insights.',\n",
       "  'categories': ['cs.CV', 'I.4.8'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1703.02563v2'},\n",
       " 709: {'ID': 709,\n",
       "  'title': 'HPatches: A benchmark and evaluation of handcrafted and learned local  descriptors',\n",
       "  'authors': ['Krystian Mikolajczyk',\n",
       "   'Andrea Vedaldi',\n",
       "   'Vassileios Balntas',\n",
       "   'Karel Lenc'],\n",
       "  'published': '2017-04-19T21:37:03Z',\n",
       "  'updated': '2017-04-19T21:37:03Z',\n",
       "  'abstract': 'In this paper, we propose a novel benchmark for evaluating local imagedescriptors. We demonstrate that the existing datasets and evaluation protocolsdo not specify unambiguously all aspects of evaluation, leading to ambiguitiesand inconsistencies in results reported in the literature. Furthermore, thesedatasets are nearly saturated due to the recent improvements in localdescriptors obtained by learning them from large annotated datasets. Therefore,we introduce a new large dataset suitable for training and testing moderndescriptors, together with strictly defined evaluation protocols in severaltasks such as matching, retrieval and classification. This allows for morerealistic, and thus more reliable comparisons in different applicationscenarios. We evaluate the performance of several state-of-the-art descriptorsand analyse their properties. We show that a simple normalisation oftraditional hand-crafted descriptors can boost their performance to the levelof deep learning based descriptors within a realistic benchmarks evaluation.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1704.05939v1'},\n",
       " 710: {'ID': 710,\n",
       "  'title': 'Learning Stochastic Recurrent Networks',\n",
       "  'authors': ['Justin Bayer', 'Christian Osendorfer'],\n",
       "  'published': '2014-11-27T14:22:36Z',\n",
       "  'updated': '2015-03-05T21:55:38Z',\n",
       "  'abstract': 'Leveraging advances in variational inference, we propose to enhance recurrentneural networks with latent variables, resulting in Stochastic RecurrentNetworks (STORNs). The model i) can be trained with stochastic gradientmethods, ii) allows structured and multi-modal conditionals at each time step,iii) features a reliable estimator of the marginal likelihood and iv) is ageneralisation of deterministic recurrent neural networks. We evaluate themethod on four polyphonic musical data sets and motion capture data.',\n",
       "  'categories': ['stat.ML', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1411.7610v3'},\n",
       " 711: {'ID': 711,\n",
       "  'title': 'Jointly Sparse Convolutional Neural Networks in Dual Spatial-Winograd  Domains',\n",
       "  'authors': ['Mostafa El-Khamy', 'Yoojin Choi', 'Jungwon Lee'],\n",
       "  'published': '2019-02-21T01:03:04Z',\n",
       "  'updated': '2019-02-21T01:03:04Z',\n",
       "  'abstract': 'We consider the optimization of deep convolutional neural networks (CNNs)such that they provide good performance while having reduced complexity ifdeployed on either conventional systems with spatial-domain convolution orlower-complexity systems designed for Winograd convolution. The proposedframework produces one compressed model whose convolutional filters can be madesparse either in the spatial domain or in the Winograd domain. Hence, thecompressed model can be deployed universally on any platform, without need forre-training on the deployed platform. To get a better compression ratio, thesparse model is compressed in the spatial domain that has a fewer number ofparameters. From our experiments, we obtain $24.2\\\\times$ and $47.7\\\\times$compressed models for ResNet-18 and AlexNet trained on the ImageNet dataset,while their computational cost is also reduced by $4.5\\\\times$ and $5.1\\\\times$,respectively.',\n",
       "  'categories': ['cs.CV', 'cs.LG', 'cs.NE'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1902.08192v1'},\n",
       " 712: {'ID': 712,\n",
       "  'title': 'Effective Data Fusion with Generalized Vegetation Index: Evidence from  Land Cover Segmentation in Agriculture',\n",
       "  'authors': ['Jingyi Su',\n",
       "   'Andrew Ng',\n",
       "   'Hao Sheng',\n",
       "   'Xiao Chen',\n",
       "   'Ram Rajagopal'],\n",
       "  'published': '2020-05-07T20:41:20Z',\n",
       "  'updated': '2020-05-07T20:41:20Z',\n",
       "  'abstract': 'How can we effectively leverage the domain knowledge from remote sensing tobetter segment agriculture land cover from satellite images? In this paper, wepropose a novel, model-agnostic, data-fusion approach for vegetation-relatedcomputer vision tasks. Motivated by the various Vegetation Indices (VIs), whichare introduced by domain experts, we systematically reviewed the VIs that arewidely used in remote sensing and their feasibility to be incorporated in deepneural networks. To fully leverage the Near-Infrared channel, the traditionalRed-Green-Blue channels, and Vegetation Index or its variants, we propose aGeneralized Vegetation Index (GVI), a lightweight module that can be easilyplugged into many neural network architectures to serve as an additionalinformation input. To smoothly train models with our GVI, we developed anAdditive Group Normalization (AGN) module that does not require extraparameters of the prescribed neural networks. Our approach has improved theIoUs of vegetation-related classes by 0.9-1.3 percent and consistently improvesthe overall mIoU by 2 percent on our baseline.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2005.03743v1'},\n",
       " 713: {'ID': 713,\n",
       "  'title': 'Computed Tomography Reconstruction Using Deep Image Prior and Learned  Reconstruction Methods',\n",
       "  'authors': ['Johannes Leuschner',\n",
       "   'Maximilian Schmidt',\n",
       "   'Daniel Otero Baguer'],\n",
       "  'published': '2020-03-10T21:03:34Z',\n",
       "  'updated': '2020-03-12T12:09:52Z',\n",
       "  'abstract': 'In this work, we investigate the application of deep learning methods forcomputed tomography in the context of having a low-data regime. As motivation,we review some of the existing approaches and obtain quantitative results aftertraining them with different amounts of data. We find that the learnedprimal-dual has an outstanding performance in terms of reconstruction qualityand data efficiency. However, in general, end-to-end learned methods have twoissues: a) lack of classical guarantees in inverse problems and b) lack ofgeneralization when not trained with enough data. To overcome these issues, webring in the deep image prior approach in combination with classicalregularization. The proposed methods improve the state-of-the-art results inthe low data-regime.',\n",
       "  'categories': ['eess.IV', 'cs.CV', 'cs.LG', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2003.04989v2'},\n",
       " 714: {'ID': 714,\n",
       "  'title': 'An Efficient Hardware Accelerator for Structured Sparse Convolutional  Neural Networks on FPGAs',\n",
       "  'authors': ['Hejia Zhang',\n",
       "   'Chaoyang Zhu',\n",
       "   'Haibin Shen',\n",
       "   'Kejie Huang',\n",
       "   'Ziqi Zhu',\n",
       "   'Shuyuan Yang'],\n",
       "  'published': '2020-01-07T10:21:07Z',\n",
       "  'updated': '2020-01-07T10:21:07Z',\n",
       "  'abstract': 'Deep Convolutional Neural Networks (CNNs) have achieved state-of-the-artperformance in a wide range of applications. However, deeper CNN models, whichare usually computation consuming, are widely required for complex ArtificialIntelligence (AI) tasks. Though recent research progress on network compressionsuch as pruning has emerged as a promising direction to mitigate computationalburden, existing accelerators are still prevented from completely utilizing thebenefits of leveraging sparsity owing to the irregularity caused by pruning. Onthe other hand, Field-Programmable Gate Arrays (FPGAs) have been regarded as apromising hardware platform for CNN inference acceleration. However, mostexisting FPGA accelerators focus on dense CNN and cannot address theirregularity problem. In this paper, we propose a sparse wise dataflow to skipthe cycles of processing Multiply-and-Accumulates (MACs) with zero weights andexploit data statistics to minimize energy through zeros gating to avoidunnecessary computations. The proposed sparse wise dataflow leads to a lowbandwidth requirement and a high data sharing. Then we design an FPGAaccelerator containing a Vector Generator Module (VGM) which can match theindex between sparse weights and input activations according to the proposeddataflow. Experimental results demonstrate that our implementation can achieve987 imag/s and 48 imag/s performance for AlexNet and VGG-16 on Xilinx ZCU102,respectively, which provides 1.5x to 6.7x speedup and 2.0x to 6.2xenergy-efficiency over previous CNN FPGA accelerators.',\n",
       "  'categories': ['eess.SY', 'cs.SY', 'eess.IV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2001.01955v1'},\n",
       " 715: {'ID': 715,\n",
       "  'title': 'Ensemble based discriminative models for Visual Dialog Challenge 2018',\n",
       "  'authors': ['Raghav Goyal', 'Shubham Agarwal'],\n",
       "  'published': '2020-01-15T08:20:54Z',\n",
       "  'updated': '2020-01-15T08:20:54Z',\n",
       "  'abstract': \"This manuscript describes our approach for the Visual Dialog Challenge 2018.We use an ensemble of three discriminative models with different encoders anddecoders for our final submission. Our best performing model on 'test-std'split achieves the NDCG score of 55.46 and the MRR value of 63.77, securingthird position in the challenge.\",\n",
       "  'categories': ['cs.CV', 'cs.CL', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2001.05865v1'},\n",
       " 716: {'ID': 716,\n",
       "  'title': 'Query-guided End-to-End Person Search',\n",
       "  'authors': ['Federico Tombari',\n",
       "   'Fabio Galasso',\n",
       "   'Sikandar Amin',\n",
       "   'Bharti Munjal'],\n",
       "  'published': '2019-05-03T14:31:23Z',\n",
       "  'updated': '2019-05-03T14:31:23Z',\n",
       "  'abstract': 'Person search has recently gained attention as the novel task of finding aperson, provided as a cropped sample, from a gallery of non-cropped images,whereby several other people are also visible. We believe that i. persondetection and re-identification should be pursued in a joint optimizationframework and that ii. the person search should leverage the query imageextensively (e.g. emphasizing unique query patterns). However, so far, no priorart realizes this. We introduce a novel query-guided end-to-end person searchnetwork (QEEPS) to address both aspects. We leverage a most recent jointdetector and re-identification work, OIM [37]. We extend this with i. aquery-guided Siamese squeeze-and-excitation network (QSSE-Net) that uses globalcontext from both the query and gallery images, ii. a query-guided regionproposal network (QRPN) to produce query-relevant proposals, and iii. aquery-guided similarity subnetwork (QSimNet), to learn a query-guidedreidentification score. QEEPS is the first end-to-end query-guided detectionand re-id network. On both the most recent CUHK-SYSU [37] and PRW [46]datasets, we outperform the previous state-of-the-art by a large margin.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1905.01203v1'},\n",
       " 717: {'ID': 717,\n",
       "  'title': 'Deep image prior for 3D magnetic particle imaging: A quantitative  comparison of regularization techniques on Open MPI dataset',\n",
       "  'authors': ['Mads Thorstein Roar Henriksen',\n",
       "   'Sören Dittmer',\n",
       "   'Tobias Kluth',\n",
       "   'Peter Maass'],\n",
       "  'published': '2020-07-03T10:13:10Z',\n",
       "  'updated': '2020-07-03T10:13:10Z',\n",
       "  'abstract': 'Magnetic particle imaging (MPI) is an imaging modality exploiting thenonlinear magnetization behavior of (super-)paramagnetic nanoparticles toobtain a space- and often also time-dependent concentration of a tracerconsisting of these nanoparticles. MPI has a continuously increasing number ofpotential medical applications. One prerequisite for successful performance inthese applications is a proper solution to the image reconstruction problem.More classical methods from inverse problems theory, as well as novelapproaches from the field of machine learning, have the potential to deliverhigh-quality reconstructions in MPI. We investigate a novel reconstructionapproach based on a deep image prior, which builds on representing the solutionby a deep neural network. Novel approaches, as well as variational anditerative regularization techniques, are compared quantitatively in terms ofpeak signal-to-noise ratios and structural similarity indices on the publiclyavailable Open MPI dataset.',\n",
       "  'categories': ['eess.IV', 'cs.CV', 'cs.LG', 'math.FA', 'math.OC'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2007.01593v1'},\n",
       " 718: {'ID': 718,\n",
       "  'title': 'Security of Facial Forensics Models Against Adversarial Attacks',\n",
       "  'authors': ['Fuming Fang',\n",
       "   'Huy H. Nguyen',\n",
       "   'Rong Huang',\n",
       "   'Junichi Yamagishi',\n",
       "   'Isao Echizen'],\n",
       "  'published': '2019-11-02T06:03:51Z',\n",
       "  'updated': '2020-05-20T12:56:34Z',\n",
       "  'abstract': 'Deep neural networks (DNNs) have been used in digital forensics to identifyfake facial images. We investigated several DNN-based forgery forensics models(FFMs) to examine whether they are secure against adversarial attacks. Weexperimentally demonstrated the existence of individual adversarialperturbations (IAPs) and universal adversarial perturbations (UAPs) that canlead a well-performed FFM to misbehave. Based on iterative procedure, gradientinformation is used to generate two kinds of IAPs that can be used to fabricateclassification and segmentation outputs. In contrast, UAPs are generated on thebasis of over-firing. We designed a new objective function that encouragesneurons to over-fire, which makes UAP generation feasible even without usingtraining data. Experiments demonstrated the transferability of UAPs acrossunseen datasets and unseen FFMs. Moreover, we conducted subjective assessmentfor imperceptibility of the adversarial perturbations, revealing that thecrafted UAPs are visually negligible. These findings provide a baseline forevaluating the adversarial security of FFMs.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1911.00660v2'},\n",
       " 719: {'ID': 719,\n",
       "  'title': 'Self-Selective Correlation Ship Tracking Method for Smart Ocean System',\n",
       "  'authors': ['Mohsen Guizani',\n",
       "   'Bin Song',\n",
       "   'Jie Guo',\n",
       "   'Xu Kang',\n",
       "   'Xiaojiang Du'],\n",
       "  'published': '2019-02-26T01:19:35Z',\n",
       "  'updated': '2019-02-26T01:19:35Z',\n",
       "  'abstract': 'In recent years, with the development of the marine industry, navigationenvironment becomes more complicated. Some artificial intelligencetechnologies, such as computer vision, can recognize, track and count thesailing ships to ensure the maritime security and facilitates the managementfor Smart Ocean System. Aiming at the scaling problem and boundary effectproblem of traditional correlation filtering methods, we propose aself-selective correlation filtering method based on box regression (BRCF). Theproposed method mainly include: 1) A self-selective model with negative samplesmining method which effectively reduces the boundary effect in strengtheningthe classification ability of classifier at the same time; 2) A bounding boxregression method combined with a key points matching method for the scaleprediction, leading to a fast and efficient calculation. The experimentalresults show that the proposed method can effectively deal with the problem ofship size changes and background interference. The success rates and precisionswere higher than Discriminative Scale Space Tracking (DSST) by over 8percentage points on the marine traffic dataset of our laboratory. In terms ofprocessing speed, the proposed method is higher than DSST by nearly 22 FramesPer Second (FPS).',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1902.09690v1'},\n",
       " 720: {'ID': 720,\n",
       "  'title': 'Procedural Noise Adversarial Examples for Black-Box Attacks on Deep  Convolutional Networks',\n",
       "  'authors': ['Kenneth T. Co',\n",
       "   'Sixte de Maupeou',\n",
       "   'Luis Muñoz-González',\n",
       "   'Emil C. Lupu'],\n",
       "  'published': '2018-09-30T21:45:39Z',\n",
       "  'updated': '2019-11-23T13:02:08Z',\n",
       "  'abstract': 'Deep Convolutional Networks (DCNs) have been shown to be vulnerable toadversarial examples---perturbed inputs specifically designed to produceintentional errors in the learning algorithms at test time. Existinginput-agnostic adversarial perturbations exhibit interesting visual patternsthat are currently unexplained. In this paper, we introduce a structuredapproach for generating Universal Adversarial Perturbations (UAPs) withprocedural noise functions. Our approach unveils the systemic vulnerability ofpopular DCN models like Inception v3 and YOLO v3, with single noise patternsable to fool a model on up to 90% of the dataset. Procedural noise allows us togenerate a distribution of UAPs with high universal evasion rates using only afew parameters. Additionally, we propose Bayesian optimization to efficientlylearn procedural noise parameters to construct inexpensive untargeted black-boxattacks. We demonstrate that it can achieve an average of less than 10 queriesper successful attack, a 100-fold improvement on existing methods. We furthermotivate the use of input-agnostic defences to increase the stability of modelsto adversarial perturbations. The universality of our attacks suggests that DCNmodels may be sensitive to aggregations of low-level class-agnostic features.These findings give insight on the nature of some universal adversarialperturbations and how they could be generated in other applications.',\n",
       "  'categories': ['cs.CR', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1810.00470v4'},\n",
       " 721: {'ID': 721,\n",
       "  'title': 'Mimic and Fool: A Task Agnostic Adversarial Attack',\n",
       "  'authors': ['Utpal Garain', 'Akshay Chaturvedi'],\n",
       "  'published': '2019-06-11T13:56:12Z',\n",
       "  'updated': '2020-04-12T18:37:42Z',\n",
       "  'abstract': 'At present, adversarial attacks are designed in a task-specific fashion.However, for downstream computer vision tasks such as image captioning, imagesegmentation etc., the current deep learning systems use an image classifierlike VGG16, ResNet50, Inception-v3 etc. as a feature extractor. Keeping this inmind, we propose Mimic and Fool, a task agnostic adversarial attack. Given afeature extractor, the proposed attack finds an adversarial image which canmimic the image feature of the original image. This ensures that the two imagesgive the same (or similar) output regardless of the task. We randomly select1000 MSCOCO validation images for experimentation. We perform experiments ontwo image captioning models, Show and Tell, Show Attend and Tell and one VQAmodel, namely, end-to-end neural module network (N2NMN). The proposed attackachieves success rate of 74.0%, 81.0% and 87.1% for Show and Tell, Show Attendand Tell and N2NMN respectively. We also propose a slight modification to ourattack to generate natural-looking adversarial images. In addition, we alsoshow the applicability of the proposed attack for invertible architecture.Since Mimic and Fool only requires information about the feature extractor ofthe model, it can be considered as a gray-box attack.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1906.04606v2'},\n",
       " 722: {'ID': 722,\n",
       "  'title': 'Policy Consolidation for Continual Reinforcement Learning',\n",
       "  'authors': ['Christos Kaplanis', 'Claudia Clopath', 'Murray Shanahan'],\n",
       "  'published': '2019-02-01T09:59:10Z',\n",
       "  'updated': '2019-06-17T12:45:41Z',\n",
       "  'abstract': \"We propose a method for tackling catastrophic forgetting in deepreinforcement learning that is \\\\textit{agnostic} to the timescale of changes inthe distribution of experiences, does not require knowledge of task boundaries,and can adapt in \\\\textit{continuously} changing environments. In our\\\\textit{policy consolidation} model, the policy network interacts with acascade of hidden networks that simultaneously remember the agent's policy at arange of timescales and regularise the current policy by its own history,thereby improving its ability to learn without forgetting. We find that themodel improves continual learning relative to baselines on a number ofcontinuous control tasks in single-task, alternating two-task, and multi-agentcompetitive self-play settings.\",\n",
       "  'categories': ['cs.LG', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1902.00255v2'},\n",
       " 723: {'ID': 723,\n",
       "  'title': 'Adversarial Turing Patterns from Cellular Automata',\n",
       "  'authors': ['Ivan Oseledets',\n",
       "   'Nurislam Tursynbek',\n",
       "   'Ilya Vilkoviskiy',\n",
       "   'Maria Sindeeva'],\n",
       "  'published': '2020-11-18T16:50:54Z',\n",
       "  'updated': '2021-02-08T07:51:43Z',\n",
       "  'abstract': 'State-of-the-art deep classifiers are intriguingly vulnerable to universaladversarial perturbations: single disturbances of small magnitude that lead tomisclassification of most in-puts. This phenomena may potentially result in aserious security problem. Despite the extensive research in this area,there isa lack of theoretical understanding of the structure of these perturbations. Inimage domain, there is a certain visual similarity between patterns, thatrepresent these perturbations, and classical Turing patterns, which appear as asolution of non-linear partial differential equations and are underlyingconcept of many processes in nature. In this paper,we provide a theoreticalbridge between these two different theories, by mapping a simplified algorithmfor crafting universal perturbations to (inhomogeneous) cellular automata,thelatter is known to generate Turing patterns. Furthermore,we propose to useTuring patterns, generated by cellular automata, as universal perturbations,and experimentally show that they significantly degrade the performance of deeplearning models. We found this method to be a fast and efficient way to createa data-agnostic quasi-imperceptible perturbation in the black-box scenario. Thesource code is available at https://github.com/NurislamT/advTuring.',\n",
       "  'categories': ['cs.NE', 'cs.AI', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2011.09393v2'},\n",
       " 724: {'ID': 724,\n",
       "  'title': 'Dynamic Feature Pyramid Networks for Object Detection',\n",
       "  'authors': ['Mingjian Zhu', 'Changbin Yu', 'Yunhe Wang', 'Kai Han'],\n",
       "  'published': '2020-12-01T19:03:55Z',\n",
       "  'updated': '2020-12-01T19:03:55Z',\n",
       "  'abstract': 'This paper studies feature pyramid network (FPN), which is a widely usedmodule for aggregating multi-scale feature information in the object detectionsystem. The performance gain in most of the existing works is mainlycontributed to the increase of computation burden, especially the floatingnumber operations (FLOPs). In addition, the multi-scale information within eachlayer in FPN has not been well investigated. To this end, we first introduce aninception FPN in which each layer contains convolution filters with differentkernel sizes to enlarge the receptive field and integrate more usefulinformation. Moreover, we point out that not all objects need such acomplicated calculation module and propose a new dynamic FPN (DyFPN). Eachlayer in the DyFPN consists of multiple branches with different computationalcosts. Specifically, the output features of DyFPN will be calculated by usingthe adaptively selected branch according to a learnable gating operation.Therefore, the proposed method can provide a more efficient dynamic inferencefor achieving a better trade-off between accuracy and detection performance.Extensive experiments conducted on benchmarks demonstrate that the proposedDyFPN significantly improves performance with the optimal allocation ofcomputation resources. For instance, replacing the FPN with the inception FPNimproves detection accuracy by 1.6 AP using the Faster R-CNN paradigm on COCOminival, and the DyFPN further reduces about 40% of its FLOPs while maintainingsimilar performance.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2012.00779v1'},\n",
       " 725: {'ID': 725,\n",
       "  'title': 'Fast-UAP: An Algorithm for Speeding up Universal Adversarial  Perturbation Generation with Orientation of Perturbation Vectors',\n",
       "  'authors': ['Le Shu', 'Jiazhu Dai'],\n",
       "  'published': '2019-11-04T12:57:17Z',\n",
       "  'updated': '2020-01-06T10:28:26Z',\n",
       "  'abstract': 'Convolutional neural networks (CNN) have become one of the most popularmachine learning tools and are being applied in various tasks, however, CNNmodels are vulnerable to universal perturbations, which are usuallyhuman-imperceptible but can cause natural images to be misclassified with highprobability. One of the state-of-the-art algorithms to generate universalperturbations is known as UAP. UAP only aggregates the minimal perturbations inevery iteration, which will lead to generated universal perturbation whosemagnitude cannot rise up efficiently and cause a slow generation. In thispaper, we proposed an optimized algorithm to improve the performance ofcrafting universal perturbations based on orientation of perturbation vectors.At each iteration, instead of choosing minimal perturbation vector with respectto each image, we aggregate the current instance of universal perturbation withthe perturbation which has similar orientation to the former so that themagnitude of the aggregation will rise up as large as possible at everyiteration. The experiment results show that we get universal perturbations in ashorter time and with a smaller number of training images. Furthermore, weobserve in experiments that universal perturbations generated by our proposedalgorithm have an average increment of fooling rate by 9% in white-box attacksand black-box attacks comparing with universal perturbations generated by UAP.',\n",
       "  'categories': ['cs.LG', 'stat.ML', 'I.2.0', 'I.2.0'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1911.01172v3'},\n",
       " 726: {'ID': 726,\n",
       "  'title': 'A Histogram Thresholding Improvement to Mask R-CNN for Scalable  Segmentation of New and Old Rural Buildings',\n",
       "  'authors': ['Weipan Xu', 'Xun Li', 'Junhao Jiang', 'Ying Li', 'Haohui Chen'],\n",
       "  'published': '2021-02-08T02:09:11Z',\n",
       "  'updated': '2021-02-08T02:09:11Z',\n",
       "  'abstract': \"Mapping new and old buildings are of great significance for understandingsocio-economic development in rural areas. In recent years, deep neuralnetworks have achieved remarkable building segmentation results inhigh-resolution remote sensing images. However, the scarce training data andthe varying geographical environments have posed challenges for scalablebuilding segmentation. This study proposes a novel framework based on MaskR-CNN, named HTMask R-CNN, to extract new and old rural buildings even when thelabel is scarce. The framework adopts the result of single-object instancesegmentation from the orthodox Mask R-CNN. Further, it classifies the ruralbuildings into new and old ones based on a dynamic grayscale threshold inferredfrom the result of a two-object instance segmentation task where training datais scarce. We found that the framework can extract more buildings and achieve amuch higher mean Average Precision (mAP) than the orthodox Mask R-CNN model. Wetested the novel framework's performance with increasing training data andfound that it converged even when the training samples were limited. Thisframework's main contribution is to allow scalable segmentation by usingsignificantly fewer training samples than traditional machine learningpractices. That makes mapping China's new and old rural buildings viable.\",\n",
       "  'categories': ['cs.CV', 'eess.IV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2102.04838v1'},\n",
       " 727: {'ID': 727,\n",
       "  'title': 'Supervised Discrete Hashing with Relaxation',\n",
       "  'authors': ['Tongliang Liu',\n",
       "   'Jie Gui',\n",
       "   'Zhenan Sun',\n",
       "   'Dacheng Tao',\n",
       "   'Tieniu Tan'],\n",
       "  'published': '2019-04-07T00:09:19Z',\n",
       "  'updated': '2019-04-07T00:09:19Z',\n",
       "  'abstract': 'Data-dependent hashing has recently attracted attention due to being able tosupport efficient retrieval and storage of high-dimensional data such asdocuments, images, and videos. In this paper, we propose a novel learning-basedhashing method called \"Supervised Discrete Hashing with Relaxation\" (SDHR)based on \"Supervised Discrete Hashing\" (SDH). SDH uses ordinary least squaresregression and traditional zero-one matrix encoding of class label informationas the regression target (code words), thus fixing the regression target. InSDHR, the regression target is instead optimized. The optimized regressiontarget matrix satisfies a large margin constraint for correct classification ofeach example. Compared with SDH, which uses the traditional zero-one matrix,SDHR utilizes the learned regression target matrix and, therefore, moreaccurately measures the classification error of the regression model and ismore flexible. As expected, SDHR generally outperforms SDH. Experimentalresults on two large-scale image datasets (CIFAR-10 and MNIST) and alarge-scale and challenging face dataset (FRGC) demonstrate the effectivenessand efficiency of SDHR.',\n",
       "  'categories': ['cs.LG', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1904.03549v1'},\n",
       " 728: {'ID': 728,\n",
       "  'title': 'Random Hinge Forest for Differentiable Learning',\n",
       "  'authors': ['Sharon Schreiber',\n",
       "   'Adam P. Harrison',\n",
       "   'Nathan Lay',\n",
       "   'Gitesh Dawer',\n",
       "   'Adrian Barbu'],\n",
       "  'published': '2018-02-12T04:08:53Z',\n",
       "  'updated': '2018-03-01T06:25:27Z',\n",
       "  'abstract': 'We propose random hinge forests, a simple, efficient, and novel variant ofdecision forests. Importantly, random hinge forests can be readily incorporatedas a general component within arbitrary computation graphs that are optimizedend-to-end with stochastic gradient descent or variants thereof. We deriverandom hinge forest and ferns, focusing on their sparse and efficient nature,their min-max margin property, strategies to initialize them for arbitrarynetwork architectures, and the class of optimizers most suitable for optimizingrandom hinge forest. The performance and versatility of random hinge forestsare demonstrated by experiments incorporating a variety of of small and largeUCI machine learning data sets and also ones involving the MNIST, Letter, andUSPS image datasets. We compare random hinge forests with random forests andthe more recent backpropagating deep neural decision forests.',\n",
       "  'categories': ['stat.ML', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1802.03882v2'},\n",
       " 729: {'ID': 729,\n",
       "  'title': 'A deep Large Binocular Telescope view of the Canes Venatici I dwarf  galaxy',\n",
       "  'authors': ['Roberto Ragazzoni',\n",
       "   'Eric F. Bell',\n",
       "   'Matthew G. Coleman',\n",
       "   'Andrea Grazian',\n",
       "   'Fernando Pedichini',\n",
       "   'Federico Gasparo',\n",
       "   'Jelte T. A. De Jong',\n",
       "   'Hans-Walter Rix',\n",
       "   'David J. Sand',\n",
       "   'David Thompson',\n",
       "   'Emiliano Diolaiti',\n",
       "   'Vadim Burwitz',\n",
       "   'John M. Hill',\n",
       "   'Emanuele Giallongo',\n",
       "   'Nicolas F. Martin',\n",
       "   'Jill Bechtold'],\n",
       "  'published': '2007-09-21T08:09:07Z',\n",
       "  'updated': '2007-11-06T11:04:13Z',\n",
       "  'abstract': 'We present the first deep color-magnitude diagram of the Canes Venatici I(CVnI) dwarf galaxy from observations with the wide field Large BinocularCamera on the Large Binocular Telescope. Reaching down to the main-sequenceturnoff of the oldest stars, it reveals a dichotomy in the stellar populationsof CVnI: it harbors an old (&gt; 10 Gyr), metal-poor ([Fe/H] ~ -2.0) and spatiallyextended population along with a much younger (~ 1.4-2.0 Gyr), 0.5 dex moremetal-rich, and spatially more concentrated population. These young stars arealso offset by 64_{-20}^{+40} pc to the East of the galaxy center. The datasuggest that this young population, which represent ~ 3-5 % of the stellar massof the galaxy within its half-light radius, should be identified with thekinematically cold stellar component found by Ibata et al. (2006). CVnItherefore follows the behavior of the other remote MW dwarf spheroidals whichall contain intermediate age and/or young populations: a complex star formationhistory is possible in extremely low-mass galaxies.',\n",
       "  'categories': ['astro-ph'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/0709.3365v2'},\n",
       " 730: {'ID': 730,\n",
       "  'title': 'Expolring Architectures for CNN-Based Word Spotting',\n",
       "  'authors': ['Sebastian Sudholt',\n",
       "   'Gernot A. Fink',\n",
       "   'Fabian Wolf',\n",
       "   'Eugen Rusakov'],\n",
       "  'published': '2018-06-28T10:20:41Z',\n",
       "  'updated': '2018-06-28T10:20:41Z',\n",
       "  'abstract': 'The goal in word spotting is to retrieve parts of document images which arerelevant with respect to a certain user-defined query. The recent past has seenattribute-based Convolutional Neural Networks take over this field of research.As is common for other fields of computer vision, the CNNs used for this taskare already considerably deep. The question that arises, however, is: Howcomplex does a CNN have to be for word spotting? Are increasingly deeper modelsgiving increasingly bet- ter results or does performance behave asymptoticallyfor these architectures? On the other hand, can similar results be obtainedwith a much smaller CNN? The goal of this paper is to give an answer to thesequestions. Therefore, the recently successful TPP- PHOCNet will be compared toa Residual Network, a Densely Connected Convolutional Network and a LeNetarchitecture empirically. As will be seen in the evaluation, a complex modelcan be beneficial for word spotting on harder tasks such as the IAM OfflineDatabase but gives no advantage for easier benchmarks such as the GeorgeWashington Database.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1806.10866v1'},\n",
       " 731: {'ID': 731,\n",
       "  'title': 'NAS-DIP: Learning Deep Image Prior with Neural Architecture Search',\n",
       "  'authors': ['Yun-Chun Chen', 'Esther Robb', 'Chen Gao', 'Jia-Bin Huang'],\n",
       "  'published': '2020-08-26T17:59:36Z',\n",
       "  'updated': '2020-08-26T17:59:36Z',\n",
       "  'abstract': 'Recent work has shown that the structure of deep convolutional neuralnetworks can be used as a structured image prior for solving various inverseimage restoration tasks. Instead of using hand-designed architectures, wepropose to search for neural architectures that capture stronger image priors.Building upon a generic U-Net architecture, our core contribution lies indesigning new search spaces for (1) an upsampling cell and (2) a pattern ofcross-scale residual connections. We search for an improved network byleveraging an existing neural architecture search algorithm (usingreinforcement learning with a recurrent neural network controller). We validatethe effectiveness of our method via a wide variety of applications, includingimage restoration, dehazing, image-to-image translation, and matrixfactorization. Extensive experimental results show that our algorithm performsfavorably against state-of-the-art learning-free approaches and reachescompetitive performance with existing learning-based methods in some cases.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2008.11713v1'},\n",
       " 732: {'ID': 732,\n",
       "  'title': 'Structured Query-Based Image Retrieval Using Scene Graphs',\n",
       "  'authors': ['Subarna Tripathi', 'Brigit Schroeder'],\n",
       "  'published': '2020-05-13T22:40:32Z',\n",
       "  'updated': '2020-05-13T22:40:32Z',\n",
       "  'abstract': \"A structured query can capture the complexity of object interactions (e.g.'woman rides motorcycle') unlike single objects (e.g. 'woman' or 'motorcycle').Retrieval using structured queries therefore is much more useful than singleobject retrieval, but a much more challenging problem. In this paper we presenta method which uses scene graph embeddings as the basis for an approach toimage retrieval. We examine how visual relationships, derived from scenegraphs, can be used as structured queries. The visual relationships aredirected subgraphs of the scene graph with a subject and object as nodesconnected by a predicate relationship. Notably, we are able to achieve highrecall even on low to medium frequency objects found in the long-tailedCOCO-Stuff dataset, and find that adding a visual relationship-inspired lossboosts our recall by 10% in the best case.\",\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2005.06653v1'},\n",
       " 733: {'ID': 733,\n",
       "  'title': 'An Effective Single-Image Super-Resolution Model Using  Squeeze-and-Excitation Networks',\n",
       "  'authors': ['Kangfu Mei',\n",
       "   'Jihua Ye',\n",
       "   'Aiwen Jiang',\n",
       "   'Juncheng Li',\n",
       "   'Mingwen Wang'],\n",
       "  'published': '2018-10-03T16:29:37Z',\n",
       "  'updated': '2018-10-03T16:29:37Z',\n",
       "  'abstract': 'Recent works on single-image super-resolution are concentrated on improvingperformance through enhancing spatial encoding between convolutional layers. Inthis paper, we focus on modeling the correlations between channels ofconvolutional features. We present an effective deep residual network based onsqueeze-and-excitation blocks (SEBlock) to reconstruct high-resolution (HR)image from low-resolution (LR) image. SEBlock is used to adaptively recalibratechannel-wise feature mappings. Further, short connections between each SEBlockare used to remedy information loss. Extensive experiments show that our modelcan achieve the state-of-the-art performance and get finer texture details.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1810.01831v1'},\n",
       " 734: {'ID': 734,\n",
       "  'title': 'Liver Segmentation from Multimodal Images using HED-Mask R-CNN',\n",
       "  'authors': ['Mohanasankar Sivaprakasam',\n",
       "   'Jeevakala S',\n",
       "   'Deepika G',\n",
       "   'Keerthi Ram',\n",
       "   'Supriti Mulay'],\n",
       "  'published': '2019-10-23T12:04:45Z',\n",
       "  'updated': '2019-10-23T12:04:45Z',\n",
       "  'abstract': \"Precise segmentation of the liver is critical for computer-aided diagnosissuch as pre-evaluation of the liver for living donor-based transplantationsurgery. This task is challenging due to the weak boundaries of organs,countless anatomical variations, and the complexity of the background. Computedtomography (CT) scanning and magnetic resonance imaging (MRI) images havedifferent parameters and settings. Thus, images acquired from differentmodalities differ from one another making liver segmentation challenging task.We propose an efficient liver segmentation with the combination ofholistically-nested edge detection (HED) and the Mask-region-convolutionalneural network (R-CNN) to address these challenges. The proposed HED-Mask R-CNNapproach is based on effective identification of edge maps from multimodalimages. The proposed system firstly applies a preprocessing step of imageenhancement to get the 'primal sketches' of the abdomen. Then the HED networkis applied to enhanced CT and MRI modality images to get a better edge map.Finally, the Mask R-CNN is used to segment the liver from edge map images. Weused a dataset of 20 CT patients and 9 MR patients from the CHAOS challenge.The system is trained on CT and MRI images separately and then converted to 2Dslices. We significantly improved the segmentation accuracy of CT and MRIimages on a database with a Dice value of 0.94 for CT, 0.89 for T2-weighted MRIand 0.91 for T1-weighted MRI.\",\n",
       "  'categories': ['eess.IV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1910.10504v1'},\n",
       " 735: {'ID': 735,\n",
       "  'title': 'Extracting Electron Scattering Cross Sections from Swarm Data using Deep  Neural Networks',\n",
       "  'authors': ['Bhaskar Chaudhury', 'Vishrut Jetly'],\n",
       "  'published': '2020-11-30T11:48:15Z',\n",
       "  'updated': '2020-11-30T11:48:15Z',\n",
       "  'abstract': 'Electron-neutral scattering cross sections are fundamental quantities insimulations of low temperature plasmas used for many technological applicationstoday. From these microscopic cross sections, several macro-scale quantities(called \"swarm\" parameters) can be calculated. However, measurements as well astheoretical calculations of cross sections are challenging. Since the 1960sresearchers have attempted to solve the inverse swarm problem of obtainingcross sections from swarm data; but the solutions are not necessarily unique.To address this issues, we examine the use of deep learning models which aretrained using the previous determinations of elastic momentum transfer,ionization and excitation cross sections for different gases available on theLXCat website and their corresponding swarm parameters calculated using theBOLSIG+ solver for the numerical solution of the Boltzmann equation forelectrons in weakly ionized gases. We implement artificial neural network(ANN), convolutional neural network (CNN) and densely connected convolutionalnetwork (DenseNet) for this investigation. To the best of our knowledge, thereis no study exploring the use of CNN and DenseNet for the inverse swarmproblem. We test the validity of predictions by all these trained networks fora broad range of gas species and we deduce that DenseNet effectively extractsboth long and short term features from the swarm data and hence, it predictscross sections with significantly higher accuracy compared to ANN. Further, weapply Monte Carlo dropout as Bayesian approximation to estimate the probabilitydistribution of the cross sections to determine all plausible solutions of thisinverse problem.',\n",
       "  'categories': ['physics.plasm-ph', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2011.14711v1'},\n",
       " 736: {'ID': 736,\n",
       "  'title': 'Boundary-preserving Mask R-CNN',\n",
       "  'authors': ['Xinggang Wang', 'Lichao Huang', 'Wenyu Liu', 'Tianheng Cheng'],\n",
       "  'published': '2020-07-17T11:54:02Z',\n",
       "  'updated': '2020-07-17T11:54:02Z',\n",
       "  'abstract': 'Tremendous efforts have been made to improve mask localization accuracy ininstance segmentation. Modern instance segmentation methods relying on fullyconvolutional networks perform pixel-wise classification, which ignores objectboundaries and shapes, leading coarse and indistinct mask prediction resultsand imprecise localization. To remedy these problems, we propose a conceptuallysimple yet effective Boundary-preserving Mask R-CNN (BMask R-CNN) to leverageobject boundary information to improve mask localization accuracy. BMask R-CNNcontains a boundary-preserving mask head in which object boundary and mask aremutually learned via feature fusion blocks. As a result, the predicted masksare better aligned with object boundaries. Without bells and whistles, BMaskR-CNN outperforms Mask R-CNN by a considerable margin on the COCO dataset; inthe Cityscapes dataset, there are more accurate boundary groundtruthsavailable, so that BMask R-CNN obtains remarkable improvements over Mask R-CNN.Besides, it is not surprising to observe that BMask R-CNN obtains more obviousimprovement when the evaluation criterion requires better localization (e.g.,AP$_{75}$) as shown in Fig.1. Code and models are available at\\\\url{https://github.com/hustvl/BMaskR-CNN}.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2007.08921v1'},\n",
       " 737: {'ID': 737,\n",
       "  'title': 'An Analysis of Visual Question Answering Algorithms',\n",
       "  'authors': ['Christopher Kanan', 'Kushal Kafle'],\n",
       "  'published': '2017-03-28T17:48:07Z',\n",
       "  'updated': '2017-09-13T18:56:45Z',\n",
       "  'abstract': 'In visual question answering (VQA), an algorithm must answer text-basedquestions about images. While multiple datasets for VQA have been created sincelate 2014, they all have flaws in both their content and the way algorithms areevaluated on them. As a result, evaluation scores are inflated andpredominantly determined by answering easier questions, making it difficult tocompare different methods. In this paper, we analyze existing VQA algorithmsusing a new dataset. It contains over 1.6 million questions organized into 12different categories. We also introduce questions that are meaningless for agiven image to force a VQA system to reason about image content. We propose newevaluation schemes that compensate for over-represented question-types and makeit easier to study the strengths and weaknesses of algorithms. We analyze theperformance of both baseline and state-of-the-art VQA models, includingmulti-modal compact bilinear pooling (MCB), neural module networks, andrecurrent answering units. Our experiments establish how attention helpscertain categories more than others, determine which models work better thanothers, and explain how simple models (e.g. MLP) can surpass more complexmodels (MCB) by simply learning to answer large, easy question categories.',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.CL'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1703.09684v2'},\n",
       " 738: {'ID': 738,\n",
       "  'title': 'ComboLoss for Facial Attractiveness Analysis with Squeeze-and-Excitation  Networks',\n",
       "  'authors': ['Jinhai Xiang', 'Lu Xu'],\n",
       "  'published': '2020-10-21T02:27:31Z',\n",
       "  'updated': '2020-10-21T02:27:31Z',\n",
       "  'abstract': 'Loss function is crucial for model training and feature representationlearning, conventional models usually regard facial attractiveness recognitiontask as a regression problem, and adopt MSE loss or Huber variant loss assupervision to train a deep convolutional neural network (CNN) to predictfacial attractiveness score. Little work has been done to systematicallycompare the performance of diverse loss functions. In this paper, we firstlysystematically analyze model performance under diverse loss functions. Then anovel loss function named ComboLoss is proposed to guide the SEResNeXt50network. The proposed method achieves state-of-the-art performance on SCUT-FBP,HotOrNot and SCUT-FBP5500 datasets with an improvement of 1.13%, 2.1% and 0.57%compared with prior arts, respectively. Code and models are available athttps://github.com/lucasxlu/ComboLoss.git.',\n",
       "  'categories': ['cs.MM'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2010.10721v1'},\n",
       " 739: {'ID': 739,\n",
       "  'title': 'Adversarial Network Traffic: Towards Evaluating the Robustness of Deep  Learning-Based Network Traffic Classification',\n",
       "  'authors': ['Rasool Jalili', 'Saeed Shiravi', 'Amir Mahdi Sadeghzadeh'],\n",
       "  'published': '2020-03-03T00:19:35Z',\n",
       "  'updated': '2021-01-20T23:52:11Z',\n",
       "  'abstract': 'Network traffic classification is used in various applications such asnetwork traffic management, policy enforcement, and intrusion detectionsystems. Although most applications encrypt their network traffic and some ofthem dynamically change their port numbers, Machine Learning (ML) andespecially Deep Learning (DL)-based classifiers have shown impressiveperformance in network traffic classification. In this paper, we evaluate therobustness of DL-based network traffic classifiers against Adversarial NetworkTraffic (ANT). ANT causes DL-based network traffic classifiers to predictincorrectly using Universal Adversarial Perturbation (UAP) generating methods.Since there is no need to buffer network traffic before sending ANT, it isgenerated live. We partition the input space of the DL-based network trafficclassification into three categories: packet classification, flow contentclassification, and flow time series classification. To generate ANT, wepropose three new attacks injecting UAP into network traffic. AdvPad attackinjects a UAP into the content of packets to evaluate the robustness of packetclassifiers. AdvPay attack injects a UAP into the payload of a dummy packet toevaluate the robustness of flow content classifiers. AdvBurst attack injects aspecific number of dummy packets with crafted statistical features based on aUAP into a selected burst of a flow to evaluate the robustness of flow timeseries classifiers. The results indicate injecting a little UAP into networktraffic, highly decreases the performance of DL-based network trafficclassifiers in all categories.',\n",
       "  'categories': ['cs.CR'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2003.01261v4'},\n",
       " 740: {'ID': 740,\n",
       "  'title': 'WCE Polyp Detection with Triplet based Embeddings',\n",
       "  'authors': ['Jordi Vitrià',\n",
       "   'Fernando Azpiroz',\n",
       "   'Carolina Malagelada',\n",
       "   'Santi Seguí',\n",
       "   'Hagen Wenzek',\n",
       "   'Pablo Laiz'],\n",
       "  'published': '2019-12-10T11:08:45Z',\n",
       "  'updated': '2020-10-02T11:51:38Z',\n",
       "  'abstract': \"Wireless capsule endoscopy is a medical procedure used to visualize theentire gastrointestinal tract and to diagnose intestinal conditions, such aspolyps or bleeding. Current analyses are performed by manually inspectingnearly each one of the frames of the video, a tedious and error-prone task.Automatic image analysis methods can be used to reduce the time needed forphysicians to evaluate a capsule endoscopy video, however these methods arestill in a research phase. In this paper we focus on computer-aided polypdetection in capsule endoscopy images. This is a challenging problem because ofthe diversity of polyp appearance, the imbalanced dataset structure and thescarcity of data. We have developed a new polyp computer-aided decision systemthat combines a deep convolutional neural network and metric learning. The keypoint of the method is the use of the triplet loss function with the aim ofimproving feature extraction from the images when having small dataset. Thetriplet loss function allows to train robust detectors by forcing images fromthe same category to be represented by similar embedding vectors while ensuringthat images from different categories are represented by dissimilar vectors.Empirical results show a meaningful increase of AUC values compared to baselinemethods. A good performance is not the only requirement when considering theadoption of this technology to clinical practice. Trust and explainability ofdecisions are as important as performance. With this purpose, we also provide amethod to generate visual explanations of the outcome of our polyp detector.These explanations can be used to build a physician's trust in the system andalso to convey information about the inner working of the method to thedesigner for debugging purposes.\",\n",
       "  'categories': ['eess.IV', 'cs.CV', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1912.04643v3'},\n",
       " 741: {'ID': 741,\n",
       "  'title': 'Latent Filter Scaling for Multimodal Unsupervised Image-to-Image  Translation',\n",
       "  'authors': ['Peter Wonka', 'Yazeed Alharbi', 'Neil Smith'],\n",
       "  'published': '2018-12-24T10:07:50Z',\n",
       "  'updated': '2019-04-07T07:47:18Z',\n",
       "  'abstract': 'In multimodal unsupervised image-to-image translation tasks, the goal is totranslate an image from the source domain to many images in the target domain.We present a simple method that produces higher quality images than currentstate-of-the-art while maintaining the same amount of multimodal diversity.Previous methods follow the unconditional approach of trying to map the latentcode directly to a full-size image. This leads to complicated networkarchitectures with several introduced hyperparameters to tune. By treating thelatent code as a modifier of the convolutional filters, we produce multimodaloutput while maintaining the traditional Generative Adversarial Network (GAN)loss and without additional hyperparameters. The only tuning required by ourmethod controls the tradeoff between variability and quality of generatedimages. Furthermore, we achieve disentanglement between source domain contentand target domain style for free as a by-product of our formulation. We performqualitative and quantitative experiments showing the advantages of our methodcompared with the state-of-the art on multiple benchmark image-to-imagetranslation datasets.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1812.09877v3'},\n",
       " 742: {'ID': 742,\n",
       "  'title': 'A Hermite-Padé perspective on Gell-Mann--Low renormalization group: an  application to the correlation function of Lieb-Liniger gas',\n",
       "  'authors': ['Maxim Olshanii', 'Vanja Dunjko'],\n",
       "  'published': '2009-10-03T18:47:51Z',\n",
       "  'updated': '2009-10-03T18:47:51Z',\n",
       "  'abstract': \"While Pad\\\\'e approximation is a general method for improving convergence ofseries expansions, Gell-Mann--Low renormalization group normally relies on thepresence of special symmetries. We show that in the single-variable case, thelatter becomes an integral Hermite-Pad\\\\'e approximation, needing no specialsymmetries. It is especially useful for interpolating between expansions forsmall values of a variable and a scaling law of known exponent for largevalues. As an example, we extract the scaling-law prefactor for the one-bodydensity matrix of the Lieb-Liniger gas. Using a new result for the 4th-orderterm in the short-distance expansion, we find a remarkable agreement with knownab initio numerical results.\",\n",
       "  'categories': ['cond-mat.quant-gas'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/0910.0565v1'},\n",
       " 743: {'ID': 743,\n",
       "  'title': 'CCD Photometry of Galactic Globular Clusters V. NGC 2808',\n",
       "  'authors': ['Alistair R. Walker'],\n",
       "  'published': '1999-04-28T13:28:38Z',\n",
       "  'updated': '1999-04-28T13:28:38Z',\n",
       "  'abstract': 'We present a deep color magnitude diagram for the galactic globular clusterNGC 2808 (C0911-646), reaching 3 magnitudes below the main sequence turnoff. Aset of local photometric standards are provided, and the mean cluster reddeningand metallicity are constrained using several photometric techniques. Theunusual horizontal branch morphology discovered by Sosin et al. (1997, ApJ,480, L35) is confirmed. No significant radial population gradients areidentified to within the limits of the data. We investigate the redwards colorspread on the main sequemce found by Ferraro et al. (1997, A&amp;A, 327, 598) in afield centered 6.3 arcmin from the cluster center, attributed to binaries. Weconclude that the spread is in the most part due to a non-Gaussian distributionof photometric errors about the main sequence ridgeline.',\n",
       "  'categories': ['astro-ph'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/astro-ph/9904387v1'},\n",
       " 744: {'ID': 744,\n",
       "  'title': 'Universal Adversarial Training',\n",
       "  'authors': ['Zheng Xu',\n",
       "   'Ali Shafahi',\n",
       "   'John Dickerson',\n",
       "   'Tom Goldstein',\n",
       "   'Larry S. Davis',\n",
       "   'Mahyar Najibi'],\n",
       "  'published': '2018-11-27T23:09:27Z',\n",
       "  'updated': '2019-11-20T20:57:36Z',\n",
       "  'abstract': 'Standard adversarial attacks change the predicted class label of a selectedimage by adding specially tailored small perturbations to its pixels. Incontrast, a universal perturbation is an update that can be added to any imagein a broad class of images, while still changing the predicted class label. Westudy the efficient generation of universal adversarial perturbations, and alsoefficient methods for hardening networks to these attacks. We propose a simpleoptimization-based universal attack that reduces the top-1 accuracy of variousnetwork architectures on ImageNet to less than 20%, while learning theuniversal perturbation 13X faster than the standard method.  To defend against these perturbations, we propose universal adversarialtraining, which models the problem of robust classifier generation as atwo-player min-max game, and produces robust models with only 2X the cost ofnatural training. We also propose a simultaneous stochastic gradient methodthat is almost free of extra computation, which allows us to do universaladversarial training on ImageNet.',\n",
       "  'categories': ['cs.CV', 'cs.CR', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1811.11304v2'},\n",
       " 745: {'ID': 745,\n",
       "  'title': 'Adversarial Privacy-preserving Filter',\n",
       "  'authors': ['Jiaming Zhang',\n",
       "   'Yongli Hu',\n",
       "   'Xian Zhao',\n",
       "   'Xiaowen Huang',\n",
       "   'Jitao Sang',\n",
       "   'Yanfeng Sun'],\n",
       "  'published': '2020-07-25T05:41:00Z',\n",
       "  'updated': '2020-08-04T05:12:11Z',\n",
       "  'abstract': 'While widely adopted in practical applications, face recognition has beencritically discussed regarding the malicious use of face images and thepotential privacy problems, e.g., deceiving payment system and causing personalsabotage. Online photo sharing services unintentionally act as the mainrepository for malicious crawler and face recognition applications. This workaims to develop a privacy-preserving solution, called AdversarialPrivacy-preserving Filter (APF), to protect the online shared face images frombeing maliciously used.We propose an end-cloud collaborated adversarial attacksolution to satisfy requirements of privacy, utility and nonaccessibility.Specifically, the solutions consist of three modules: (1) image-specificgradient generation, to extract image-specific gradient in the user end with acompressed probe model; (2) adversarial gradient transfer, to fine-tune theimage-specific gradient in the server cloud; and (3) universal adversarialperturbation enhancement, to append image-independent perturbation to derivethe final adversarial noise. Extensive experiments on three datasets validatethe effectiveness and efficiency of the proposed solution. A prototypeapplication is also released for further evaluation.We hope the end-cloudcollaborated attack framework could shed light on addressing the issue ofonline multimedia sharing privacy-preserving issues from user side.',\n",
       "  'categories': ['cs.CR', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2007.12861v2'},\n",
       " 746: {'ID': 746,\n",
       "  'title': 'Semi-Supervised Image-to-Image Translation',\n",
       "  'authors': ['Manan Oza', 'Himanshu Vaghela', 'Sudhir Bagul'],\n",
       "  'published': '2019-01-24T03:26:00Z',\n",
       "  'updated': '2019-01-24T03:26:00Z',\n",
       "  'abstract': 'Image-to-image translation is a long-established and a difficult problem incomputer vision. In this paper we propose an adversarial based model forimage-to-image translation. The regular deep neural-network based methodsperform the task of image-to-image translation by comparing gram matrices andusing image segmentation which requires human intervention. Our generativeadversarial network based model works on a conditional probability approach.This approach makes the image translation independent of any local, global andcontent or style features. In our approach we use a bidirectionalreconstruction model appended with the affine transform factor that helps inconserving the content and photorealism as compared to other models. Theadvantage of using such an approach is that the image-to-image translation issemi-supervised, independant of image segmentation and inherits the propertiesof generative adversarial networks tending to produce realistic. This methodhas proven to produce better results than Multimodal UnsupervisedImage-to-image translation.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1901.08212v1'},\n",
       " 747: {'ID': 747,\n",
       "  'title': 'The origin of the LMC stellar bar: clues from the SFH of the bar and  inner disk',\n",
       "  'authors': ['E. J. Bernard',\n",
       "   'C. Gallart',\n",
       "   'L. Monteagudo',\n",
       "   'M. Monelli',\n",
       "   'P. B. Stetson'],\n",
       "  'published': '2017-10-04T11:41:03Z',\n",
       "  'updated': '2017-10-04T11:41:03Z',\n",
       "  'abstract': 'We discuss the origin of the LMC stellar bar by comparing the star formationhistories (SFH) obtained from deep color-magnitude diagrams (CMDs) in the barand in a number of fields in different directions within the inner disk. TheCMDs, reaching the oldest main sequence turnoffs in these very crowded fields,have been obtained with VIMOS on the VLT in service mode, under very goodseeing conditions. We show that the SFHs of all fields share the same patterns,with consistent variations of the star formation rate as a function of time inall of them. We therefore conclude that no specific event of star formation canbe identified with the formation of the LMC bar, which instead likely formedfrom a redistribution of disk material that occurred when the LMC disk becamebar unstable, and shared a common SFH with the inner disk thereafter. Thestrong similarity between the SFH of the center and edge of the bar rules outsignificant spatial variations of the SFH across the bar, which are predictedby scenarios of classic bar formation through buckling mechanisms.',\n",
       "  'categories': ['astro-ph.GA'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1710.01556v1'},\n",
       " 748: {'ID': 748,\n",
       "  'title': 'Improving Generative Visual Dialog by Answering Diverse Questions',\n",
       "  'authors': ['Vishvak Murahari',\n",
       "   'Abhishek Das',\n",
       "   'Prithvijit Chattopadhyay',\n",
       "   'Dhruv Batra',\n",
       "   'Devi Parikh'],\n",
       "  'published': '2019-09-23T16:47:15Z',\n",
       "  'updated': '2019-10-03T03:01:48Z',\n",
       "  'abstract': \"Prior work on training generative Visual Dialog models with reinforcementlearning(Das et al.) has explored a Qbot-Abot image-guessing game and shownthat this 'self-talk' approach can lead to improved performance at thedownstream dialog-conditioned image-guessing task. However, this improvementsaturates and starts degrading after a few rounds of interaction, and does notlead to a better Visual Dialog model. We find that this is due in part torepeated interactions between Qbot and Abot during self-talk, which are notinformative with respect to the image. To improve this, we devise a simpleauxiliary objective that incentivizes Qbot to ask diverse questions, thusreducing repetitions and in turn enabling Abot to explore a larger state spaceduring RL ie. be exposed to more visual concepts to talk about, and variedquestions to answer. We evaluate our approach via a host of automatic metricsand human studies, and demonstrate that it leads to better dialog, ie. dialogthat is more diverse (ie. less repetitive), consistent (ie. has fewerconflicting exchanges), fluent (ie. more human-like),and detailed, while stillbeing comparably image-relevant as prior work and ablations.\",\n",
       "  'categories': ['cs.LG', 'cs.AI', 'cs.CL', 'cs.CV', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1909.10470v2'},\n",
       " 749: {'ID': 749,\n",
       "  'title': 'Group Whitening: Balancing Learning Efficiency and Representational  Capacity',\n",
       "  'authors': ['Fan Zhu', 'Ling Shao', 'Lei Huang', 'Yi Zhou', 'Li Liu'],\n",
       "  'published': '2020-09-28T14:00:07Z',\n",
       "  'updated': '2020-11-24T09:46:16Z',\n",
       "  'abstract': \"Batch normalization (BN) is an important technique commonly incorporated intodeep learning models to perform standardization within mini-batches. The meritsof BN in improving a model's learning efficiency can be further amplified byapplying whitening, while its drawbacks in estimating population statistics forinference can be avoided through group normalization (GN). This paper proposesgroup whitening (GW), which exploits the advantages of the whitening operationand avoids the disadvantages of normalization within mini-batches. In addition,we analyze the constraints imposed on features by normalization, and show howthe batch size (group number) affects the performance of batch (group)normalized networks, from the perspective of model's representational capacity.This analysis provides theoretical guidance for applying GW in practice.Finally, we apply the proposed GW to ResNet and ResNeXt architectures andconduct experiments on the ImageNet and COCO benchmarks. Results show that GWconsistently improves the performance of different architectures, with absolutegains of $1.02\\\\%$ $\\\\sim$ $1.49\\\\%$ in top-1 accuracy on ImageNet and $1.82\\\\%$$\\\\sim$ $3.21\\\\%$ in bounding box AP on COCO.\",\n",
       "  'categories': ['cs.LG', 'cs.CV', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2009.13333v3'},\n",
       " 750: {'ID': 750,\n",
       "  'title': 'Visual Reference Resolution using Attention Memory for Visual Dialog',\n",
       "  'authors': ['Leonid Sigal',\n",
       "   'Paul Hongsuck Seo',\n",
       "   'Andreas Lehrmann',\n",
       "   'Bohyung Han'],\n",
       "  'published': '2017-09-23T02:53:48Z',\n",
       "  'updated': '2018-08-06T21:03:18Z',\n",
       "  'abstract': 'Visual dialog is a task of answering a series of inter-dependent questionsgiven an input image, and often requires to resolve visual references among thequestions. This problem is different from visual question answering (VQA),which relies on spatial attention (a.k.a. visual grounding) estimated from animage and question pair. We propose a novel attention mechanism that exploitsvisual attentions in the past to resolve the current reference in the visualdialog scenario. The proposed model is equipped with an associative attentionmemory storing a sequence of previous (attention, key) pairs. From this memory,the model retrieves the previous attention, taking into account recency, whichis most relevant for the current question, in order to resolve potentiallyambiguous references. The model then merges the retrieved attention with atentative one to obtain the final attention for the current question;specifically, we use dynamic parameter prediction to combine the two attentionsconditioned on the question. Through extensive experiments on a new syntheticvisual dialog dataset, we show that our model significantly outperforms thestate-of-the-art (by ~16 % points) in situations, where visual referenceresolution plays an important role. Moreover, the proposed model achievessuperior performance (~ 2 % points improvement) in the Visual Dialog dataset,despite having significantly fewer parameters than the baselines.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1709.07992v3'},\n",
       " 751: {'ID': 751,\n",
       "  'title': 'Unsupervised Domain-Adaptive Person Re-identification Based on  Attributes',\n",
       "  'authors': ['Vittorio Murino', 'Xiangping Zhu', 'Pietro Morerio'],\n",
       "  'published': '2019-08-27T17:56:05Z',\n",
       "  'updated': '2019-08-27T17:56:05Z',\n",
       "  'abstract': 'Pedestrian attributes, e.g., hair length, clothes type and color, locallydescribe the semantic appearance of a person. Training person re-identification(ReID) algorithms under the supervision of such attributes have proven to beeffective in extracting local features which are important for ReID. Unlikeperson identity, attributes are consistent across different domains (ordatasets). However, most of ReID datasets lack attribute annotations. On theother hand, there are several datasets labeled with sufficient attributes forthe case of pedestrian attribute recognition. Exploiting such data for ReIDpurpose can be a way to alleviate the shortage of attribute annotations in ReIDcase. In this work, an unsupervised domain adaptive ReID feature learningframework is proposed to make full use of attribute annotations. We propose totransfer attribute-related features from their original domain to the ReID one:to this end, we introduce an adversarial discriminative domain adaptationmethod in order to learn domain invariant features for encoding semanticattributes. Experiments on three large-scale datasets validate theeffectiveness of the proposed ReID framework.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1908.10359v1'},\n",
       " 752: {'ID': 752,\n",
       "  'title': 'Deep Convolutional Encoder-Decoders with Aggregated Multi-Resolution  Skip Connections for Skin Lesion Segmentation',\n",
       "  'authors': ['Ahmed H. Shahin', 'Karim Amer', 'Mustafa A. Elattar'],\n",
       "  'published': '2019-01-26T10:55:27Z',\n",
       "  'updated': '2019-04-04T13:27:37Z',\n",
       "  'abstract': 'The prevalence of skin melanoma is rapidly increasing as well as the recordeddeath cases of its patients. Automatic image segmentation tools play animportant role in providing standardized computer-assisted analysis for skinmelanoma patients. Current state-of-the-art segmentation methods are based onfully convolutional neural networks, which utilize an encoder-decoder approach.However, these methods produce coarse segmentation masks due to the loss oflocation information during the encoding layers. Inspired by Pyramid SceneParsing Network (PSP-Net), we propose an encoder-decoder model that utilizespyramid pooling modules in the deep skip connections which aggregate the globalcontext and compensate for the lost spatial information. We trained andvalidated our approach using ISIC 2018: Skin Lesion Analysis Towards MelanomaDetection grand challenge dataset. Our approach showed a validation accuracywith a Jaccard index of 0.837, which outperforms U-Net. We believe that withthis reported reliable accuracy, this method can be introduced for clinicalpractice.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1901.09197v2'},\n",
       " 753: {'ID': 753,\n",
       "  'title': 'Cut-and-Paste Neural Rendering',\n",
       "  'authors': ['David A. Forsyth', 'Anand Bhattad'],\n",
       "  'published': '2020-10-12T17:59:55Z',\n",
       "  'updated': '2020-10-12T17:59:55Z',\n",
       "  'abstract': \"Cut-and-paste methods take an object from one image and insert it intoanother. Doing so often results in unrealistic looking images because theinserted object's shading is inconsistent with the target scene's shading.Existing reshading methods require a geometric and physical model of theinserted object, which is then rendered using environment parameters.Accurately constructing such a model only from a single image is beyond thecurrent understanding of computer vision. We describe an alternative procedure-- cut-and-paste neural rendering, to render the inserted fragment's shadingfield consistent with the target scene. We use a Deep Image Prior (DIP) as aneural renderer trained to render an image with consistent image decompositioninferences. The resulting rendering from DIP should have an albedo consistentwith composite albedo; it should have a shading field that, outside theinserted fragment, is the same as the target scene's shading field; andcomposite surface normals are consistent with the final rendering's shadingfield. The result is a simple procedure that produces convincing and realisticshading. Moreover, our procedure does not require rendered images orimage-decomposition from real images in the training or labeled annotations. Infact, our only use of simulated ground truth is our use of a pre-trained normalestimator. Qualitative results are strong, supported by a user study comparingagainst the state-of-the-art image harmonization baseline.\",\n",
       "  'categories': ['cs.CV', 'cs.GR', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2010.05907v1'},\n",
       " 754: {'ID': 754,\n",
       "  'title': 'Bilinear Supervised Hashing Based on 2D Image Features',\n",
       "  'authors': ['Yujuan Ding', 'Zheng Zhang', 'Zhihui Lai', 'Wai Kueng Wong'],\n",
       "  'published': '2019-01-05T23:39:27Z',\n",
       "  'updated': '2019-01-05T23:39:27Z',\n",
       "  'abstract': 'Hashing has been recognized as an efficient representation learning method toeffectively handle big data due to its low computational complexity and memorycost. Most of the existing hashing methods focus on learning thelow-dimensional vectorized binary features based on the high-dimensional rawvectorized features. However, studies on how to obtain preferable binary codesfrom the original 2D image features for retrieval is very limited. This paperproposes a bilinear supervised discrete hashing (BSDH) method based on 2D imagefeatures which utilizes bilinear projections to binarize the image matrixfeatures such that the intrinsic characteristics in the 2D image space arepreserved in the learned binary codes. Meanwhile, the bilinear projectionapproximation and vectorization binary codes regression are seamlesslyintegrated together to formulate the final robust learning framework.Furthermore, a discrete optimization strategy is developed to alternativelyupdate each variable for obtaining the high-quality binary codes. In addition,two 2D image features, traditional SURF-based FVLAD feature and CNN-basedAlexConv5 feature are designed for further improving the performance of theproposed BSDH method. Results of extensive experiments conducted on fourbenchmark datasets show that the proposed BSDH method almost outperforms allcompeting hashing methods with different input features by different evaluationprotocols.',\n",
       "  'categories': ['cs.CV', 'cs.MM'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1901.01474v1'},\n",
       " 755: {'ID': 755,\n",
       "  'title': 'SeNA-CNN: Overcoming Catastrophic Forgetting in Convolutional Neural  Networks by Selective Network Augmentation',\n",
       "  'authors': ['Luís A. Alexandre', 'Abel S. Zacarias'],\n",
       "  'published': '2018-02-22T10:23:36Z',\n",
       "  'updated': '2018-05-09T08:59:04Z',\n",
       "  'abstract': 'Lifelong learning aims to develop machine learning systems that can learn newtasks while preserving the performance on previous learned tasks. In this paperwe present a method to overcome catastrophic forgetting on convolutional neuralnetworks, that learns new tasks and preserves the performance on old taskswithout accessing the data of the original model, by selective networkaugmentation. The experiment results showed that SeNA-CNN, in some scenarios,outperforms the state-of-art Learning without Forgetting algorithm. Resultsalso showed that in some situations it is better to use SeNA-CNN instead oftraining a neural network using isolated learning.',\n",
       "  'categories': ['cs.LG', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1802.08250v2'},\n",
       " 756: {'ID': 756,\n",
       "  'title': 'Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial  Networks',\n",
       "  'authors': ['Alexei A. Efros',\n",
       "   'Taesung Park',\n",
       "   'Phillip Isola',\n",
       "   'Jun-Yan Zhu'],\n",
       "  'published': '2017-03-30T17:44:17Z',\n",
       "  'updated': '2020-08-24T16:51:03Z',\n",
       "  'abstract': 'Image-to-image translation is a class of vision and graphics problems wherethe goal is to learn the mapping between an input image and an output imageusing a training set of aligned image pairs. However, for many tasks, pairedtraining data will not be available. We present an approach for learning totranslate an image from a source domain $X$ to a target domain $Y$ in theabsence of paired examples. Our goal is to learn a mapping $G: X \\\\rightarrow Y$such that the distribution of images from $G(X)$ is indistinguishable from thedistribution $Y$ using an adversarial loss. Because this mapping is highlyunder-constrained, we couple it with an inverse mapping $F: Y \\\\rightarrow X$and introduce a cycle consistency loss to push $F(G(X)) \\\\approx X$ (and viceversa). Qualitative results are presented on several tasks where pairedtraining data does not exist, including collection style transfer, objecttransfiguration, season transfer, photo enhancement, etc. Quantitativecomparisons against several prior methods demonstrate the superiority of ourapproach.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1703.10593v7'},\n",
       " 757: {'ID': 757,\n",
       "  'title': 'Blind Image Restoration with Flow Based Priors',\n",
       "  'authors': ['Leonhard Helminger',\n",
       "   'Michael Bernasconi',\n",
       "   'Markus Gross',\n",
       "   'Abdelaziz Djelouah',\n",
       "   'Christopher Schroers'],\n",
       "  'published': '2020-09-09T21:40:11Z',\n",
       "  'updated': '2020-09-09T21:40:11Z',\n",
       "  'abstract': 'Image restoration has seen great progress in the last years thanks to theadvances in deep neural networks. Most of these existing techniques are trainedusing full supervision with suitable image pairs to tackle a specificdegradation. However, in a blind setting with unknown degradations this is notpossible and a good prior remains crucial. Recently, neural network basedapproaches have been proposed to model such priors by leveraging eitherdenoising autoencoders or the implicit regularization captured by the neuralnetwork structure itself. In contrast to this, we propose using normalizingflows to model the distribution of the target content and to use this as aprior in a maximum a posteriori (MAP) formulation. By expressing the MAPoptimization process in the latent space through the learned bijective mapping,we are able to obtain solutions through gradient descent. To the best of ourknowledge, this is the first work that explores normalizing flows as prior inimage enhancement problems. Furthermore, we present experimental results for anumber of different degradations on data sets varying in complexity and showcompetitive results when comparing with the deep image prior approach.',\n",
       "  'categories': ['eess.IV', 'cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2009.04583v1'},\n",
       " 758: {'ID': 758,\n",
       "  'title': 'Deep Image Prior for Sparse-sampling Photoacoustic Microscopy',\n",
       "  'authors': ['Xiaoyi Zhu',\n",
       "   'Zixuan Zhang',\n",
       "   'Roarke Horstmeyer',\n",
       "   'Junjie Yao',\n",
       "   'Maomao Chen',\n",
       "   'Daiwei Li',\n",
       "   'Dong Zhang',\n",
       "   'Yu Shrike Zhang',\n",
       "   'Jianwen Luo',\n",
       "   'Tri Vu',\n",
       "   'Anthony DiSpirito III'],\n",
       "  'published': '2020-10-15T15:46:19Z',\n",
       "  'updated': '2020-10-15T15:46:19Z',\n",
       "  'abstract': 'Photoacoustic microscopy (PAM) is an emerging method for imaging bothstructural and functional information without the need for exogenous contrastagents. However, state-of-the-art PAM faces a tradeoff between imaging speedand spatial sampling density within the same field-of-view (FOV). Limited bythe pulsed laser\\'s repetition rate, the imaging speed is inversely proportionalto the total number of effective pixels. To cover the same FOV in a shorteramount of time with the same PAM hardware, there is currently no other optionthan to decrease spatial sampling density (i.e., sparse sampling). Deeplearning methods have recently been used to improve sparsely sampled PAMimages; however, these methods often require time-consuming pre-training and alarge training dataset that has fully sampled, co-registered ground truth. Inthis paper, we propose using a method known as \"deep image prior\" to improvethe image quality of sparsely sampled PAM images. The network does not needprior learning or fully sampled ground truth, making its implementation moreflexible and much quicker. Our results show promising improvement in PAvasculature images with as few as 2% of the effective pixels. Our deep imageprior approach produces results that outperform interpolation methods and canbe readily translated to other high-speed, sparse-sampling imaging modalities.',\n",
       "  'categories': ['eess.IV', 'cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2010.12041v1'},\n",
       " 759: {'ID': 759,\n",
       "  'title': 'Dense Extreme Inception Network: Towards a Robust CNN Model for Edge  Detection',\n",
       "  'authors': ['Angel D. Sappa', 'Xavier Soria', 'Edgar Riba'],\n",
       "  'published': '2019-09-04T17:27:46Z',\n",
       "  'updated': '2020-02-04T00:08:24Z',\n",
       "  'abstract': 'This paper proposes a Deep Learning based edge detector, which is inspired onboth HED (Holistically-Nested Edge Detection) and Xception networks. Theproposed approach generates thin edge-maps that are plausible for human eyes;it can be used in any edge detection task without previous training or finetuning process. As a second contribution, a large dataset with carefullyannotated edges has been generated. This dataset has been used for training theproposed approach as well the state-of-the-art algorithms for comparisons.Quantitative and qualitative evaluations have been performed on differentbenchmarks showing improvements with the proposed method when F-measure of ODSand OIS are considered.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1909.01955v2'},\n",
       " 760: {'ID': 760,\n",
       "  'title': 'Learning Deep Image Priors for Blind Image Denoising',\n",
       "  'authors': ['Guoping Qiu',\n",
       "   'Ke Sun',\n",
       "   'Jingxin Liu',\n",
       "   'Bolei Xu',\n",
       "   'Bozhi Liu',\n",
       "   'Xianxu Hou',\n",
       "   'Yuanhao Gong',\n",
       "   'Hongming Luo'],\n",
       "  'published': '2019-06-04T08:16:01Z',\n",
       "  'updated': '2019-06-04T08:16:01Z',\n",
       "  'abstract': 'Image denoising is the process of removing noise from noisy images, which isan image domain transferring task, i.e., from a single or several noise leveldomains to a photo-realistic domain. In this paper, we propose an effectiveimage denoising method by learning two image priors from the perspective ofdomain alignment. We tackle the domain alignment on two levels. 1) thefeature-level prior is to learn domain-invariant features for corrupted imageswith different level noise; 2) the pixel-level prior is used to push thedenoised images to the natural image manifold. The two image priors are basedon $\\\\mathcal{H}$-divergence theory and implemented by learning classifiers inadversarial training manners. We evaluate our approach on multiple datasets.The results demonstrate the effectiveness of our approach for robust imagedenoising on both synthetic and real-world noisy images. Furthermore, we showthat the feature-level prior is capable of alleviating the discrepancy betweendifferent level noise. It can be used to improve the blind denoisingperformance in terms of distortion measures (PSNR and SSIM), while pixel-levelprior can effectively improve the perceptual quality to ensure the realisticoutputs, which is further validated by subjective evaluation.',\n",
       "  'categories': ['eess.IV', 'cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1906.01259v1'},\n",
       " 761: {'ID': 761,\n",
       "  'title': 'Granular Multimodal Attention Networks for Visual Dialog',\n",
       "  'authors': ['Badri N. Patro', 'Shivansh Patel', 'Vinay P. Namboodiri'],\n",
       "  'published': '2019-10-13T10:49:41Z',\n",
       "  'updated': '2019-10-13T10:49:41Z',\n",
       "  'abstract': 'Vision and language tasks have benefited from attention. There have been anumber of different attention models proposed. However, the scale at whichattention needs to be applied has not been well examined. Particularly, in thiswork, we propose a new method Granular Multi-modal Attention, where we aim toparticularly address the question of the right granularity at which one needsto attend while solving the Visual Dialog task. The proposed method showsimprovement in both image and text attention networks. We then propose agranular Multi-modal Attention network that jointly attends on the image andtext granules and shows the best performance. With this work, we observe thatobtaining granular attention and doing exhaustive Multi-modal Attention appearsto be the best way to attend while solving visual dialog.',\n",
       "  'categories': ['cs.CV', 'cs.CL', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1910.05728v1'},\n",
       " 762: {'ID': 762,\n",
       "  'title': 'An Efficient Accelerator Design Methodology for Deformable Convolutional  Networks',\n",
       "  'authors': ['Jung-Woo Chang', 'Saehyun Ahn', 'Suk-Ju Kang'],\n",
       "  'published': '2020-06-09T13:16:44Z',\n",
       "  'updated': '2020-06-13T10:40:25Z',\n",
       "  'abstract': 'Deformable convolutional networks have demonstrated outstanding performancein object recognition tasks with an effective feature extraction. Unlikestandard convolution, the deformable convolution decides the receptive fieldsize using dynamically generated offsets, which leads to an irregular memoryaccess. Especially, the memory access pattern varies both spatially andtemporally, making static optimization ineffective. Thus, a naiveimplementation would lead to an excessive memory footprint. In this paper, wepresent a novel approach to accelerate deformable convolution on FPGA. First,we propose a novel training method to reduce the size of the receptive field inthe deformable convolutional layer without compromising accuracy. By optimizingthe receptive field, we can compress the maximum size of the receptive field by12.6 times. Second, we propose an efficient systolic architecture to maximizeits efficiency. We then implement our design on FPGA to support the optimizeddataflow. Experimental results show that our accelerator achieves up to 17.25times speedup over the state-of-the-art accelerator.',\n",
       "  'categories': ['cs.DC', 'cs.NE'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2006.05238v2'},\n",
       " 763: {'ID': 763,\n",
       "  'title': 'TAB-VCR: Tags and Attributes based Visual Commonsense Reasoning  Baselines',\n",
       "  'authors': ['Unnat Jain', 'Jingxiang Lin', 'Alexander G. Schwing'],\n",
       "  'published': '2019-10-31T17:59:57Z',\n",
       "  'updated': '2020-01-09T15:55:26Z',\n",
       "  'abstract': 'Reasoning is an important ability that we learn from a very early age. Yet,reasoning is extremely hard for algorithms. Despite impressive recent progressthat has been reported on tasks that necessitate reasoning, such as visualquestion answering and visual dialog, models often exploit biases in datasets.To develop models with better reasoning abilities, recently, the new visualcommonsense reasoning (VCR) task has been introduced. Not only do models haveto answer questions, but also do they have to provide a reason for the givenanswer. The proposed baseline achieved compelling results, leveraging ameticulously designed model composed of LSTM modules and attention nets. Herewe show that a much simpler model obtained by ablating and pruning the existingintricate baseline can perform better with half the number of trainableparameters. By associating visual features with attribute information andbetter text to image grounding, we obtain further improvements for our simpler&amp; effective baseline, TAB-VCR. We show that this approach results in a 5.3%,4.4% and 6.5% absolute improvement over the previous state-of-the-art onquestion answering, answer justification and holistic VCR.',\n",
       "  'categories': ['cs.CV', 'cs.CL', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1910.14671v2'},\n",
       " 764: {'ID': 764,\n",
       "  'title': 'Zero-Shot Learning -- A Comprehensive Evaluation of the Good, the Bad  and the Ugly',\n",
       "  'authors': ['Bernt Schiele',\n",
       "   'Christoph H. Lampert',\n",
       "   'Yongqin Xian',\n",
       "   'Zeynep Akata'],\n",
       "  'published': '2017-07-03T15:41:41Z',\n",
       "  'updated': '2020-09-23T15:02:08Z',\n",
       "  'abstract': 'Due to the importance of zero-shot learning, i.e. classifying images wherethere is a lack of labeled training data, the number of proposed approaches hasrecently increased steadily. We argue that it is time to take a step back andto analyze the status quo of the area. The purpose of this paper is three-fold.First, given the fact that there is no agreed upon zero-shot learningbenchmark, we first define a new benchmark by unifying both the evaluationprotocols and data splits of publicly available datasets used for this task.This is an important contribution as published results are often not comparableand sometimes even flawed due to, e.g. pre-training on zero-shot test classes.Moreover, we propose a new zero-shot learning dataset, the Animals withAttributes 2 (AWA2) dataset which we make publicly available both in terms ofimage features and the images themselves. Second, we compare and analyze asignificant number of the state-of-the-art methods in depth, both in theclassic zero-shot setting but also in the more realistic generalized zero-shotsetting. Finally, we discuss in detail the limitations of the current status ofthe area which can be taken as a basis for advancing it.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1707.00600v4'},\n",
       " 765: {'ID': 765,\n",
       "  'title': 'U-Net Training with Instance-Layer Normalization',\n",
       "  'authors': ['Xiao-Yun Zhou',\n",
       "   'Zhao-Yang Wang',\n",
       "   'Guang-Zhong Yang',\n",
       "   'Peichao Li'],\n",
       "  'published': '2019-08-21T11:24:25Z',\n",
       "  'updated': '2019-08-25T14:18:15Z',\n",
       "  'abstract': 'Normalization layers are essential in a Deep Convolutional Neural Network(DCNN). Various normalization methods have been proposed. The statistics usedto normalize the feature maps can be computed at batch, channel, or instancelevel. However, in most of existing methods, the normalization for each layeris fixed. Batch-Instance Normalization (BIN) is one of the first proposedmethods that combines two different normalization methods and achieve diversenormalization for different layers. However, two potential issues exist in BIN:first, the Clip function is not differentiable at input values of 0 and 1;second, the combined feature map is not with a normalized distribution which isharmful for signal propagation in DCNN. In this paper, an Instance-LayerNormalization (ILN) layer is proposed by using the Sigmoid function for thefeature map combination, and cascading group normalization. The performance ofILN is validated on image segmentation of the Right Ventricle (RV) and LeftVentricle (LV) using U-Net as the network architecture. The results show thatthe proposed ILN outperforms previous traditional and popular normalizationmethods with noticeable accuracy improvements for most validations, supportingthe effectiveness of the proposed ILN.',\n",
       "  'categories': ['eess.IV', 'cs.CV', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1908.08466v2'},\n",
       " 766: {'ID': 766,\n",
       "  'title': 'Differentiable Learning-to-Normalize via Switchable Normalization',\n",
       "  'authors': ['Ruimao Zhang',\n",
       "   'Zhanglin Peng',\n",
       "   'Ping Luo',\n",
       "   'Jingyu Li',\n",
       "   'Jiamin Ren'],\n",
       "  'published': '2018-06-28T05:55:57Z',\n",
       "  'updated': '2019-04-24T05:31:23Z',\n",
       "  'abstract': 'We address a learning-to-normalize problem by proposing SwitchableNormalization (SN), which learns to select different normalizers for differentnormalization layers of a deep neural network. SN employs three distinct scopesto compute statistics (means and variances) including a channel, a layer, and aminibatch. SN switches between them by learning their importance weights in anend-to-end manner. It has several good properties. First, it adapts to variousnetwork architectures and tasks (see Fig.1). Second, it is robust to a widerange of batch sizes, maintaining high performance even when small minibatch ispresented (e.g. 2 images/GPU). Third, SN does not have sensitivehyper-parameter, unlike group normalization that searches the number of groupsas a hyper-parameter. Without bells and whistles, SN outperforms itscounterparts on various challenging benchmarks, such as ImageNet, COCO,CityScapes, ADE20K, and Kinetics. Analyses of SN are also presented. We hope SNwill help ease the usage and understand the normalization techniques in deeplearning. The code of SN has been made available inhttps://github.com/switchablenorms/.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1806.10779v5'},\n",
       " 767: {'ID': 767,\n",
       "  'title': 'Crowd Counting on Images with Scale Variation and Isolated Clusters',\n",
       "  'authors': ['Haoyue Bai', 'S. -H. Gary Chan', 'Song Wen'],\n",
       "  'published': '2019-09-09T13:17:26Z',\n",
       "  'updated': '2019-09-09T13:17:26Z',\n",
       "  'abstract': 'Crowd counting is to estimate the number of objects (e.g., people orvehicles) in an image of unconstrained congested scenes. Designing a generalcrowd counting algorithm applicable to a wide range of crowd images ischallenging, mainly due to the possibly large variation in object scales andthe presence of many isolated small clusters. Previous approaches based onconvolution operations with multi-branch architecture are effective for onlysome narrow bands of scales and have not captured the long-range contextualrelationship due to isolated clustering. To address that, we propose SACANet, anovel scale-adaptive long-range context-aware network for crowd counting.SACANet consists of three major modules: the pyramid contextual module whichextracts long-range contextual information and enlarges the receptive field, ascale-adaptive self-attention multi-branch module to attain high scalesensitivity and detection accuracy of isolated clusters, and a hierarchicalfusion module to fuse multi-level self-attention features. With groupnormalization, SACANet achieves better optimality in the training process. Wehave conducted extensive experiments using the VisDrone2019 People dataset, theVisDrone2019 Vehicle dataset, and some other challenging benchmarks. Ascompared with the state-of-the-art methods, SACANet is shown to be effective,especially for extremely crowded conditions with diverse scales and scatteredclusters, and achieves much lower MAE as compared with baselines.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1909.03839v1'},\n",
       " 768: {'ID': 768,\n",
       "  'title': 'Assessment of Breast Cancer Histology using Densely Connected  Convolutional Networks',\n",
       "  'authors': ['Maximilian Baust',\n",
       "   'Stefan Braunewell',\n",
       "   'Florian Ludwig',\n",
       "   'Matthias Kohl',\n",
       "   'Christoph Walz'],\n",
       "  'published': '2018-04-09T19:40:02Z',\n",
       "  'updated': '2018-04-09T19:40:02Z',\n",
       "  'abstract': 'Breast cancer is the most frequently diagnosed cancer and leading cause ofcancer-related death among females worldwide. In this article, we investigatethe applicability of densely connected convolutional neural networks to theproblems of histology image classification and whole slide image segmentationin the area of computer-aided diagnoses for breast cancer. To this end, westudy various approaches for transfer learning and apply them to the data setfrom the 2018 grand challenge on breast cancer histology images (BACH).',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1804.04595v1'},\n",
       " 769: {'ID': 769,\n",
       "  'title': 'Neural Module Networks for Reasoning over Text',\n",
       "  'authors': ['Nitish Gupta',\n",
       "   'Sameer Singh',\n",
       "   'Matt Gardner',\n",
       "   'Kevin Lin',\n",
       "   'Dan Roth'],\n",
       "  'published': '2019-12-10T20:36:07Z',\n",
       "  'updated': '2020-02-15T19:26:05Z',\n",
       "  'abstract': 'Answering compositional questions that require multiple steps of reasoningagainst text is challenging, especially when they involve discrete, symbolicoperations. Neural module networks (NMNs) learn to parse such questions asexecutable programs composed of learnable modules, performing well on syntheticvisual QA domains. However, we find that it is challenging to learn thesemodels for non-synthetic questions on open-domain text, where a model needs todeal with the diversity of natural language and perform a broader range ofreasoning. We extend NMNs by: (a) introducing modules that reason over aparagraph of text, performing symbolic reasoning (such as arithmetic, sorting,counting) over numbers and dates in a probabilistic and differentiable manner;and (b) proposing an unsupervised auxiliary loss to help extract argumentsassociated with the events in text. Additionally, we show that a limited amountof heuristically-obtained question program and intermediate module outputsupervision provides sufficient inductive bias for accurate learning. Ourproposed model significantly outperforms state-of-the-art models on a subset ofthe DROP dataset that poses a variety of reasoning challenges that are coveredby our modules.',\n",
       "  'categories': ['cs.CL'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1912.04971v2'},\n",
       " 770: {'ID': 770,\n",
       "  'title': 'Analysis of Dominant Classes in Universal Adversarial Perturbations',\n",
       "  'authors': ['Roberto Santana', 'Jon Vadillo', 'Jose A. Lozano'],\n",
       "  'published': '2020-12-28T16:42:46Z',\n",
       "  'updated': '2021-01-11T10:16:30Z',\n",
       "  'abstract': 'The reasons why Deep Neural Networks are susceptible to being fooled byadversarial examples remains an open discussion. Indeed, many differentstrategies can be employed to efficiently generate adversarial attacks, some ofthem relying on different theoretical justifications. Among these strategies,universal (input-agnostic) perturbations are of particular interest, due totheir capability to fool a network independently of the input in which theperturbation is applied. In this work, we investigate an intriguing phenomenonof universal perturbations, which has been reported previously in theliterature, yet without a proven justification: universal perturbations changethe predicted classes for most inputs into one particular (dominant) class,even if this behavior is not specified during the creation of the perturbation.In order to justify the cause of this phenomenon, we propose a number ofhypotheses and experimentally test them using a speech command classificationproblem in the audio domain as a testbed. Our analyses reveal interestingproperties of universal perturbations, suggest new methods to generate suchattacks and provide an explanation of dominant classes, under both a geometricand a data-feature perspective.',\n",
       "  'categories': ['cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2012.14352v2'},\n",
       " 771: {'ID': 771,\n",
       "  'title': 'Multi-View Intact Space Learning',\n",
       "  'authors': ['Chao Xu', 'Dacheng Tao', 'Chang Xu'],\n",
       "  'published': '2019-04-04T04:04:57Z',\n",
       "  'updated': '2019-04-04T04:04:57Z',\n",
       "  'abstract': 'It is practical to assume that an individual view is unlikely to besufficient for effective multi-view learning. Therefore, integration ofmulti-view information is both valuable and necessary. In this paper, wepropose the Multi-view Intact Space Learning (MISL) algorithm, which integratesthe encoded complementary information in multiple views to discover a latentintact representation of the data. Even though each view on its own isinsufficient, we show theoretically that by combing multiple views we canobtain abundant information for latent intact space learning. Employing theCauchy loss (a technique used in statistical learning) as the error measurementstrengthens robustness to outliers. We propose a new definition of multi-viewstability and then derive the generalization error bound based on multi-viewstability and Rademacher complexity, and show that the complementarity betweenmultiple views is beneficial for the stability and generalization. MISL isefficiently optimized using a novel Iteratively Reweight Residuals (IRR)technique, whose convergence is theoretically analyzed. Experiments onsynthetic data and real-world datasets demonstrate that MISL is an effectiveand promising algorithm for practical applications.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1904.02340v1'},\n",
       " 772: {'ID': 772,\n",
       "  'title': 'ASLFeat: Learning Local Features of Accurate Shape and Localization',\n",
       "  'authors': ['Shiwei Li',\n",
       "   'Lei Zhou',\n",
       "   'Tian Fang',\n",
       "   'Xuyang Bai',\n",
       "   'Hongkai Chen',\n",
       "   'Zixin Luo',\n",
       "   'Jiahui Zhang',\n",
       "   'Yao Yao',\n",
       "   'Long Quan'],\n",
       "  'published': '2020-03-23T04:03:03Z',\n",
       "  'updated': '2020-04-19T12:47:53Z',\n",
       "  'abstract': 'This work focuses on mitigating two limitations in the joint learning oflocal feature detectors and descriptors. First, the ability to estimate thelocal shape (scale, orientation, etc.) of feature points is often neglectedduring dense feature extraction, while the shape-awareness is crucial toacquire stronger geometric invariance. Second, the localization accuracy ofdetected keypoints is not sufficient to reliably recover camera geometry, whichhas become the bottleneck in tasks such as 3D reconstruction. In this paper, wepresent ASLFeat, with three light-weight yet effective modifications tomitigate above issues. First, we resort to deformable convolutional networks todensely estimate and apply local transformation. Second, we take advantage ofthe inherent feature hierarchy to restore spatial resolution and low-leveldetails for accurate keypoint localization. Finally, we use a peakinessmeasurement to relate feature responses and derive more indicative detectionscores. The effect of each modification is thoroughly studied, and theevaluation is extensively conducted across a variety of practical scenarios.State-of-the-art results are reported that demonstrate the superiority of ourmethods.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2003.10071v2'},\n",
       " 773: {'ID': 773,\n",
       "  'title': 'Long-term Recurrent Convolutional Networks for Visual Recognition and  Description',\n",
       "  'authors': ['Subhashini Venugopalan',\n",
       "   'Jeff Donahue',\n",
       "   'Trevor Darrell',\n",
       "   'Kate Saenko',\n",
       "   'Sergio Guadarrama',\n",
       "   'Marcus Rohrbach',\n",
       "   'Lisa Anne Hendricks'],\n",
       "  'published': '2014-11-17T08:25:17Z',\n",
       "  'updated': '2016-05-31T22:57:33Z',\n",
       "  'abstract': 'Models based on deep convolutional networks have dominated recent imageinterpretation tasks; we investigate whether models which are also recurrent,or \"temporally deep\", are effective for tasks involving sequences, visual andotherwise. We develop a novel recurrent convolutional architecture suitable forlarge-scale visual learning which is end-to-end trainable, and demonstrate thevalue of these models on benchmark video recognition tasks, image descriptionand retrieval problems, and video narration challenges. In contrast to currentmodels which assume a fixed spatio-temporal receptive field or simple temporalaveraging for sequential processing, recurrent convolutional models are \"doublydeep\"\\' in that they can be compositional in spatial and temporal \"layers\". Suchmodels may have advantages when target concepts are complex and/or trainingdata are limited. Learning long-term dependencies is possible whennonlinearities are incorporated into the network state updates. Long-term RNNmodels are appealing in that they directly can map variable-length inputs(e.g., video frames) to variable length outputs (e.g., natural language text)and can model complex temporal dynamics; yet they can be optimized withbackpropagation. Our recurrent long-term models are directly connected tomodern visual convnet models and can be jointly trained to simultaneously learntemporal dynamics and convolutional perceptual representations. Our resultsshow such models have distinct advantages over state-of-the-art models forrecognition or generation which are separately defined and/or optimized.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1411.4389v4'},\n",
       " 774: {'ID': 774,\n",
       "  'title': 'LevelSet R-CNN: A Deep Variational Method for Instance Segmentation',\n",
       "  'authors': ['Raquel Urtasun',\n",
       "   'Justin Liang',\n",
       "   'Yuwen Xiong',\n",
       "   'Namdar Homayounfar',\n",
       "   'Wei-Chiu Ma'],\n",
       "  'published': '2020-07-30T17:52:18Z',\n",
       "  'updated': '2020-07-30T17:52:18Z',\n",
       "  'abstract': 'Obtaining precise instance segmentation masks is of high importance in manymodern applications such as robotic manipulation and autonomous driving.Currently, many state of the art models are based on the Mask R-CNN frameworkwhich, while very powerful, outputs masks at low resolutions which could resultin imprecise boundaries. On the other hand, classic variational methods forsegmentation impose desirable global and local data and geometry constraints onthe masks by optimizing an energy functional. While mathematically elegant,their direct dependence on good initialization, non-robust image cues andmanual setting of hyperparameters renders them unsuitable for modernapplications. We propose LevelSet R-CNN, which combines the best of both worldsby obtaining powerful feature representations that are combined in anend-to-end manner with a variational segmentation framework. We demonstrate theeffectiveness of our approach on COCO and Cityscapes datasets.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2007.15629v1'},\n",
       " 775: {'ID': 775,\n",
       "  'title': 'Learning Sight from Sound: Ambient Sound Provides Supervision for Visual  Learning',\n",
       "  'authors': ['Josh H. McDermott',\n",
       "   'Jiajun Wu',\n",
       "   'Antonio Torralba',\n",
       "   'Andrew Owens',\n",
       "   'William T. Freeman'],\n",
       "  'published': '2017-12-20T00:10:40Z',\n",
       "  'updated': '2017-12-20T00:10:40Z',\n",
       "  'abstract': 'The sound of crashing waves, the roar of fast-moving cars -- sound conveysimportant information about the objects in our surroundings. In this work, weshow that ambient sounds can be used as a supervisory signal for learningvisual models. To demonstrate this, we train a convolutional neural network topredict a statistical summary of the sound associated with a video frame. Weshow that, through this process, the network learns a representation thatconveys information about objects and scenes. We evaluate this representationon several recognition tasks, finding that its performance is comparable tothat of other state-of-the-art unsupervised learning methods. Finally, we showthrough visualizations that the network learns units that are selective toobjects that are often associated with characteristic sounds. This paperextends an earlier conference paper, Owens et al. 2016, with additionalexperiments and discussion.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1712.07271v1'},\n",
       " 776: {'ID': 776,\n",
       "  'title': 'Where Are You? Localization from Embodied Dialog',\n",
       "  'authors': ['James M. Rehg',\n",
       "   'Peter Anderson',\n",
       "   'Meera Hahn',\n",
       "   'Jacob Krantz',\n",
       "   'Stefan Lee',\n",
       "   'Dhruv Batra',\n",
       "   'Devi Parikh'],\n",
       "  'published': '2020-11-16T21:09:43Z',\n",
       "  'updated': '2020-11-16T21:09:43Z',\n",
       "  'abstract': \"We present Where Are You? (WAY), a dataset of ~6k dialogs in which two humans-- an Observer and a Locator -- complete a cooperative localization task. TheObserver is spawned at random in a 3D environment and can navigate fromfirst-person views while answering questions from the Locator. The Locator mustlocalize the Observer in a detailed top-down map by asking questions and givinginstructions. Based on this dataset, we define three challenging tasks:Localization from Embodied Dialog or LED (localizing the Observer from dialoghistory), Embodied Visual Dialog (modeling the Observer), and CooperativeLocalization (modeling both agents). In this paper, we focus on the LED task --providing a strong baseline model with detailed ablations characterizing bothdataset biases and the importance of various modeling choices. Our best modelachieves 32.7% success at identifying the Observer's location within 3m inunseen buildings, vs. 70.4% for human Locators.\",\n",
       "  'categories': ['cs.CV', 'cs.CL'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2011.08277v1'},\n",
       " 777: {'ID': 777,\n",
       "  'title': 'Team RUC_AIM3 Technical Report at Activitynet 2020 Task 2: Exploring  Sequential Events Detection for Dense Video Captioning',\n",
       "  'authors': ['Yuqing Song', 'Yida Zhao', 'Qin Jin', 'Shizhe Chen'],\n",
       "  'published': '2020-06-14T13:21:37Z',\n",
       "  'updated': '2020-06-14T13:21:37Z',\n",
       "  'abstract': 'Detecting meaningful events in an untrimmed video is essential for densevideo captioning. In this work, we propose a novel and simple model for eventsequence generation and explore temporal relationships of the event sequence inthe video. The proposed model omits inefficient two-stage proposal generationand directly generates event boundaries conditioned on bi-directional temporaldependency in one pass. Experimental results show that the proposed eventsequence generation model can generate more accurate and diverse events withina small number of proposals. For the event captioning, we follow our previouswork to employ the intra-event captioning models into our pipeline system. Theoverall system achieves state-of-the-art performance on the dense-captioningevents in video task with 9.894 METEOR score on the challenge testing set.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2006.07896v1'},\n",
       " 778: {'ID': 778,\n",
       "  'title': 'Improving Fair Predictions Using Variational Inference In Causal Models',\n",
       "  'authors': ['Patrick Forré', 'Christos Louizos', 'Rik Helwegen'],\n",
       "  'published': '2020-08-25T08:27:11Z',\n",
       "  'updated': '2020-08-25T08:27:11Z',\n",
       "  'abstract': \"The importance of algorithmic fairness grows with the increasing impactmachine learning has on people's lives. Recent work on fairness metrics showsthe need for causal reasoning in fairness constraints. In this work, apractical method named FairTrade is proposed for creating flexible predictionmodels which integrate fairness constraints on sensitive causal paths. Themethod uses recent advances in variational inference in order to account forunobserved confounders. Further, a method outline is proposed which uses thecausal mechanism estimates to audit black box models. Experiments are conductedon simulated data and on a real dataset in the context of detecting unlawfulsocial welfare. This research aims to contribute to machine learning techniqueswhich honour our ethical and legal boundaries.\",\n",
       "  'categories': ['cs.LG', 'cs.AI', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2008.10880v1'},\n",
       " 779: {'ID': 779,\n",
       "  'title': 'Continual Learning for Domain Adaptation in Chest X-ray Classification',\n",
       "  'authors': ['Matthias Lenga', 'Heinrich Schulz', 'Axel Saalbach'],\n",
       "  'published': '2020-01-16T16:20:43Z',\n",
       "  'updated': '2020-01-16T16:20:43Z',\n",
       "  'abstract': 'Over the last years, Deep Learning has been successfully applied to a broadrange of medical applications. Especially in the context of chest X-rayclassification, results have been reported which are on par, or even superiorto experienced radiologists. Despite this success in controlled experimentalenvironments, it has been noted that the ability of Deep Learning models togeneralize to data from a new domain (with potentially different tasks) isoften limited. In order to address this challenge, we investigate techniquesfrom the field of Continual Learning (CL) including Joint Training (JT),Elastic Weight Consolidation (EWC) and Learning Without Forgetting (LWF). Usingthe ChestX-ray14 and the MIMIC-CXR datasets, we demonstrate empirically thatthese methods provide promising options to improve the performance of DeepLearning models on a target domain and to mitigate effectively catastrophicforgetting for the source domain. To this end, the best overall performance wasobtained using JT, while for LWF competitive results could be achieved - evenwithout accessing data from the source domain.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2001.05922v1'},\n",
       " 780: {'ID': 780,\n",
       "  'title': 'SMC: Stellar Populations through deep CMDs',\n",
       "  'authors': ['Noelia E. D. Noel',\n",
       "   'Carme Gallart',\n",
       "   'Edgardo Costa',\n",
       "   'Rene Mendez'],\n",
       "  'published': '2005-06-21T12:28:47Z',\n",
       "  'updated': '2005-06-21T12:28:47Z',\n",
       "  'abstract': \"We present deep color-magnitud diagrams (CMDs) reaching the oldestmain-sequence turnoffs for 12 fields in the SMC. The {\\\\it B}-band and {\\\\itR}-band observations were performed using the 100-inch Ir\\\\'en\\\\'ee du Ponttelescope at Las Campanas Observatory, Chile, during four different campaigns(2001-2004). Our fields cover a wide range of galactocentric distance rangingfrom $\\\\sim1\\\\deg$ to $\\\\sim4\\\\deg$ from the center of the galaxy and are located adifferent position angles. Photometry was carried out using DAOPHOTII/ALLSTAR/ALLFRAME. Teramo isochrones have been overlapped. All ourunprecedented deep ground-based CMDs reach the old MS turnoffs with very goodphotometric accuracy. They clearly show stellar population gradients as afunction of both galactocentric distance and position angle. The mostconspicuous difference involves the young population (age$&lt;$1 Gyr): the youngMS is much more populated on the eastern fields, located on the SMC wing area,than on the western fields located at similar galactocentric radius. Inaddition, the main stellar population gets progresively older on average as wego to larger galactocentric radius.\",\n",
       "  'categories': ['astro-ph'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/astro-ph/0506488v1'},\n",
       " 781: {'ID': 781,\n",
       "  'title': 'Attention-guided Context Feature Pyramid Network for Object Detection',\n",
       "  'authors': ['Jun Guo', 'Ruichao Shi', 'Junxu Cao', 'Qi Chen'],\n",
       "  'published': '2020-05-23T05:24:50Z',\n",
       "  'updated': '2020-05-23T05:24:50Z',\n",
       "  'abstract': 'For object detection, how to address the contradictory requirement betweenfeature map resolution and receptive field on high-resolution inputs stillremains an open question. In this paper, to tackle this issue, we build a novelarchitecture, called Attention-guided Context Feature Pyramid Network (AC-FPN),that exploits discriminative information from various large receptive fieldsvia integrating attention-guided multi-path features. The model contains twomodules. The first one is Context Extraction Module (CEM) that explores largecontextual information from multiple receptive fields. As redundant contextualrelations may mislead localization and recognition, we also design the secondmodule named Attention-guided Module (AM), which can adaptively capture thesalient dependencies over objects by using the attention mechanism. AM consistsof two sub-modules, i.e., Context Attention Module (CxAM) and Content AttentionModule (CnAM), which focus on capturing discriminative semantics and locatingprecise positions, respectively. Most importantly, our AC-FPN can be readilyplugged into existing FPN-based models. Extensive experiments on objectdetection and instance segmentation show that existing models with our proposedCEM and AM significantly surpass their counterparts without them, and our modelsuccessfully obtains state-of-the-art results. We have released the source codeat https://github.com/Caojunxu/AC-FPN.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2005.11475v1'},\n",
       " 782: {'ID': 782,\n",
       "  'title': 'Data recovery in computational fluid dynamics through deep image priors',\n",
       "  'authors': ['Ray W. Grout', 'Marc T. Henry de Frahan'],\n",
       "  'published': '2019-01-30T21:42:01Z',\n",
       "  'updated': '2019-02-17T17:32:45Z',\n",
       "  'abstract': 'One of the challenges encountered by computational simulations at exascale isthe reliability of simulations in the face of hardware and software faults.These faults, expected to increase with the complexity of the computationalsystems, will lead to the loss of simulation data and simulation failure andare currently addressed through a checkpoint-restart paradigm. Focusingspecifically on computational fluid dynamics simulations, this work proposes amethod that uses a deep convolutional neural network to recover simulationdata. This data recovery method (i) is agnostic to the flow configuration andgeometry, (ii) does not require extensive training data, and (iii) is accuratefor very different physical flows. Results indicate that the use of deep imagepriors for data recovery is more accurate than standard recovery techniques,such as the Gaussian process regression, also known as Kriging. Data recoveryis performed for two canonical fluid flows: laminar flow around a cylinder andhomogeneous isotropic turbulence. For data recovery of the laminar flow arounda cylinder, results indicate similar performance between the proposed methodand Gaussian process regression across a wide range of mask sizes. Forhomogeneous isotropic turbulence, data recovery through the deep convolutionalneural network exhibits an error in relevant turbulent quantities approximatelythree times smaller than that for the Gaussian process regression,. Forwardsimulations using recovered data illustrate that the enstrophy decay iscaptured within 10% using the deep convolutional neural network approach.Although demonstrated specifically for data recovery of fluid flows, thistechnique can be used in a wide range of applications, including particle imagevelocimetry, visualization, and computational simulations of physical processesbeyond the Navier-Stokes equations.',\n",
       "  'categories': ['physics.flu-dyn',\n",
       "   'physics.comp-ph',\n",
       "   '60G15, 62M45, 68U10, 76F05, 76D17'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1901.11113v2'},\n",
       " 783: {'ID': 783,\n",
       "  'title': 'Reducing the Representation Error of GAN Image Priors Using the Deep  Decoder',\n",
       "  'authors': ['Paul Hand', 'Reinhard Heckel', 'Max Daniels'],\n",
       "  'published': '2020-01-23T18:37:24Z',\n",
       "  'updated': '2020-01-23T18:37:24Z',\n",
       "  'abstract': 'Generative models, such as GANs, learn an explicit low-dimensionalrepresentation of a particular class of images, and so they may be used asnatural image priors for solving inverse problems such as image restoration andcompressive sensing. GAN priors have demonstrated impressive performance onthese tasks, but they can exhibit substantial representation error for bothin-distribution and out-of-distribution images, because of the mismatch betweenthe learned, approximate image distribution and the data generatingdistribution. In this paper, we demonstrate a method for reducing therepresentation error of GAN priors by modeling images as the linear combinationof a GAN prior with a Deep Decoder. The deep decoder is an underparameterizedand most importantly unlearned natural signal model similar to the Deep ImagePrior. No knowledge of the specific inverse problem is needed in the trainingof the GAN underlying our method. For compressive sensing and imagesuperresolution, our hybrid model exhibits consistently higher PSNRs than boththe GAN priors and Deep Decoder separately, both on in-distribution andout-of-distribution images. This model provides a method for extensibly andcheaply leveraging both the benefits of learned and unlearned image recoverypriors in inverse problems.',\n",
       "  'categories': ['cs.LG', 'cs.CV', 'eess.IV', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2001.08747v1'},\n",
       " 784: {'ID': 784,\n",
       "  'title': 'CS-VQA: Visual Question Answering with Compressively Sensed Images',\n",
       "  'authors': ['Kuldeep Kulkarni',\n",
       "   'Suhas Lohit',\n",
       "   'Anik Jha',\n",
       "   'Pavan Turaga',\n",
       "   'Suren Jayasuriya',\n",
       "   'Li-Chi Huang'],\n",
       "  'published': '2018-06-08T23:26:22Z',\n",
       "  'updated': '2018-06-08T23:26:22Z',\n",
       "  'abstract': 'Visual Question Answering (VQA) is a complex semantic task requiring bothnatural language processing and visual recognition. In this paper, we explorewhether VQA is solvable when images are captured in a sub-Nyquist compressiveparadigm. We develop a series of deep-network architectures that exploitavailable compressive data to increasing degrees of accuracy, and show that VQAis indeed solvable in the compressed domain. Our results show that there isnominal degradation in VQA performance when using compressive measurements, butthat accuracy can be recovered when VQA pipelines are used in conjunction withstate-of-the-art deep neural networks for CS reconstruction. The resultspresented yield important implications for resource-constrained VQAapplications.',\n",
       "  'categories': ['cs.CV', 'cs.AI', '68'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1806.03379v1'},\n",
       " 785: {'ID': 785,\n",
       "  'title': 'Suzaku Observations of the X-ray Brightest Fossil Group ESO 3060170',\n",
       "  'authors': ['Yuanyuan Su', 'Eric D. Miller', 'Raymond E. White'],\n",
       "  'published': '2013-08-01T18:01:44Z',\n",
       "  'updated': '2013-08-01T18:01:44Z',\n",
       "  'abstract': '\"Fossil\" galaxy groups, each dominated by a relatively isolated giantelliptical galaxy, have many properties intermediate between groups andclusters of galaxies. We used the {\\\\sl Suzaku} X-ray observatory to observe theX-ray brightest fossil group, ESO 3060170, out to $R_{200}$, in order to betterelucidate the relation between fossil groups, normal groups, and clusters. Wedetermined the intragroup gas temperature, density, and metal abundancedistributions and derived the entropy, pressure and mass profiles for thisgroup. The entropy and pressure profiles in the outer regions are flatter thanin simulated clusters, similar to what is seen in observations of massiveclusters. This may indicate that the gas is clumpy and/or the gas has beenredistributed. Assuming hydrostatic equilibrium, the total mass is estimated tobe $\\\\sim1.7\\\\times10^{14}$ $M_{\\\\odot}$ within a radius $R_{200}$ of $\\\\sim1.15$Mpc, with an enclosed baryon mass fraction of 0.14. The integrated ironmass-to-light ratio of this fossil group is larger than in most groups andcomparable to those of clusters, indicating that this fossil group has retainedthe bulk of its metals. A galaxy luminosity density map on a scale of 25 Mpcshows that this fossil group resides in a relatively isolated environment,unlike the filamentary structures in which typical groups and clusters areembedded.',\n",
       "  'categories': ['astro-ph.CO'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1308.0283v1'},\n",
       " 786: {'ID': 786,\n",
       "  'title': 'Tight Integration of Feature-Based Relocalization in Monocular Direct  Visual Odometry',\n",
       "  'authors': ['Rui Wang',\n",
       "   'Daniel Cremers',\n",
       "   'Mariia Gladkova',\n",
       "   'Niclas Zeller'],\n",
       "  'published': '2021-02-01T21:41:05Z',\n",
       "  'updated': '2021-02-08T13:15:27Z',\n",
       "  'abstract': 'In this paper we propose a framework for integrating map-based relocalizationinto online direct visual odometry. To achieve map-based relocalization fordirect methods, we integrate image features into Direct Sparse Odometry (DSO)and rely on feature matching to associate online visual odometry (VO) with apreviously built map. The integration of the relocalization poses is threefold.Firstly, they are treated as pose priors and tightly integrated into the directimage alignment of the front-end tracking. Secondly, they are also tightlyintegrated into the back-end bundle adjustment. An online fusion module isfurther proposed to combine relative VO poses and global relocalization posesin a pose graph to estimate keyframe-wise smooth and globally accurate poses.We evaluate our method on two multi-weather datasets showing the benefits ofintegrating different handcrafted and learned features and demonstratingpromising improvements on camera tracking accuracy.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2102.01191v2'},\n",
       " 787: {'ID': 787,\n",
       "  'title': 'Passing Expectation Propagation Messages with Kernel Methods',\n",
       "  'authors': ['Wittawat Jitkrittum', 'Nicolas Heess', 'Arthur Gretton'],\n",
       "  'published': '2015-01-02T10:00:07Z',\n",
       "  'updated': '2015-01-02T10:00:07Z',\n",
       "  'abstract': 'We propose to learn a kernel-based message operator which takes as input allexpectation propagation (EP) incoming messages to a factor node and produces anoutgoing message. In ordinary EP, computing an outgoing message involvesestimating a multivariate integral which may not have an analytic expression.Learning such an operator allows one to bypass the expensive computation of theintegral during inference by directly mapping all incoming messages into anoutgoing message. The operator can be learned from training data (examples ofinput and output messages) which allows automated inference to be made on anykind of factor that can be sampled.',\n",
       "  'categories': ['stat.ML', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1501.00375v1'},\n",
       " 788: {'ID': 788,\n",
       "  'title': 'Infinite Feature Selection: A Graph-based Feature Filtering Approach',\n",
       "  'authors': ['Alessandro Vinciarelli',\n",
       "   'Marco Cristani',\n",
       "   'Umberto Castellani',\n",
       "   'Simone Melzi',\n",
       "   'Giorgio Roffo'],\n",
       "  'published': '2020-06-15T07:20:40Z',\n",
       "  'updated': '2020-06-15T07:20:40Z',\n",
       "  'abstract': 'We propose a filtering feature selection framework that considers subsets offeatures as paths in a graph, where a node is a feature and an edge indicatespairwise (customizable) relations among features, dealing with relevance andredundancy principles. By two different interpretations (exploiting propertiesof power series of matrices and relying on Markov chains fundamentals) we canevaluate the values of paths (i.e., feature subsets) of arbitrary lengths,eventually go to infinite, from which we dub our framework Infinite FeatureSelection (Inf-FS). Going to infinite allows to constrain the computationalcomplexity of the selection process, and to rank the features in an elegantway, that is, considering the value of any path (subset) containing aparticular feature. We also propose a simple unsupervised strategy to cut theranking, so providing the subset of features to keep. In the experiments, weanalyze diverse settings with heterogeneous features, for a total of 11benchmarks, comparing against 18 widely-known comparative approaches. Theresults show that Inf-FS behaves better in almost any situation, that is, whenthe number of features to keep are fixed a priori, or when the decision of thesubset cardinality is part of the process.',\n",
       "  'categories': ['cs.CV', 'cs.LG', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2006.08184v1'},\n",
       " 789: {'ID': 789,\n",
       "  'title': 'Classifying Breast Histopathology Images with a Ductal Instance-Oriented  Pipeline',\n",
       "  'authors': ['Sachin Mehta',\n",
       "   'Donald L. Weaver',\n",
       "   'Beibin Li',\n",
       "   'Joann G. Elmore',\n",
       "   'Corey W. Arnold',\n",
       "   'Stevan Knezevich',\n",
       "   'Ezgi Mercan',\n",
       "   'Linda G. Shapiro'],\n",
       "  'published': '2020-12-11T05:43:12Z',\n",
       "  'updated': '2020-12-11T05:43:12Z',\n",
       "  'abstract': 'In this study, we propose the Ductal Instance-Oriented Pipeline (DIOP) thatcontains a duct-level instance segmentation model, a tissue-level semanticsegmentation model, and three-levels of features for diagnostic classification.Based on recent advancements in instance segmentation and the Mask R-CNN model,our duct-level segmenter tries to identify each ductal individual inside amicroscopic image; then, it extracts tissue-level information from theidentified ductal instances. Leveraging three levels of information obtainedfrom these ductal instances and also the histopathology image, the proposedDIOP outperforms previous approaches (both feature-based and CNN-based) in alldiagnostic tasks; for the four-way classification task, the DIOP achievescomparable performance to general pathologists in this unique dataset. Theproposed DIOP only takes a few seconds to run in the inference time, whichcould be used interactively on most modern computers. More clinicalexplorations are needed to study the robustness and generalizability of thissystem in the future.',\n",
       "  'categories': ['cs.CV', 'cs.AI'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2012.06136v1'},\n",
       " 790: {'ID': 790,\n",
       "  'title': 'FD-FCN: 3D Fully Dense and Fully Convolutional Network for Semantic  Segmentation of Brain Anatomy',\n",
       "  'authors': ['Binbin Yang', 'Weiwei Zhang'],\n",
       "  'published': '2019-07-22T09:19:05Z',\n",
       "  'updated': '2020-04-30T10:39:17Z',\n",
       "  'abstract': 'In this paper, a 3D patch-based fully dense and fully convolutional network(FD-FCN) is proposed for fast and accurate segmentation of subcorticalstructures in T1-weighted magnetic resonance images. Developed from the seminalFCN with an end-to-end learning-based approach and constructed by newlydesigned dense blocks including a dense fully-connected layer, the proposedFD-FCN is different from other FCN-based methods and leads to an outperformancein the perspective of both efficiency and accuracy. Compared with the U-shapedarchitecture, FD-FCN discards the upsampling path for model fitness. Toalleviate the problem of parameter explosion, the inputs of dense blocks are nolonger directly passed to subsequent layers. This architecture of FD-FCN bringsa great reduction on both memory and time consumption in training process.Although FD-FCN is slimmed down, in model competence it gains better capabilityof dense inference than other conventional networks. This benefits from theconstruction of network architecture and the incorporation of redesigned denseblocks. The multi-scale FD-FCN models both local and global context byembedding intermediate-layer outputs in the final prediction, which encouragesconsistency between features extracted at different scales and embedsfine-grained information directly in the segmentation process. In addition,dense blocks are rebuilt to enlarge the receptive fields without significantlyincreasing parameters, and spectral coordinates are exploited for spatialcontext of the original input patch. The experiments were performed over theIBSR dataset, and FD-FCN produced an accurate segmentation result of overallDice overlap value of 89.81% for 11 brain structures in 53 seconds, with atleast 3.66% absolute improvement of dice accuracy than state-of-the-art 3DFCN-based methods.',\n",
       "  'categories': ['eess.IV', 'cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1907.09194v2'},\n",
       " 791: {'ID': 791,\n",
       "  'title': 'Image Restoration by Deep Projected GSURE',\n",
       "  'authors': ['Se Young Chun',\n",
       "   'Shady Abu-Hussein',\n",
       "   'Tom Tirer',\n",
       "   'Raja Giryes',\n",
       "   'Yonina C. Eldar'],\n",
       "  'published': '2021-02-04T08:52:46Z',\n",
       "  'updated': '2021-02-04T08:52:46Z',\n",
       "  'abstract': 'Ill-posed inverse problems appear in many image processing applications, suchas deblurring and super-resolution. In recent years, solutions that are basedon deep Convolutional Neural Networks (CNNs) have shown great promise. Yet,most of these techniques, which train CNNs using external data, are restrictedto the observation models that have been used in the training phase. A recentalternative that does not have this drawback relies on learning the targetimage using internal learning. One such prominent example is the Deep ImagePrior (DIP) technique that trains a network directly on the input image with aleast-squares loss. In this paper, we propose a new image restoration frameworkthat is based on minimizing a loss function that includes a \"projected-version\"of the Generalized SteinUnbiased Risk Estimator (GSURE) and parameterization ofthe latent image by a CNN. We demonstrate two ways to use our framework. In thefirst one, where no explicit prior is used, we show that the proposed approachoutperforms other internal learning methods, such as DIP. In the second one, weshow that our GSURE-based loss leads to improved performance when used within aplug-and-play priors scheme.',\n",
       "  'categories': ['cs.CV', 'eess.IV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2102.02485v1'},\n",
       " 792: {'ID': 792,\n",
       "  'title': 'Gather-Excite: Exploiting Feature Context in Convolutional Neural  Networks',\n",
       "  'authors': ['Gang Sun',\n",
       "   'Li Shen',\n",
       "   'Jie Hu',\n",
       "   'Andrea Vedaldi',\n",
       "   'Samuel Albanie'],\n",
       "  'published': '2018-10-29T18:52:37Z',\n",
       "  'updated': '2019-01-12T10:34:06Z',\n",
       "  'abstract': 'While the use of bottom-up local operators in convolutional neural networks(CNNs) matches well some of the statistics of natural images, it may alsoprevent such models from capturing contextual long-range feature interactions.In this work, we propose a simple, lightweight approach for better contextexploitation in CNNs. We do so by introducing a pair of operators: gather,which efficiently aggregates feature responses from a large spatial extent, andexcite, which redistributes the pooled information to local features. Theoperators are cheap, both in terms of number of added parameters andcomputational complexity, and can be integrated directly in existingarchitectures to improve their performance. Experiments on several datasetsshow that gather-excite can bring benefits comparable to increasing the depthof a CNN at a fraction of the cost. For example, we find ResNet-50 withgather-excite operators is able to outperform its 101-layer counterpart onImageNet with no additional learnable parameters. We also propose a parametricgather-excite operator pair which yields further performance gains, relate itto the recently-introduced Squeeze-and-Excitation Networks, and analyse theeffects of these changes to the CNN feature activation statistics.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1810.12348v3'},\n",
       " 793: {'ID': 793,\n",
       "  'title': 'Probing the Galactic Bulge with deep Adaptive Optics imaging: the age of  NGC 6440',\n",
       "  'authors': ['F. R. Ferraro',\n",
       "   'S. Lena',\n",
       "   'E. Diolaiti',\n",
       "   'L. Origlia',\n",
       "   'E. Valenti',\n",
       "   'S. Fabbri',\n",
       "   'G. Beccari'],\n",
       "  'published': '2008-09-23T15:21:30Z',\n",
       "  'updated': '2008-09-23T15:21:30Z',\n",
       "  'abstract': 'We present first results of a pilot project aimed at exploiting thepotentiality of ground based adaptive optics imaging in the near infrared todetermine the age of stellar clusters in the Galactic Bulge. We have used acombination of high resolution adaptive optics (ESO-VLT NAOS-CONICA) andwide-field (ESO-NTT-SOFI) photometry of the metal rich globular cluster NGC6440 located towards the inner Bulge, to compute a deep color magnitude diagramfrom the tip of the Red Giant Branch down to J~22$, two magnitudes below theMain Sequence Turn Off (TO). The magnitude difference between the TO level andthe red Horizontal Branch has been used as an age indicator. It is the firsttime that such a measurement for a bulge globular cluster has been obtainedwith a ground based telescope. From a direct comparison with 47 Tuc and with aset of theoretical isochrones, we concluded that NGC 6440 is old and likelycoeval to 47 Tuc. This result adds a new evidence that the Galactic Bulge is ~2Gyr younger at most than the pristine, metal poor population of the GalacticHalo.',\n",
       "  'categories': ['astro-ph'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/0809.3939v1'},\n",
       " 794: {'ID': 794,\n",
       "  'title': 'Dermoscopic Image Analysis for ISIC Challenge 2018',\n",
       "  'authors': ['Yao Zhang', 'Xiao Ma', 'Cheng Zhong', 'Jinyi Zou'],\n",
       "  'published': '2018-07-24T08:12:54Z',\n",
       "  'updated': '2018-07-24T08:12:54Z',\n",
       "  'abstract': 'This short paper reports the algorithms we used and the evaluationperformances for ISIC Challenge 2018. Our team participates in all the tasks inthis challenge. In lesion segmentation task, the pyramid scene parsing network(PSPNet) is modified to segment the lesions. In lesion attribute detectiontask, the modified PSPNet is also adopted in a multi-label way. In diseaseclassification task, the DenseNet-169 is adopted for multi-classclassification.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1807.08948v1'},\n",
       " 795: {'ID': 795,\n",
       "  'title': 'Towards Deeper Graph Neural Networks with Differentiable Group  Normalization',\n",
       "  'authors': ['Kaixiong Zhou',\n",
       "   'Daochen Zha',\n",
       "   'Xia Hu',\n",
       "   'Rui Chen',\n",
       "   'Yuening Li',\n",
       "   'Xiao Huang'],\n",
       "  'published': '2020-06-12T07:18:02Z',\n",
       "  'updated': '2020-06-12T07:18:02Z',\n",
       "  'abstract': 'Graph neural networks (GNNs), which learn the representation of a node byaggregating its neighbors, have become an effective computational tool indownstream applications. Over-smoothing is one of the key issues which limitthe performance of GNNs as the number of layers increases. It is because thestacked aggregators would make node representations converge toindistinguishable vectors. Several attempts have been made to tackle the issueby bringing linked node pairs close and unlinked pairs distinct. However, theyoften ignore the intrinsic community structures and would result in sub-optimalperformance. The representations of nodes within the same community/class needbe similar to facilitate the classification, while different classes areexpected to be separated in embedding space. To bridge the gap, we introducetwo over-smoothing metrics and a novel technique, i.e., differentiable groupnormalization (DGN). It normalizes nodes within the same group independently toincrease their smoothness, and separates node distributions among differentgroups to significantly alleviate the over-smoothing issue. Experiments onreal-world datasets demonstrate that DGN makes GNN models more robust toover-smoothing and achieves better performance with deeper GNNs.',\n",
       "  'categories': ['cs.LG', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2006.06972v1'},\n",
       " 796: {'ID': 796,\n",
       "  'title': 'Stochastic Neural Network with Kronecker Flow',\n",
       "  'authors': ['Gintare Karolina Dziugaite',\n",
       "   'Ahmed Touati',\n",
       "   'Alexandre Lacoste',\n",
       "   'Aaron Courville',\n",
       "   'Chin-Wei Huang',\n",
       "   'Pascal Vincent'],\n",
       "  'published': '2019-06-10T21:33:25Z',\n",
       "  'updated': '2020-02-13T21:41:38Z',\n",
       "  'abstract': 'Recent advances in variational inference enable the modelling of highlystructured joint distributions, but are limited in their capacity to scale tothe high-dimensional setting of stochastic neural networks. This limitationmotivates a need for scalable parameterizations of the noise generationprocess, in a manner that adequately captures the dependencies among thevarious parameters. In this work, we address this need and present theKronecker Flow, a generalization of the Kronecker product to invertiblemappings designed for stochastic neural networks. We apply our method tovariational Bayesian neural networks on predictive tasks, PAC-Bayesgeneralization bound estimation, and approximate Thompson sampling incontextual bandits. In all setups, our methods prove to be competitive withexisting methods and better than the baselines.',\n",
       "  'categories': ['cs.LG', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1906.04282v2'},\n",
       " 797: {'ID': 797,\n",
       "  'title': 'Biochemical analysis of human breast tissues using FT-Raman spectroscopy',\n",
       "  'authors': ['Renata Andrade Bitar',\n",
       "   'Leandra Naira Zambelli Ramalho',\n",
       "   'Herculano da Silva Martinho',\n",
       "   'Airton Abrahao Martin',\n",
       "   'Mario Mourao Netto',\n",
       "   'Carlos Julio Tierra Criollo'],\n",
       "  'published': '2006-03-31T18:52:11Z',\n",
       "  'updated': '2006-03-31T18:52:11Z',\n",
       "  'abstract': 'In this work we employ the Fourier Transform Raman Spectroscopy to studynormal and tumoral human breast tissues, including several subtypes of cancers.We analyzed 194 Raman spectra from breast tissues that were separated into 9groups according to their corresponding histopathological diagnosis. Theassignment of the relevant Raman bands enabled us to connect the several kindsof breast tissues (normal and pathological) to their corresponding biochemicalmoieties alterations and distinguish among 7 groups: normal breast, fibrocysticcondition, duct carcinoma-in-situ, duct carcinoma-in-situ with necrosis,infiltrating duct carcinoma not otherwise specified, colloid infiltrating ductcarcinoma and invasive lobular carcinomas. We were able to establish thebiochemical basis for each spectrum, relating the observed peaks to specificbiomolecules that play special role in the carcinogenesis process. This work isvery useful for the premature optical diagnosis of a broad range of breastpathologies. We noticed that we were not able to differentiate inflammatory andmedullary duct carcinomas from infiltrating duct carcinoma not otherwisespecified.',\n",
       "  'categories': ['q-bio.TO', 'q-bio.QM'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/q-bio/0603037v1'},\n",
       " 798: {'ID': 798,\n",
       "  'title': 'Tracing out the northern stream of the Sagittarius dwarf galaxy with  color- magnitude diagram techniques',\n",
       "  'authors': ['A. Aparicio', 'M. A. Gomez-Flechoso', 'D. Martinez-Delgado'],\n",
       "  'published': '2001-10-31T15:29:45Z',\n",
       "  'updated': '2001-10-31T15:29:45Z',\n",
       "  'abstract': 'Standard cosmology predicts that dwarfs were the first galaxies to be formedin the Universe and that many of them merge afterwards to form bigger galaxiessuch as the Milky Way. This process would have left behind traces such as tidaldebris or tidal streams in the outer halo. We report here the detection of twonew tidal debris of the northern stream of the Sagittarius dwarf galaxy, basedin the analysis of wide field, deep color- magnitude diagrams. These detectionsprovide strong observational evidence that the stripped debris of Sagittariusextends up to 60 degrees from its center, suggesting that the stream of thisgalaxy completely wraps the Milky Way in an almost polar orbit. Our negativedetections also suggest the stream is narrow, supporting a nearly sphericalMilky Way dark matter halo potential.',\n",
       "  'categories': ['astro-ph'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/astro-ph/0110703v1'},\n",
       " 799: {'ID': 799,\n",
       "  'title': 'SERank: Optimize Sequencewise Learning to Rank Using  Squeeze-and-Excitation Network',\n",
       "  'authors': ['Kuan Fang',\n",
       "   'Zhan Shen',\n",
       "   'RiKang Zhou',\n",
       "   'RuiXing Wang',\n",
       "   'LiWen Fan'],\n",
       "  'published': '2020-06-07T08:29:58Z',\n",
       "  'updated': '2020-06-07T08:29:58Z',\n",
       "  'abstract': \"Learning-to-rank (LTR) is a set of supervised machine learning algorithmsthat aim at generating optimal ranking order over a list of items. A lot ofranking models have been studied during the past decades. And most of themtreat each query document pair independently during training and inference.Recently, there are a few methods have been proposed which focused on mininginformation across ranking candidates list for further improvements, such aslearning multivariant scoring function or learning contextual embedding.However, these methods usually greatly increase computational cost duringonline inference, especially when with large candidates size in real-world websearch systems. What's more, there are few studies that focus on novel designof model structure for leveraging information across ranking candidates. Inthis work, we propose an effective and efficient method named as SERank whichis a Sequencewise Ranking model by using Squeeze-and-Excitation network to takeadvantage of cross-document information. Moreover, we examine our proposedmethods on several public benchmark datasets, as well as click logs collectedfrom a commercial Question Answering search engine, Zhihu. In addition, we alsoconduct online A/B testing at Zhihu search engine to further verify theproposed approach. Results on both offline datasets and online A/B testingdemonstrate that our method contributes to a significant improvement.\",\n",
       "  'categories': ['cs.IR'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2006.04084v1'},\n",
       " 800: {'ID': 800,\n",
       "  'title': 'Vulnerability of deep neural networks for detecting COVID-19 cases from  chest X-ray images to universal adversarial attacks',\n",
       "  'authors': ['Kazuki Koga', 'Hokuto Hirano', 'Kazuhiro Takemoto'],\n",
       "  'published': '2020-05-22T08:54:41Z',\n",
       "  'updated': '2020-05-22T08:54:41Z',\n",
       "  'abstract': 'Under the epidemic of the novel coronavirus disease 2019 (COVID-19), chestX-ray computed tomography imaging is being used for effectively screeningCOVID-19 patients. The development of computer-aided systems based on deepneural networks (DNNs) has been advanced, to rapidly and accurately detectCOVID-19 cases, because the need for expert radiologists, who are limited innumber, forms a bottleneck for the screening. However, so far, thevulnerability of DNN-based systems has been poorly evaluated, although DNNs arevulnerable to a single perturbation, called universal adversarial perturbation(UAP), which can induce DNN failure in most classification tasks. Thus, wefocus on representative DNN models for detecting COVID-19 cases from chestX-ray images and evaluate their vulnerability to UAPs generated using simpleiterative algorithms. We consider nontargeted UAPs, which cause a task failureresulting in an input being assigned an incorrect label, and targeted UAPs,which cause the DNN to classify an input into a specific class. The resultsdemonstrate that the models are vulnerable to nontargeted and targeted UAPs,even in case of small UAPs. In particular, 2% norm of the UPAs to the averagenorm of an image in the image dataset achieves &gt;85% and &gt;90% success rates forthe nontargeted and targeted attacks, respectively. Due to the nontargetedUAPs, the DNN models judge most chest X-ray images as COVID-19 cases. Thetargeted UAPs make the DNN models classify most chest X-ray images into a giventarget class. The results indicate that careful consideration is required inpractical applications of DNNs to COVID-19 diagnosis; in particular, theyemphasize the need for strategies to address security concerns. As an example,we show that iterative fine-tuning of the DNN models using UAPs improves therobustness of the DNN models against UAPs.',\n",
       "  'categories': ['cs.CV', 'cs.CR', 'cs.LG', 'eess.IV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2005.11061v1'},\n",
       " 801: {'ID': 801,\n",
       "  'title': 'Attention-SLAM: A Visual Monocular SLAM Learning from Human Gaze',\n",
       "  'authors': ['Zhen Sun',\n",
       "   'Danping Zou',\n",
       "   'Jinquan Li',\n",
       "   'Qi Wu',\n",
       "   'Songpengcheng Xia',\n",
       "   'Ling Pei',\n",
       "   'Wenxian Yu',\n",
       "   'Tao Li'],\n",
       "  'published': '2020-09-15T06:59:12Z',\n",
       "  'updated': '2020-09-15T06:59:12Z',\n",
       "  'abstract': 'This paper proposes a novel simultaneous localization and mapping (SLAM)approach, namely Attention-SLAM, which simulates human navigation mode bycombining a visual saliency model (SalNavNet) with traditional monocular visualSLAM. Most SLAM methods treat all the features extracted from the images asequal importance during the optimization process. However, the salient featurepoints in scenes have more significant influence during the human navigationprocess. Therefore, we first propose a visual saliency model called SalVavNetin which we introduce a correlation module and propose an adaptive ExponentialMoving Average (EMA) module. These modules mitigate the center bias to enablethe saliency maps generated by SalNavNet to pay more attention to the samesalient object. Moreover, the saliency maps simulate the human behavior for therefinement of SLAM results. The feature points extracted from the salientregions have greater importance in optimization process. We add semanticsaliency information to the Euroc dataset to generate an open-source saliencySLAM dataset. Comprehensive test results prove that Attention-SLAM outperformsbenchmarks such as Direct Sparse Odometry (DSO), ORB-SLAM, and Salient DSO interms of efficiency, accuracy, and robustness in most test cases.',\n",
       "  'categories': ['cs.CV', 'cs.RO'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2009.06886v1'},\n",
       " 802: {'ID': 802,\n",
       "  'title': 'Two can play this Game: Visual Dialog with Discriminative Question  Generation and Answering',\n",
       "  'authors': ['Unnat Jain', 'Alexander Schwing', 'Svetlana Lazebnik'],\n",
       "  'published': '2018-03-29T17:58:43Z',\n",
       "  'updated': '2018-03-29T17:58:43Z',\n",
       "  'abstract': 'Human conversation is a complex mechanism with subtle nuances. It is hence anambitious goal to develop artificial intelligence agents that can participatefluently in a conversation. While we are still far from achieving this goal,recent progress in visual question answering, image captioning, and visualquestion generation shows that dialog systems may be realizable in the not toodistant future. To this end, a novel dataset was introduced recently andencouraging results were demonstrated, particularly for question answering. Inthis paper, we demonstrate a simple symmetric discriminative baseline, that canbe applied to both predicting an answer as well as predicting a question. Weshow that this method performs on par with the state of the art, even memorynet based methods. In addition, for the first time on the visual dialogdataset, we assess the performance of a system asking questions, anddemonstrate how visual dialog can be generated from discriminative questiongeneration and question answering.',\n",
       "  'categories': ['cs.CV', 'cs.CL'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1803.11186v1'},\n",
       " 803: {'ID': 803,\n",
       "  'title': 'The CAPIO 2017 Conversational Speech Recognition System',\n",
       "  'authors': ['Ian Lane',\n",
       "   'Jungsuk Kim',\n",
       "   'Kyu J. Han',\n",
       "   'Akshay Chandrashekaran'],\n",
       "  'published': '2017-12-29T23:31:05Z',\n",
       "  'updated': '2018-04-10T00:17:37Z',\n",
       "  'abstract': 'In this paper we show how we have achieved the state-of-the-art performanceon the industry-standard NIST 2000 Hub5 English evaluation set. We exploredensely connected LSTMs, inspired by the densely connected convolutionalnetworks recently introduced for image classification tasks. We also propose anacoustic model adaptation scheme that simply averages the parameters of a seedneural network acoustic model and its adapted version. This method was appliedwith the CallHome training corpus and improved individual system performancesby on average 6.1% (relative) against the CallHome portion of the evaluationset with no performance loss on the Switchboard portion. With RNN-LM rescoringand lattice combination on the 5 systems trained across three different phonesets, our 2017 speech recognition system has obtained 5.0% and 9.1% onSwitchboard and CallHome, respectively, both of which are the best word errorrates reported thus far. According to IBM in their latest work to compare humanand machine transcriptions, our reported Switchboard word error rate can beconsidered to surpass the human parity (5.1%) of transcribing conversationaltelephone speech.',\n",
       "  'categories': ['cs.CL'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1801.00059v2'},\n",
       " 804: {'ID': 804,\n",
       "  'title': 'Weakly Supervised Neuro-Symbolic Module Networks for Numerical Reasoning',\n",
       "  'authors': ['Steven C. H. Hoi', 'Shafiq Joty', 'Amrita Saha'],\n",
       "  'published': '2021-01-28T03:36:09Z',\n",
       "  'updated': '2021-01-28T03:36:09Z',\n",
       "  'abstract': 'Neural Module Networks (NMNs) have been quite successful in incorporatingexplicit reasoning as learnable modules in various question answering tasks,including the most generic form of numerical reasoning over text in MachineReading Comprehension (MRC). However, to achieve this, contemporary NMNs needstrong supervision in executing the query as a specialized program overreasoning modules and fail to generalize to more open-ended settings withoutsuch supervision. Hence we propose Weakly-Supervised Neuro-Symbolic ModuleNetwork (WNSMN) trained with answers as the sole supervision for numericalreasoning based MRC. It learns to execute a noisy heuristic program obtainedfrom the dependency parsing of the query, as discrete actions over both neuraland symbolic reasoning modules and trains it end-to-end in a reinforcementlearning framework with discrete reward from answer matching. On thenumerical-answer subset of DROP, WNSMN out-performs NMN by 32% and thereasoning-free language model GenBERT by 8% in exact match accuracy whentrained under comparable weak supervised settings. This showcases theeffectiveness and generalizability of modular networks that can handle explicitdiscrete reasoning over noisy programs in an end-to-end manner.',\n",
       "  'categories': ['cs.CL'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2101.11802v1'},\n",
       " 805: {'ID': 805,\n",
       "  'title': 'Visual Relationship Detection with Language prior and Softmax',\n",
       "  'authors': ['Jongyoul Park', 'Jaewon Jung'],\n",
       "  'published': '2019-04-16T16:29:52Z',\n",
       "  'updated': '2019-04-16T16:29:52Z',\n",
       "  'abstract': 'Visual relationship detection is an intermediate image understanding taskthat detects two objects and classifies a predicate that explains therelationship between two objects in an image. The three components arelinguistically and visually correlated (e.g. \"wear\" is related to \"person\" and\"shirt\", while \"laptop\" is related to \"table\" and \"on\") thus, the solutionspace is huge because there are many possible cases between them. Language andvisual modules are exploited and a sophisticated spatial vector is proposed.The models in this work outperformed the state of arts without costlylinguistic knowledge distillation from a large text corpus and building complexloss functions. All experiments were only evaluated on Visual RelationshipDetection and Visual Genome dataset.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1904.07798v1'},\n",
       " 806: {'ID': 806,\n",
       "  'title': 'Object Detection for Understanding Assembly Instruction Using  Context-aware Data Augmentation and Cascade Mask R-CNN',\n",
       "  'authors': ['Sungho Shin',\n",
       "   'Seunghyeok Back',\n",
       "   'Joosoon Lee',\n",
       "   'Seongju Lee',\n",
       "   'Kyoobin Lee'],\n",
       "  'published': '2021-01-07T12:10:27Z',\n",
       "  'updated': '2021-01-08T02:38:51Z',\n",
       "  'abstract': 'Understanding assembly instruction has the potential to enhance the robot stask planning ability and enables advanced robotic applications. To recognizethe key components from the 2D assembly instruction image, We mainly focus onsegmenting the speech bubble area, which contains lots of information aboutinstructions. For this, We applied Cascade Mask R-CNN and developed acontext-aware data augmentation scheme for speech bubble segmentation, whichrandomly combines images cuts by considering the context of assemblyinstructions. We showed that the proposed augmentation scheme achieves a bettersegmentation performance compared to the existing augmentation algorithm byincreasing the diversity of trainable data while considering the distributionof components locations. Also, we showed that deep learning can be useful tounderstand assembly instruction by detecting the essential objects in theassembly instruction, such as tools and parts.',\n",
       "  'categories': ['cs.RO', 'cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2101.02509v2'},\n",
       " 807: {'ID': 807,\n",
       "  'title': 'Speech-Gesture Mapping and Engagement Evaluation in Human Robot  Interaction',\n",
       "  'authors': ['Abhinav Dhall', 'Bishal Ghosh', 'Ekta Singla'],\n",
       "  'published': '2018-12-09T13:22:01Z',\n",
       "  'updated': '2018-12-09T13:22:01Z',\n",
       "  'abstract': 'A robot needs contextual awareness, effective speech production andcomplementing non-verbal gestures for successful communication in society. Inthis paper, we present our end-to-end system that tries to enhance theeffectiveness of non-verbal gestures. For achieving this, we identifiedprominently used gestures in performances by TED speakers and mapped them totheir corresponding speech context and modulated speech based upon theattention of the listener. The proposed method utilized Convolutional PoseMachine [4] to detect the human gesture. Dominant gestures of TED speakers wereused for learning the gesture-to-speech mapping. The speeches by them were usedfor training the model. We also evaluated the engagement of the robot withpeople by conducting a social survey. The effectiveness of the performance wasmonitored by the robot and it self-improvised its speech pattern on the basisof the attention level of the audience, which was calculated using visualfeedback from the camera. The effectiveness of interaction as well as thedecisions made during improvisation was further evaluated based on thehead-pose detection and interaction survey.',\n",
       "  'categories': ['cs.RO', 'cs.HC'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1812.03484v1'},\n",
       " 808: {'ID': 808,\n",
       "  'title': 'Self-Paced Deep Regression Forests with Consideration on  Underrepresented Examples',\n",
       "  'authors': ['Yazhou Ren', 'Shijie Ai', 'Zenglin Xu', 'Lili Pan'],\n",
       "  'published': '2020-04-03T10:18:05Z',\n",
       "  'updated': '2020-08-06T01:06:32Z',\n",
       "  'abstract': 'Deep discriminative models (e.g. deep regression forests, deep neuraldecision forests) have achieved remarkable success recently to solve problemssuch as facial age estimation and head pose estimation. Most existing methodspursue robust and unbiased solutions either through learning discriminativefeatures, or reweighting samples. We argue what is more desirable is learninggradually to discriminate like our human beings, and hence we resort toself-paced learning (SPL). Then, a natural question arises: can self-pacedregime lead deep discriminative models to achieve more robust and less biasedsolutions? To this end, this paper proposes a new deep discriminativemodel--self-paced deep regression forests with consideration onunderrepresented examples (SPUDRFs). It tackles the fundamental ranking andselecting problem in SPL from a new perspective: fairness. This paradigm isfundamental and could be easily combined with a variety of deep discriminativemodels (DDMs). Extensive experiments on two computer vision tasks, i.e., facialage estimation and head pose estimation, demonstrate the efficacy of SPUDRFs,where state-of-the-art performances are achieved.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2004.01459v4'},\n",
       " 809: {'ID': 809,\n",
       "  'title': 'Transfer Learning Through Weighted Loss Function and Group Normalization  for Vessel Segmentation from Retinal Images',\n",
       "  'authors': ['Jon Rokne',\n",
       "   'Reda Alhajj',\n",
       "   'Andrew Crichton',\n",
       "   'Abdullah Sarhan'],\n",
       "  'published': '2020-12-16T20:34:48Z',\n",
       "  'updated': '2020-12-16T20:34:48Z',\n",
       "  'abstract': 'The vascular structure of blood vessels is important in diagnosing retinalconditions such as glaucoma and diabetic retinopathy. Accurate segmentation ofthese vessels can help in detecting retinal objects such as the optic disc andoptic cup and hence determine if there are damages to these areas. Moreover,the structure of the vessels can help in diagnosing glaucoma. The rapiddevelopment of digital imaging and computer-vision techniques has increased thepotential for developing approaches for segmenting retinal vessels. In thispaper, we propose an approach for segmenting retinal vessels that uses deeplearning along with transfer learning. We adapted the U-Net structure to use acustomized InceptionV3 as the encoder and used multiple skip connections toform the decoder. Moreover, we used a weighted loss function to handle theissue of class imbalance in retinal images. Furthermore, we contributed a newdataset to this field. We tested our approach on six publicly availabledatasets and a newly created dataset. We achieved an average accuracy of 95.60%and a Dice coefficient of 80.98%. The results obtained from comprehensiveexperiments demonstrate the robustness of our approach to the segmentation ofblood vessels in retinal images obtained from different sources. Our approachresults in greater segmentation accuracy than other approaches.',\n",
       "  'categories': ['eess.IV', 'cs.CV', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2012.09250v1'},\n",
       " 810: {'ID': 810,\n",
       "  'title': 'Local Context Normalization: Revisiting Local Normalization',\n",
       "  'authors': ['Christopher Kiekintveld',\n",
       "   'Dan Morris',\n",
       "   'Md Mahmudulla Hassan',\n",
       "   'Caleb Robinson',\n",
       "   'Anthony Ortiz',\n",
       "   'Nebojsa Jojic',\n",
       "   'Olac Fuentes'],\n",
       "  'published': '2019-12-12T09:28:24Z',\n",
       "  'updated': '2020-05-09T09:27:12Z',\n",
       "  'abstract': 'Normalization layers have been shown to improve convergence in deep neuralnetworks, and even add useful inductive biases. In many vision applications thelocal spatial context of the features is important, but most commonnormalization schemes including Group Normalization (GN), InstanceNormalization (IN), and Layer Normalization (LN) normalize over the entirespatial dimension of a feature. This can wash out important signals and degradeperformance. For example, in applications that use satellite imagery, inputimages can be arbitrarily large; consequently, it is nonsensical to normalizeover the entire area. Positional Normalization (PN), on the other hand, onlynormalizes over a single spatial position at a time. A natural compromise is tonormalize features by local context, while also taking into account group levelinformation. In this paper, we propose Local Context Normalization (LCN): anormalization layer where every feature is normalized based on a window aroundit and the filters in its group. We propose an algorithmic solution to make LCNefficient for arbitrary window sizes, even if every point in the image has aunique window. LCN outperforms its Batch Normalization (BN), GN, IN, and LNcounterparts for object detection, semantic segmentation, and instancesegmentation applications in several benchmark datasets, while keepingperformance independent of the batch size and facilitating transfer learning.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1912.05845v3'},\n",
       " 811: {'ID': 811,\n",
       "  'title': 'RefinedMPL: Refined Monocular PseudoLiDAR for 3D Object Detection in  Autonomous Driving',\n",
       "  'authors': ['Jean Marie Uwabeza Vianney', 'Bingbing Liu', 'Shubhra Aich'],\n",
       "  'published': '2019-11-21T19:21:32Z',\n",
       "  'updated': '2019-11-21T19:21:32Z',\n",
       "  'abstract': 'In this paper, we strive for solving the ambiguities arisen by theastoundingly high density of raw PseudoLiDAR for monocular 3D object detectionfor autonomous driving. Without much computational overhead, we propose asupervised and an unsupervised sparsification scheme of PseudoLiDAR prior to 3Ddetection. Both the strategies assist the standard 3D detector gain betterperformance over the raw PseudoLiDAR baseline using only ~5% of its points onthe KITTI object detection benchmark, thus making our monocular framework andLiDAR-based counterparts computationally equivalent (Figure 1). Moreover, ourarchitecture agnostic refinements provide state-of-the-art results on KITTI3Dtest set for \"Car\" and \"Pedestrian\" categories with 54% relative improvementfor \"Pedestrian\". Finally, exploratory analysis is performed on the discrepancybetween monocular and LiDAR-based 3D detection frameworks to guide futureendeavours.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1911.09712v1'},\n",
       " 812: {'ID': 812,\n",
       "  'title': 'Noise Conscious Training of Non Local Neural Network powered by Self  Attentive Spectral Normalized Markovian Patch GAN for Low Dose CT Denoising',\n",
       "  'authors': ['Sutanu Bera', 'Prabir Kumar Biswas'],\n",
       "  'published': '2020-11-11T10:44:52Z',\n",
       "  'updated': '2020-11-11T10:44:52Z',\n",
       "  'abstract': \"The explosive rise of the use of Computer tomography (CT) imaging in medicalpractice has heightened public concern over the patient's associated radiationdose. However, reducing the radiation dose leads to increased noise andartifacts, which adversely degrades the scan's interpretability. Consequently,an advanced image reconstruction algorithm to improve the diagnosticperformance of low dose ct arose as the primary concern among the researchers,which is challenging due to the ill-posedness of the problem. In recent times,the deep learning-based technique has emerged as a dominant method for low doseCT(LDCT) denoising. However, some common bottleneck still exists, which hindersdeep learning-based techniques from furnishing the best performance. In thisstudy, we attempted to mitigate these problems with three novel accretions.First, we propose a novel convolutional module as the first attempt to utilizeneighborhood similarity of CT images for denoising tasks. Our proposed moduleassisted in boosting the denoising by a significant margin. Next, we movedtowards the problem of non-stationarity of CT noise and introduced a new noiseaware mean square error loss for LDCT denoising. Moreover, the loss mentionedabove also assisted to alleviate the laborious effort required while trainingCT denoising network using image patches. Lastly, we propose a noveldiscriminator function for CT denoising tasks. The conventional vanilladiscriminator tends to overlook the fine structural details and focus on theglobal agreement. Our proposed discriminator leverage self-attention andpixel-wise GANs for restoring the diagnostic quality of LDCT images. Our methodvalidated on a publicly available dataset of the 2016 NIH-AAPM-Mayo Clinic LowDose CT Grand Challenge performed remarkably better than the existing state ofthe art method.\",\n",
       "  'categories': ['cs.CV', 'cs.LG', 'eess.IV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2011.05684v1'},\n",
       " 813: {'ID': 813,\n",
       "  'title': 'RUC+CMU: System Report for Dense Captioning Events in Videos',\n",
       "  'authors': ['Yuqing Song',\n",
       "   'Yida Zhao',\n",
       "   'Alexander Hauptmann',\n",
       "   'Shizhe Chen',\n",
       "   'Qin Jin',\n",
       "   'Jiarong Qiu'],\n",
       "  'published': '2018-06-22T21:03:47Z',\n",
       "  'updated': '2018-06-22T21:03:47Z',\n",
       "  'abstract': 'This notebook paper presents our system in the ActivityNet Dense Captioningin Video task (task 3). Temporal proposal generation and caption generation areboth important to the dense captioning task. Therefore, we propose a proposalranking model to employ a set of effective feature representations for proposalgeneration, and ensemble a series of caption models enhanced with contextinformation to generate captions robustly on predicted proposals. Our approachachieves the state-of-the-art performance on the dense video captioning taskwith 8.529 METEOR score on the challenge testing set.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1806.08854v1'},\n",
       " 814: {'ID': 814,\n",
       "  'title': 'Hybrid-S2S: Video Object Segmentation with Recurrent Networks and  Correspondence Matching',\n",
       "  'authors': ['Andreas Dengel',\n",
       "   'Stanislav Frolov',\n",
       "   'Fatemeh Azimi',\n",
       "   'Joern Hees',\n",
       "   'Federico Raue'],\n",
       "  'published': '2020-10-10T19:00:43Z',\n",
       "  'updated': '2020-11-07T09:33:51Z',\n",
       "  'abstract': \"One-shot Video Object Segmentation~(VOS) is the task of pixel-wise trackingan object of interest within a video sequence, where the segmentation mask ofthe first frame is given at inference time. In recent years, Recurrent NeuralNetworks~(RNNs) have been widely used for VOS tasks, but they often suffer fromlimitations such as drift and error propagation. In this work, we study anRNN-based architecture and address some of these issues by proposing a hybridsequence-to-sequence architecture named HS2S, utilizing a dual mask propagationstrategy that allows incorporating the information obtained from correspondencematching. Our experiments show that augmenting the RNN with correspondencematching is a highly effective solution to reduce the drift problem. Theadditional information helps the model to predict more accurate masks and makesit robust against error propagation. We evaluate our HS2S model on theDAVIS2017 dataset as well as Youtube-VOS. On the latter, we achieve animprovement of 11.2pp in the overall segmentation accuracy over RNN-basedstate-of-the-art methods in VOS. We analyze our model's behavior in challengingcases such as occlusion and long sequences and show that our hybridarchitecture significantly enhances the segmentation quality in these difficultscenarios.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2010.05069v2'},\n",
       " 815: {'ID': 815,\n",
       "  'title': 'Robust Synthesis of Adversarial Visual Examples Using a Deep Image Prior',\n",
       "  'authors': ['John Collomosse', 'Steve Schneider', 'Thomas Gittings'],\n",
       "  'published': '2019-07-03T15:40:05Z',\n",
       "  'updated': '2019-07-03T15:40:05Z',\n",
       "  'abstract': \"We present a novel method for generating robust adversarial image examplesbuilding upon the recent `deep image prior' (DIP) that exploits convolutionalnetwork architectures to enforce plausible texture in image synthesis.Adversarial images are commonly generated by perturbing images to introducehigh frequency noise that induces image misclassification, but that is fragileto subsequent digital manipulation of the image. We show that using DIP toreconstruct an image under adversarial constraint induces perturbations thatare more robust to affine deformation, whilst remaining visually imperceptible.Furthermore we show that our DIP approach can also be adapted to produce localadversarial patches (`adversarial stickers'). We demonstrate robust adversarialexamples over a broad gamut of images and object classes drawn from theImageNet dataset.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1907.01996v1'},\n",
       " 816: {'ID': 816,\n",
       "  'title': 'CDeC-Net: Composite Deformable Cascade Network for Table Detection in  Document Images',\n",
       "  'authors': ['Ajoy Mondal', 'Madhav Agarwal', 'C. V. Jawahar'],\n",
       "  'published': '2020-08-25T05:53:59Z',\n",
       "  'updated': '2020-08-25T05:53:59Z',\n",
       "  'abstract': 'Localizing page elements/objects such as tables, figures, equations, etc. isthe primary step in extracting information from document images. We propose anovel end-to-end trainable deep network, (CDeC-Net) for detecting tablespresent in the documents. The proposed network consists of a multistageextension of Mask R-CNN with a dual backbone having deformable convolution fordetecting tables varying in scale with high detection accuracy at higher IoUthreshold. We empirically evaluate CDeC-Net on all the publicly availablebenchmark datasets - ICDAR-2013, ICDAR-2017, ICDAR-2019,UNLV, Marmot,PubLayNet, and TableBank - with extensive experiments.  Our solution has three important properties: (i) a single trained modelCDeC-Net{\\\\ddag} performs well across all the popular benchmark datasets; (ii)we report excellent performances across multiple, including higher, thresholdsof IoU; (iii) by following the same protocol of the recent papers for each ofthe benchmarks, we consistently demonstrate the superior quantitativeperformance. Our code and models will be publicly released for enabling thereproducibility of the results.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2008.10831v1'},\n",
       " 817: {'ID': 817,\n",
       "  'title': 'Qiniu Submission to ActivityNet Challenge 2018',\n",
       "  'authors': ['Qinzhu He',\n",
       "   'Kai Hu',\n",
       "   'Yining Lin',\n",
       "   'Yixin Bao',\n",
       "   'Liang Zhu',\n",
       "   'Xiaoteng Zhang',\n",
       "   'Yao Peng',\n",
       "   'Jie Shao',\n",
       "   'Yicheng Wang',\n",
       "   'Feiyun Zhang'],\n",
       "  'published': '2018-06-12T08:42:55Z',\n",
       "  'updated': '2018-06-12T08:42:55Z',\n",
       "  'abstract': 'In this paper, we introduce our submissions for the tasks of trimmed activityrecognition (Kinetics) and trimmed event recognition (Moments in Time) forActivitynet Challenge 2018. In the two tasks, non-local neural networks andtemporal segment networks are implemented as our base models. Multi-modal cuessuch as RGB image, optical flow and acoustic signal have also been used in ourmethod. We also propose new non-local-based models for further improvement onthe recognition accuracy. The final submissions after ensembling the modelsachieve 83.5% top-1 accuracy and 96.8% top-5 accuracy on the Kineticsvalidation set, 35.81% top-1 accuracy and 62.59% top-5 accuracy on the MITvalidation set.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1806.04391v1'},\n",
       " 818: {'ID': 818,\n",
       "  'title': 'Multi-Scale Attention with Dense Encoder for Handwritten Mathematical  Expression Recognition',\n",
       "  'authors': ['Lirong Dai', 'Jun Du', 'Jianshu Zhang'],\n",
       "  'published': '2018-01-05T09:22:42Z',\n",
       "  'updated': '2018-01-31T01:52:21Z',\n",
       "  'abstract': 'Handwritten mathematical expression recognition is a challenging problem dueto the complicated two-dimensional structures, ambiguous handwriting input andvariant scales of handwritten math symbols. To settle this problem, we utilizethe attention based encoder-decoder model that recognizes mathematicalexpression images from two-dimensional layouts to one-dimensional LaTeXstrings. We improve the encoder by employing densely connected convolutionalnetworks as they can strengthen feature extraction and facilitate gradientpropagation especially on a small training set. We also present a novelmulti-scale attention model which is employed to deal with the recognition ofmath symbols in different scales and save the fine-grained details that will bedropped by pooling operations. Validated on the CROHME competition task, theproposed method significantly outperforms the state-of-the-art methods with anexpression recognition accuracy of 52.8% on CROHME 2014 and 50.1% on CROHME2016, by only using the official training dataset.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1801.03530v2'},\n",
       " 819: {'ID': 819,\n",
       "  'title': 'Harmonic holes as the submodules of brain network and network  dissimilarity',\n",
       "  'authors': ['Seunggyun Ha',\n",
       "   'Yu Kyeong Kim',\n",
       "   'Hyejin Kang',\n",
       "   'Hongyoon Choi',\n",
       "   'Moo K. Chung',\n",
       "   'Dong Soo Lee',\n",
       "   'Hyekyoung Lee'],\n",
       "  'published': '2018-11-11T05:30:35Z',\n",
       "  'updated': '2018-11-11T05:30:35Z',\n",
       "  'abstract': \"Persistent homology has been applied to brain network analysis for findingthe shape of brain networks across multiple thresholds. In the persistenthomology, the shape of networks is often quantified by the sequence of$k$-dimensional holes and Betti numbers.The Betti numbers are more widely usedthan holes themselves in topological brain network analysis. However, the holesshow the local connectivity of networks, and they can be very informativefeatures in analysis. In this study, we propose a new method of measuringnetwork differences based on the dissimilarity measure of harmonic holes (HHs).The HHs, which represent the substructure of brain networks, are extracted bythe Hodge Laplacian of brain networks. We also find the most contributed HHs tothe network difference based on the HH dissimilarity. We applied our proposedmethod to clustering the networks of 4 groups, normal control (NC), stable andprogressive mild cognitive impairment (sMCI and pMCI), and Alzheimer's disease(AD). The results showed that the clustering performance of the proposed methodwas better than that of network distances based on only the global change oftopology.\",\n",
       "  'categories': ['q-bio.QM'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1811.04355v1'},\n",
       " 820: {'ID': 820,\n",
       "  'title': 'A New Mask R-CNN Based Method for Improved Landslide Detection',\n",
       "  'authors': ['Shaik Ejaz Ahamed',\n",
       "   'Ramji Dwivedi',\n",
       "   'G. R. Sinha',\n",
       "   'Basant Kumar',\n",
       "   'Alessandro Sebastianelli',\n",
       "   'Silvia Liberata Ullo',\n",
       "   'Amrita Mohan'],\n",
       "  'published': '2020-10-04T07:46:37Z',\n",
       "  'updated': '2020-10-04T07:46:37Z',\n",
       "  'abstract': 'This paper presents a novel method of landslide detection by exploiting theMask R-CNN capability of identifying an object layout by using a pixel-basedsegmentation, along with transfer learning used to train the proposed model. Adata set of 160 elements is created containing landslide and non-landslideimages. The proposed method consists of three steps: (i) augmenting trainingimage samples to increase the volume of the training data, (ii) fine tuningwith limited image samples, and (iii) performance evaluation of the algorithmin terms of precision, recall and F1 measure, on the considered landslideimages, by adopting ResNet-50 and 101 as backbone models. The experimentalresults are quite encouraging as the proposed method achieves Precision equalsto 1.00, Recall 0.93 and F1 measure 0.97, when ResNet-101 is used as backbonemodel, and with a low number of landslide photographs used as training samples.The proposed algorithm can be potentially useful for land use planners andpolicy makers of hilly areas where intermittent slope deformations necessitatelandslide detection as prerequisite before planning.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2010.01499v1'},\n",
       " 821: {'ID': 821,\n",
       "  'title': 'IGCV$2$: Interleaved Structured Sparse Convolutional Neural Networks',\n",
       "  'authors': ['Jingdong Wang',\n",
       "   'Guotian Xie',\n",
       "   'Ting Zhang',\n",
       "   'Richang Hong',\n",
       "   'Guo-Jun Qi',\n",
       "   'Jianhuang Lai'],\n",
       "  'published': '2018-04-17T12:36:36Z',\n",
       "  'updated': '2018-04-17T12:36:36Z',\n",
       "  'abstract': 'In this paper, we study the problem of designing efficient convolutionalneural network architectures with the interest in eliminating the redundancy inconvolution kernels. In addition to structured sparse kernels, low-rank kernelsand the product of low-rank kernels, the product of structured sparse kernels,which is a framework for interpreting the recently-developed interleaved groupconvolutions (IGC) and its variants (e.g., Xception), has been attractingincreasing interests.  Motivated by the observation that the convolutions contained in a groupconvolution in IGC can be further decomposed in the same manner, we present amodularized building block, {IGCV$2$:} interleaved structured sparseconvolutions. It generalizes interleaved group convolutions, which is composedof two structured sparse kernels, to the product of more structured sparsekernels, further eliminating the redundancy. We present the complementarycondition and the balance condition to guide the design of structured sparsekernels, obtaining a balance among three aspects: model size, computationcomplexity and classification accuracy. Experimental results demonstrate theadvantage on the balance among these three aspects compared to interleavedgroup convolutions and Xception, and competitive performance compared to otherstate-of-the-art architecture design methods.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1804.06202v1'},\n",
       " 822: {'ID': 822,\n",
       "  'title': 'Contextualized Non-local Neural Networks for Sequence Learning',\n",
       "  'authors': ['Xuanjing Huang',\n",
       "   'Jian Tang',\n",
       "   'Jackie Chi Kit Cheung',\n",
       "   'Pengfei Liu',\n",
       "   'Shuaichen Chang'],\n",
       "  'published': '2018-11-21T05:14:37Z',\n",
       "  'updated': '2018-11-21T05:14:37Z',\n",
       "  'abstract': 'Recently, a large number of neural mechanisms and models have been proposedfor sequence learning, of which self-attention, as exemplified by theTransformer model, and graph neural networks (GNNs) have attracted muchattention. In this paper, we propose an approach that combines and draws on thecomplementary strengths of these two methods. Specifically, we proposecontextualized non-local neural networks (CN$^{\\\\textbf{3}}$), which can bothdynamically construct a task-specific structure of a sentence and leverage richlocal dependencies within a particular neighborhood.  Experimental results on ten NLP tasks in text classification, semanticmatching, and sequence labeling show that our proposed model outperformscompetitive baselines and discovers task-specific dependency structures, thusproviding better interpretability to users.',\n",
       "  'categories': ['cs.CL'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1811.08600v1'},\n",
       " 823: {'ID': 823,\n",
       "  'title': 'The Local Group galaxy candidates Cas dSph, Peg dSph=AndVI, and Cam A',\n",
       "  'authors': ['A. Aparicio',\n",
       "   'C. Gallart',\n",
       "   'D. Martinez-Delgado',\n",
       "   'W. L. Freedman'],\n",
       "  'published': '1999-05-05T22:27:19Z',\n",
       "  'updated': '1999-05-05T22:27:19Z',\n",
       "  'abstract': 'We present observations of the new Local Group galaxy candidates CassiopeiadSph, Pegasus dSph=AndVI and Camelopardalis A. Our deep color-magnitudediagrams show that the first two galaxies are certainly Local group members,and likely dSph galaxies at a distance similar to that of the Andromeda galaxy.Cam A seems to be a star forming galaxy considerably further away.',\n",
       "  'categories': ['astro-ph'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/astro-ph/9905051v1'},\n",
       " 824: {'ID': 824,\n",
       "  'title': 'Benchmark Visual Question Answer Models by using Focus Map',\n",
       "  'authors': ['Zhekai Zhang', 'Yueyang Xianzang', 'Wenda Qiu'],\n",
       "  'published': '2018-01-13T09:09:33Z',\n",
       "  'updated': '2018-01-13T09:09:33Z',\n",
       "  'abstract': 'Inferring and Executing Programs for Visual Reasoning proposes a model forvisual reasoning that consists of a program generator and an execution engineto avoid end-to-end models. To show that the model actually learns whichobjects to focus on to answer the questions, the authors give a visualizationof the norm of the gradient of the sum of the predicted answer scores withrespect to the final feature map. However, the authors do not evaluate theefficiency of focus map. This paper purposed a method for evaluating it. Wegenerate several kinds of questions to test different keywords. We infer focusmaps from the model by asking these questions and evaluate them by comparingwith the segmentation graph. Furthermore, this method can be applied to anymodel if focus maps can be inferred from it. By evaluating focus map ofdifferent models on the CLEVR dataset, we will show that CLEVR-iep model haslearned where to focus more than end-to-end models.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1801.05302v1'},\n",
       " 825: {'ID': 825,\n",
       "  'title': 'End-to-end Learning of Deep Visual Representations for Image Retrieval',\n",
       "  'authors': ['Albert Gordo', 'Diane Larlus', 'Jerome Revaud', 'Jon Almazan'],\n",
       "  'published': '2016-10-25T16:02:42Z',\n",
       "  'updated': '2017-05-05T15:34:09Z',\n",
       "  'abstract': 'While deep learning has become a key ingredient in the top performing methodsfor many computer vision tasks, it has failed so far to bring similarimprovements to instance-level image retrieval. In this article, we argue thatreasons for the underwhelming results of deep methods on image retrieval arethreefold: i) noisy training data, ii) inappropriate deep architecture, andiii) suboptimal training procedure. We address all three issues.  First, we leverage a large-scale but noisy landmark dataset and develop anautomatic cleaning method that produces a suitable training set for deepretrieval. Second, we build on the recent R-MAC descriptor, show that it can beinterpreted as a deep and differentiable architecture, and present improvementsto enhance it. Last, we train this network with a siamese architecture thatcombines three streams with a triplet loss. At the end of the training process,the proposed architecture produces a global image representation in a singleforward pass that is well suited for image retrieval. Extensive experimentsshow that our approach significantly outperforms previous retrieval approaches,including state-of-the-art methods based on costly local descriptor indexingand spatial verification. On Oxford 5k, Paris 6k and Holidays, we respectivelyreport 94.7, 96.6, and 94.8 mean average precision. Our representations canalso be heavily compressed using product quantization with little loss inaccuracy. For additional material, please seewww.xrce.xerox.com/Deep-Image-Retrieval.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1610.07940v2'},\n",
       " 826: {'ID': 826,\n",
       "  'title': 'Reasoning Visual Dialogs with Structural and Partial Observations',\n",
       "  'authors': ['Zilong Zheng', 'Song-Chun Zhu', 'Wenguan Wang', 'Siyuan Qi'],\n",
       "  'published': '2019-04-11T06:46:15Z',\n",
       "  'updated': '2019-05-28T23:40:33Z',\n",
       "  'abstract': 'We propose a novel model to address the task of Visual Dialog which exhibitscomplex dialog structures. To obtain a reasonable answer based on the currentquestion and the dialog history, the underlying semantic dependencies betweendialog entities are essential. In this paper, we explicitly formalize this taskas inference in a graphical model with partially observed nodes and unknowngraph structures (relations in dialog). The given dialog entities are viewed asthe observed nodes. The answer to a given question is represented by a nodewith missing value. We first introduce an Expectation Maximization algorithm toinfer both the underlying dialog structures and the missing node values(desired answers). Based on this, we proceed to propose a differentiable graphneural network (GNN) solution that approximates this process. Experimentresults on the VisDial and VisDial-Q datasets show that our model outperformscomparative methods. It is also observed that our method can infer theunderlying dialog structure for better dialog reasoning.',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1904.05548v2'},\n",
       " 827: {'ID': 827,\n",
       "  'title': 'DenseNet for Dense Flow',\n",
       "  'authors': ['Shawn Newsam', 'Yi Zhu'],\n",
       "  'published': '2017-07-19T22:37:46Z',\n",
       "  'updated': '2017-07-19T22:37:46Z',\n",
       "  'abstract': 'Classical approaches for estimating optical flow have achieved rapid progressin the last decade. However, most of them are too slow to be applied inreal-time video analysis. Due to the great success of deep learning, recentwork has focused on using CNNs to solve such dense prediction problems. In thispaper, we investigate a new deep architecture, Densely Connected ConvolutionalNetworks (DenseNet), to learn optical flow. This specific architecture is idealfor the problem at hand as it provides shortcut connections throughout thenetwork, which leads to implicit deep supervision. We extend current DenseNetto a fully convolutional network to learn motion estimation in an unsupervisedmanner. Evaluation results on three standard benchmarks demonstrate thatDenseNet is a better fit than other widely adopted CNN architectures foroptical flow estimation.',\n",
       "  'categories': ['cs.CV', 'cs.MM'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1707.06316v1'},\n",
       " 828: {'ID': 828,\n",
       "  'title': 'Accelerated MRI with Un-trained Neural Networks',\n",
       "  'authors': ['Reinhard Heckel', 'Mohammad Zalbagi Darestani'],\n",
       "  'published': '2020-07-06T00:01:25Z',\n",
       "  'updated': '2020-11-20T23:06:26Z',\n",
       "  'abstract': 'Convolutional Neural Networks (CNNs) are highly effective for imagereconstruction problems. Typically, CNNs are trained on large amounts oftraining images. Recently, however, un-trained CNNs such as the Deep ImagePrior and Deep Decoder have achieved excellent performance for imagereconstruction problems such as denoising and inpainting, \\\\emph{without usingany training data}. Motivated by this development, we address thereconstruction problem arising in accelerated MRI with un-trained neuralnetworks. We propose a highly-optimized un-trained recovery approach based on avariation of the Deep Decoder. We show that the resulting method significantlyoutperforms conventional un-trained methods such as total-variation normminimization, as well as naive applications of un-trained networks. Mostimportantly, we achieve on-par performance with a standard trained baseline,the U-net, on the FastMRI dataset, a dataset for benchmarking deep learningbased reconstruction methods. While state-of-the-art trained methods stilloutperform our un-trained method, our work demonstrates that current trainedmethods only achieve a minor performance gain over un-trained methods, at thecost of a loss in robustness to out-of-distribution examples. Therefore,un-trained neural networks are a serious competitor to trained ones for medicalimaging.',\n",
       "  'categories': ['eess.IV', 'cs.CV', 'cs.LG', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2007.02471v2'},\n",
       " 829: {'ID': 829,\n",
       "  'title': 'Over-the-Air Adversarial Flickering Attacks against Video Recognition  Networks',\n",
       "  'authors': ['Itay Naeh', 'Roi Pony', 'Shie Mannor'],\n",
       "  'published': '2020-02-12T17:58:12Z',\n",
       "  'updated': '2020-11-27T14:39:25Z',\n",
       "  'abstract': 'Deep neural networks for video classification, just like image classificationnetworks, may be subjected to adversarial manipulation. The main differencebetween image classifiers and video classifiers is that the latter usually usetemporal information contained within the video. In this work we present amanipulation scheme for fooling video classifiers by introducing a flickeringtemporal perturbation that is practically unnoticeable by human observers andis implementable in the real world. After demonstrating the manipulation ofaction classification of single videos, we generalize the procedure to makeuniversal adversarial perturbation, achieving high fooling ratio. In addition,we generalize the universal perturbation and produce a temporal-invariantperturbation, which can be applied to the video without synchronizing theperturbation to the input. The attack was implemented on several target modelsand the transferability of the attack was demonstrated. These properties allowus to bridge the gap between simulated environment and real-world application,as will be demonstrated in this paper for the first time for an over-the-airflickering attack.',\n",
       "  'categories': ['cs.LG', 'cs.CR', 'cs.CV', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2002.05123v3'},\n",
       " 830: {'ID': 830,\n",
       "  'title': 'Feature Selection for Unsupervised Domain Adaptation using Optimal  Transport',\n",
       "  'authors': ['Ievgen Redko', 'Léo Gautheron', 'Carole Lartizien'],\n",
       "  'published': '2018-06-28T10:04:54Z',\n",
       "  'updated': '2018-06-28T10:04:54Z',\n",
       "  'abstract': 'In this paper, we propose a new feature selection method for unsuperviseddomain adaptation based on the emerging optimal transportation theory. We buildupon a recent theoretical analysis of optimal transport in domain adaptationand show that it can directly suggest a feature selection procedure leveragingthe shift between the domains. Based on this, we propose a novel algorithm thataims to sort features by their similarity across the source and target domains,where the order is obtained by analyzing the coupling matrix representing thesolution of the proposed optimal transportation problem. We evaluate our methodon a well-known benchmark data set and illustrate its capability of selectingcorrelated features leading to better classification performances. Furthermore,we show that the proposed algorithm can be used as a pre-processing step forexisting domain adaptation techniques ensuring an important speed-up in termsof the computational time while maintaining comparable results. Finally, wevalidate our algorithm on clinical imaging databases for computer-aideddiagnosis task with promising results.',\n",
       "  'categories': ['cs.LG', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1806.10861v1'},\n",
       " 831: {'ID': 831,\n",
       "  'title': 'Direct Visual-Inertial Odometry with Semi-Dense Mapping',\n",
       "  'authors': ['Guanghui Wang', 'Wenju Xu', 'Dongkyu Choi'],\n",
       "  'published': '2019-10-04T18:49:22Z',\n",
       "  'updated': '2019-10-04T18:49:22Z',\n",
       "  'abstract': 'The paper presents a direct visual-inertial odometry system. In particular, atightly coupled nonlinear optimization based method is proposed by integratingthe recent advances in direct dense tracking and Inertial Measurement Unit(IMU) pre-integration, and a factor graph optimization is adapted to estimatethe pose of the camera and rebuild a semi-dense map. Two sliding windows aremaintained in the proposed approach. The first one, based on Direct SparseOdometry (DSO), is to estimate the depths of candidate points for mapping anddense visual tracking. In the second one, measurements from the IMUpre-integration and dense visual tracking are fused probabilistically using atightly-coupled, optimization-based sensor fusion framework. As a result, theIMU pre-integration provides additional constraints to suppress the scale driftinduced by the visual odometry. Evaluations on real-world benchmark datasetsshow that the proposed method achieves competitive results in indoor scenes.',\n",
       "  'categories': ['cs.RO', 'cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1910.02106v1'},\n",
       " 832: {'ID': 832,\n",
       "  'title': 'CrDoCo: Pixel-level Domain Transfer with Cross-Domain Consistency',\n",
       "  'authors': ['Yun-Chun Chen',\n",
       "   'Yen-Yu Lin',\n",
       "   'Jia-Bin Huang',\n",
       "   'Ming-Hsuan Yang'],\n",
       "  'published': '2020-01-09T19:00:35Z',\n",
       "  'updated': '2020-01-09T19:00:35Z',\n",
       "  'abstract': 'Unsupervised domain adaptation algorithms aim to transfer the knowledgelearned from one domain to another (e.g., synthetic to real images). Theadapted representations often do not capture pixel-level domain shifts that arecrucial for dense prediction tasks (e.g., semantic segmentation). In thispaper, we present a novel pixel-wise adversarial domain adaptation algorithm.By leveraging image-to-image translation methods for data augmentation, our keyinsight is that while the translated images between domains may differ instyles, their predictions for the task should be consistent. We exploit thisproperty and introduce a cross-domain consistency loss that enforces ouradapted model to produce consistent predictions. Through extensive experimentalresults, we show that our method compares favorably against thestate-of-the-art on a wide variety of unsupervised domain adaptation tasks.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2001.03182v1'},\n",
       " 833: {'ID': 833,\n",
       "  'title': 'Unsupervised Single-Image Reflection Separation Using Perceptual Deep  Image Priors',\n",
       "  'authors': ['Mohamed Hefeeda',\n",
       "   'Seyed Mohammad Nourbakhsh',\n",
       "   'Hamed RahmaniKhezri',\n",
       "   'Suhong Kim'],\n",
       "  'published': '2020-09-01T21:08:30Z',\n",
       "  'updated': '2020-09-01T21:08:30Z',\n",
       "  'abstract': \"Reflections often degrade the quality of the image by obstructing thebackground scene. This is not desirable for everyday users, and it negativelyimpacts the performance of multimedia applications that process images withreflections. Most current methods for removing reflections utilizesupervised-learning models. However, these models require an extensive numberof image pairs to perform well, especially on natural images with reflection,which is difficult to achieve in practice. In this paper, we propose a novelunsupervised framework for single-image reflection separation. Instead oflearning from a large dataset, we optimize the parameters of two cross-coupleddeep convolutional networks on a target image to generate two exclusivebackground and reflection layers. In particular, we design a new architectureof the network to embed semantic features extracted from a pre-trained deepclassification network, which gives more meaningful separation similar to humanperception. Quantitative and qualitative results on commonly used datasets inthe literature show that our method's performance is at least on par with thestate-of-the-art supervised methods and, occasionally, better without requiringlarge training datasets. Our results also show that our method significantlyoutperforms the closest unsupervised method in the literature for removingreflections from single images.\",\n",
       "  'categories': ['cs.CV', 'cs.MM', 'I.5.4; I.4.9'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2009.00702v1'},\n",
       " 834: {'ID': 834,\n",
       "  'title': 'Ask, Acquire, and Attack: Data-free UAP Generation using Class  Impressions',\n",
       "  'authors': ['R. Venkatesh Babu',\n",
       "   'Phani Krishna Uppala',\n",
       "   'Konda Reddy Mopuri'],\n",
       "  'published': '2018-08-03T11:02:26Z',\n",
       "  'updated': '2018-08-03T11:02:26Z',\n",
       "  'abstract': 'Deep learning models are susceptible to input specific noise, calledadversarial perturbations. Moreover, there exist input-agnostic noise, calledUniversal Adversarial Perturbations (UAP) that can affect inference of themodels over most input samples. Given a model, there exist broadly twoapproaches to craft UAPs: (i) data-driven: that require data, and (ii)data-free: that do not require data samples. Data-driven approaches requireactual samples from the underlying data distribution and craft UAPs with highsuccess (fooling) rate. However, data-free approaches craft UAPs withoututilizing any data samples and therefore result in lesser success rates. Inthis paper, for data-free scenarios, we propose a novel approach that emulatesthe effect of data samples with class impressions in order to craft UAPs usingdata-driven objectives. Class impression for a given pair of category and modelis a generic representation (in the input space) of the samples belonging tothat category. Further, we present a neural network based generative model thatutilizes the acquired class impressions to learn crafting UAPs. Experimentalevaluation demonstrates that the learned generative model, (i) readily craftsUAPs via simple feed-forwarding through neural network layers, and (ii)achieves state-of-the-art success rates for data-free scenario and closer tothat for data-driven setting without actually utilizing any data samples.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1808.01153v1'},\n",
       " 835: {'ID': 835,\n",
       "  'title': 'On Universalized Adversarial and Invariant Perturbations',\n",
       "  'authors': ['Sandesh Kamath', 'Amit Deshpande', 'K V Subrahmanyam'],\n",
       "  'published': '2020-06-08T10:08:20Z',\n",
       "  'updated': '2020-06-08T10:08:20Z',\n",
       "  'abstract': 'Convolutional neural networks or standard CNNs (StdCNNs) aretranslation-equivariant models that achieve translation invariance when trainedon data augmented with sufficient translations. Recent work on equivariantmodels for a given group of transformations (e.g., rotations) has lead togroup-equivariant convolutional neural networks (GCNNs). GCNNs trained on dataaugmented with sufficient rotations achieve rotation invariance. Recent work byauthors arXiv:2002.11318 studies a trade-off between invariance and robustnessto adversarial attacks. In another related work arXiv:2005.08632, given anymodel and any input-dependent attack that satisfies a certain spectralproperty, the authors propose a universalization technique called SVD-Universalto produce a universal adversarial perturbation by looking at very few testexamples. In this paper, we study the effectiveness of SVD-Universal on GCNNsas they gain rotation invariance through higher degree of trainingaugmentation. We empirically observe that as GCNNs gain rotation invariancethrough training augmented with larger rotations, the fooling rate ofSVD-Universal gets better. To understand this phenomenon, we introduceuniversal invariant directions and study their relation to the universaladversarial direction produced by SVD-Universal.',\n",
       "  'categories': ['cs.LG', 'cs.CV', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2006.04449v1'},\n",
       " 836: {'ID': 836,\n",
       "  'title': 'Making History Matter: History-Advantage Sequence Training for Visual  Dialog',\n",
       "  'authors': ['Hanwang Zhang', 'Tianhao Yang', 'Zheng-Jun Zha'],\n",
       "  'published': '2019-02-25T14:58:35Z',\n",
       "  'updated': '2019-04-17T15:09:16Z',\n",
       "  'abstract': \"We study the multi-round response generation in visual dialog, where aresponse is generated according to a visually grounded conversational history.Given a triplet: an image, Q&amp;A history, and current question, all theprevailing methods follow a codec (i.e., encoder-decoder) fashion in asupervised learning paradigm: a multimodal encoder encodes the triplet into afeature vector, which is then fed into the decoder for the current answergeneration, supervised by the ground-truth. However, this conventionalsupervised learning does NOT take into account the impact of imperfect history,violating the conversational nature of visual dialog and thus making the codecmore inclined to learn history bias but not contextual reasoning. To this end,inspired by the actor-critic policy gradient in reinforcement learning, wepropose a novel training paradigm called History Advantage Sequence Training(HAST). Specifically, we intentionally impose wrong answers in the history,obtaining an adverse critic, and see how the historic error impacts the codec'sfuture behavior by History Advantage-a quantity obtained by subtracting theadverse critic from the gold reward of ground-truth history. Moreover, to makethe codec more sensitive to the history, we propose a novel attention networkcalled History-Aware Co-Attention Network (HACAN) which can be effectivelytrained by using HAST. Experimental results on three benchmarks: VisDialv0.9&amp;v1.0 and GuessWhat?!, show that the proposed HAST strategy consistentlyoutperforms the state-of-the-art supervised counterparts.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1902.09326v3'},\n",
       " 837: {'ID': 837,\n",
       "  'title': 'Adversarial Perturbations Fool Deepfake Detectors',\n",
       "  'authors': ['Shomik Jain', 'Apurva Gandhi'],\n",
       "  'published': '2020-03-24T00:54:02Z',\n",
       "  'updated': '2020-05-15T05:41:32Z',\n",
       "  'abstract': 'This work uses adversarial perturbations to enhance deepfake images and foolcommon deepfake detectors. We created adversarial perturbations using the FastGradient Sign Method and the Carlini and Wagner L2 norm attack in both blackboxand whitebox settings. Detectors achieved over 95% accuracy on unperturbeddeepfakes, but less than 27% accuracy on perturbed deepfakes. We also exploretwo improvements to deepfake detectors: (i) Lipschitz regularization, and (ii)Deep Image Prior (DIP). Lipschitz regularization constrains the gradient of thedetector with respect to the input in order to increase robustness to inputperturbations. The DIP defense removes perturbations using generativeconvolutional neural networks in an unsupervised manner. Regularizationimproved the detection of perturbed deepfakes on average, including a 10%accuracy boost in the blackbox case. The DIP defense achieved 95% accuracy onperturbed deepfakes that fooled the original detector, while retaining 98%accuracy in other cases on a 100 image subsample.',\n",
       "  'categories': ['cs.CV', 'cs.LG', 'cs.NE', 'stat.ML', 'I.4'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2003.10596v2'},\n",
       " 838: {'ID': 838,\n",
       "  'title': 'The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for  Semantic Segmentation',\n",
       "  'authors': ['Michal Drozdzal',\n",
       "   'Yoshua Bengio',\n",
       "   'Adriana Romero',\n",
       "   'Simon Jégou',\n",
       "   'David Vazquez'],\n",
       "  'published': '2016-11-28T20:27:54Z',\n",
       "  'updated': '2017-10-31T13:10:48Z',\n",
       "  'abstract': 'State-of-the-art approaches for semantic image segmentation are built onConvolutional Neural Networks (CNNs). The typical segmentation architecture iscomposed of (a) a downsampling path responsible for extracting coarse semanticfeatures, followed by (b) an upsampling path trained to recover the input imageresolution at the output of the model and, optionally, (c) a post-processingmodule (e.g. Conditional Random Fields) to refine the model predictions.  Recently, a new CNN architecture, Densely Connected Convolutional Networks(DenseNets), has shown excellent results on image classification tasks. Theidea of DenseNets is based on the observation that if each layer is directlyconnected to every other layer in a feed-forward fashion then the network willbe more accurate and easier to train.  In this paper, we extend DenseNets to deal with the problem of semanticsegmentation. We achieve state-of-the-art results on urban scene benchmarkdatasets such as CamVid and Gatech, without any further post-processing modulenor pretraining. Moreover, due to smart construction of the model, our approachhas much less parameters than currently published best entries for thesedatasets.  Code to reproduce the experiments is available here :https://github.com/SimJeg/FC-DenseNet/blob/master/train.py',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1611.09326v3'},\n",
       " 839: {'ID': 839,\n",
       "  'title': 'Learning to Deblur and Generate High Frame Rate Video with an Event  Camera',\n",
       "  'authors': ['Shi Boxin',\n",
       "   'Wang YIzhou',\n",
       "   'Teng Minggui',\n",
       "   'Huang Tiejun',\n",
       "   'Chen Haoyu'],\n",
       "  'published': '2020-03-02T13:02:05Z',\n",
       "  'updated': '2020-03-20T04:09:55Z',\n",
       "  'abstract': \"Event cameras are bio-inspired cameras which can measure the change ofintensity asynchronously with high temporal resolution. One of the eventcameras' advantages is that they do not suffer from motion blur when recordinghigh-speed scenes. In this paper, we formulate the deblurring task ontraditional cameras directed by events to be a residual learning one, and wepropose corresponding network architectures for effective learning ofdeblurring and high frame rate video generation tasks. We first train amodified U-Net network to restore a sharp image from a blurry image usingcorresponding events. Then we train another similar network with differentdownsampling blocks to generate high frame rate video using the restored sharpimage and events. Experiment results show that our method can restore sharperimages and videos than state-of-the-art methods.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2003.00847v2'},\n",
       " 840: {'ID': 840,\n",
       "  'title': 'Multi-scale Multi-band DenseNets for Audio Source Separation',\n",
       "  'authors': ['Yuki Mitsufuji', 'Naoya Takahashi'],\n",
       "  'published': '2017-06-29T05:56:06Z',\n",
       "  'updated': '2017-06-29T05:56:06Z',\n",
       "  'abstract': 'This paper deals with the problem of audio source separation. To handle thecomplex and ill-posed nature of the problems of audio source separation, thecurrent state-of-the-art approaches employ deep neural networks to obtaininstrumental spectra from a mixture. In this study, we propose a novel networkarchitecture that extends the recently developed densely connectedconvolutional network (DenseNet), which has shown excellent results on imageclassification tasks. To deal with the specific problem of audio sourceseparation, an up-sampling layer, block skip connection and band-dedicateddense blocks are incorporated on top of DenseNet. The proposed approach takesadvantage of long contextual information and outperforms state-of-the-artresults on SiSEC 2016 competition by a large margin in terms ofsignal-to-distortion ratio. Moreover, the proposed architecture requiressignificantly fewer parameters and considerably less training time comparedwith other methods.',\n",
       "  'categories': ['cs.SD', 'cs.CL', 'cs.MM'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1706.09588v1'},\n",
       " 841: {'ID': 841,\n",
       "  'title': 'Generalizable Data-free Objective for Crafting Universal Adversarial  Perturbations',\n",
       "  'authors': ['R. Venkatesh Babu', 'Konda Reddy Mopuri', 'Aditya Ganeshan'],\n",
       "  'published': '2018-01-24T17:36:57Z',\n",
       "  'updated': '2018-07-24T08:19:43Z',\n",
       "  'abstract': \"Machine learning models are susceptible to adversarial perturbations: smallchanges to input that can cause large changes in output. It is alsodemonstrated that there exist input-agnostic perturbations, called universaladversarial perturbations, which can change the inference of target model onmost of the data samples. However, existing methods to craft universalperturbations are (i) task specific, (ii) require samples from the trainingdata distribution, and (iii) perform complex optimizations. Additionally,because of the data dependence, fooling ability of the crafted perturbations isproportional to the available training data. In this paper, we present a novel,generalizable and data-free approaches for crafting universal adversarialperturbations. Independent of the underlying task, our objective achievesfooling via corrupting the extracted features at multiple layers. Therefore,the proposed objective is generalizable to craft image-agnostic perturbationsacross multiple vision tasks such as object recognition, semantic segmentation,and depth estimation. In the practical setting of black-box attack scenario(when the attacker does not have access to the target model and it's trainingdata), we show that our objective outperforms the data dependent objectives tofool the learned models. Further, via exploiting simple priors related to thedata distribution, our objective remarkably boosts the fooling ability of thecrafted perturbations. Significant fooling rates achieved by our objectiveemphasize that the current deep learning models are now at an increased risk,since our objective generalizes across multiple tasks without the requirementof training data for crafting the perturbations. To encourage reproducibleresearch, we have released the codes for our proposed algorithm.\",\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1801.08092v3'},\n",
       " 842: {'ID': 842,\n",
       "  'title': 'Omnidirectional DSO: Direct Sparse Odometry with Fisheye Cameras',\n",
       "  'authors': ['Vladyslav Usenko',\n",
       "   'Daniel Cremers',\n",
       "   'Jörg Stückler',\n",
       "   'Hidenobu Matsuki',\n",
       "   'Lukas von Stumberg'],\n",
       "  'published': '2018-08-08T13:34:31Z',\n",
       "  'updated': '2018-08-08T13:34:31Z',\n",
       "  'abstract': 'We propose a novel real-time direct monocular visual odometry foromnidirectional cameras. Our method extends direct sparse odometry (DSO) byusing the unified omnidirectional model as a projection function, which can beapplied to fisheye cameras with a field-of-view (FoV) well above 180 degrees.This formulation allows for using the full area of the input image even withstrong distortion, while most existing visual odometry methods can only use arectified and cropped part of it. Model parameters within an active keyframewindow are jointly optimized, including the intrinsic/extrinsic cameraparameters, 3D position of points, and affine brightness parameters. Thanks tothe wide FoV, image overlap between frames becomes bigger and points are morespatially distributed. Our results demonstrate that our method providesincreased accuracy and robustness over state-of-the-art visual odometryalgorithms.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1808.02775v1'},\n",
       " 843: {'ID': 843,\n",
       "  'title': 'Efficient Deep Learning of Non-local Features for Hyperspectral Image  Classification',\n",
       "  'authors': ['Liang Xiao',\n",
       "   'Jianyu Chen',\n",
       "   'Delu Pan',\n",
       "   'Chen Chen',\n",
       "   'Qian Du',\n",
       "   'Yu Shen',\n",
       "   'Sijie Zhu'],\n",
       "  'published': '2020-08-02T19:13:22Z',\n",
       "  'updated': '2020-08-02T19:13:22Z',\n",
       "  'abstract': \"Deep learning based methods, such as Convolution Neural Network (CNN), havedemonstrated their efficiency in hyperspectral image (HSI) classification.These methods can automatically learn spectral-spatial discriminative featureswithin local patches. However, for each pixel in an HSI, it is not only relatedto its nearby pixels but also has connections to pixels far away from itself.Therefore, to incorporate the long-range contextual information, a deep fullyconvolutional network (FCN) with an efficient non-local module, named ENL-FCN,is proposed for HSI classification. In the proposed framework, a deep FCNconsiders an entire HSI as input and extracts spectral-spatial information in alocal receptive field. The efficient non-local module is embedded in thenetwork as a learning unit to capture the long-range contextual information.Different from the traditional non-local neural networks, the long-rangecontextual information is extracted in a specially designed criss-cross pathfor computation efficiency. Furthermore, by using a recurrent operation, eachpixel's response is aggregated from all pixels of HSI. The benefits of ourproposed ENL-FCN are threefold: 1) the long-range contextual information isincorporated effectively, 2) the efficient module can be freely embedded in adeep neural network in a plug-and-play fashion, and 3) it has much fewerlearning parameters and requires less computational resources. The experimentsconducted on three popular HSI datasets demonstrate that the proposed methodachieves state-of-the-art classification performance with lower computationalcost in comparison with several leading deep neural networks for HSI.\",\n",
       "  'categories': ['cs.CV', 'eess.IV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2008.00542v1'},\n",
       " 844: {'ID': 844,\n",
       "  'title': 'Channel Normalization in Convolutional Neural Network avoids Vanishing  Gradients',\n",
       "  'authors': ['Reinhard Heckel', 'Zhenwei Dai'],\n",
       "  'published': '2019-07-22T19:25:25Z',\n",
       "  'updated': '2019-07-22T19:25:25Z',\n",
       "  'abstract': 'Normalization layers are widely used in deep neural networks to stabilizetraining. In this paper, we consider the training of convolutional neuralnetworks with gradient descent on a single training example. This optimizationproblem arises in recent approaches for solving inverse problems such as thedeep image prior or the deep decoder. We show that for this setup, channelnormalization, which centers and normalizes each channel individually, avoidsvanishing gradients, whereas, without normalization, gradients vanish whichprevents efficient optimization. This effect prevails in deep single-channellinear convolutional networks, and we show that without channel normalization,gradient descent takes at least exponentially many steps to come close to anoptimum. Contrary, with channel normalization, the gradients remain bounded,thus avoiding exploding gradients.',\n",
       "  'categories': ['cs.LG', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1907.09539v1'},\n",
       " 845: {'ID': 845,\n",
       "  'title': 'Make One-Shot Video Object Segmentation Efficient Again',\n",
       "  'authors': ['Laura Leal-Taixe', 'Tim Meinhardt'],\n",
       "  'published': '2020-12-03T12:21:23Z',\n",
       "  'updated': '2020-12-03T12:21:23Z',\n",
       "  'abstract': 'Video object segmentation (VOS) describes the task of segmenting a set ofobjects in each frame of a video. In the semi-supervised setting, the firstmask of each object is provided at test time. Following the one-shot principle,fine-tuning VOS methods train a segmentation model separately on each givenobject mask. However, recently the VOS community has deemed such a test timeoptimization and its impact on the test runtime as unfeasible. To mitigate theinefficiencies of previous fine-tuning approaches, we present efficientOne-Shot Video Object Segmentation (e-OSVOS). In contrast to most VOSapproaches, e-OSVOS decouples the object detection task and predicts only localsegmentation masks by applying a modified version of Mask R-CNN. The one-shottest runtime and performance are optimized without a laborious and handcraftedhyperparameter search. To this end, we meta learn the model initialization andlearning rates for the test time optimization. To achieve optimal learningbehavior, we predict individual learning rates at a neuron level. Furthermore,we apply an online adaptation to address the common performance degradationthroughout a sequence by continuously fine-tuning the model on previous maskpredictions supported by a frame-to-frame bounding box propagation. e-OSVOSprovides state-of-the-art results on DAVIS 2016, DAVIS 2017, and YouTube-VOSfor one-shot fine-tuning methods while reducing the test runtime substantially.  Code is available at https://github.com/dvl-tum/e-osvos.',\n",
       "  'categories': ['cs.CV', 'cs.LG', 'cs.RO'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2012.01866v1'},\n",
       " 846: {'ID': 846,\n",
       "  'title': 'Conditional Random Fields as Recurrent Neural Networks for 3D Medical  Imaging Segmentation',\n",
       "  'authors': ['Miguel Monteiro',\n",
       "   'Arlindo L. Oliveira',\n",
       "   'Mário A. T. Figueiredo'],\n",
       "  'published': '2018-07-19T14:37:43Z',\n",
       "  'updated': '2018-07-19T14:37:43Z',\n",
       "  'abstract': 'The Conditional Random Field as a Recurrent Neural Network layer is arecently proposed algorithm meant to be placed on top of an existingFully-Convolutional Neural Network to improve the quality of semanticsegmentation. In this paper, we test whether this algorithm, which was shown toimprove semantic segmentation for 2D RGB images, is able to improvesegmentation quality for 3D multi-modal medical images. We developed animplementation of the algorithm which works for any number of spatialdimensions, input/output image channels, and reference image channels. As faras we know this is the first publicly available implementation of this sort. Wetested the algorithm with two distinct 3D medical imaging datasets, weconcluded that the performance differences observed were not statisticallysignificant. Finally, in the discussion section of the paper, we go into thereasons as to why this technique transfers poorly from natural images tomedical images.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1807.07464v1'},\n",
       " 847: {'ID': 847,\n",
       "  'title': 'HAR-Net: Joint Learning of Hybrid Attention for Single-stage Object  Detection',\n",
       "  'authors': ['Shengjin Wang', 'Ya-Li Li'],\n",
       "  'published': '2019-04-25T03:37:19Z',\n",
       "  'updated': '2019-04-25T03:37:19Z',\n",
       "  'abstract': 'Object detection has been a challenging task in computer vision. Althoughsignificant progress has been made in object detection with deep neuralnetworks, the attention mechanism is far from development. In this paper, wepropose the hybrid attention mechanism for single-stage object detection.First, we present the modules of spatial attention, channel attention andaligned attention for single-stage object detection. In particular, stackeddilated convolution layers with symmetrically fixed rates are constructed tolearn spatial attention. The channel attention is proposed with the cross-levelgroup normalization and squeeze-and-excitation module. Aligned attention isconstructed with organized deformable filters. Second, the three kinds ofattention are unified to construct the hybrid attention mechanism. We thenembed the hybrid attention into Retina-Net and propose the efficientsingle-stage HAR-Net for object detection. The attention modules and theproposed HAR-Net are evaluated on the COCO detection dataset. Experimentsdemonstrate that hybrid attention can significantly improve the detectionaccuracy and the HAR-Net can achieve the state-of-the-art 45.8\\\\% mAP,outperform existing single-stage object detectors.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1904.11141v1'},\n",
       " 848: {'ID': 848,\n",
       "  'title': 'Cascaded Sparse Spatial Bins for Efficient and Effective Generic Object  Detection',\n",
       "  'authors': ['Jiri Matas', 'David Novotny'],\n",
       "  'published': '2015-04-27T11:14:27Z',\n",
       "  'updated': '2015-10-13T10:30:22Z',\n",
       "  'abstract': 'A novel efficient method for extraction of object proposals is introduced.Its \"objectness\" function exploits deep spatial pyramid features, a novelfast-to-compute HoG-based edge statistic and the EdgeBoxes score. Theefficiency is achieved by the use of spatial bins in a novel combination withsparsity-inducing group normalized SVM. State-of-the-art recall performance isachieved on Pascal VOC07, significantly outperforming methods with comparablespeed. Interestingly, when only 100 proposals per image are considered themethod attains 78% recall on VOC07. The method improves mAP of the RCNNstate-of-the-art class-specific detector, increasing it by 10 points when only50 proposals are used in each image. The system trained on twenty classesperforms well on the two hundred class ILSVRC2013 set confirming generalizationcapability.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1504.07029v2'},\n",
       " 849: {'ID': 849,\n",
       "  'title': 'Nuclear Norm based Matrix Regression with Applications to Face  Recognition with Occlusion and Illumination Changes',\n",
       "  'authors': ['Jian Yang',\n",
       "   'Jianjun Qian',\n",
       "   'Fanlong Zhang',\n",
       "   'Lei Luo',\n",
       "   'Yicheng Gao'],\n",
       "  'published': '2014-05-06T09:46:28Z',\n",
       "  'updated': '2014-05-06T09:46:28Z',\n",
       "  'abstract': 'Recently regression analysis becomes a popular tool for face recognition. Theexisting regression methods all use the one-dimensional pixel-based errormodel, which characterizes the representation error pixel by pixel individuallyand thus neglects the whole structure of the error image. We observe thatocclusion and illumination changes generally lead to a low-rank error image. Tomake use of this low-rank structural information, this paper presents atwo-dimensional image matrix based error model, i.e. matrix regression, forface representation and classification. Our model uses the minimal nuclear normof representation error image as a criterion, and the alternating directionmethod of multipliers method to calculate the regression coefficients. Comparedwith the current regression methods, the proposed Nuclear Norm based MatrixRegression (NMR) model is more robust for alleviating the effect ofillumination, and more intuitive and powerful for removing the structural noisecaused by occlusion. We experiment using four popular face image databases, theExtended Yale B database, the AR database, the Multi-PIE and the FRGC database.Experimental results demonstrate the performance advantage of NMR over thestate-of-the-art regression based face recognition methods.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1405.1207v1'},\n",
       " 850: {'ID': 850,\n",
       "  'title': 'Learning Goal-Oriented Visual Dialog via Tempered Policy Gradient',\n",
       "  'authors': ['Volker Tresp', 'Rui Zhao'],\n",
       "  'published': '2018-07-02T15:14:43Z',\n",
       "  'updated': '2020-05-24T08:03:58Z',\n",
       "  'abstract': 'Learning goal-oriented dialogues by means of deep reinforcement learning hasrecently become a popular research topic. However, commonly used policy-baseddialogue agents often end up focusing on simple utterances and suboptimalpolicies. To mitigate this problem, we propose a class of noveltemperature-based extensions for policy gradient methods, which are referred toas Tempered Policy Gradients (TPGs). On a recent AI-testbed, i.e., theGuessWhat?! game, we achieve significant improvements with two innovations. Thefirst one is an extension of the state-of-the-art solutions with Seq2Seq andMemory Network structures that leads to an improvement of 7%. The second one isthe application of our newly developed TPG methods, which improves theperformance additionally by around 5% and, even more importantly, helps producemore convincing utterances.',\n",
       "  'categories': ['cs.LG', 'cs.AI', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1807.00737v5'},\n",
       " 851: {'ID': 851,\n",
       "  'title': 'EPNAS: Efficient Progressive Neural Architecture Search',\n",
       "  'authors': ['Feng Yan',\n",
       "   'Greg Diamos',\n",
       "   'Syed Zawad',\n",
       "   'Sercan Arik',\n",
       "   'Peng Wang',\n",
       "   'Yanqi Zhou',\n",
       "   'Haonan Yu'],\n",
       "  'published': '2019-07-07T03:50:42Z',\n",
       "  'updated': '2019-07-07T03:50:42Z',\n",
       "  'abstract': 'In this paper, we propose Efficient Progressive Neural Architecture Search(EPNAS), a neural architecture search (NAS) that efficiently handles largesearch space through a novel progressive search policy with performanceprediction based on REINFORCE~\\\\cite{Williams.1992.PG}. EPNAS is designed tosearch target networks in parallel, which is more scalable on parallel systemssuch as GPU/TPU clusters. More importantly, EPNAS can be generalized toarchitecture search with multiple resource constraints, \\\\eg, model size,compute complexity or intensity, which is crucial for deployment in widespreadplatforms such as mobile and cloud. We compare EPNAS against otherstate-of-the-art (SoTA) network architectures (\\\\eg,MobileNetV2~\\\\cite{mobilenetv2}) and efficient NAS algorithms (\\\\eg,ENAS~\\\\cite{pham2018efficient}, and PNAS~\\\\cite{Liu2017b}) on image recognitiontasks using CIFAR10 and ImageNet. On both datasets, EPNAS is superior \\\\wrtarchitecture searching speed and recognition accuracy.',\n",
       "  'categories': ['cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1907.04648v1'},\n",
       " 852: {'ID': 852,\n",
       "  'title': 'IIIT-AR-13K: A New Dataset for Graphical Object Detection in Documents',\n",
       "  'authors': ['Ajoy Mondal', 'Peter Lipps', 'C. V. Jawahar'],\n",
       "  'published': '2020-08-06T10:59:20Z',\n",
       "  'updated': '2020-08-06T10:59:20Z',\n",
       "  'abstract': 'We introduce a new dataset for graphical object detection in businessdocuments, more specifically annual reports. This dataset, IIIT-AR-13k, iscreated by manually annotating the bounding boxes of graphical or page objectsin publicly available annual reports. This dataset contains a total of 13kannotated page images with objects in five different popular categories -table, figure, natural image, logo, and signature. It is the largest manuallyannotated dataset for graphical object detection. Annual reports created inmultiple languages for several years from various companies bring highdiversity into this dataset. We benchmark IIIT-AR-13K dataset with two state ofthe art graphical object detection techniques using Faster R-CNN [20] and MaskR-CNN [11] and establish high baselines for further research. Our dataset ishighly effective as training data for developing practical solutions forgraphical object detection in both business documents and technical articles.By training with IIIT-AR-13K, we demonstrate the feasibility of a singlesolution that can report superior performance compared to the equivalent onestrained with a much larger amount of data, for table detection. We hope thatour dataset helps in advancing the research for detecting various types ofgraphical objects in business documents.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2008.02569v1'},\n",
       " 853: {'ID': 853,\n",
       "  'title': 'Escoin: Efficient Sparse Convolutional Neural Network Inference on GPUs',\n",
       "  'authors': ['Xuhao Chen'],\n",
       "  'published': '2018-02-28T06:31:45Z',\n",
       "  'updated': '2019-04-03T21:11:27Z',\n",
       "  'abstract': 'Deep neural networks have achieved remarkable accuracy in many artificialintelligence applications, e.g. computer vision, at the cost of a large numberof parameters and high computational complexity. Weight pruning can compressDNN models by removing redundant parameters in the networks, but it bringssparsity in the weight matrix, and therefore makes the computation inefficienton GPUs. Although pruning can remove more than 80% of the weights, it actuallyhurts inference performance (speed) when running models on GPUs.  Two major problems cause this unsatisfactory performance on GPUs. First,lowering convolution onto matrix multiplication reduces data reuseopportunities and wastes memory bandwidth. Second, the sparsity brought bypruning makes the computation irregular, which leads to inefficiency whenrunning on massively parallel GPUs. To overcome these two limitations, wepropose Escort, an efficient sparse convolutional neural networks on GPUs.Instead of using the lowering method, we choose to compute the sparseconvolutions directly. We then orchestrate the parallelism and locality for thedirect sparse convolution kernel, and apply customized optimization techniquesto further improve performance. Evaluation on NVIDIA GPUs show that Escort canimprove sparse convolution speed by 2.63x and 3.07x, and inference speed by1.43x and 1.69x, compared to CUBLAS and CUSPARSE respectively.',\n",
       "  'categories': ['cs.DC', 'cs.CV', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1802.10280v2'},\n",
       " 854: {'ID': 854,\n",
       "  'title': 'Combining Noise-to-Image and Image-to-Image GANs: Brain MR Image  Augmentation for Tumor Detection',\n",
       "  'authors': ['Ryosuke Araki',\n",
       "   'Yujiro Furukawa',\n",
       "   'Hideki Nakayama',\n",
       "   'Giancarlo Mauri',\n",
       "   'Yudai Nagano',\n",
       "   'Leonardo Rundo',\n",
       "   'Hideaki Hayashi',\n",
       "   'Changhee Han'],\n",
       "  'published': '2019-05-31T08:14:19Z',\n",
       "  'updated': '2019-10-09T12:20:15Z',\n",
       "  'abstract': 'Convolutional Neural Networks (CNNs) achieve excellent computer-assisteddiagnosis with sufficient annotated training data. However, most medicalimaging datasets are small and fragmented. In this context, GenerativeAdversarial Networks (GANs) can synthesize realistic/diverse additionaltraining images to fill the data lack in the real image distribution;researchers have improved classification by augmenting data with noise-to-image(e.g., random noise samples to diverse pathological images) or image-to-imageGANs (e.g., a benign image to a malignant one). Yet, no research has reportedresults combining noise-to-image and image-to-image GANs for furtherperformance boost. Therefore, to maximize the DA effect with the GANcombinations, we propose a two-step GAN-based DA that generates and refinesbrain Magnetic Resonance (MR) images with/without tumors separately: (i)Progressive Growing of GANs (PGGANs), multi-stage noise-to-image GAN forhigh-resolution MR image generation, first generates realistic/diverse 256 X256 images; (ii) Multimodal UNsupervised Image-to-image Translation (MUNIT)that combines GANs/Variational AutoEncoders or SimGAN that uses a DA-focusedGAN loss, further refines the texture/shape of the PGGAN-generated imagessimilarly to the real ones. We thoroughly investigate CNN-based tumorclassification results, also considering the influence of pre-training onImageNet and discarding weird-looking GAN-generated images. The results showthat, when combined with classic DA, our two-step GAN-based DA cansignificantly outperform the classic DA alone, in tumor detection (i.e.,boosting sensitivity 93.67% to 97.48%) and also in other medical imaging tasks.',\n",
       "  'categories': ['eess.IV', 'cs.AI', 'cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1905.13456v3'},\n",
       " 855: {'ID': 855,\n",
       "  'title': 'Online Adaptation of Convolutional Neural Networks for Video Object  Segmentation',\n",
       "  'authors': ['Bastian Leibe', 'Paul Voigtlaender'],\n",
       "  'published': '2017-06-28T17:02:39Z',\n",
       "  'updated': '2017-08-01T15:18:18Z',\n",
       "  'abstract': 'We tackle the task of semi-supervised video object segmentation, i.e.segmenting the pixels belonging to an object in the video using the groundtruth pixel mask for the first frame. We build on the recently introducedone-shot video object segmentation (OSVOS) approach which uses a pretrainednetwork and fine-tunes it on the first frame. While achieving impressiveperformance, at test time OSVOS uses the fine-tuned network in unchanged formand is not able to adapt to large changes in object appearance. To overcomethis limitation, we propose Online Adaptive Video Object Segmentation (OnAVOS)which updates the network online using training examples selected based on theconfidence of the network and the spatial configuration. Additionally, we add apretraining step based on objectness, which is learned on PASCAL. Ourexperiments show that both extensions are highly effective and improve thestate of the art on DAVIS to an intersection-over-union score of 85.7%.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1706.09364v2'},\n",
       " 856: {'ID': 856,\n",
       "  'title': 'DMRM: A Dual-channel Multi-hop Reasoning Model for Visual Dialog',\n",
       "  'authors': ['Jiaming Xu',\n",
       "   'Peng Li',\n",
       "   'Jie Zhou',\n",
       "   'Bo Xu',\n",
       "   'Fandong Meng',\n",
       "   'Feilong Chen'],\n",
       "  'published': '2019-12-18T03:09:12Z',\n",
       "  'updated': '2019-12-18T03:09:12Z',\n",
       "  'abstract': 'Visual Dialog is a vision-language task that requires an AI agent to engagein a conversation with humans grounded in an image. It remains a challengingtask since it requires the agent to fully understand a given question beforemaking an appropriate response not only from the textual dialog history, butalso from the visually-grounded information. While previous models typicallyleverage single-hop reasoning or single-channel reasoning to deal with thiscomplex multimodal reasoning task, which is intuitively insufficient. In thispaper, we thus propose a novel and more powerful Dual-channel Multi-hopReasoning Model for Visual Dialog, named DMRM. DMRM synchronously capturesinformation from the dialog history and the image to enrich the semanticrepresentation of the question by exploiting dual-channel reasoning.Specifically, DMRM maintains a dual channel to obtain the question- andhistory-aware image features and the question- and image-aware dialog historyfeatures by a mulit-hop reasoning process in each channel. Additionally, wealso design an effective multimodal attention to further enhance the decoder togenerate more accurate responses. Experimental results on the VisDial v0.9 andv1.0 datasets demonstrate that the proposed model is effective and outperformscompared models by a significant margin.',\n",
       "  'categories': ['cs.CL'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1912.08360v1'},\n",
       " 857: {'ID': 857,\n",
       "  'title': 'PyramidBox++: High Performance Detector for Finding Tiny Face',\n",
       "  'authors': ['Jingtuo Liu', 'Xu Tang', 'Zhihang Li', 'Ran He', 'Junyu Han'],\n",
       "  'published': '2019-03-31T11:44:31Z',\n",
       "  'updated': '2019-03-31T11:44:31Z',\n",
       "  'abstract': 'With the rapid development of deep convolutional neural network, facedetection has made great progress in recent years. WIDER FACE dataset, as amain benchmark, contributes greatly to this area. A large amount of methodshave been put forward where PyramidBox designs an effective data augmentationstrategy (Data-anchor-sampling) and context-based module for face detector. Inthis report, we improve each part to further boost the performance, includingBalanced-data-anchor-sampling, Dual-PyramidAnchors and Dense Context Module.Specifically, Balanced-data-anchor-sampling obtains more uniform sampling offaces with different sizes. Dual-PyramidAnchors facilitate feature learning byintroducing progressive anchor loss. Dense Context Module with dense connectionnot only enlarges receptive filed, but also passes information efficiently.Integrating these techniques, PyramidBox++ is constructed and achievesstate-of-the-art performance in hard set.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1904.00386v1'},\n",
       " 858: {'ID': 858,\n",
       "  'title': 'Learning to Deblur Images with Exemplars',\n",
       "  'authors': ['Zhe Hu', 'Jinshan Pan', 'Wenqi Ren', 'Ming-Hsuan Yang'],\n",
       "  'published': '2018-05-15T00:26:15Z',\n",
       "  'updated': '2018-05-15T00:26:15Z',\n",
       "  'abstract': 'Human faces are one interesting object class with numerous applications.While significant progress has been made in the generic deblurring problem,existing methods are less effective for blurry face images. The success of thestate-of-the-art image deblurring algorithms stems mainly from implicit orexplicit restoration of salient edges for kernel estimation. However, existingmethods are less effective as only few edges can be restored from blurry faceimages for kernel estimation. In this paper, we address the problem ofdeblurring face images by exploiting facial structures. We propose a deblurringalgorithm based on an exemplar dataset without using coarse-to-fine strategiesor heuristic edge selections. In addition, we develop a convolutional neuralnetwork to restore sharp edges from blurry images for deblurring. Extensiveexperiments against the state-of-the-art methods demonstrate the effectivenessof the proposed algorithms for deblurring face images. In addition, we show theproposed algorithms can be applied to image deblurring for other objectclasses.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1805.05503v1'},\n",
       " 859: {'ID': 859,\n",
       "  'title': 'Disentangling Homophemes in Lip Reading using Perplexity Analysis',\n",
       "  'authors': ['Perry Xiao', 'Daqing Chen', 'Souheil Fenghour', 'Kun Guo'],\n",
       "  'published': '2020-11-28T12:12:17Z',\n",
       "  'updated': '2020-11-28T12:12:17Z',\n",
       "  'abstract': 'The performance of automated lip reading using visemes as a classificationschema has achieved less success compared with the use of ASCII characters andwords largely due to the problem of different words sharing identical visemes.The Generative Pre-Training transformer is an effective autoregressive languagemodel used for many tasks in Natural Language Processing, including sentenceprediction and text classification.  This paper proposes a new application for this model and applies it in thecontext of lip reading, where it serves as a language model to convert visualspeech in the form of visemes, to language in the form of words and sentences.The network uses the search for optimal perplexity to perform theviseme-to-word mapping and is thus a solution to the one-to-many mappingproblem that exists whereby various words that sound different when spoken lookidentical. This paper proposes a method to tackle the one-to-many mappingproblem when performing automated lip reading using solely visual cues in twoseparate scenarios: the first scenario is where the word boundary, that is, thebeginning and the ending of a word, is unknown; and the second scenario iswhere the boundary is known.  Sentences from the benchmark BBC dataset \"Lip Reading Sentences in theWild\"(LRS2), are classified with a character error rate of 10.7% and a worderror rate of 18.0%. The main contribution of this paper is to propose a methodof predicting words through the use of perplexity analysis when only visualcues are present, using an autoregressive language model.',\n",
       "  'categories': ['cs.CL'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2012.07528v1'},\n",
       " 860: {'ID': 860,\n",
       "  'title': 'DCIL: Deep Contextual Internal Learning for Image Restoration and Image  Retargeting',\n",
       "  'authors': ['Shanmuganathan Raman', 'Indra Deep Mastan'],\n",
       "  'published': '2019-12-09T18:12:49Z',\n",
       "  'updated': '2019-12-09T18:12:49Z',\n",
       "  'abstract': 'Recently, there is a vast interest in developing methods which areindependent of the training samples such as deep image prior, zero-shotlearning, and internal learning. The methods above are based on the common goalof maximizing image features learning from a single image despite inherenttechnical diversity. In this work, we bridge the gap between the variousunsupervised approaches above and propose a general framework for imagerestoration and image retargeting. We use contextual feature learning andinternal learning to improvise the structure similarity between the source andthe target images. We perform image resize application in the following setups:classical image resize using super-resolution, a challenging image resize wherethe low-resolution image contains noise, and content-aware image resize usingimage retargeting. We also provide comparisons to the relevant state-of-the-artmethods.',\n",
       "  'categories': ['eess.IV', 'cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1912.04229v1'},\n",
       " 861: {'ID': 861,\n",
       "  'title': 'Conditional deep surrogate models for stochastic, high-dimensional, and  multi-fidelity systems',\n",
       "  'authors': ['Paris Perdikaris', 'Yibo Yang'],\n",
       "  'published': '2019-01-15T15:20:33Z',\n",
       "  'updated': '2019-01-15T15:20:33Z',\n",
       "  'abstract': 'We present a probabilistic deep learning methodology that enables theconstruction of predictive data-driven surrogates for stochastic systems.Leveraging recent advances in variational inference with implicitdistributions, we put forth a statistical inference framework that enables theend-to-end training of surrogate models on paired input-output observationsthat may be stochastic in nature, originate from different information sourcesof variable fidelity, or be corrupted by complex noise processes. The resultingsurrogates can accommodate high-dimensional inputs and outputs and are able toreturn predictions with quantified uncertainty. The effectiveness our approachis demonstrated through a series of canonical studies, including the regressionof noisy data, multi-fidelity modeling of stochastic processes, and uncertaintypropagation in high-dimensional dynamical systems.',\n",
       "  'categories': ['stat.ML', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1901.04878v1'},\n",
       " 862: {'ID': 862,\n",
       "  'title': 'Four Things Everyone Should Know to Improve Batch Normalization',\n",
       "  'authors': ['Michael J. Dinneen', 'Cecilia Summers'],\n",
       "  'published': '2019-06-09T01:14:48Z',\n",
       "  'updated': '2020-02-14T05:20:53Z',\n",
       "  'abstract': 'A key component of most neural network architectures is the use ofnormalization layers, such as Batch Normalization. Despite its common use andlarge utility in optimizing deep architectures, it has been challenging both togenerically improve upon Batch Normalization and to understand thecircumstances that lend themselves to other enhancements. In this paper, weidentify four improvements to the generic form of Batch Normalization and thecircumstances under which they work, yielding performance gains across allbatch sizes while requiring no additional computation during training. Thesecontributions include proposing a method for reasoning about the currentexample in inference normalization statistics, fixing a training vs. inferencediscrepancy; recognizing and validating the powerful regularization effect ofGhost Batch Normalization for small and medium batch sizes; examining theeffect of weight decay regularization on the scaling and shifting parametersgamma and beta; and identifying a new normalization algorithm for very smallbatch sizes by combining the strengths of Batch and Group Normalization. Wevalidate our results empirically on six datasets: CIFAR-100, SVHN, Caltech-256,Oxford Flowers-102, CUB-2011, and ImageNet.',\n",
       "  'categories': ['cs.LG', 'cs.CV', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1906.03548v2'},\n",
       " 863: {'ID': 863,\n",
       "  'title': 'Accurate Prostate Cancer Detection and Segmentation on Biparametric MRI  using Non-local Mask R-CNN with Histopathological Ground Truth',\n",
       "  'authors': ['Otto Ettala',\n",
       "   'Ning Wen',\n",
       "   'Ivan Jambor',\n",
       "   'Zhenzhen Dai',\n",
       "   'Pekka Taimen',\n",
       "   'Peter Boström',\n",
       "   'Harri Merisaari',\n",
       "   'Craig Rogers',\n",
       "   'Milan Pantelic',\n",
       "   'Hannu Aronen',\n",
       "   'Mohamed Elshaikh'],\n",
       "  'published': '2020-10-28T21:07:09Z',\n",
       "  'updated': '2020-10-28T21:07:09Z',\n",
       "  'abstract': 'Purpose: We aimed to develop deep machine learning (DL) models to improve thedetection and segmentation of intraprostatic lesions (IL) on bp-MRI by usingwhole amount prostatectomy specimen-based delineations. We also aimed toinvestigate whether transfer learning and self-training would improve resultswith small amount labelled data.  Methods: 158 patients had suspicious lesions delineated on MRI based onbp-MRI, 64 patients had ILs delineated on MRI based on whole mountprostatectomy specimen sections, 40 patients were unlabelled. A non-local MaskR-CNN was proposed to improve the segmentation accuracy. Transfer learning wasinvestigated by fine-tuning a model trained using MRI-based delineations withprostatectomy-based delineations. Two label selection strategies wereinvestigated in self-training. The performance of models was evaluated by 3Ddetection rate, dice similarity coefficient (DSC), 95 percentile Hausdrauff (95HD, mm) and true positive ratio (TPR).  Results: With prostatectomy-based delineations, the non-local Mask R-CNN withfine-tuning and self-training significantly improved all evaluation metrics.For the model with the highest detection rate and DSC, 80.5% (33/41) of lesionsin all Gleason Grade Groups (GGG) were detected with DSC of 0.548[0.165], 95 HDof 5.72[3.17] and TPR of 0.613[0.193]. Among them, 94.7% (18/19) of lesionswith GGG &gt; 2 were detected with DSC of 0.604[0.135], 95 HD of 6.26[3.44] andTPR of 0.580[0.190].  Conclusion: DL models can achieve high prostate cancer detection andsegmentation accuracy on bp-MRI based on annotations from histologic images. Tofurther improve the performance, more data with annotations of both MRI andwhole amount prostatectomy specimens are required.',\n",
       "  'categories': ['eess.IV', 'cs.CV', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2010.15233v1'},\n",
       " 864: {'ID': 864,\n",
       "  'title': 'CLOSURE: Assessing Systematic Generalization of CLEVR Models',\n",
       "  'authors': ['Shikhar Murty',\n",
       "   'Aaron Courville',\n",
       "   'Dzmitry Bahdanau',\n",
       "   'Yoshua Bengio',\n",
       "   'Harm de Vries',\n",
       "   'Philippe Beaudoin',\n",
       "   \"Timothy J. O'Donnell\"],\n",
       "  'published': '2019-12-12T05:56:53Z',\n",
       "  'updated': '2020-10-17T23:58:06Z',\n",
       "  'abstract': 'The CLEVR dataset of natural-looking questions about 3D-rendered scenes hasrecently received much attention from the research community. A number ofmodels have been proposed for this task, many of which achieved very highaccuracies of around 97-99%. In this work, we study how systematic thegeneralization of such models is, that is to which extent they are capable ofhandling novel combinations of known linguistic constructs. To this end, wetest models\\' understanding of referring expressions based on matching objectproperties (such as e.g. \"another cube that is the same size as the browncube\") in novel contexts. Our experiments on the thereby constructed CLOSUREbenchmark show that state-of-the-art models often do not exhibit systematicityafter being trained on CLEVR. Surprisingly, we find that an explicitlycompositional Neural Module Network model also generalizes badly on CLOSURE,even when it has access to the ground-truth programs at test time. We improvethe NMN\\'s systematic generalization by developing a novel Vector-NMN modulearchitecture with vector-valued inputs and outputs. Lastly, we investigate howmuch few-shot transfer learning can help models that are pretrained on CLEVR toadapt to CLOSURE. Our few-shot learning experiments contrast the adaptationbehavior of the models with intermediate discrete programs with that of theend-to-end continuous models.',\n",
       "  'categories': ['cs.AI', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1912.05783v2'},\n",
       " 865: {'ID': 865,\n",
       "  'title': 'Vision and Language: from Visual Perception to Content Creation',\n",
       "  'authors': ['Tao Mei', 'Ting Yao', 'Wei Zhang'],\n",
       "  'published': '2019-12-26T14:07:20Z',\n",
       "  'updated': '2019-12-26T14:07:20Z',\n",
       "  'abstract': 'Vision and language are two fundamental capabilities of human intelligence.Humans routinely perform tasks through the interactions between vision andlanguage, supporting the uniquely human capacity to talk about what they see orhallucinate a picture on a natural-language description. The valid question ofhow language interacts with vision motivates us researchers to expand thehorizons of computer vision area. In particular, \"vision to language\" isprobably one of the most popular topics in the past five years, with asignificant growth in both volume of publications and extensive applications,e.g., captioning, visual question answering, visual dialog, languagenavigation, etc. Such tasks boost visual perception with more comprehensiveunderstanding and diverse linguistic representations. Going beyond theprogresses made in \"vision to language,\" language can also contribute to visionunderstanding and offer new possibilities of visual content creation, i.e.,\"language to vision.\" The process performs as a prism through which to createvisual content conditioning on the language inputs. This paper reviews therecent advances along these two dimensions: \"vision to language\" and \"languageto vision.\" More concretely, the former mainly focuses on the development ofimage/video captioning, as well as typical encoder-decoder structures andbenchmarks, while the latter summarizes the technologies of visual contentcreation. The real-world deployment or services of vision and language areelaborated as well.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1912.11872v1'},\n",
       " 866: {'ID': 866,\n",
       "  'title': 'Universal Adversarial Perturbations Generative Network for Speaker  Recognition',\n",
       "  'authors': ['Yue Wang',\n",
       "   'Jizheng Xu',\n",
       "   'Li Zhang',\n",
       "   'Chuanmin Jia',\n",
       "   'Wen Gao',\n",
       "   'Xinfeng Zhang',\n",
       "   'Jiguo Li',\n",
       "   'Siwei Ma'],\n",
       "  'published': '2020-04-07T14:22:10Z',\n",
       "  'updated': '2020-04-07T14:22:10Z',\n",
       "  'abstract': 'Attacking deep learning based biometric systems has drawn more and moreattention with the wide deployment of fingerprint/face/speaker recognitionsystems, given the fact that the neural networks are vulnerable to theadversarial examples, which have been intentionally perturbed to remain almostimperceptible for human. In this paper, we demonstrated the existence of theuniversal adversarial perturbations~(UAPs) for the speaker recognition systems.We proposed a generative network to learn the mapping from the low-dimensionalnormal distribution to the UAPs subspace, then synthesize the UAPs to perturbeany input signals to spoof the well-trained speaker recognition model with highprobability. Experimental results on TIMIT and LibriSpeech datasets demonstratethe effectiveness of our model.',\n",
       "  'categories': ['eess.AS', 'cs.CR', 'cs.SD'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2004.03428v1'},\n",
       " 867: {'ID': 867,\n",
       "  'title': 'Probabilistic framework for solving Visual Dialog',\n",
       "  'authors': ['Badri N. Patro', 'Vinay P. Namboodiri', 'Anupriy'],\n",
       "  'published': '2019-09-11T00:25:12Z',\n",
       "  'updated': '2019-10-17T07:30:39Z',\n",
       "  'abstract': \"In this paper, we propose a probabilistic framework for solving the task of`Visual Dialog'. Solving this task requires reasoning and understanding ofvisual modality, language modality, and common sense knowledge to answer.Various architectures have been proposed to solve this task by variants ofmulti-modal deep learning techniques that combine visual and languagerepresentations. However, we believe that it is crucial to understand andanalyze the sources of uncertainty for solving this task. Our approach allowsfor estimating uncertainty and also aids a diverse generation of answers. Theproposed approach is obtained through a probabilistic representation modulethat provides us with representations for image, question and conversationhistory, a module that ensures that diverse latent representations forcandidate answers are obtained given the probabilistic representations and anuncertainty representation module that chooses the appropriate answer thatminimizes uncertainty. We thoroughly evaluate the model with a detailedablation analysis, comparison with state of the art and visualization of theuncertainty that aids in the understanding of the method. Using the proposedprobabilistic framework, we thus obtain an improved visual dialog system thatis also more explainable.\",\n",
       "  'categories': ['cs.CV', 'cs.CL', 'cs.LG', 'cs.MM'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1909.04800v2'},\n",
       " 868: {'ID': 868,\n",
       "  'title': 'Meta Module Network for Compositional Visual Reasoning',\n",
       "  'authors': ['William Wang',\n",
       "   'Linjie Li',\n",
       "   'Yu Cheng',\n",
       "   'Jingjing Liu',\n",
       "   'Wenhu Chen',\n",
       "   'Zhe Gan'],\n",
       "  'published': '2019-10-08T06:28:24Z',\n",
       "  'updated': '2020-11-08T02:52:51Z',\n",
       "  'abstract': 'Neural Module Network (NMN) exhibits strong interpretability andcompositionality thanks to its handcrafted neural modules with explicitmulti-hop reasoning capability. However, most NMNs suffer from two criticaldrawbacks: 1) scalability: customized module for specific function renders itimpractical when scaling up to a larger set of functions in complex tasks; 2)generalizability: rigid pre-defined module inventory makes it difficult togeneralize to unseen functions in new tasks/domains. To design a more powerfulNMN architecture for practical use, we propose Meta Module Network (MMN)centered on a novel meta module, which can take in function recipes and morphinto diverse instance modules dynamically. The instance modules are then woveninto an execution graph for complex visual reasoning, inheriting the strongexplainability and compositionality of NMN. With such a flexible instantiationmechanism, the parameters of instance modules are inherited from the centralmeta module, retaining the same model complexity as the function set grows,which promises better scalability. Meanwhile, as functions are encoded into theembedding space, unseen functions can be readily represented based on itsstructural similarity with previously observed ones, which ensures bettergeneralizability. Experiments on GQA and CLEVR datasets validate thesuperiority of MMN over state-of-the-art NMN designs. Synthetic experiments onheld-out unseen functions from GQA dataset also demonstrate the stronggeneralizability of MMN. Our code and model are released in Githubhttps://github.com/wenhuchen/Meta-Module-Network.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1910.03230v5'},\n",
       " 869: {'ID': 869,\n",
       "  'title': 'Residual Squeeze-and-Excitation Network for Fast Image Deraining',\n",
       "  'authors': ['Kazuyuki Tasaka', 'Jianfeng Xu', 'Zhibo Chen', 'Jun Fu'],\n",
       "  'published': '2020-06-01T07:17:01Z',\n",
       "  'updated': '2020-06-01T07:17:01Z',\n",
       "  'abstract': 'Image deraining is an important image processing task as rain streaks notonly severely degrade the visual quality of images but also significantlyaffect the performance of high-level vision tasks. Traditional methodsprogressively remove rain streaks via different recurrent neural networks.However, these methods fail to yield plausible rain-free images in an efficientmanner. In this paper, we propose a residual squeeze-and-excitation networkcalled RSEN for fast image deraining as well as superior deraining performancecompared with state-of-the-art approaches. Specifically, RSEN adopts alightweight encoder-decoder architecture to conduct rain removal in one stage.Besides, both encoder and decoder adopt a novel residual squeeze-and-excitationblock as the core of feature extraction, which contains a residual block forproducing hierarchical features, followed by a squeeze-and-excitation block forchannel-wisely enhancing the resulted hierarchical features. Experimentalresults demonstrate that our method can not only considerably reduce thecomputational complexity but also significantly improve the derainingperformance compared with state-of-the-art methods.',\n",
       "  'categories': ['eess.IV', 'cs.CV', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2006.00757v1'},\n",
       " 870: {'ID': 870,\n",
       "  'title': 'Densely Connected Convolutional Networks and Signal Quality Analysis to  Detect Atrial Fibrillation Using Short Single-Lead ECG Recordings',\n",
       "  'authors': ['Bryan Conroy',\n",
       "   'Saman Parvaneh',\n",
       "   'Saeed Babaeizadeh',\n",
       "   'Asif Rahman',\n",
       "   'Jonathan Rubin'],\n",
       "  'published': '2017-10-10T18:58:45Z',\n",
       "  'updated': '2017-10-10T18:58:45Z',\n",
       "  'abstract': \"The development of new technology such as wearables that record high-qualitysingle channel ECG, provides an opportunity for ECG screening in a largerpopulation, especially for atrial fibrillation screening. The main goal of thisstudy is to develop an automatic classification algorithm for normal sinusrhythm (NSR), atrial fibrillation (AF), other rhythms (O), and noise from asingle channel short ECG segment (9-60 seconds). For this purpose, signalquality index (SQI) along with dense convolutional neural networks was used.Two convolutional neural network (CNN) models (main model that accepts 15seconds ECG and secondary model that processes 9 seconds shorter ECG) weretrained using the training data set. If the recording is determined to be oflow quality by SQI, it is immediately classified as noisy. Otherwise, it istransformed to a time-frequency representation and classified with the CNN asNSR, AF, O, or noise. At the final step, a feature-based post-processingalgorithm classifies the rhythm as either NSR or O in case the CNN model'sdiscrimination between the two is indeterminate. The best result achieved atthe official phase of the PhysioNet/CinC challenge on the blind test set was0.80 (F1 for NSR, AF, and O were 0.90, 0.80, and 0.70, respectively).\",\n",
       "  'categories': ['eess.SP', 'cs.CV', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1710.05817v1'},\n",
       " 871: {'ID': 871,\n",
       "  'title': 'Towards Visually Explaining Similarity Models',\n",
       "  'authors': ['Meng Zheng',\n",
       "   'Ziyan Wu',\n",
       "   'Srikrishna Karanam',\n",
       "   'Richard J. Radke',\n",
       "   'Terrence Chen'],\n",
       "  'published': '2020-08-13T17:47:41Z',\n",
       "  'updated': '2020-10-13T17:00:38Z',\n",
       "  'abstract': 'We consider the problem of visually explaining similarity models, i.e.,explaining why a model predicts two images to be similar in addition toproducing a scalar score. While much recent work in visual modelinterpretability has focused on gradient-based attention, these methods rely ona classification module to generate visual explanations. Consequently, theycannot readily explain other kinds of models that do not use or needclassification-like loss functions (e.g., similarity models trained with ametric learning loss). In this work, we bridge this crucial gap, presenting amethod to generate gradient-based visual attention for image similaritypredictors. By relying solely on the learned feature embedding, we show thatour approach can be applied to any kind of CNN-based similarity architecture,an important step towards generic visual explainability. We show that ourresulting attention maps serve more than just interpretability; they can beinfused into the model learning process itself with new trainable constraints.We show that the resulting similarity models perform, and can be visuallyexplained, better than the corresponding baseline models trained without theseconstraints. We demonstrate our approach using extensive experiments on threedifferent kinds of tasks: generic image retrieval, person re-identification,and low-shot semantic segmentation.',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.LG', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2008.06035v2'},\n",
       " 872: {'ID': 872,\n",
       "  'title': 'Revisiting Visual Grounding',\n",
       "  'authors': ['Erik Conser',\n",
       "   'Melanie Mitchell',\n",
       "   'Kennedy Hahn',\n",
       "   'Chandler M. Watson'],\n",
       "  'published': '2019-04-03T20:11:02Z',\n",
       "  'updated': '2019-04-03T20:11:02Z',\n",
       "  'abstract': 'We revisit a particular visual grounding method: the \"Image Retrieval UsingScene Graphs\" (IRSG) system of Johnson et al. (2015). Our experiments indicatethat the system does not effectively use its learned object-relationshipmodels. We also look closely at the IRSG dataset, as well as the widely usedVisual Relationship Dataset (VRD) that is adapted from it. We find that thesedatasets exhibit biases that allow methods that ignore relationships to performrelatively well. We also describe several other problems with the IRSG dataset,and report on experiments using a subset of the dataset in which the biases andother problems are removed. Our studies contribute to a more general effort:that of better understanding what machine learning methods that combinelanguage and vision actually learn and what popular datasets actually test.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1904.02225v1'},\n",
       " 873: {'ID': 873,\n",
       "  'title': 'BYOL works even without batch statistics',\n",
       "  'authors': ['Corentin Tallec',\n",
       "   'Michal Valko',\n",
       "   'Andrew Brock',\n",
       "   'Bilal Piot',\n",
       "   'Florent Altché',\n",
       "   'Razvan Pascanu',\n",
       "   'Pierre H. Richemond',\n",
       "   'Samuel Smith',\n",
       "   'Soham De',\n",
       "   'Florian Strub',\n",
       "   'Jean-Bastien Grill'],\n",
       "  'published': '2020-10-20T13:05:05Z',\n",
       "  'updated': '2020-10-20T13:05:05Z',\n",
       "  'abstract': 'Bootstrap Your Own Latent (BYOL) is a self-supervised learning approach forimage representation. From an augmented view of an image, BYOL trains an onlinenetwork to predict a target network representation of a different augmentedview of the same image. Unlike contrastive methods, BYOL does not explicitlyuse a repulsion term built from negative pairs in its training objective. Yet,it avoids collapse to a trivial, constant representation. Thus, it has recentlybeen hypothesized that batch normalization (BN) is critical to prevent collapsein BYOL. Indeed, BN flows gradients across batch elements, and could leakinformation about negative views in the batch, which could act as an implicitnegative (contrastive) term. However, we experimentally show that replacing BNwith a batch-independent normalization scheme (namely, a combination of groupnormalization and weight standardization) achieves performance comparable tovanilla BYOL ($73.9\\\\%$ vs. $74.3\\\\%$ top-1 accuracy under the linear evaluationprotocol on ImageNet with ResNet-$50$). Our finding disproves the hypothesisthat the use of batch statistics is a crucial ingredient for BYOL to learnuseful representations.',\n",
       "  'categories': ['stat.ML', 'cs.CV', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2010.10241v1'},\n",
       " 874: {'ID': 874,\n",
       "  'title': 'Imaging through glass diffusers using densely connected convolutional  networks',\n",
       "  'authors': ['Justin Lee',\n",
       "   'George Barbastathis',\n",
       "   'Mo Deng',\n",
       "   'Ayan Sinha',\n",
       "   'Shuai Li'],\n",
       "  'published': '2017-11-18T04:14:01Z',\n",
       "  'updated': '2017-11-18T04:14:01Z',\n",
       "  'abstract': 'Computational imaging through scatter generally is accomplished by firstcharacterizing the scattering medium so that its forward operator is obtained;and then imposing additional priors in the form of regularizers on thereconstruction functional so as to improve the condition of the originallyill-posed inverse problem. In the functional, the forward operator andregularizer must be entered explicitly or parametrically (e.g. scatteringmatrices and dictionaries, respectively.) However, the process of determiningthese representations is often incomplete, prone to errors, or infeasible.Recently, deep learning architectures have been proposed to instead learn boththe forward operator and regularizer through examples. Here, we propose for thefirst time, to our knowledge, a convolutional neural network architecturecalled \"IDiffNet\" for the problem of imaging through diffuse media anddemonstrate that IDiffNet has superior generalization capability throughextensive tests with well-calibrated diffusers. We found that the NegativePearson Correlation Coefficient loss function for training is more appropriatefor spatially sparse objects and strong scattering conditions. Our results showthat the convolutional architecture is robust to the choice of prior, asdemonstrated by the use of multiple training and testing object databases, andcapable of achieving higher space-bandwidth product reconstructions thanpreviously reported.',\n",
       "  'categories': ['physics.optics'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1711.06810v1'},\n",
       " 875: {'ID': 875,\n",
       "  'title': 'A Pretrained DenseNet Encoder for Brain Tumor Segmentation',\n",
       "  'authors': ['Jean Stawiaski'],\n",
       "  'published': '2018-11-19T08:00:22Z',\n",
       "  'updated': '2018-11-19T08:00:22Z',\n",
       "  'abstract': 'This article presents a convolutional neural network for the automaticsegmentation of brain tumors in multimodal 3D MR images based on a U-netarchitecture.We evaluate the use of a densely connected convolutional networkencoder (DenseNet) which was pretrained on the ImageNet data set. We detail twonetwork architectures that can take into account multiple 3D images as inputs.This work aims to identify if a generic pretrained network can be used for veryspecific medical applications where the target data differ both in the numberof spatial dimensions as well as in the number of inputs channels. Moreover inorder to regularize this transfer learning task we only train the decoder partof the U-net architecture. We evaluate the effectiveness of the proposedapproach on the BRATS 2018 segmentation challenge where we obtained dice scoresof 0.79, 0.90, 0.85 and 95/% Hausdorff distance of 2.9mm, 3.95mm, and 6.48mmfor enhanced tumor core, whole tumor and tumor core respectively on thevalidation set. This scores degrades to 0.77, 0.88, 0.78 and 95 /% Hausdorffdistance of 3.6mm, 5.72mm, and 5.83mm on the testing set.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1811.07542v1'},\n",
       " 876: {'ID': 876,\n",
       "  'title': 'Learning Goal-Oriented Visual Dialog Agents: Imitating and Surpassing  Analytic Experts',\n",
       "  'authors': ['Yen-Wei Chang', 'Wen-Hsiao Peng'],\n",
       "  'published': '2019-07-24T15:08:38Z',\n",
       "  'updated': '2019-07-24T15:08:38Z',\n",
       "  'abstract': 'This paper tackles the problem of learning a questioner in the goal-orientedvisual dialog task. Several previous works adopt model-free reinforcementlearning. Most pretrain the model from a finite set of human-generated data. Weargue that using limited demonstrations to kick-start the questioner isinsufficient due to the large policy search space. Inspired by a recentlyproposed information theoretic approach, we develop two analytic experts toserve as a source of high-quality demonstrations for imitation learning. Wethen take advantage of reinforcement learning to refine the model towards thegoal-oriented objective. Experimental results on the GuessWhat?! dataset showthat our method has the combined merits of imitation and reinforcementlearning, achieving the state-of-the-art performance.',\n",
       "  'categories': ['cs.AI', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1907.10500v1'},\n",
       " 877: {'ID': 877,\n",
       "  'title': 'Nested Hierarchical Dirichlet Processes for Multi-Level Non-Parametric  Admixture Modeling',\n",
       "  'authors': ['Lavanya Sita Tekumalla',\n",
       "   'Indrajit Bhattacharya',\n",
       "   'Priyanka Agrawal'],\n",
       "  'published': '2015-08-26T11:24:36Z',\n",
       "  'updated': '2015-08-27T08:14:45Z',\n",
       "  'abstract': 'Dirichlet Process(DP) is a Bayesian non-parametric prior for infinite mixturemodeling, where the number of mixture components grows with the number of dataitems. The Hierarchical Dirichlet Process (HDP), is an extension of DP forgrouped data, often used for non-parametric topic modeling, where each group isa mixture over shared mixture densities. The Nested Dirichlet Process (nDP), onthe other hand, is an extension of the DP for learning group leveldistributions from data, simultaneously clustering the groups. It allows grouplevel distributions to be shared across groups in a non-parametric setting,leading to a non-parametric mixture of mixtures. The nCRF extends the nDP formultilevel non-parametric mixture modeling, enabling modeling topichierarchies. However, the nDP and nCRF do not allow sharing of distributions asrequired in many applications, motivating the need for multi-levelnon-parametric admixture modeling. We address this gap by proposing multi-levelnested HDPs (nHDP) where the base distribution of the HDP is itself a HDP ateach level thereby leading to admixtures of admixtures at each level. Becauseof couplings between various HDP levels, scaling up is naturally a challengeduring inference. We propose a multi-level nested Chinese Restaurant Franchise(nCRF) representation for the nested HDP, with which we outline an inferencealgorithm based on Gibbs Sampling. We evaluate our model with the two levelnHDP for non-parametric entity topic modeling where an inner HDP creates acountably infinite set of topic mixtures and associates them with authorentities, while an outer HDP associates documents with these author entities.In our experiments on two real world research corpora, the nHDP is able togeneralize significantly better than existing models and detect missing authorentities with a reasonable level of accuracy.',\n",
       "  'categories': ['stat.ML', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1508.06446v2'},\n",
       " 878: {'ID': 878,\n",
       "  'title': 'DeepCFL: Deep Contextual Features Learning from a Single Image',\n",
       "  'authors': ['Shanmuganathan Raman', 'Indra Deep Mastan'],\n",
       "  'published': '2020-11-07T06:54:59Z',\n",
       "  'updated': '2020-11-07T06:54:59Z',\n",
       "  'abstract': 'Recently, there is a vast interest in developing image feature learningmethods that are independent of the training data, such as deep image prior,InGAN, SinGAN, and DCIL. These methods are unsupervised and are used to performlow-level vision tasks such as image restoration, image editing, and imagesynthesis. In this work, we proposed a new training data-independent framework,called Deep Contextual Features Learning (DeepCFL), to perform image synthesisand image restoration based on the semantics of the input image. The contextualfeatures are simply the high dimensional vectors representing the semantics ofthe given image. DeepCFL is a single image GAN framework that learns thedistribution of the context vectors from the input image. We show theperformance of contextual learning in various challenging scenarios:outpainting, inpainting, and restoration of randomly removed pixels. DeepCFL isapplicable when the input source image and the generated target image are notaligned. We illustrate image synthesis using DeepCFL for the task of imageresizing.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2011.03712v1'},\n",
       " 879: {'ID': 879,\n",
       "  'title': 'Deformable PV-RCNN: Improving 3D Object Detection with Learned  Deformations',\n",
       "  'authors': ['Prarthana Bhattacharyya', 'Krzysztof Czarnecki'],\n",
       "  'published': '2020-08-20T04:11:17Z',\n",
       "  'updated': '2020-08-20T04:11:17Z',\n",
       "  'abstract': 'We present Deformable PV-RCNN, a high-performing point-cloud based 3D objectdetector. Currently, the proposal refinement methods used by thestate-of-the-art two-stage detectors cannot adequately accommodate differingobject scales, varying point-cloud density, part-deformation and clutter. Wepresent a proposal refinement module inspired by 2D deformable convolutionnetworks that can adaptively gather instance-specific features from locationswhere informative content exists. We also propose a simple context gatingmechanism which allows the keypoints to select relevant context information forthe refinement stage. We show state-of-the-art results on the KITTI dataset.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2008.08766v1'},\n",
       " 880: {'ID': 880,\n",
       "  'title': 'A Bayesian Perspective on the Deep Image Prior',\n",
       "  'authors': ['Matheus Gadelha',\n",
       "   'Daniel Sheldon',\n",
       "   'Subhransu Maji',\n",
       "   'Zezhou Cheng'],\n",
       "  'published': '2019-04-16T04:39:29Z',\n",
       "  'updated': '2019-04-16T04:39:29Z',\n",
       "  'abstract': 'The deep image prior was recently introduced as a prior for natural images.It represents images as the output of a convolutional network with randominputs. For \"inference\", gradient descent is performed to adjust networkparameters to make the output match observations. This approach yields goodperformance on a range of image reconstruction tasks. We show that the deepimage prior is asymptotically equivalent to a stationary Gaussian process priorin the limit as the number of channels in each layer of the network goes toinfinity, and derive the corresponding kernel. This informs a Bayesian approachto inference. We show that by conducting posterior inference using stochasticgradient Langevin we avoid the need for early stopping, which is a drawback ofthe current approach, and improve results for denoising and impainting tasks.We illustrate these intuitions on a number of 1D and 2D signal reconstructiontasks.',\n",
       "  'categories': ['cs.CV', 'cs.LG', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1904.07457v1'},\n",
       " 881: {'ID': 881,\n",
       "  'title': 'Filter Response Normalization Layer: Eliminating Batch Dependence in the  Training of Deep Neural Networks',\n",
       "  'authors': ['Saurabh Singh', 'Shankar Krishnan'],\n",
       "  'published': '2019-11-21T20:32:04Z',\n",
       "  'updated': '2020-04-01T04:19:08Z',\n",
       "  'abstract': 'Batch Normalization (BN) uses mini-batch statistics to normalize theactivations during training, introducing dependence between mini-batchelements. This dependency can hurt the performance if the mini-batch size istoo small, or if the elements are correlated. Several alternatives, such asBatch Renormalization and Group Normalization (GN), have been proposed toaddress this issue. However, they either do not match the performance of BN forlarge batches, or still exhibit degradation in performance for smaller batches,or introduce artificial constraints on the model architecture. In this paper wepropose the Filter Response Normalization (FRN) layer, a novel combination of anormalization and an activation function, that can be used as a replacement forother normalizations and activations. Our method operates on each activationchannel of each batch element independently, eliminating the dependency onother batch elements. Our method outperforms BN and other alternatives in avariety of settings for all batch sizes. FRN layer performs $\\\\approx 0.7-1.0\\\\%$better than BN on top-1 validation accuracy with large mini-batch sizes forImagenet classification using InceptionV3 and ResnetV2-50 architectures.Further, it performs $&gt;1\\\\%$ better than GN on the same problem in the smallmini-batch size regime. For object detection problem on COCO dataset, FRN layeroutperforms all other methods by at least $0.3-0.5\\\\%$ in all batch sizeregimes.',\n",
       "  'categories': ['cs.LG', 'cs.CV', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1911.09737v2'},\n",
       " 882: {'ID': 882,\n",
       "  'title': 'In defense of OSVOS',\n",
       "  'authors': ['Anh-Dzung Doan',\n",
       "   'Yu Liu',\n",
       "   'Yutong Dai',\n",
       "   'Lingqiao Liu',\n",
       "   'Ian Reid'],\n",
       "  'published': '2019-08-19T11:07:17Z',\n",
       "  'updated': '2019-08-20T01:24:35Z',\n",
       "  'abstract': \"As a milestone for video object segmentation, one-shot video objectsegmentation (OSVOS) has achieved a large margin compared to the conventionaloptical-flow based methods regarding to the segmentation accuracy. Itsexcellent performance mainly benefit from the three-step training mechanism,that are: (1) acquiring object features on the base dataset (i.e. ImageNet),(2) training the parent network on the training set of the target dataset (i.e.DAVIS-2016) to be capable of differentiating the object of interest from thebackground. (3) online fine-tuning the interested object on the first frame ofthe target test set to overfit its appearance, then the model can be utilizedto segment the same object in the rest frames of that video. In this paper, weargue that for the step (2), OSVOS has the limitation to 'overemphasize' thegeneric semantic object information while 'dilute' the instance cues of theobject(s), which largely block the whole training process. Through adding acommon module, video loss, which we formulate with various forms of constraints(including weighted BCE loss, high-dimensional triplet loss, as well as a novelmixed instance-aware video loss), to train the parent network in the step (2),the network is then better prepared for the step (3), i.e. online fine-tuningon the target instance. Through extensive experiments using different networkstructures as the backbone, we show that the proposed video loss module canimprove the segmentation performance significantly, compared to that of OSVOS.Meanwhile, since video loss is a common module, it can be generalized to otherfine-tuning based methods and similar vision tasks such as depth estimation andsaliency detection.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1908.06692v2'},\n",
       " 883: {'ID': 883,\n",
       "  'title': 'Pixel-level Corrosion Detection on Metal Constructions by Fusion of Deep  Learning Semantic and Contour Segmentation',\n",
       "  'authors': ['Eftychios Protopapadakis',\n",
       "   'Iason Katsamenis',\n",
       "   'Nikolaos Doulamis',\n",
       "   'Athanasios Voulodimos',\n",
       "   'Anastasios Doulamis'],\n",
       "  'published': '2020-08-12T09:54:17Z',\n",
       "  'updated': '2020-08-12T09:54:17Z',\n",
       "  'abstract': 'Corrosion detection on metal constructions is a major challenge in civilengineering for quick, safe and effective inspection. Existing image analysisapproaches tend to place bounding boxes around the defected region which is notadequate both for structural analysis and pre-fabrication, an innovativeconstruction concept which reduces maintenance cost, time and improves safety.In this paper, we apply three semantic segmentation-oriented deep learningmodels (FCN, U-Net and Mask R-CNN) for corrosion detection, which performbetter in terms of accuracy and time and require a smaller number of annotatedsamples compared to other deep models, e.g. CNN. However, the final imagesderived are still not sufficiently accurate for structural analysis andpre-fabrication. Thus, we adopt a novel data projection scheme that fuses theresults of color segmentation, yielding accurate but over-segmented contours ofa region, with a processed area of the deep masks, resulting in high-confidencecorroded pixels.',\n",
       "  'categories': ['cs.CV',\n",
       "   'cs.LG',\n",
       "   'eess.IV',\n",
       "   '68T07 (Primary) 68T45 (Secondary)',\n",
       "   'I.2.10; I.4.6'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2008.05204v1'},\n",
       " 884: {'ID': 884,\n",
       "  'title': 'Interactive Video Object Segmentation in the Wild',\n",
       "  'authors': ['Michael Gygli', 'Arnaud Benard'],\n",
       "  'published': '2017-12-31T11:46:54Z',\n",
       "  'updated': '2017-12-31T11:46:54Z',\n",
       "  'abstract': 'In this paper we present our system for human-in-the-loop video objectsegmentation. The backbone of our system is a method for one-shot video objectsegmentation. While fast, this method requires an accurate pixel-levelsegmentation of one (or several) frames as input. As manually annotating such asegmentation is impractical, we propose a deep interactive image segmentationmethod, that can accurately segment objects with only a handful of clicks. Onthe GrabCut dataset, our method obtains 90% IOU with just 3.8 clicks onaverage, setting the new state of the art. Furthermore, as our methoditeratively refines an initial segmentation, it can effectively correct frameswhere the video object segmentation fails, thus allowing users to quicklyobtain high quality results even on challenging sequences. Finally, weinvestigate usage patterns and give insights in how many steps users take toannotate frames, what kind of corrections they provide, etc., thus givingimportant insights for further improving interactive video segmentation.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1801.00269v1'},\n",
       " 885: {'ID': 885,\n",
       "  'title': 'Learning a Dilated Residual Network for SAR Image Despeckling',\n",
       "  'authors': ['Xiaoshuang Ma',\n",
       "   'Qiang Zhang',\n",
       "   'Jie Li',\n",
       "   'Zhen Yang',\n",
       "   'Qiangqiang Yuan'],\n",
       "  'published': '2017-09-09T03:22:26Z',\n",
       "  'updated': '2018-01-24T03:06:32Z',\n",
       "  'abstract': 'In this paper, to break the limit of the traditional linear models forsynthetic aperture radar (SAR) image despeckling, we propose a novel deeplearning approach by learning a non-linear end-to-end mapping between the noisyand clean SAR images with a dilated residual network (SAR-DRN). SAR-DRN isbased on dilated convolutions, which can both enlarge the receptive field andmaintain the filter size and layer depth with a lightweight structure. Inaddition, skip connections and residual learning strategy are added to thedespeckling model to maintain the image details and reduce the vanishinggradient problem. Compared with the traditional despeckling methods, theproposed method shows superior performance over the state-of-the-art methods onboth quantitative and visual assessments, especially for strong speckle noise.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1709.02898v3'},\n",
       " 886: {'ID': 886,\n",
       "  'title': 'In-Bed Pose Estimation: Deep Learning with Shallow Dataset',\n",
       "  'authors': ['Yu Yin', 'Shuangjun Liu', 'Sarah Ostadabbas'],\n",
       "  'published': '2017-11-03T03:05:05Z',\n",
       "  'updated': '2018-07-07T20:37:06Z',\n",
       "  'abstract': 'Although human pose estimation for various computer vision (CV) applicationshas been studied extensively in the last few decades, yet in-bed poseestimation using camera-based vision methods has been ignored by the CVcommunity because it is assumed to be identical to the general purpose poseestimation methods. However, in-bed pose estimation has its own specializedaspects and comes with specific challenges including the notable differences inlighting conditions throughout a day and also having different posedistribution from the common human surveillance viewpoint. In this paper, wedemonstrate that these challenges significantly lessen the effectiveness ofexisting general purpose pose estimation models. In order to address thelighting variation challenge, infrared selective (IRS) image acquisitiontechnique is proposed to provide uniform quality data under various lightingconditions. In addition, to deal with unconventional pose perspective, a 2-endhistogram of oriented gradient (HOG) rectification method is presented. In thiswork, we explored the idea of employing a pre-trained convolutional neuralnetwork (CNN) model trained on large public datasets of general human poses andfine-tuning the model using our own shallow in-bed IRS dataset. We developed anIRS imaging system and collected IRS image data from several realisticlife-size mannequins in a simulated hospital room environment. A pre-trainedCNN called convolutional pose machine (CPM) was repurposed for in-bed poseestimation by fine-tuning its specific intermediate layers. Using the HOGrectification method, the pose estimation performance of CPM significantlyimproved by 26.4% in PCK0.1 criteria compared to the model without suchrectification.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1711.01005v3'},\n",
       " 887: {'ID': 887,\n",
       "  'title': 'M-ADDA: Unsupervised Domain Adaptation with Deep Metric Learning',\n",
       "  'authors': ['Issam Laradji', 'Reza Babanezhad'],\n",
       "  'published': '2018-07-06T19:21:59Z',\n",
       "  'updated': '2018-07-06T19:21:59Z',\n",
       "  'abstract': \"Unsupervised domain adaptation techniques have been successful for a widerange of problems where supervised labels are limited. The task is to classifyan unlabeled `target' dataset by leveraging a labeled `source' dataset thatcomes from a slightly similar distribution. We propose metric-based adversarialdiscriminative domain adaptation (M-ADDA) which performs two main steps. First,it uses a metric learning approach to train the source model on the sourcedataset by optimizing the triplet loss function. This results in clusters whereembeddings of the same label are close to each other and those with differentlabels are far from one another. Next, it uses the adversarial approach (asthat used in ADDA \\\\cite{2017arXiv170205464T}) to make the extracted featuresfrom the source and target datasets indistinguishable. Simultaneously, weoptimize a novel loss function that encourages the target dataset's embeddingsto form clusters. While ADDA and M-ADDA use similar architectures, we show thatM-ADDA performs significantly better on the digits adaptation datasets of MNISTand USPS. This suggests that using metric-learning for domain adaptation canlead to large improvements in classification accuracy for the domain adaptationtask. The code is available at \\\\url{https://github.com/IssamLaradji/M-ADDA}.\",\n",
       "  'categories': ['cs.LG', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1807.02552v1'},\n",
       " 888: {'ID': 888,\n",
       "  'title': 'Are You Talking to Me? Reasoned Visual Dialog Generation through  Adversarial Learning',\n",
       "  'authors': ['Peng Wang',\n",
       "   'Qi Wu',\n",
       "   'Chunhua Shen',\n",
       "   'Anton van den Hengel',\n",
       "   'Ian Reid'],\n",
       "  'published': '2017-11-21T03:11:49Z',\n",
       "  'updated': '2017-11-21T03:11:49Z',\n",
       "  'abstract': 'The Visual Dialogue task requires an agent to engage in a conversation aboutan image with a human. It represents an extension of the Visual QuestionAnswering task in that the agent needs to answer a question about an image, butit needs to do so in light of the previous dialogue that has taken place. Thekey challenge in Visual Dialogue is thus maintaining a consistent, and naturaldialogue while continuing to answer questions correctly. We present a novelapproach that combines Reinforcement Learning and Generative AdversarialNetworks (GANs) to generate more human-like responses to questions. The GANhelps overcome the relative paucity of training data, and the tendency of thetypical MLE-based approach to generate overly terse answers. Critically, theGAN is tightly integrated into the attention mechanism that generateshuman-interpretable reasons for each answer. This means that the discriminativemodel of the GAN has the task of assessing whether a candidate answer isgenerated by a human or not, given the provided reason. This is significantbecause it drives the generative model to produce high quality answers that arewell supported by the associated reasoning. The method also generates thestate-of-the-art results on the primary benchmark.',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.CL'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1711.07613v1'},\n",
       " 889: {'ID': 889,\n",
       "  'title': 'Recovering the Imperfect: Cell Segmentation in the Presence of  Dynamically Localized Proteins',\n",
       "  'authors': ['Özgün Çiçek',\n",
       "   'Yassine Marrakchi',\n",
       "   'Barbara Di Ventura',\n",
       "   'Enoch Boasiako Antwi',\n",
       "   'Thomas Brox'],\n",
       "  'published': '2020-11-20T16:30:55Z',\n",
       "  'updated': '2020-11-20T16:30:55Z',\n",
       "  'abstract': 'Deploying off-the-shelf segmentation networks on biomedical data has becomecommon practice, yet if structures of interest in an image sequence are visibleonly temporarily, existing frame-by-frame methods fail. In this paper, weprovide a solution to segmentation of imperfect data through time based ontemporal propagation and uncertainty estimation. We integrate uncertaintyestimation into Mask R-CNN network and propagate motion-corrected segmentationmasks from frames with low uncertainty to those frames with high uncertainty tohandle temporary loss of signal for segmentation. We demonstrate the value ofthis approach over frame-by-frame segmentation and regular temporal propagationon data from human embryonic kidney (HEK293T) cells transiently transfectedwith a fluorescent protein that moves in and out of the nucleus over time. Themethod presented here will empower microscopic experiments aimed atunderstanding molecular and cellular function.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2011.10486v1'},\n",
       " 890: {'ID': 890,\n",
       "  'title': 'Compact Approximation for Polynomial of Covariance Feature',\n",
       "  'authors': ['Tatsuya Harada', 'Yusuke Mukuta', 'Tatsuaki Machida'],\n",
       "  'published': '2019-06-05T06:46:58Z',\n",
       "  'updated': '2019-06-05T06:46:58Z',\n",
       "  'abstract': 'Covariance pooling is a feature pooling method with good classificationaccuracy. Because covariance features consist of second-order statistics, thescale of the feature elements are varied. Therefore, normalizing covariancefeatures using a matrix square root affects the performance improvement. Whenpooling methods are applied to local features extracted from CNN models, theaccuracy increases when the pooling function is back-propagatable and thefeature-extraction model is learned in an end-to-end manner. Recently, theiterative polynomial approximation method for the matrix square root of acovariance feature was proposed, and resulted in a faster and more stabletraining than the methods based on singular-value decomposition. In this paper,we propose an extension of compact bilinear pooling, which is a compactapproximation of the standard covariance feature, to the polynomials of thecovariance feature. Subsequently, we apply the proposed approximation to thepolynomial corresponding to the matrix square root to obtain a compactapproximation for the square root of the covariance feature. Our methodapproximates a higher-dimensional polynomial of a covariance by the weightedsum of the approximate features corresponding to a pair of local features basedon the similarity of the local features. We apply our method for standardfine-grained image recognition datasets and demonstrate that the proposedmethod shows comparable accuracy with fewer dimensions than the originalfeature.',\n",
       "  'categories': ['cs.CV', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1906.01851v1'},\n",
       " 891: {'ID': 891,\n",
       "  'title': 'Regularization by architecture: A deep prior approach for inverse  problems',\n",
       "  'authors': ['Daniel Otero Baguer',\n",
       "   'Sören Dittmer',\n",
       "   'Tobias Kluth',\n",
       "   'Peter Maass'],\n",
       "  'published': '2018-12-10T15:54:33Z',\n",
       "  'updated': '2020-03-18T14:46:22Z',\n",
       "  'abstract': 'The present paper studies so-called deep image prior (DIP) techniques in thecontext of ill-posed inverse problems. DIP networks have been recentlyintroduced for applications in image processing; also first experimentalresults for applying DIP to inverse problems have been reported. This paperaims at discussing different interpretations of DIP and to obtain analyticresults for specific network designs and linear operators. The maincontribution is to introduce the idea of viewing these approaches as theoptimization of Tikhonov functionals rather than optimizing networks. Besidestheoretical results, we present numerical verifications.',\n",
       "  'categories': ['cs.LG', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1812.03889v2'},\n",
       " 892: {'ID': 892,\n",
       "  'title': 'Stop Bugging Me! Evading Modern-Day Wiretapping Using Adversarial  Perturbations',\n",
       "  'authors': ['Tal Ben Senior',\n",
       "   'Yael Mathov',\n",
       "   'Yuval Elovici',\n",
       "   'Asaf Shabtai'],\n",
       "  'published': '2020-10-24T06:56:35Z',\n",
       "  'updated': '2020-10-24T06:56:35Z',\n",
       "  'abstract': \"Mass surveillance systems for voice over IP (VoIP) conversations pose a hugerisk to privacy. These automated systems use learning models to analyzeconversations, and upon detecting calls that involve specific topics, routethem to a human agent. In this study, we present an adversarial learning-basedframework for privacy protection for VoIP conversations. We present a novelalgorithm that finds a universal adversarial perturbation (UAP), which, whenadded to the audio stream, prevents an eavesdropper from automaticallydetecting the conversation's topic. As shown in our experiments, the UAP isagnostic to the speaker or audio length, and its volume can be changed inreal-time, as needed. In a real-world demonstration, we use a Teensymicrocontroller that acts as an external microphone and adds the UAP to theaudio in real-time. We examine different speakers, VoIP applications (Skype,Zoom), audio lengths, and speech-to-text models (Deep Speech, Kaldi). Ourresults in the real world suggest that our approach is a feasible solution forprivacy protection.\",\n",
       "  'categories': ['cs.SD', 'cs.CR', 'cs.LG', 'eess.AS'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2010.12809v1'},\n",
       " 893: {'ID': 893,\n",
       "  'title': 'Semantic Sentence Matching with Densely-connected Recurrent and  Co-attentive Information',\n",
       "  'authors': ['Inho Kang', 'Nojun Kwak', 'Seonhoon Kim'],\n",
       "  'published': '2018-05-29T11:29:56Z',\n",
       "  'updated': '2018-11-02T09:20:01Z',\n",
       "  'abstract': 'Sentence matching is widely used in various natural language tasks such asnatural language inference, paraphrase identification, and question answering.For these tasks, understanding logical and semantic relationship between twosentences is required but it is yet challenging. Although attention mechanismis useful to capture the semantic relationship and to properly align theelements of two sentences, previous methods of attention mechanism simply use asummation operation which does not retain original features enough. Inspired byDenseNet, a densely connected convolutional network, we propose adensely-connected co-attentive recurrent neural network, each layer of whichuses concatenated information of attentive features as well as hidden featuresof all the preceding recurrent layers. It enables preserving the original andthe co-attentive feature information from the bottommost word embedding layerto the uppermost recurrent layer. To alleviate the problem of anever-increasing size of feature vectors due to dense concatenation operations,we also propose to use an autoencoder after dense concatenation. We evaluateour proposed architecture on highly competitive benchmark datasets related tosentence matching. Experimental results show that our architecture, whichretains recurrent and attentive features, achieves state-of-the-artperformances for most of the tasks.',\n",
       "  'categories': ['cs.CL'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1805.11360v2'},\n",
       " 894: {'ID': 894,\n",
       "  'title': 'Manifold Modeling in Embedded Space: A Perspective for Interpreting Deep  Image Prior',\n",
       "  'authors': ['Qibin Zhao',\n",
       "   'Tatsuya Yokota',\n",
       "   'Andrzej Cichocki',\n",
       "   'Hidekata Hontani'],\n",
       "  'published': '2019-08-08T10:05:09Z',\n",
       "  'updated': '2020-01-21T08:14:35Z',\n",
       "  'abstract': \"Deep image prior (DIP), which utilizes a deep convolutional network (ConvNet)structure itself as an image prior, has attracted attentions in computer visionand machine learning communities. It empirically shows the effectiveness ofConvNet structure for various image restoration applications. However, why theDIP works so well is still unknown, and why convolution operation is useful forimage reconstruction or enhancement is not very clear. In this study, we tacklethese questions. The proposed approach is dividing the convolution into``delay-embedding'' and ``transformation (\\\\ie encoder-decoder)'', and proposinga simple, but essential, image/tensor modeling method which is closely relatedto dynamical systems and self-similarity. The proposed method named as manifoldmodeling in embedded space (MMES) is implemented by using a noveldenoising-auto-encoder in combination with multi-way delay-embedding transform.In spite of its simplicity, the image/tensor completion, super-resolution,deconvolution, and denoising results of MMES are quite similar even competitiveto DIP in our extensive experiments, and these results would help us forreinterpreting/characterizing the DIP from a perspective of ``low-dimensionalpatch-manifold prior''.\",\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1908.02995v2'},\n",
       " 895: {'ID': 895,\n",
       "  'title': 'M32: Is There An Ancient, Metal-Poor Population?',\n",
       "  'authors': ['T. Lauer',\n",
       "   'A. Dressler',\n",
       "   'C. J. Grillmair',\n",
       "   'K. Mighell',\n",
       "   'A. Monachesi',\n",
       "   'W. L. Freedman',\n",
       "   'G. Fiorentino',\n",
       "   'S. Trager',\n",
       "   'A. Saha',\n",
       "   'E. Tolstoy'],\n",
       "  'published': '2009-09-01T15:00:00Z',\n",
       "  'updated': '2009-09-01T15:00:00Z',\n",
       "  'abstract': \"We observed two fields near M32 with the ACS/HRC on board the Hubble SpaceTelescope, located at distances of about 1.8' and 5.4' (hereafter F1 and F2,respectively) from the center of M32. To obtain a very detailed and deepcolor-magnitude diagram (CMD) and to look for short period variability, weobtained time-series imaging of each field in 32-orbit-long exposures using theF435W (B) and F555W (V) filters, spanning a temporal range of 2 days perfilter. We focus on our detection of variability on RR Lyrae variable stars,which represents the only way to obtain information about the presence of avery old population (larger than 10 Gyr) in M32 from optical data. Here wepresent results obtained from the detection of 31 RR Lyrae in these fields: 17in F1 and 14 in F2.\",\n",
       "  'categories': ['astro-ph.GA'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/0909.0202v1'},\n",
       " 896: {'ID': 896,\n",
       "  'title': 'Bottleneck Transformers for Visual Recognition',\n",
       "  'authors': ['Aravind Srinivas',\n",
       "   'Niki Parmar',\n",
       "   'Ashish Vaswani',\n",
       "   'Pieter Abbeel',\n",
       "   'Jonathon Shlens',\n",
       "   'Tsung-Yi Lin'],\n",
       "  'published': '2021-01-27T18:55:27Z',\n",
       "  'updated': '2021-01-27T18:55:27Z',\n",
       "  'abstract': 'We present BoTNet, a conceptually simple yet powerful backbone architecturethat incorporates self-attention for multiple computer vision tasks includingimage classification, object detection and instance segmentation. By justreplacing the spatial convolutions with global self-attention in the finalthree bottleneck blocks of a ResNet and no other changes, our approach improvesupon the baselines significantly on instance segmentation and object detectionwhile also reducing the parameters, with minimal overhead in latency. Throughthe design of BoTNet, we also point out how ResNet bottleneck blocks withself-attention can be viewed as Transformer blocks. Without any bells andwhistles, BoTNet achieves 44.4% Mask AP and 49.7% Box AP on the COCO InstanceSegmentation benchmark using the Mask R-CNN framework; surpassing the previousbest published single model and single scale results of ResNeSt evaluated onthe COCO validation set. Finally, we present a simple adaptation of the BoTNetdesign for image classification, resulting in models that achieve a strongperformance of 84.7% top-1 accuracy on the ImageNet benchmark while being up to2.33x faster in compute time than the popular EfficientNet models on TPU-v3hardware. We hope our simple and effective approach will serve as a strongbaseline for future research in self-attention models for vision.',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2101.11605v1'},\n",
       " 897: {'ID': 897,\n",
       "  'title': 'Compact Tensor Pooling for Visual Question Answering',\n",
       "  'authors': ['Yang Shi', 'Tommaso Furlanello', 'Anima Anandkumar'],\n",
       "  'published': '2017-06-20T23:55:32Z',\n",
       "  'updated': '2017-06-20T23:55:32Z',\n",
       "  'abstract': 'Performing high level cognitive tasks requires the integration of featuremaps with drastically different structure. In Visual Question Answering (VQA)image descriptors have spatial structures, while lexical inputs inherentlyfollow a temporal sequence. The recently proposed Multimodal Compact Bilinearpooling (MCB) forms the outer products, via count-sketch approximation, of thevisual and textual representation at each spatial location. While thisprocedure preserves spatial information locally, outer-products are takenindependently for each fiber of the activation tensor, and therefore do notinclude spatial context. In this work, we introduce multi-dimensional sketch({MD-sketch}), a novel extension of count-sketch to tensors. Using this newformulation, we propose Multimodal Compact Tensor Pooling (MCT) to fullyexploit the global spatial context during bilinear pooling operations.Contrarily to MCB, our approach preserves spatial context by directlyconvolving the MD-sketch from the visual tensor features with the text vectorfeature using higher order FFT. Furthermore we apply MCT incrementally at eachstep of the question embedding and accumulate the multi-modal vectors with asecond LSTM layer before the final answer is chosen.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1706.06706v1'},\n",
       " 898: {'ID': 898,\n",
       "  'title': 'Mnemonics Training: Multi-Class Incremental Learning without Forgetting',\n",
       "  'authors': ['Bernt Schiele',\n",
       "   'Yaoyao Liu',\n",
       "   'Qianru Sun',\n",
       "   'An-An Liu',\n",
       "   'Yuting Su'],\n",
       "  'published': '2020-02-24T12:55:25Z',\n",
       "  'updated': '2020-09-15T19:44:51Z',\n",
       "  'abstract': 'Multi-Class Incremental Learning (MCIL) aims to learn new concepts byincrementally updating a model trained on previous concepts. However, there isan inherent trade-off to effectively learning new concepts without catastrophicforgetting of previous ones. To alleviate this issue, it has been proposed tokeep around a few examples of the previous concepts but the effectiveness ofthis approach heavily depends on the representativeness of these examples. Thispaper proposes a novel and automatic framework we call mnemonics, where weparameterize exemplars and make them optimizable in an end-to-end manner. Wetrain the framework through bilevel optimizations, i.e., model-level andexemplar-level. We conduct extensive experiments on three MCIL benchmarks,CIFAR-100, ImageNet-Subset and ImageNet, and show that using mnemonicsexemplars can surpass the state-of-the-art by a large margin. Interestingly andquite intriguingly, the mnemonics exemplars tend to be on the boundariesbetween different classes.',\n",
       "  'categories': ['cs.CV', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2002.10211v5'},\n",
       " 899: {'ID': 899,\n",
       "  'title': \"Large-Scale Answerer in Questioner's Mind for Visual Dialog Question  Generation\",\n",
       "  'authors': ['Jung-Woo Ha',\n",
       "   'Tong Gao',\n",
       "   'Jaejun Yoo',\n",
       "   'Sang-Woo Lee',\n",
       "   'Sohee Yang'],\n",
       "  'published': '2019-02-22T03:46:53Z',\n",
       "  'updated': '2019-02-22T03:46:53Z',\n",
       "  'abstract': \"Answerer in Questioner's Mind (AQM) is an information-theoretic frameworkthat has been recently proposed for task-oriented dialog systems. AQM benefitsfrom asking a question that would maximize the information gain when it isasked. However, due to its intrinsic nature of explicitly calculating theinformation gain, AQM has a limitation when the solution space is very large.To address this, we propose AQM+ that can deal with a large-scale problem andask a question that is more coherent to the current context of the dialog. Weevaluate our method on GuessWhich, a challenging task-oriented visual dialogproblem, where the number of candidate classes is near 10K. Our experimentalresults and ablation studies show that AQM+ outperforms the state-of-the-artmodels by a remarkable margin with a reasonable approximation. In particular,the proposed AQM+ reduces more than 60% of error as the dialog proceeds, whilethe comparative algorithms diminish the error by less than 6%. Based on ourresults, we argue that AQM+ is a general task-oriented dialog algorithm thatcan be applied for non-yes-or-no responses.\",\n",
       "  'categories': ['cs.CL', 'cs.CV', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1902.08355v1'},\n",
       " 900: {'ID': 900,\n",
       "  'title': 'Instance and Panoptic Segmentation Using Conditional Convolutions',\n",
       "  'authors': ['Bowen Zhang', 'Zhi Tian', 'Hao Chen', 'Chunhua Shen'],\n",
       "  'published': '2021-02-05T06:57:02Z',\n",
       "  'updated': '2021-02-05T06:57:02Z',\n",
       "  'abstract': 'We propose a simple yet effective framework for instance and panopticsegmentation, termed CondInst (conditional convolutions for instance andpanoptic segmentation). In the literature, top-performing instance segmentationmethods typically follow the paradigm of Mask R-CNN and rely on ROI operations(typically ROIAlign) to attend to each instance. In contrast, we propose toattend to the instances with dynamic conditional convolutions. Instead of usinginstance-wise ROIs as inputs to the instance mask head of fixed weights, wedesign dynamic instance-aware mask heads, conditioned on the instances to bepredicted. CondInst enjoys three advantages: 1.) Instance and panopticsegmentation are unified into a fully convolutional network, eliminating theneed for ROI cropping and feature alignment. 2.) The elimination of the ROIcropping also significantly improves the output instance mask resolution. 3.)Due to the much improved capacity of dynamically-generated conditionalconvolutions, the mask head can be very compact (e.g., 3 conv. layers, eachhaving only 8 channels), leading to significantly faster inference time perinstance and making the overall inference time almost constant, irrelevant tothe number of instances. We demonstrate a simpler method that can achieveimproved accuracy and inference speed on both instance and panopticsegmentation tasks. On the COCO dataset, we outperform a few state-of-the-artmethods. We hope that CondInst can be a strong baseline for instance andpanoptic segmentation. Code is available at: https://git.io/AdelaiDet',\n",
       "  'categories': ['cs.CV', 'eess.IV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2102.03026v1'},\n",
       " 901: {'ID': 901,\n",
       "  'title': 'Computational Mirrors: Blind Inverse Light Transport by Deep Matrix  Factorization',\n",
       "  'authors': ['Lukas Murmann',\n",
       "   'Fredo Durand',\n",
       "   'Adam B. Yedidia',\n",
       "   'Prafull Sharma',\n",
       "   'Gregory W. Wornell',\n",
       "   'Miika Aittala',\n",
       "   'William T. Freeman'],\n",
       "  'published': '2019-12-05T00:06:20Z',\n",
       "  'updated': '2019-12-05T00:06:20Z',\n",
       "  'abstract': 'We recover a video of the motion taking place in a hidden scene by observingchanges in indirect illumination in a nearby uncalibrated visible region. Wesolve this problem by factoring the observed video into a matrix productbetween the unknown hidden scene video and an unknown light transport matrix.This task is extremely ill-posed, as any non-negative factorization willsatisfy the data. Inspired by recent work on the Deep Image Prior, weparameterize the factor matrices using randomly initialized convolutionalneural networks trained in a one-off manner, and show that this results indecompositions that reflect the true motion in the hidden scene.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1912.02314v1'},\n",
       " 902: {'ID': 902,\n",
       "  'title': 'A Photometric Study of the Outer Halo Globular Cluster NGC 5824',\n",
       "  'authors': ['A. R. Walker',\n",
       "   'S. Cassisi',\n",
       "   \"M. Dall'Ora\",\n",
       "   'A. M. Kunder',\n",
       "   'A. K. Vivas',\n",
       "   'C. E. Martínez-Vázquez',\n",
       "   'G. Bono',\n",
       "   'G. Andreuzzi',\n",
       "   'M. Monelli',\n",
       "   'P. B. Stetson'],\n",
       "  'published': '2017-05-15T15:05:54Z',\n",
       "  'updated': '2017-05-15T15:05:54Z',\n",
       "  'abstract': 'Multi-wavelength CCD photometry over 21 years has been used to produce deepcolor-magnitude diagrams together with light curves for the variables in theGalactic globular cluster NGC 5824. Twenty-one new cluster RR Lyrae stars areidentified, bringing the total to 47, of which 42 have reliable periodsdetermined for the first time. The color-magnitude diagram is matched usingBaSTI isochrones with age of $13$~Gyr. and reddening is found to be $E(B-V) =0.15 \\\\pm0.02$; using the period-Wesenheit relation in two colors the distancemodulus is $(m-M)_0=17.45 \\\\pm 0.07$ corresponding to a distance of 30.9 Kpc.The observations show no signs of populations that are significantly youngerthan the $13$~Gyr stars. The width of the red giant branch does not allow for aspread in [Fe/H] greater than $\\\\sigma = 0.05$ dex, and there is no photometricevidence for widened or parallel sequences. The $V, c_{UBI}$ pseudo-colormagnitude diagram shows a bifurcation of the red giant branch that by analogywith other clusters is interpreted as being due to differing spectralsignatures of the first (75\\\\%) and second (25\\\\%) generations of stars whose agedifference is close enough that main sequence turnoffs in the color-magnitudediagram are unresolved. The cluster main sequence is visible against thebackground out to a radial distance of $\\\\sim17$ arcmin. We conclude that NGC5824 appears to be a classical Oosterhoff Type II globular cluster, withoutovert signs of being a remnant of a now-disrupted dwarf galaxy.',\n",
       "  'categories': ['astro-ph.GA'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1705.05280v1'},\n",
       " 903: {'ID': 903,\n",
       "  'title': '5G Utility Pole Planner Using Google Street View and Mask R-CNN',\n",
       "  'authors': ['Yanyu Zhang', 'Osama Alshaykh'],\n",
       "  'published': '2020-08-26T17:27:52Z',\n",
       "  'updated': '2020-08-26T17:27:52Z',\n",
       "  'abstract': 'With the advances of fifth-generation (5G) cellular networks technology, manystudies and work have been carried out on how to build 5G networks for smartcities. In the previous research, street lighting poles and smart light polesare capable of being a 5G access point. In order to determine the position ofthe points, this paper discusses a new way to identify poles based on MaskR-CNN, which extends Fast R-CNNs by making it employ recursive Bayesianfiltering and perform proposal propagation and reuse. The dataset contains3,000 high-resolution images from google map. To make training faster, we useda very efficient GPU implementation of the convolution operation. We achieved atrain error rate of 7.86% and a test error rate of 32.03%. At last, we used theimmune algorithm to set 5G poles in the smart cities.',\n",
       "  'categories': ['cs.CV', 'cs.LG', 'eess.IV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2008.11689v1'},\n",
       " 904: {'ID': 904,\n",
       "  'title': 'Two Causal Principles for Improving Visual Dialog',\n",
       "  'authors': ['Hanwang Zhang', 'Yulei Niu', 'Jianqiang Huang', 'Jiaxin Qi'],\n",
       "  'published': '2019-11-24T10:35:35Z',\n",
       "  'updated': '2020-03-02T17:09:34Z',\n",
       "  'abstract': 'This paper unravels the design tricks adopted by us, the champion teamMReaL-BDAI, for Visual Dialog Challenge 2019: two causal principles forimproving Visual Dialog (VisDial). By \"improving\", we mean that they canpromote almost every existing VisDial model to the state-of-the-art performanceon the leader-board. Such a major improvement is only due to our carefulinspection on the causality behind the model and data, finding that thecommunity has overlooked two causalities in VisDial. Intuitively, Principle 1suggests: we should remove the direct input of the dialog history to the answermodel, otherwise a harmful shortcut bias will be introduced; Principle 2 says:there is an unobserved confounder for history, question, and answer, leading tospurious correlations from training data. In particular, to remove theconfounder suggested in Principle 2, we propose several causal interventionalgorithms, which make the training fundamentally different from thetraditional likelihood estimation. Note that the two principles aremodel-agnostic, so they are applicable in any VisDial model. The code isavailable at https://github.com/simpleshinobu/visdial-principles.',\n",
       "  'categories': ['cs.CV', 'cs.CL'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1911.10496v2'},\n",
       " 905: {'ID': 905,\n",
       "  'title': 'Cross-Language Aphasia Detection using Optimal Transport Domain  Adaptation',\n",
       "  'authors': ['Marzyeh Ghassemi',\n",
       "   'Jekaterina Novikova',\n",
       "   'Aparna Balagopalan',\n",
       "   'Tristan Naumann',\n",
       "   'Bret Nestor',\n",
       "   'Matthew B. A. McDermott'],\n",
       "  'published': '2019-12-04T19:48:54Z',\n",
       "  'updated': '2019-12-04T19:48:54Z',\n",
       "  'abstract': 'Multi-language speech datasets are scarce and often have small sample sizesin the medical domain. Robust transfer of linguistic features across languagescould improve rates of early diagnosis and therapy for speakers of low-resourcelanguages when detecting health conditions from speech. We utilizeout-of-domain, unpaired, single-speaker, healthy speech data for trainingmultiple Optimal Transport (OT) domain adaptation systems. We learn mappingsfrom other languages to English and detect aphasia from linguisticcharacteristics of speech, and show that OT domain adaptation improves aphasiadetection over unilingual baselines for French (6% increased F1) and Mandarin(5% increased F1). Further, we show that adding aphasic data to the domainadaptation system significantly increases performance for both French andMandarin, increasing the F1 scores further (10% and 8% increase in F1 scoresfor French and Mandarin, respectively, over unilingual baselines).',\n",
       "  'categories': ['eess.AS', 'cs.CL', 'cs.LG', 'cs.SD', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1912.04370v1'},\n",
       " 906: {'ID': 906,\n",
       "  'title': 'HDR Denoising and Deblurring by Learning Spatio-temporal Distortion  Models',\n",
       "  'authors': ['Tobias Ritschel',\n",
       "   'Hans-Peter Seidel',\n",
       "   'Karol Myszkowski',\n",
       "   'Mojtaba Bemana',\n",
       "   'Uğur Çoğalan'],\n",
       "  'published': '2020-12-22T13:53:26Z',\n",
       "  'updated': '2020-12-23T10:14:56Z',\n",
       "  'abstract': 'We seek to reconstruct sharp and noise-free high-dynamic range (HDR) videofrom a dual-exposure sensor that records different low-dynamic range (LDR)information in different pixel columns: Odd columns provide low-exposure,sharp, but noisy information; even columns complement this with less noisy,high-exposure, but motion-blurred data. Previous LDR work learns to deblur anddenoise (DISTORTED-&gt;CLEAN) supervised by pairs of CLEAN and DISTORTED images.Regrettably, capturing DISTORTED sensor readings is time-consuming; as well,there is a lack of CLEAN HDR videos. We suggest a method to overcome those twolimitations. First, we learn a different function instead: CLEAN-&gt;DISTORTED,which generates samples containing correlated pixel noise, and row and columnnoise, as well as motion blur from a low number of CLEAN sensor readings.Second, as there is not enough CLEAN HDR video available, we devise a method tolearn from LDR video in-stead. Our approach compares favorably to severalstrong baselines, and can boost existing methods when they are re-trained onour data. Combined with spatial and temporal super-resolution, it enablesapplications such as re-lighting with low noise or blur.',\n",
       "  'categories': ['eess.IV', 'cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2012.12009v2'},\n",
       " 907: {'ID': 907,\n",
       "  'title': 'Exploring the Regularity of Sparse Structure in Convolutional Neural  Networks',\n",
       "  'authors': ['Jeff Pool',\n",
       "   'Song Han',\n",
       "   'Yu Wang',\n",
       "   'William J. Dally',\n",
       "   'Wenshuo Li',\n",
       "   'Xingyu Liu',\n",
       "   'Huizi Mao'],\n",
       "  'published': '2017-05-24T18:35:41Z',\n",
       "  'updated': '2017-06-05T00:22:25Z',\n",
       "  'abstract': 'Sparsity helps reduce the computational complexity of deep neural networks byskipping zeros. Taking advantage of sparsity is listed as a high priority innext generation DNN accelerators such as TPU. The structure of sparsity, i.e.,the granularity of pruning, affects the efficiency of hardware acceleratordesign as well as the prediction accuracy. Coarse-grained pruning createsregular sparsity patterns, making it more amenable for hardware accelerationbut more challenging to maintain the same accuracy. In this paper wequantitatively measure the trade-off between sparsity regularity and predictionaccuracy, providing insights in how to maintain accuracy while having more amore structured sparsity pattern. Our experimental results show thatcoarse-grained pruning can achieve a sparsity ratio similar to unstructuredpruning without loss of accuracy. Moreover, due to the index saving effect,coarse-grained pruning is able to obtain a better compression ratio thanfine-grained sparsity at the same accuracy threshold. Based on the recentsparse convolutional neural network accelerator (SCNN), our experiments furtherdemonstrate that coarse-grained sparsity saves about 2x the memory referencescompared to fine-grained sparsity. Since memory reference is more than twoorders of magnitude more expensive than arithmetic operations, the regularityof sparse structure leads to more efficient hardware design.',\n",
       "  'categories': ['cs.LG', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1705.08922v3'},\n",
       " 908: {'ID': 908,\n",
       "  'title': 'Skin lesion classification with ensemble of squeeze-and-excitation  networks and semi-supervised learning',\n",
       "  'authors': ['Shunsuke Kitada', 'Hitoshi Iyatomi'],\n",
       "  'published': '2018-09-07T16:24:21Z',\n",
       "  'updated': '2018-09-07T16:24:21Z',\n",
       "  'abstract': 'In this report, we introduce the outline of our system in Task 3: DiseaseClassification of ISIC 2018: Skin Lesion Analysis Towards Melanoma Detection.We fine-tuned multiple pre-trained neural network models based onSqueeze-and-Excitation Networks (SENet) which achieved state-of-the-art resultsin the field of image recognition. In addition, we used the mean teachers as asemi-supervised learning framework and introduced some specially designed dataaugmentation strategies for skin lesion analysis. We confirmed our dataaugmentation strategy improved classification performance and demonstrated87.2% in balanced accuracy on the official ISIC2018 validation dataset.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1809.02568v1'},\n",
       " 909: {'ID': 909,\n",
       "  'title': 'Explainable and Explicit Visual Reasoning over Scene Graphs',\n",
       "  'authors': ['Hanwang Zhang', 'Juanzi Li', 'Jiaxin Shi'],\n",
       "  'published': '2018-12-05T08:35:05Z',\n",
       "  'updated': '2019-03-19T12:55:14Z',\n",
       "  'abstract': 'We aim to dismantle the prevalent black-box neural architectures used incomplex visual reasoning tasks, into the proposed eXplainable and eXplicitNeural Modules (XNMs), which advance beyond existing neural module networkstowards using scene graphs --- objects as nodes and the pairwise relationshipsas edges --- for explainable and explicit reasoning with structured knowledge.XNMs allow us to pay more attention to teach machines how to \"think\",regardless of what they \"look\". As we will show in the paper, by using scenegraphs as an inductive bias, 1) we can design XNMs in a concise and flexiblefashion, i.e., XNMs merely consist of 4 meta-types, which significantly reducethe number of parameters by 10 to 100 times, and 2) we can explicitly trace thereasoning-flow in terms of graph attentions. XNMs are so generic that theysupport a wide range of scene graph implementations with various qualities. Forexample, when the graphs are detected perfectly, XNMs achieve 100% accuracy onboth CLEVR and CLEVR CoGenT, establishing an empirical performance upper-boundfor visual reasoning; when the graphs are noisily detected from real-worldimages, XNMs are still robust to achieve a competitive 67.5% accuracy onVQAv2.0, surpassing the popular bag-of-objects attention models without graphstructures.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1812.01855v2'},\n",
       " 910: {'ID': 910,\n",
       "  'title': 'Dual Attention Networks for Visual Reference Resolution in Visual Dialog',\n",
       "  'authors': ['Byoung-Tak Zhang', 'Gi-Cheon Kang', 'Jaeseo Lim'],\n",
       "  'published': '2019-02-25T15:32:56Z',\n",
       "  'updated': '2019-08-29T02:24:23Z',\n",
       "  'abstract': 'Visual dialog (VisDial) is a task which requires an AI agent to answer aseries of questions grounded in an image. Unlike in visual question answering(VQA), the series of questions should be able to capture a temporal contextfrom a dialog history and exploit visually-grounded information. A problemcalled visual reference resolution involves these challenges, requiring theagent to resolve ambiguous references in a given question and find thereferences in a given image. In this paper, we propose Dual Attention Networks(DAN) for visual reference resolution. DAN consists of two kinds of attentionnetworks, REFER and FIND. Specifically, REFER module learns latentrelationships between a given question and a dialog history by employing aself-attention mechanism. FIND module takes image features and reference-awarerepresentations (i.e., the output of REFER module) as input, and performsvisual grounding via bottom-up attention mechanism. We qualitatively andquantitatively evaluate our model on VisDial v1.0 and v0.9 datasets, showingthat DAN outperforms the previous state-of-the-art model by a significantmargin.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1902.09368v3'},\n",
       " 911: {'ID': 911,\n",
       "  'title': 'Computer Vision based Accident Detection for Autonomous Vehicles',\n",
       "  'authors': ['Ilaiah Kavati', 'Savyasachi Gupta', 'Dhananjai Chand'],\n",
       "  'published': '2020-12-20T08:51:10Z',\n",
       "  'updated': '2020-12-20T08:51:10Z',\n",
       "  'abstract': 'Numerous Deep Learning and sensor-based models have been developed to detectpotential accidents with an autonomous vehicle. However, a self-driving carneeds to be able to detect accidents between other vehicles in its path andtake appropriate actions such as to slow down or stop and inform the concernedauthorities. In this paper, we propose a novel support system for self-drivingcars that detects vehicular accidents through a dashboard camera. The systemleverages the Mask R-CNN framework for vehicle detection and a centroidtracking algorithm to track the detected vehicle. Additionally, the frameworkcalculates various parameters such as speed, acceleration, and trajectory todetermine whether an accident has occurred between any of the tracked vehicles.The framework has been tested on a custom dataset of dashcam footage andachieves a high accident detection rate while maintaining a low false alarmrate.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2012.10870v1'},\n",
       " 912: {'ID': 912,\n",
       "  'title': 'Relative Depth Order Estimation Using Multi-scale Densely Connected  Convolutional Networks',\n",
       "  'authors': ['Shengjun Liu', 'Tianqi Zhao', 'Chunhua Shen', 'Ruoxi Deng'],\n",
       "  'published': '2017-07-25T16:11:01Z',\n",
       "  'updated': '2017-07-27T10:19:13Z',\n",
       "  'abstract': 'We study the problem of estimating the relative depth order of point pairs ina monocular image. Recent advances mainly focus on using deep convolutionalneural networks (DCNNs) to learn and infer the ordinal information frommultiple contextual information of the points pair such as global scenecontext, local contextual information, and the locations. However, it remainsunclear how much each context contributes to the task. To address this, wefirst examine the contribution of each context cue [1], [2] to the performancein the context of depth order estimation. We find out the local contextsurrounding the points pair contributes the most and the global scene contexthelps little. Based on the findings, we propose a simple method, using amulti-scale densely-connected network to tackle the task. Instead of learningthe global structure, we dedicate to explore the local structure by learning toregress from regions of multiple sizes around the point pairs. Moreover, we usethe recent densely connected network [3] to encourage substantial feature reuseas well as deepen our network to boost the performance. We show in experimentsthat the results of our approach is on par with or better than thestate-of-the-art methods with the benefit of using only a small number oftraining data.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1707.08063v2'},\n",
       " 913: {'ID': 913,\n",
       "  'title': 'Robust pedestrian detection in thermal imagery using synthesized images',\n",
       "  'authors': ['Andrew D. Bagdanov',\n",
       "   'Leonardo Galteri',\n",
       "   'Alberto Del Bimbo',\n",
       "   'Marco Bertini',\n",
       "   'Lorenzo Berlincioni',\n",
       "   'My Kieu'],\n",
       "  'published': '2021-02-03T11:08:31Z',\n",
       "  'updated': '2021-02-03T11:08:31Z',\n",
       "  'abstract': 'In this paper we propose a method for improving pedestrian detection in thethermal domain using two stages: first, a generative data augmentation approachis used, then a domain adaptation method using generated data adapts an RGBpedestrian detector. Our model, based on the Least-Squares GenerativeAdversarial Network, is trained to synthesize realistic thermal versions ofinput RGB images which are then used to augment the limited amount of labeledthermal pedestrian images available for training. We apply our generative dataaugmentation strategy in order to adapt a pretrained YOLOv3 pedestrian detectorto detection in the thermal-only domain. Experimental results demonstrate theeffectiveness of our approach: using less than 50\\\\% of available real thermaltraining data, and relying on synthesized data generated by our model in thedomain adaptation phase, our detector achieves state-of-the-art results on theKAIST Multispectral Pedestrian Detection Benchmark; even if more real thermaldata is available adding GAN generated images to the training data results inimproved performance, thus showing that these images act as an effective formof data augmentation. To the best of our knowledge, our detector achieves thebest single-modality detection results on KAIST with respect to thestate-of-the-art.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2102.02005v1'},\n",
       " 914: {'ID': 914,\n",
       "  'title': 'Finding Action Tubes with a Sparse-to-Dense Framework',\n",
       "  'authors': ['John See',\n",
       "   'Weiyao Lin',\n",
       "   'Ning Xu',\n",
       "   'Rui Qian',\n",
       "   'Limin Wang',\n",
       "   'Yuxi Li',\n",
       "   'Tao Wang',\n",
       "   'Shugong Xu'],\n",
       "  'published': '2020-08-30T15:38:44Z',\n",
       "  'updated': '2020-08-30T15:38:44Z',\n",
       "  'abstract': 'The task of spatial-temporal action detection has attracted increasingattention among researchers. Existing dominant methods solve this problem byrelying on short-term information and dense serial-wise detection on eachindividual frames or clips. Despite their effectiveness, these methods showedinadequate use of long-term information and are prone to inefficiency. In thispaper, we propose for the first time, an efficient framework that generatesaction tube proposals from video streams with a single forward pass in asparse-to-dense manner. There are two key characteristics in this framework:(1) Both long-term and short-term sampled information are explicitly utilizedin our spatiotemporal network, (2) A new dynamic feature sampling module (DTS)is designed to effectively approximate the tube output while keeping the systemtractable. We evaluate the efficacy of our model on the UCF101-24, JHMDB-21 andUCFSports benchmark datasets, achieving promising results that are competitiveto state-of-the-art methods. The proposed sparse-to-dense strategy rendered ourframework about 7.6 times more efficient than the nearest competitor.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2008.13196v1'},\n",
       " 915: {'ID': 915,\n",
       "  'title': 'An Integral Pose Regression System for the ECCV2018 PoseTrack Challenge',\n",
       "  'authors': ['Xiao Sun', 'Stephen Lin', 'Chuankang Li'],\n",
       "  'published': '2018-09-17T08:59:22Z',\n",
       "  'updated': '2018-09-17T08:59:22Z',\n",
       "  'abstract': 'For the ECCV 2018 PoseTrack Challenge, we present a 3D human pose estimationsystem based mainly on the integral human pose regression method. We show acomprehensive ablation study to examine the key performance factors of theproposed system. Our system obtains 47mm MPJPE on the CHALL_H80K test dataset,placing second in the ECCV2018 3D human pose estimation challenge. Code will bereleased to facilitate future work.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1809.06079v1'},\n",
       " 916: {'ID': 916,\n",
       "  'title': 'Groups as Graphs',\n",
       "  'authors': ['W. B. Vasantha Kandasamy', 'Florentin Smarandache'],\n",
       "  'published': '2009-06-28T16:06:18Z',\n",
       "  'updated': '2009-06-28T16:06:18Z',\n",
       "  'abstract': 'For the first time we represent every finite group in the form of a graph inthis book. The authors choose to call these graphs as identity graph, since themain role in obtaining the graph is played by the identity element of thegroup.  This study is innovative because through this description one can immediatelylook at the graph and say the number of elements in the group G which areself-inversed. Also study of different properties, like the subgroups of agroup, normal subgroups of a group, p-sylow subgroups of a group and conjugateelements of a group are carried out using the identity graph of the group inthis book.  This book has four chapters. The first chapter is introductory. The secondchapter represents groups as graphs. In the third chapter, we have definedsimilar types of graphs for algebraic structures like commutative semigroups,loops, commutative groupoids and commutative rings. The final chapter poses 52problems.',\n",
       "  'categories': ['math.GM', '20-02, 20NXX, 05CXX'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/0906.5144v1'},\n",
       " 917: {'ID': 917,\n",
       "  'title': 'Tangent Convolutions for Dense Prediction in 3D',\n",
       "  'authors': ['Qian-Yi Zhou',\n",
       "   'Jaesik Park',\n",
       "   'Vladlen Koltun',\n",
       "   'Maxim Tatarchenko'],\n",
       "  'published': '2018-07-06T15:14:51Z',\n",
       "  'updated': '2018-07-06T15:14:51Z',\n",
       "  'abstract': 'We present an approach to semantic scene analysis using deep convolutionalnetworks. Our approach is based on tangent convolutions - a new constructionfor convolutional networks on 3D data. In contrast to volumetric approaches,our method operates directly on surface geometry. Crucially, the constructionis applicable to unstructured point clouds and other noisy real-world data. Weshow that tangent convolutions can be evaluated efficiently on large-scalepoint clouds with millions of points. Using tangent convolutions, we design adeep fully-convolutional network for semantic segmentation of 3D point clouds,and apply it to challenging real-world datasets of indoor and outdoor 3Denvironments. Experimental results show that the presented approach outperformsother recent deep network constructions in detailed analysis of large 3Dscenes.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1807.02443v1'},\n",
       " 918: {'ID': 918,\n",
       "  'title': 'Visual Coreference Resolution in Visual Dialog using Neural Module  Networks',\n",
       "  'authors': ['Satwik Kottur',\n",
       "   'Dhruv Batra',\n",
       "   'Marcus Rohrbach',\n",
       "   'José M. F. Moura',\n",
       "   'Devi Parikh'],\n",
       "  'published': '2018-09-06T04:36:22Z',\n",
       "  'updated': '2018-09-06T04:36:22Z',\n",
       "  'abstract': \"Visual dialog entails answering a series of questions grounded in an image,using dialog history as context. In addition to the challenges found in visualquestion answering (VQA), which can be seen as one-round dialog, visual dialogencompasses several more. We focus on one such problem called visualcoreference resolution that involves determining which words, typically nounphrases and pronouns, co-refer to the same entity/object instance in an image.This is crucial, especially for pronouns (e.g., `it'), as the dialog agent mustfirst link it to a previous coreference (e.g., `boat'), and only then can relyon the visual grounding of the coreference `boat' to reason about the pronoun`it'. Prior work (in visual dialog) models visual coreference resolution either(a) implicitly via a memory network over history, or (b) at a coarse level forthe entire question; and not explicitly at a phrase level of granularity. Inthis work, we propose a neural module network architecture for visual dialog byintroducing two novel modules - Refer and Exclude - that perform explicit,grounded, coreference resolution at a finer word level. We demonstrate theeffectiveness of our model on MNIST Dialog, a visually simple yetcoreference-wise complex dataset, by achieving near perfect accuracy, and onVisDial, a large and challenging visual dialog dataset on real images, whereour model outperforms other approaches, and is more interpretable, grounded,and consistent qualitatively.\",\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.CL'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1809.01816v1'},\n",
       " 919: {'ID': 919,\n",
       "  'title': 'Mango Tree Net -- A fully convolutional network for semantic  segmentation and individual crown detection of mango trees',\n",
       "  'authors': ['Vikas Agaradahalli Gurumurthy',\n",
       "   'Omkar Narasipura',\n",
       "   'Ramesh Kestur'],\n",
       "  'published': '2019-07-16T09:41:13Z',\n",
       "  'updated': '2019-07-16T09:41:13Z',\n",
       "  'abstract': 'This work presents a method for semantic segmentation of mango trees in highresolution aerial imagery, and, a novel method for individual crown detectionof mango trees using segmentation output. Mango Tree Net, a fully convolutionalneural network (FCN), is trained using supervised learning to perform semanticsegmentation of mango trees in imagery acquired using an unmanned aerialvehicle (UAV). The proposed network is retrained to separatetouching/overlapping tree crowns in segmentation output. Contour basedconnected object detection is performed on the segmentation output fromretrained network. Bounding boxes are drawn on the original images usingcoordinates of connected objects to achieve individual crown detection. Thetraining dataset consists of 8,824 image patches of size 240 x 240. Theapproach is tested for performance on segmentation and individual crowndetection tasks using test datasets containing 36 and 4 images respectively.The performance is analyzed using standard metrics precision, recall, f1-scoreand accuracy. Results obtained demonstrate the robustness of the proposedmethods despite variations in factors such as scale, occlusion, lightingconditions and surrounding vegetation.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1907.06915v1'},\n",
       " 920: {'ID': 920,\n",
       "  'title': 'Normalization in Training U-Net for 2D Biomedical Semantic Segmentation',\n",
       "  'authors': ['Xiao-Yun Zhou', 'Guang-Zhong Yang'],\n",
       "  'published': '2018-09-11T10:27:45Z',\n",
       "  'updated': '2019-01-12T21:12:14Z',\n",
       "  'abstract': \"2D biomedical semantic segmentation is important for robotic vision insurgery. Segmentation methods based on Deep Convolutional Neural Network (DCNN)can out-perform conventional methods in terms of both accuracy and levels ofautomation. One common issue in training a DCNN for biomedical semanticsegmentation is the internal covariate shift where the training ofconvolutional kernels is encumbered by the distribution change of inputfeatures, hence both the training speed and performance are decreased. BatchNormalization (BN) is the first proposed method for addressing internalcovariate shift and is widely used. Instance Normalization (IN) and LayerNormalization (LN) have also been proposed. Group Normalization (GN) isproposed more recently and has not yet been applied to 2D biomedical semanticsegmentation, however, no specific validations on GN were given. Most DCNNs forbiomedical semantic segmentation adopt BN as the normalization method bydefault, without reviewing its performance. In this paper, four normalizationmethods - BN, IN, LN and GN are compared in details, specifically for 2Dbiomedical semantic segmentation. U-Net is adopted as the basic DCNN structure.Three datasets regarding the Right Ventricle (RV), aorta, and Left Ventricle(LV) are used for the validation. The results show that detailed subdivision ofthe feature map, i.e. GN with a large group number or IN, achieves higheraccuracy. This accuracy improvement mainly comes from better modelgeneralization. Codes are uploaded and maintained at Xiao-Yun Zhou's Github.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1809.03783v3'},\n",
       " 921: {'ID': 921,\n",
       "  'title': 'Interactive Deep Colorization With Simultaneous Global and Local Inputs',\n",
       "  'authors': ['Peiyao Zhou', 'Yi Xiao', 'Yan Zheng'],\n",
       "  'published': '2018-01-27T12:36:31Z',\n",
       "  'updated': '2018-01-27T12:36:31Z',\n",
       "  'abstract': \"Colorization methods using deep neural networks have become a recent trend.However, most of them do not allow user inputs, or only allow limited userinputs (only global inputs or only local inputs), to control the outputcolorful images. The possible reason is that it's difficult to differentiatethe influence of different kind of user inputs in network training. To solvethis problem, we present a novel deep colorization method, which allowssimultaneous global and local inputs to better control the output colorizedimages. The key step is to design an appropriate loss function that candifferentiate the influence of input data, global inputs and local inputs. Withthis design, our method accepts no inputs, or global inputs, or local inputs,or both global and local inputs, which is not supported in previous deepcolorization methods. In addition, we propose a global color themerecommendation system to help users determine global inputs. Experimentalresults shows that our methods can better control the colorized images andgenerate state-of-art results.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1801.09083v1'},\n",
       " 922: {'ID': 922,\n",
       "  'title': 'Deep HST/ACS Photometry of the M81 Halo',\n",
       "  'authors': ['Ata Sarajedini', 'Patrick R. Durrell', 'Rupali Chandar'],\n",
       "  'published': '2010-06-10T13:59:25Z',\n",
       "  'updated': '2010-06-10T13:59:25Z',\n",
       "  'abstract': 'We present a deep color-magnitude diagram for individual stars in the halo ofthe nearby spiral galaxy M81, at a projected distance of 19 kpc, based on datataken with the Advanced Camera for Surveys on the Hubble Space Telescope (HST).The color magnitude diagram reveals a red giant branch that is narrow andfairly blue, and a horizontal branch that has stars that lie mostly redward ofthe RR Lyrae instability strip. We derive a mean metallicity of [M/H] = -1.15+\\\\- 0.11 and age of 9 +\\\\- 2 Gyr for the dominant population in our field, fromthe shape of the red giant branch, the magnitude of the red clump, and thelocation of the red giant branch bump. We compare our metallicity and ageresults with those found previously for stars in different locations withinM81, and in the spheroids of other nearby galaxies.',\n",
       "  'categories': ['astro-ph.CO', 'astro-ph.GA'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1006.2036v1'},\n",
       " 923: {'ID': 923,\n",
       "  'title': 'Deep Supervised Discrete Hashing',\n",
       "  'authors': ['Ran He', 'Zhenan Sun', 'Tieniu Tan', 'Qi Li'],\n",
       "  'published': '2017-05-31T09:16:38Z',\n",
       "  'updated': '2017-11-27T14:24:08Z',\n",
       "  'abstract': 'With the rapid growth of image and video data on the web, hashing has beenextensively studied for image or video search in recent years. Benefit fromrecent advances in deep learning, deep hashing methods have achieved promisingresults for image retrieval. However, there are some limitations of previousdeep hashing methods (e.g., the semantic information is not fully exploited).In this paper, we develop a deep supervised discrete hashing algorithm based onthe assumption that the learned binary codes should be ideal forclassification. Both the pairwise label information and the classificationinformation are used to learn the hash codes within one stream framework. Weconstrain the outputs of the last layer to be binary codes directly, which israrely investigated in deep hashing algorithm. Because of the discrete natureof hash codes, an alternating minimization method is used to optimize theobjective function. Experimental results have shown that our method outperformscurrent state-of-the-art methods on benchmark datasets.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1705.10999v2'},\n",
       " 924: {'ID': 924,\n",
       "  'title': 'Dictionary Learning for Deblurring and Digital Zoom',\n",
       "  'authors': ['Florent Couzinie-Devy',\n",
       "   'Francis Bach',\n",
       "   'Julien Mairal',\n",
       "   'Jean Ponce'],\n",
       "  'published': '2011-10-05T11:49:09Z',\n",
       "  'updated': '2011-10-05T11:49:09Z',\n",
       "  'abstract': 'This paper proposes a novel approach to image deblurring and digital zoomingusing sparse local models of image appearance. These models, where small imagepatches are represented as linear combinations of a few elements drawn fromsome large set (dictionary) of candidates, have proven well adapted to severalimage restoration tasks. A key to their success has been to learn dictionariesadapted to the reconstruction of small image patches. In contrast, recent workshave proposed instead to learn dictionaries which are not only adapted to datareconstruction, but also tuned for a specific task. We introduce here such anapproach to deblurring and digital zoom, using pairs of blurry/sharp (orlow-/high-resolution) images for training, as well as an effective stochasticgradient algorithm for solving the corresponding optimization task. Althoughthis learning problem is not convex, once the dictionaries have been learned,the sharp/high-resolution image can be recovered via convex optimization attest time. Experiments with synthetic and real data demonstrate theeffectiveness of the proposed approach, leading to state-of-the-art performancefor non-blind image deblurring and digital zoom.',\n",
       "  'categories': ['cs.LG', 'cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1110.0957v1'},\n",
       " 925: {'ID': 925,\n",
       "  'title': 'Continual Learning: Tackling Catastrophic Forgetting in Deep Neural  Networks with Replay Processes',\n",
       "  'authors': ['Timothée Lesort'],\n",
       "  'published': '2020-07-01T13:44:33Z',\n",
       "  'updated': '2020-12-08T17:08:29Z',\n",
       "  'abstract': 'Humans learn all their life long. They accumulate knowledge from a sequenceof learning experiences and remember the essential concepts without forgettingwhat they have learned previously. Artificial neural networks struggle to learnsimilarly. They often rely on data rigorously preprocessed to learn solutionsto specific problems such as classification or regression. In particular, theyforget their past learning experiences if trained on new ones. Therefore,artificial neural networks are often inept to deal with real-life settings suchas an autonomous-robot that has to learn on-line to adapt to new situations andovercome new problems without forgetting its past learning-experiences.Continual learning (CL) is a branch of machine learning addressing this type ofproblem. Continual algorithms are designed to accumulate and improve knowledgein a curriculum of learning-experiences without forgetting. In this thesis, wepropose to explore continual algorithms with replay processes. Replay processesgather together rehearsal methods and generative replay methods. GenerativeReplay consists of regenerating past learning experiences with a generativemodel to remember them. Rehearsal consists of saving a core-set of samples frompast learning experiences to rehearse them later. The replay processes makepossible a compromise between optimizing the current learning objective and thepast ones enabling learning without forgetting in sequences of tasks settings.We show that they are very promising methods for continual learning. Notably,they enable the re-evaluation of past data with new knowledge and theconfrontation of data from different learning-experiences. We demonstrate theirability to learn continually through unsupervised learning, supervised learningand reinforcement learning tasks.',\n",
       "  'categories': ['cs.LG', 'cs.AI', 'cs.NE'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2007.00487v3'},\n",
       " 926: {'ID': 926,\n",
       "  'title': 'An Empirical Method to Quantify the Peripheral Performance Degradation  in Deep Networks',\n",
       "  'authors': ['John K. Tsotsos', 'Calden Wloka'],\n",
       "  'published': '2020-12-04T18:00:47Z',\n",
       "  'updated': '2020-12-04T18:00:47Z',\n",
       "  'abstract': 'When applying a convolutional kernel to an image, if the output is to remainthe same size as the input then some form of padding is required around theimage boundary, meaning that for each layer of convolution in a convolutionalneural network (CNN), a strip of pixels equal to the half-width of the kernelsize is produced with a non-veridical representation. Although most CNN kernelsare small to reduce the parameter load of a network, this non-veridical areacompounds with each convolutional layer. The tendency toward deeper and deepernetworks combined with stride-based down-sampling means that the propagation ofthis region can end up covering a non-negligable portion of the image. Althoughthis issue with convolutions has been well acknowledged over the years, theimpact of this degraded peripheral representation on modern network behaviorhas not been fully quantified. What are the limits of translation invariance?Does image padding successfully mitigate the issue, or is performance affectedas an object moves between the image border and center? Using Mask R-CNN as anexperimental model, we design a dataset and methodology to quantify the spatialdependency of network performance. Our dataset is constructed by insertingobjects into high resolution backgrounds, thereby allowing us to cropsub-images which place target objects at specific locations relative to theimage border. By probing the behaviour of Mask R-CNN across a selection oftarget locations, we see clear patterns of performance degredation near theimage boundary, and in particular in the image corners. Quantifying both theextent and magnitude of this spatial anisotropy in network performance isimportant for the deployment of deep networks into unconstrained and realisticenvironments in which the location of objects or regions of interest are notguaranteed to be well localized within a given image.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2012.02749v1'},\n",
       " 927: {'ID': 927,\n",
       "  'title': 'DeepSwarm: Optimising Convolutional Neural Networks using Swarm  Intelligence',\n",
       "  'authors': ['Wei Pang', 'Edvinas Byla'],\n",
       "  'published': '2019-05-17T16:13:38Z',\n",
       "  'updated': '2019-05-17T16:13:38Z',\n",
       "  'abstract': 'In this paper we propose DeepSwarm, a novel neural architecture search (NAS)method based on Swarm Intelligence principles. At its core DeepSwarm uses AntColony Optimization (ACO) to generate ant population which uses the pheromoneinformation to collectively search for the best neural architecture.Furthermore, by using local and global pheromone update rules our methodensures the balance between exploitation and exploration. On top of this, tomake our method more efficient we combine progressive neural architecturesearch with weight reusability. Furthermore, due to the nature of ACO ourmethod can incorporate heuristic information which can further speed up thesearch process. After systematic and extensive evaluation, we discover that onthree different datasets (MNIST, Fashion-MNIST, and CIFAR-10) when compared toexisting systems our proposed method demonstrates competitive performance.Finally, we open source DeepSwarm as a NAS library and hope it can be used bymore deep learning researchers and practitioners.',\n",
       "  'categories': ['cs.LG', 'cs.NE', 'stat.ML', 'I.2.6'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1905.07350v1'},\n",
       " 928: {'ID': 928,\n",
       "  'title': 'Transferable Universal Adversarial Perturbations Using Generative Models',\n",
       "  'authors': ['Atiye Sadat Hashemi',\n",
       "   'Tim Fingscheidt',\n",
       "   'Andreas Bär',\n",
       "   'Saeed Mozaffari'],\n",
       "  'published': '2020-10-28T12:31:59Z',\n",
       "  'updated': '2020-10-29T15:19:41Z',\n",
       "  'abstract': 'Deep neural networks tend to be vulnerable to adversarial perturbations,which by adding to a natural image can fool a respective model with highconfidence. Recently, the existence of image-agnostic perturbations, also knownas universal adversarial perturbations (UAPs), were discovered. However,existing UAPs still lack a sufficiently high fooling rate, when being appliedto an unknown target model. In this paper, we propose a novel deep learningtechnique for generating more transferable UAPs. We utilize a perturbationgenerator and some given pretrained networks so-called source models togenerate UAPs using the ImageNet dataset. Due to the similar featurerepresentation of various model architectures in the first layer, we propose aloss formulation that focuses on the adversarial energy only in the respectivefirst layer of the source models. This supports the transferability of ourgenerated UAPs to any other target model. We further empirically analyze ourgenerated UAPs and demonstrate that these perturbations generalize very welltowards different target models. Surpassing the current state of the art inboth, fooling rate and model-transferability, we can show the superiority ofour proposed approach. Using our generated non-targeted UAPs, we obtain anaverage fooling rate of 93.36% on the source models (state of the art: 82.16%).Generating our UAPs on the deep ResNet-152, we obtain about a 12% absolutefooling rate advantage vs. cutting-edge methods on VGG-16 and VGG-19 targetmodels.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2010.14919v2'},\n",
       " 929: {'ID': 929,\n",
       "  'title': \"Don't Just Assume; Look and Answer: Overcoming Priors for Visual  Question Answering\",\n",
       "  'authors': ['Devi Parikh',\n",
       "   'Dhruv Batra',\n",
       "   'Aniruddha Kembhavi',\n",
       "   'Aishwarya Agrawal'],\n",
       "  'published': '2017-12-01T15:48:50Z',\n",
       "  'updated': '2018-06-03T15:32:06Z',\n",
       "  'abstract': \"A number of studies have found that today's Visual Question Answering (VQA)models are heavily driven by superficial correlations in the training data andlack sufficient image grounding. To encourage development of models gearedtowards the latter, we propose a new setting for VQA where for every questiontype, train and test sets have different prior distributions of answers.Specifically, we present new splits of the VQA v1 and VQA v2 datasets, which wecall Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2respectively). First, we evaluate several existing VQA models under this newsetting and show that their performance degrades significantly compared to theoriginal VQA setting. Second, we propose a novel Grounded Visual QuestionAnswering model (GVQA) that contains inductive biases and restrictions in thearchitecture specifically designed to prevent the model from 'cheating' byprimarily relying on priors in the training data. Specifically, GVQA explicitlydisentangles the recognition of visual concepts present in the image from theidentification of plausible answer space for a given question, enabling themodel to more robustly generalize across different distributions of answers.GVQA is built off an existing VQA model -- Stacked Attention Networks (SAN).Our experiments demonstrate that GVQA significantly outperforms SAN on bothVQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms morepowerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) inseveral cases. GVQA offers strengths complementary to SAN when trained andevaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is moretransparent and interpretable than existing VQA models.\",\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1712.00377v2'},\n",
       " 930: {'ID': 930,\n",
       "  'title': 'Study and development of a Computer-Aided Diagnosis system for  classification of chest x-ray images using convolutional neural networks  pre-trained for ImageNet and data augmentation',\n",
       "  'authors': ['Vinicius Pavanelli Vianna'],\n",
       "  'published': '2018-06-03T17:31:42Z',\n",
       "  'updated': '2018-06-03T17:31:42Z',\n",
       "  'abstract': 'Convolutional neural networks (ConvNets) are the actual standard for imagerecognizement and classification. On the present work we develop a ComputerAided-Diagnosis (CAD) system using ConvNets to classify a x-rays chest imagesdataset in two groups: Normal and Pneumonia. The study uses ConvNets modelsavailable on the PyTorch platform: AlexNet, SqueezeNet, ResNet and Inception.We initially use three training styles: complete from scratch using randominitialization, using a pre-trained ImageNet model training only the last layeradapted to our problem (transfer learning) and a pre-trained model modifiedtraining all the classifying layers of the model (fine tuning). The laststrategy of training used is with data augmentation techniques that avoid overfitting problems on ConvNets yielding the better results on this study',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1806.00839v1'},\n",
       " 931: {'ID': 931,\n",
       "  'title': 'Star Formation Histories in the Local Group',\n",
       "  'authors': ['Thomas M. Brown'],\n",
       "  'published': '2004-07-09T19:03:14Z',\n",
       "  'updated': '2004-07-09T19:03:14Z',\n",
       "  'abstract': \"Deep color magnitude diagrams extending to the main sequence provide the mostdirect measure of the detailed star formation history in a stellar population.With large investments of observing time, HST can obtain such data forpopulations out to 1 Mpc, but its field of view is extremely small incomparison to the size of Local Group galaxies. This limitation severelyconstrains our understanding of galaxy formation. For example, the largestgalaxy in the Local Group, Andromeda, offers an ideal laboratory for studyingthe formation of large spiral galaxies, but the galaxy shows substructure on avariety of scales, presumably due to its violent merger history. Within itsremaining lifetime, HST can only sample a few sight-lines through this complexgalaxy. In contrast, a wide field imager could provide a map of Andromeda'shalo, outer disk, and tidal streams, revealing the spatially-dependent starformation history in each structure. The same data would enable many secondarystudies, such as the age variation in Andromeda's globular cluster system,gigantic samples of variable stars, and microlensing tracers of the galaxy'sdark matter distribution.\",\n",
       "  'categories': ['astro-ph'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/astro-ph/0407193v1'},\n",
       " 932: {'ID': 932,\n",
       "  'title': 'DialGraph: Sparse Graph Learning Networks for Visual Dialog',\n",
       "  'authors': ['Gi-Cheon Kang',\n",
       "   'Junseok Park',\n",
       "   'Hwaran Lee',\n",
       "   'Byoung-Tak Zhang',\n",
       "   'Jin-Hwa Kim'],\n",
       "  'published': '2020-04-14T17:52:41Z',\n",
       "  'updated': '2020-04-14T17:52:41Z',\n",
       "  'abstract': 'Visual dialog is a task of answering a sequence of questions grounded in animage utilizing a dialog history. Previous studies have implicitly explored theproblem of reasoning semantic structures among the history using softmaxattention. However, we argue that the softmax attention yields dense structuresthat could distract to answer the questions requiring partial or even nocontextual information. In this paper, we formulate the visual dialog tasks asgraph structure learning tasks. To tackle the problem, we propose Sparse GraphLearning Networks (SGLNs) consisting of a multimodal node embedding module anda sparse graph learning module. The proposed model explicitly learn sparsedialog structures by incorporating binary and score edges, leveraging a newstructural loss function. Then, it finally outputs the answer, updating eachnode via a message passing framework. As a result, the proposed modeloutperforms the state-of-the-art approaches on the VisDial v1.0 dataset, onlyusing 10.95% of the dialog history, as well as improves interpretabilitycompared to baseline methods.',\n",
       "  'categories': ['cs.CV', 'cs.CL', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2004.06698v1'},\n",
       " 933: {'ID': 933,\n",
       "  'title': 'CD-UAP: Class Discriminative Universal Adversarial Perturbation',\n",
       "  'authors': ['Tooba Imtiaz', 'Chaoning Zhang', 'Philipp Benz', 'In So Kweon'],\n",
       "  'published': '2020-10-07T09:26:42Z',\n",
       "  'updated': '2020-10-07T09:26:42Z',\n",
       "  'abstract': 'A single universal adversarial perturbation (UAP) can be added to all naturalimages to change most of their predicted class labels. It is of high practicalrelevance for an attacker to have flexible control over the targeted classes tobe attacked, however, the existing UAP method attacks samples from all classes.In this work, we propose a new universal attack method to generate a singleperturbation that fools a target network to misclassify only a chosen group ofclasses, while having limited influence on the remaining classes. Since theproposed attack generates a universal adversarial perturbation that isdiscriminative to targeted and non-targeted classes, we term it classdiscriminative universal adversarial perturbation (CD-UAP). We propose onesimple yet effective algorithm framework, under which we design and comparevarious loss function configurations tailored for the class discriminativeuniversal attack. The proposed approach has been evaluated with extensiveexperiments on various benchmark datasets. Additionally, our proposed approachachieves state-of-the-art performance for the original task of UAP attackingall classes, which demonstrates the effectiveness of our approach.',\n",
       "  'categories': ['cs.CV', 'cs.CR', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2010.03300v1'},\n",
       " 934: {'ID': 934,\n",
       "  'title': 'Scalable, End-to-End, Deep-Learning-Based Data Reconstruction Chain for  Particle Imaging Detectors',\n",
       "  'authors': ['Francois Drielsma',\n",
       "   'Kazuhiro Terao',\n",
       "   'Dae Heun Koh',\n",
       "   'Laura Dominé'],\n",
       "  'published': '2021-02-01T18:10:00Z',\n",
       "  'updated': '2021-02-01T18:10:00Z',\n",
       "  'abstract': 'Recent inroads in Computer Vision (CV) and Machine Learning (ML) havemotivated a new approach to the analysis of particle imaging detector data.Unlike previous efforts which tackled isolated CV tasks, this paper introducesan end-to-end, ML-based data reconstruction chain for Liquid Argon TimeProjection Chambers (LArTPCs), the state-of-the-art in precision imaging at theintensity frontier of neutrino physics. The chain is a multi-task networkcascade which combines voxel-level feature extraction using SparseConvolutional Neural Networks and particle superstructure formation using GraphNeural Networks. Each algorithm incorporates physics-informed inductive biases,while their collective hierarchy is used to enforce a causal structure. Theoutput is a comprehensive description of an event that may be used forhigh-level physics inference. The chain is end-to-end optimizable, eliminatingthe need for time-intensive manual software adjustments. It is also the firstimplementation to handle the unprecedented pile-up of dozens of high energyneutrino interactions, expected in the 3D-imaging LArTPC of the DeepUnderground Neutrino Experiment. The chain is trained as a whole and itsperformance is assessed at each step using an open simulated data set.',\n",
       "  'categories': ['hep-ex', 'cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2102.01033v1'},\n",
       " 935: {'ID': 935,\n",
       "  'title': 'Marine Animal Classification with Correntropy Loss Based Multi-view  Learning',\n",
       "  'authors': ['Bing Ouyang',\n",
       "   'Fraser Dalgleish',\n",
       "   'Zheng Cao',\n",
       "   'Anni Vuorenkoski',\n",
       "   'Gabriel Alsenas',\n",
       "   'Shujian Yu',\n",
       "   'Jose Principe'],\n",
       "  'published': '2017-05-03T01:26:24Z',\n",
       "  'updated': '2017-05-03T01:26:24Z',\n",
       "  'abstract': 'To analyze marine animals behavior, seasonal distribution and abundance,digital imagery can be acquired by visual or Lidar camera. Depending on thequantity and properties of acquired imagery, the animals are characterized aseither features (shape, color, texture, etc.), or dissimilarity matricesderived from different shape analysis methods (shape context, internal distanceshape context, etc.). For both cases, multi-view learning is critical inintegrating more than one set of feature or dissimilarity matrix for higherclassification accuracy. This paper adopts correntropy loss as cost function inmulti-view learning, which has favorable statistical properties for rejectingnoise. For the case of features, the correntropy loss-based multi-view learningand its entrywise variation are developed based on the multi-view intact spacelearning algorithm. For the case of dissimilarity matrices, the robustEuclidean embedding algorithm is extended to its multi-view form with thecorrentropy loss function. Results from simulated data and real-world marineanimal imagery show that the proposed algorithms can effectively enhanceclassification rate, as well as suppress noise under different noiseconditions.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1705.01217v1'},\n",
       " 936: {'ID': 936,\n",
       "  'title': 'Using a modified double deep image prior for crosstalk mitigation in  multislice ptychography',\n",
       "  'authors': ['Chris Jacobsen', 'Ming Du', 'Xiaojing Huang'],\n",
       "  'published': '2021-01-29T17:00:40Z',\n",
       "  'updated': '2021-01-29T17:00:40Z',\n",
       "  'abstract': 'Multislice ptychography is a high-resolution microscopy technique used toimage multiple separate axial planes using a single illumination direction.However, multislice ptychography reconstructions are often degraded bycrosstalk, where some features on one plane erroneously contribute to thereconstructed image of another plane. Here, we demonstrate the use of amodified \"double deep image prior\" (DDIP) architecture in mitigating crosstalkartifacts in multislice ptychography. Utilizing the tendency of generativeneural networks to produce natural images, a modified DDIP method yielded goodresults on experimental data. For one of the datasets, we show that using DDIPcould remove the need of using additional experimental data, such as from x-rayfluorescence, to suppress the crosstalk. Our method may help x-ray multisliceptychography work for more general experimental scenarios.',\n",
       "  'categories': ['eess.IV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2102.00869v1'},\n",
       " 937: {'ID': 937,\n",
       "  'title': 'Accuracy Booster: Performance Boosting using Feature Map Re-calibration',\n",
       "  'authors': ['Pravendra Singh', 'Vinay P. Namboodiri', 'Pratik Mazumder'],\n",
       "  'published': '2019-03-11T16:16:03Z',\n",
       "  'updated': '2020-01-07T10:44:44Z',\n",
       "  'abstract': 'Convolution Neural Networks (CNN) have been extremely successful in solvingintensive computer vision tasks. The convolutional filters used in CNNs haveplayed a major role in this success, by extracting useful features from theinputs. Recently researchers have tried to boost the performance of CNNs byre-calibrating the feature maps produced by these filters, e.g.,Squeeze-and-Excitation Networks (SENets). These approaches have achieved betterperformance by Exciting up the important channels or feature maps whilediminishing the rest. However, in the process, architectural complexity hasincreased. We propose an architectural block that introduces much lowercomplexity than the existing methods of CNN performance boosting whileperforming significantly better than them. We carry out experiments on theCIFAR, ImageNet and MS-COCO datasets, and show that the proposed block canchallenge the state-of-the-art results. Our method boosts the ResNet-50architecture to perform comparably to the ResNet-152 architecture, which is athree times deeper network, on classification. We also show experimentally thatour method is not limited to classification but also generalizes well to othertasks such as object detection.',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1903.04407v2'},\n",
       " 938: {'ID': 938,\n",
       "  'title': 'Direct Sparse Odometry with Rolling Shutter',\n",
       "  'authors': ['Vladyslav Usenko',\n",
       "   'Daniel Cremers',\n",
       "   'Jörg Stückler',\n",
       "   'Nikolaus Demmel',\n",
       "   'David Schubert'],\n",
       "  'published': '2018-08-01T20:48:02Z',\n",
       "  'updated': '2018-08-01T20:48:02Z',\n",
       "  'abstract': 'Neglecting the effects of rolling-shutter cameras for visual odometry (VO)severely degrades accuracy and robustness. In this paper, we propose a noveldirect monocular VO method that incorporates a rolling-shutter model. Ourapproach extends direct sparse odometry which performs direct bundle adjustmentof a set of recent keyframe poses and the depths of a sparse set of imagepoints. We estimate the velocity at each keyframe and impose aconstant-velocity prior for the optimization. In this way, we obtain a nearreal-time, accurate direct VO method. Our approach achieves improved results onchallenging rolling-shutter sequences over state-of-the-art global-shutter VO.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1808.00558v1'},\n",
       " 939: {'ID': 939,\n",
       "  'title': 'Metallicities, Age--Metallicity relationships, and Kinematics of Red  Giant Branch Stars in the Outer Disk of the Large Magellanic Cloud',\n",
       "  'authors': ['E. Hardy', 'R. Carrera', 'C. Gallart', 'A. Aparicio'],\n",
       "  'published': '2011-06-17T09:12:29Z',\n",
       "  'updated': '2011-06-17T09:12:29Z',\n",
       "  'abstract': 'The outer disk of the LMC is studied in order to unveil clues about itsformation and evolution. Complementing our previous studies in innermost fields(3&lt;R&lt;7 kpc), we obtained deep color magnitude diagrams in 6 fields with radiusfrom 5.2 to 9.2 kpc. The comparison with isochrones shows that while the oldestpopulation is approximately coeval in all fields, the age of the youngestpopulations increases with increasing radius. Low-resolution spectroscopy inthe infrared CaII triplet region has been obtained for about 150 stars near thetip red giant branch in the same fields. Radial velocities and stellarmetallicities have been obtained from these spectra. The metallicitydistribution of each field has been analyzed together with those previouslystudied. The metal content of the most metal-poor objects, which are also theoldest according to the derived age-metallicity relationships, is similar inall fields independently of the radius. However, while the metallicity of themost metal-rich objects measured, which are the youngest ones, remains constantin the inner 6 kpc, it decreases with increasing radius from there off. Thesame is true for the mean metallicity. According to the derived age-metallicityrelationships, which are consistent with being the same in all fields, thisresult may be interpreted as an outside-in formation scheme in opposition withthe inside-out scenario predicted by LCDM cosmology for a galaxy like the LMC.The analysis of the radial velocities of our sample of giants shows that theyfollow a rotational cold disk kinematics. The velocity dispersion increases asmetallicity decreases indicating that the most metal-poor/oldest objects aredistributed in a thicker disk than the most metal-rich/youngest ones inagreement with the findings in other disks such as that of the Milky Way. Theydo not seem to be part of a hot halo, if one exists in the LMC.',\n",
       "  'categories': ['astro-ph.CO'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1106.3418v1'},\n",
       " 940: {'ID': 940,\n",
       "  'title': 'Image Deconvolution with Deep Image and Kernel Priors',\n",
       "  'authors': ['Qiqi Li', 'Zhunxuan Wang', 'Zipei Wang', 'Hakan Bilen'],\n",
       "  'published': '2019-10-18T12:44:31Z',\n",
       "  'updated': '2019-10-18T12:44:31Z',\n",
       "  'abstract': 'Image deconvolution is the process of recovering convolutional degradedimages, which is always a hard inverse problem because of its mathematicallyill-posed property. On the success of the recently proposed deep image prior(DIP), we build an image deconvolution model with deep image and kernel priors(DIKP). DIP is a learning-free representation which uses neural net structuresto express image prior information, and it showed great success in manyenergy-based models, e.g. denoising, super-resolution, inpainting. Instead, ourDIKP model uses such priors in image deconvolution to model not only images butalso kernels, combining the ideas of traditional learning-free deconvolutionmethods with neural nets. In this paper, we show that DIKP improve theperformance of learning-free image deconvolution, and we experimentallydemonstrate this on the standard benchmark of six standard test images in termsof PSNR and visual effects.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1910.08386v1'},\n",
       " 941: {'ID': 941,\n",
       "  'title': 'Modeling Cross-view Interaction Consistency for Paired Egocentric  Interaction Recognition',\n",
       "  'authors': ['Fan Lyu', 'Zhongguo Li', 'Song Wang', 'Wei Feng'],\n",
       "  'published': '2020-03-24T05:05:34Z',\n",
       "  'updated': '2020-03-24T05:05:34Z',\n",
       "  'abstract': 'With the development of Augmented Reality (AR), egocentric action recognition(EAR) plays important role in accurately understanding demands from the user.However, EAR is designed to help recognize human-machine interaction in singleegocentric view, thus difficult to capture interactions between twoface-to-face AR users. Paired egocentric interaction recognition (PEIR) is thetask to collaboratively recognize the interactions between two persons with thevideos in their corresponding views. Unfortunately, existing PEIR methodsalways directly use linear decision function to fuse the features extractedfrom two corresponding egocentric videos, which ignore consistency ofinteraction in paired egocentric videos. The consistency of interactions inpaired videos, and features extracted from them are correlated to each other.On top of that, we propose to build the relevance between two views usingbiliear pooling, which capture the consistency of two views in feature-level.Specifically, each neuron in the feature maps from one view connects to theneurons from another view, which guarantee the compact consistency between twoviews. Then all possible paired neurons are used for PEIR for the insideconsistent information of them. To be efficient, we use compact bilinearpooling with Count Sketch to avoid directly computing outer product inbilinear. Experimental results on dataset PEV shows the superiority of theproposed methods on the task PEIR.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2003.10663v1'},\n",
       " 942: {'ID': 942,\n",
       "  'title': '3D Semantic Scene Completion from a Single Depth Image using Adversarial  Training',\n",
       "  'authors': ['Martin Garbade', 'Yueh-Tung Chen', 'Juergen Gall'],\n",
       "  'published': '2019-05-15T15:12:41Z',\n",
       "  'updated': '2019-05-15T15:12:41Z',\n",
       "  'abstract': 'We address the task of 3D semantic scene completion, i.e. , given a singledepth image, we predict the semantic labels and occupancy of voxels in a 3Dgrid representing the scene. In light of the recently introduced generativeadversarial networks (GAN), our goal is to explore the potential of this modeland the efficiency of various important design choices. Our results show thatusing conditional GANs outperforms the vanilla GAN setup. We evaluate thesearchitecture designs on several datasets. Based on our experiments, wedemonstrate that GANs are able to outperform the performance of a baseline 3DCNN in case of clean annotations, but they suffer from poorly alignedannotations.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1905.06231v1'},\n",
       " 943: {'ID': 943,\n",
       "  'title': 'Unsupervised Data Imputation via Variational Inference of Deep Subspaces',\n",
       "  'authors': ['Mert R. Sabuncu', 'John Guttag', 'Adrian V. Dalca'],\n",
       "  'published': '2019-03-08T15:28:48Z',\n",
       "  'updated': '2019-03-08T15:28:48Z',\n",
       "  'abstract': 'A wide range of systems exhibit high dimensional incomplete data. Accurateestimation of the missing data is often desired, and is crucial for manydownstream analyses. Many state-of-the-art recovery methods involve supervisedlearning using datasets containing full observations. In contrast, we focus onunsupervised estimation of missing image data, where no full observations areavailable - a common situation in practice. Unsupervised imputation methods forimages often employ a simple linear subspace to capture correlations betweendata dimensions, omitting more complex relationships. In this work, weintroduce a general probabilistic model that describes sparse high dimensionalimaging data as being generated by a deep non-linear embedding. We derive alearning algorithm using a variational approximation based on convolutionalneural networks and discuss its relationship to linear imputation models, thevariational auto encoder, and deep image priors. We introduce sparsity-awarenetwork building blocks that explicitly model observed and missing data. Weanalyze proposed sparsity-aware network building blocks, evaluate our method onpublic domain imaging datasets, and conclude by showing that our method enablesimputation in an important real-world problem involving medical images. Thecode is freely available as part of the \\\\verb|neuron| library athttp://github.com/adalca/neuron.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1903.03503v1'},\n",
       " 944: {'ID': 944,\n",
       "  'title': 'BP-DIP: A Backprojection based Deep Image Prior',\n",
       "  'authors': ['Jenny Zukerman', 'Tom Tirer', 'Raja Giryes'],\n",
       "  'published': '2020-03-11T17:09:12Z',\n",
       "  'updated': '2020-06-30T17:01:02Z',\n",
       "  'abstract': 'Deep neural networks are a very powerful tool for many computer vision tasks,including image restoration, exhibiting state-of-the-art results. However, theperformance of deep learning methods tends to drop once the observation modelused in training mismatches the one in test time. In addition, most deeplearning methods require vast amounts of training data, which are notaccessible in many applications. To mitigate these disadvantages, we propose tocombine two image restoration approaches: (i) Deep Image Prior (DIP), whichtrains a convolutional neural network (CNN) from scratch in test time using thegiven degraded image. It does not require any training data and builds on theimplicit prior imposed by the CNN architecture; and (ii) a backprojection (BP)fidelity term, which is an alternative to the standard least squares loss thatis usually used in previous DIP works. We demonstrate the performance of theproposed method, termed BP-DIP, on the deblurring task and show its advantagesover the plain DIP, with both higher PSNR values and better inference run-time.',\n",
       "  'categories': ['eess.IV', 'cs.LG', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2003.05417v2'},\n",
       " 945: {'ID': 945,\n",
       "  'title': 'Open-World Learning Without Labels',\n",
       "  'authors': ['Steve Cruz',\n",
       "   'Mohsen Jafarzadeh',\n",
       "   'Touqeer Ahmad',\n",
       "   'Akshay Raj Dhamija',\n",
       "   'Chunchun Li',\n",
       "   'Terrance E. Boult'],\n",
       "  'published': '2020-11-25T17:41:03Z',\n",
       "  'updated': '2020-12-14T01:39:54Z',\n",
       "  'abstract': 'Open-world learning is a problem where an autonomous agent detects thingsthat it does not know and learns them over time from a non-stationary andnever-ending stream of data; in an open-world environment, the training dataand objective criteria are never available at once. The agent should grasp newknowledge from learning without forgetting acquired prior knowledge.Researchers proposed a few open-world learning agents for image classificationtasks that operate in complex scenarios. However, all prior work on open-worldlearning has all labeled data to learn the new classes from the stream ofimages. In scenarios where autonomous agents should respond in near real-timeor work in areas with limited communication infrastructure, human labeling ofdata is not possible. Therefore, supervised open-world learning agents are notscalable solutions for such applications. Herein, we propose a new frameworkthat enables agents to learn new classes from a stream of unlabeled data in anunsupervised manner. Also, we study the robustness and learning speed of suchagents with supervised and unsupervised feature representation. We alsointroduce a new metric for open-world learning without labels. We anticipateour theories and method to be a starting point for developing autonomous trueopen-world never-ending learning agents.',\n",
       "  'categories': ['cs.CV',\n",
       "   'cs.AI',\n",
       "   'cs.LG',\n",
       "   'eess.IV',\n",
       "   'eess.SP',\n",
       "   '68T45',\n",
       "   'I.4.8'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2011.12906v2'},\n",
       " 946: {'ID': 946,\n",
       "  'title': 'Universal Perturbation Attack Against Image Retrieval',\n",
       "  'authors': ['Hong Liu',\n",
       "   'Rongrong Ji',\n",
       "   'Jie Li',\n",
       "   'Xiaopeng Hong',\n",
       "   'Yue Gao',\n",
       "   'Qi Tian'],\n",
       "  'published': '2018-12-03T04:52:06Z',\n",
       "  'updated': '2019-09-11T02:27:12Z',\n",
       "  'abstract': 'Universal adversarial perturbations (UAPs), a.k.a. input-agnosticperturbations, has been proved to exist and be able to fool cutting-edge deeplearning models on most of the data samples. Existing UAP methods mainly focuson attacking image classification models. Nevertheless, little attention hasbeen paid to attacking image retrieval systems. In this paper, we make thefirst attempt in attacking image retrieval systems. Concretely, image retrievalattack is to make the retrieval system return irrelevant images to the query atthe top ranking list. It plays an important role to corrupt the neighbourhoodrelationships among features in image retrieval attack. To this end, we proposea novel method to generate retrieval-against UAP to break the neighbourhoodrelationships of image features via degrading the corresponding ranking metric.To expand the attack method to scenarios with varying input sizes oruntouchable network parameters, a multi-scale random resizing scheme and aranking distillation strategy are proposed. We evaluate the proposed method onfour widely-used image retrieval datasets, and report a significant performancedrop in terms of different metrics, such as mAP and mP@10. Finally, we test ourattack methods on the real-world visual search engine, i.e., Google Images,which demonstrates the practical potentials of our methods.',\n",
       "  'categories': ['cs.CV', 'cs.CR', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1812.00552v2'},\n",
       " 947: {'ID': 947,\n",
       "  'title': 'Virtual Testbed for Monocular Visual Navigation of Small Unmanned  Aircraft Systems',\n",
       "  'authors': ['Scott L. Nykl', 'Robert C. Leishman', 'Kyung Kim'],\n",
       "  'published': '2020-07-01T20:35:26Z',\n",
       "  'updated': '2020-07-01T20:35:26Z',\n",
       "  'abstract': 'Monocular visual navigation methods have seen significant advances in thelast decade, recently producing several real-time solutions for autonomouslynavigating small unmanned aircraft systems without relying on GPS. This iscritical for military operations which may involve environments where GPSsignals are degraded or denied. However, testing and comparing visualnavigation algorithms remains a challenge since visual data is expensive togather. Conducting flight tests in a virtual environment is an attractivesolution prior to committing to outdoor testing.  This work presents a virtual testbed for conducting simulated flight testsover real-world terrain and analyzing the real-time performance of visualnavigation algorithms at 31 Hz. This tool was created to ultimately find avisual odometry algorithm appropriate for further GPS-denied navigationresearch on fixed-wing aircraft, even though all of the algorithms weredesigned for other modalities. This testbed was used to evaluate three currentstate-of-the-art, open-source monocular visual odometry algorithms on afixed-wing platform: Direct Sparse Odometry, Semi-Direct Visual Odometry, andORB-SLAM2 (with loop closures disabled).',\n",
       "  'categories': ['cs.CV', 'cs.RO'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2007.00737v1'},\n",
       " 948: {'ID': 948,\n",
       "  'title': 'Packing Sparse Convolutional Neural Networks for Efficient Systolic  Array Implementations: Column Combining Under Joint Optimization',\n",
       "  'authors': ['Sai Qian Zhang', 'Bradley McDanel', 'H. T. Kung'],\n",
       "  'published': '2018-11-07T23:09:31Z',\n",
       "  'updated': '2018-11-07T23:09:31Z',\n",
       "  'abstract': 'This paper describes a novel approach of packing sparse convolutional neuralnetworks for their efficient systolic array implementations. By combiningsubsets of columns in the original filter matrix associated with aconvolutional layer, we increase the utilization efficiency of the systolicarray substantially (e.g., ~4x) due to the increased density of nonzeros in theresulting packed filter matrix. In combining columns, for each row, all filterweights but one with the largest magnitude are pruned. We retrain the remainingweights to preserve high accuracy. We demonstrate that in mitigating dataprivacy concerns the retraining can be accomplished with only fractions of theoriginal dataset (e.g., 10\\\\% for CIFAR-10). We study the effectiveness of thisjoint optimization for both high utilization and classification accuracy withASIC and FPGA designs based on efficient bit-serial implementations ofmultiplier-accumulators. We present analysis and empirical evidence on thesuperior performance of our column combining approach against prior arts undermetrics such as energy efficiency (3x) and inference latency (12x).',\n",
       "  'categories': ['cs.LG', 'cs.AR', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1811.04770v1'},\n",
       " 949: {'ID': 949,\n",
       "  'title': 'Combining Similarity and Adversarial Learning to Generate Visual  Explanation: Application to Medical Image Classification',\n",
       "  'authors': ['Martin Charachon',\n",
       "   'Camille Ruppli',\n",
       "   'Roberto Ardon',\n",
       "   'Céline Hudelot',\n",
       "   'Paul-Henry Cournède'],\n",
       "  'published': '2020-12-14T08:34:12Z',\n",
       "  'updated': '2020-12-14T08:34:12Z',\n",
       "  'abstract': 'Explaining decisions of black-box classifiers is paramount in sensitivedomains such as medical imaging since clinicians confidence is necessary foradoption. Various explanation approaches have been proposed, among whichperturbation based approaches are very promising. Within this class of methods,we leverage a learning framework to produce our visual explanations method.From a given classifier, we train two generators to produce from an input imagethe so called similar and adversarial images. The similar image shall beclassified as the input image whereas the adversarial shall not. Visualexplanation is built as the difference between these two generated images.Using metrics from the literature, our method outperforms state-of-the-artapproaches. The proposed approach is model-agnostic and has a low computationburden at prediction time. Thus, it is adapted for real-time systems. Finally,we show that random geometric augmentations applied to the original image playa regularization role that improves several previously proposed explanationmethods. We validate our approach on a large chest X-ray database.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2012.07332v1'},\n",
       " 950: {'ID': 950,\n",
       "  'title': 'DenseNet Models for Tiny ImageNet Classification',\n",
       "  'authors': ['Nishad Rajmalwar', 'Zoheb Abai'],\n",
       "  'published': '2019-04-23T17:20:35Z',\n",
       "  'updated': '2020-06-01T11:40:40Z',\n",
       "  'abstract': 'In this paper, we present two image classification models on the TinyImageNet dataset. We built two very different networks from scratch based onthe idea of Densely Connected Convolution Networks. The architecture of thenetworks is designed based on the image resolution of this specific dataset andby calculating the Receptive Field of the convolution layers. We also used somenon-conventional techniques related to image augmentation and Cyclical LearningRate to improve the accuracy of our models. The networks are trained under highconstraints and low computation resources. We aimed to achieve top-1 validationaccuracy of 60%; the results and error analysis are also presented.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1904.10429v2'},\n",
       " 951: {'ID': 951,\n",
       "  'title': 'Asymptotic Properties of Bayes Risk of a General Class of Shrinkage  Priors in Multiple Hypothesis Testing Under Sparsity',\n",
       "  'authors': ['Arijit Chakrabarti',\n",
       "   'Xueying Tang',\n",
       "   'Prasenjit Ghosh',\n",
       "   'Malay Ghosh'],\n",
       "  'published': '2013-10-28T15:41:02Z',\n",
       "  'updated': '2015-06-10T19:04:07Z',\n",
       "  'abstract': 'Consider the problem of simultaneous testing for the means of independentnormal observations. In this paper, we study some asymptotic optimalityproperties of certain multiple testing rules induced by a general class ofone-group shrinkage priors in a Bayesian decision theoretic framework, wherethe overall loss is taken as the number of misclassified hypotheses. We assumea two-groups normal mixture model for the data and consider the asymptoticframework adopted in Bogdan et al. (2011) who introduced the notion ofasymptotic Bayes optimality under sparsity in the context of multiple testing.The general class of one-group priors under study is rich enough to include,among others, the families of three parameter beta, generalized double Paretopriors, and in particular the horseshoe, the normal-exponential-gamma and theStrawderman-Berger priors. We establish that within our chosen asymptoticframework, the multiple testing rules under study asymptotically attain therisk of the Bayes Oracle up to a multiplicative factor, with the constant inthe risk close to the constant in the Oracle risk. This is similar to a resultobtained in Datta and Ghosh (2013) for the multiple testing rule based on thehorseshoe estimator introduced in Carvalho et al. (2009, 2010). We further showthat under very mild assumption on the underlying sparsity parameter, theinduced decision rules based on an empirical Bayes estimate of thecorresponding global shrinkage parameter proposed by van der Pas et al. (2014),attain the optimal Bayes risk up to the same multiplicative factorasymptotically. We provide a unifying argument applicable for the general classof priors under study. In the process, we settle a conjecture regardingoptimality property of the generalized double Pareto priors made in Datta andGhosh (2013). Our work also shows that the result in Datta and Ghosh (2013) canbe improved further.',\n",
       "  'categories': ['math.ST', 'stat.TH'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1310.7462v6'},\n",
       " 952: {'ID': 952,\n",
       "  'title': 'Response to \"Visual Dialogue without Vision or Dialogue\" (Massiceti et  al., 2018)',\n",
       "  'authors': ['Devi Parikh', 'Abhishek Das', 'Dhruv Batra'],\n",
       "  'published': '2019-01-16T21:27:57Z',\n",
       "  'updated': '2019-01-16T21:27:57Z',\n",
       "  'abstract': 'In a recent workshop paper, Massiceti et al. presented a baseline model andsubsequent critique of Visual Dialog (Das et al., CVPR 2017) that raises whatwe believe to be unfounded concerns about the dataset and evaluation. Thisarticle intends to rebut the critique and clarify potential confusions forpractitioners and future participants in the Visual Dialog challenge.',\n",
       "  'categories': ['cs.CV', 'cs.CL', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1901.05531v1'},\n",
       " 953: {'ID': 953,\n",
       "  'title': 'Proposal-free Network for Instance-level Object Segmentation',\n",
       "  'authors': ['Xiaodan Liang',\n",
       "   'Yunchao Wei',\n",
       "   'Xiaohui Shen',\n",
       "   'Jianchao Yang',\n",
       "   'Liang Lin',\n",
       "   'Shuicheng Yan'],\n",
       "  'published': '2015-09-09T05:05:53Z',\n",
       "  'updated': '2015-09-10T01:12:03Z',\n",
       "  'abstract': 'Instance-level object segmentation is an important yet under-explored task.The few existing studies are almost all based on region proposal methods toextract candidate segments and then utilize object classification to producefinal results. Nonetheless, generating accurate region proposals itself isquite challenging. In this work, we propose a Proposal-Free Network (PFN ) toaddress the instance-level object segmentation problem, which outputs theinstance numbers of different categories and the pixel-level information on 1)the coordinates of the instance bounding box each pixel belongs to, and 2) theconfidences of different categories for each pixel, based on pixel-to-pixeldeep convolutional neural network. All the outputs together, by using anyoff-the-shelf clustering method for simple post-processing, can naturallygenerate the ultimate instance-level object segmentation results. The whole PFNcan be easily trained in an end-to-end way without the requirement of aproposal generation stage. Extensive evaluations on the challenging PASCAL VOC2012 semantic segmentation benchmark demonstrate that the proposed PFN solutionwell beats the state-of-the-arts for instance-level object segmentation. Inparticular, the $AP^r$ over 20 classes at 0.5 IoU reaches 58.7% by PFN,significantly higher than 43.8% and 46.3% by the state-of-the-art algorithms,SDS [9] and [16], respectively.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1509.02636v2'},\n",
       " 954: {'ID': 954,\n",
       "  'title': 'Uncertainty Estimation in Medical Image Denoising with Bayesian Deep  Image Prior',\n",
       "  'authors': ['Tobias Ortmaier', 'Max-Heinrich Laves', 'Malte Tölle'],\n",
       "  'published': '2020-08-20T08:34:51Z',\n",
       "  'updated': '2020-08-20T08:34:51Z',\n",
       "  'abstract': 'Uncertainty quantification in inverse medical imaging tasks with deeplearning has received little attention. However, deep models trained on largedata sets tend to hallucinate and create artifacts in the reconstructed outputthat are not anatomically present. We use a randomly initialized convolutionalnetwork as parameterization of the reconstructed image and perform gradientdescent to match the observation, which is known as deep image prior. In thiscase, the reconstruction does not suffer from hallucinations as no priortraining is performed. We extend this to a Bayesian approach with Monte Carlodropout to quantify both aleatoric and epistemic uncertainty. The presentedmethod is evaluated on the task of denoising different medical imagingmodalities. The experimental results show that our approach yieldswell-calibrated uncertainty. That is, the predictive uncertainty correlateswith the predictive error. This allows for reliable uncertainty estimates andcan tackle the problem of hallucinations and artifacts in inverse medicalimaging tasks.',\n",
       "  'categories': ['eess.IV', 'cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2008.08837v1'},\n",
       " 955: {'ID': 955,\n",
       "  'title': 'HST/ACS observations of the old and metal-poor Sagittarius dwarf  irregular galaxy',\n",
       "  'authors': ['I. Saviane',\n",
       "   'E. V. Held',\n",
       "   'K. Kuijken',\n",
       "   'M. Gullieuszik',\n",
       "   'Rolly Bedin',\n",
       "   'M. Rich',\n",
       "   'M. Clemens',\n",
       "   'L. Rizzi',\n",
       "   'Y. Momany'],\n",
       "  'published': '2005-05-19T09:53:43Z',\n",
       "  'updated': '2005-05-19T09:53:43Z',\n",
       "  'abstract': \"We have obtained deep color-magnitude diagrams of the Sagittarius dwarfirregular galaxy with the HST/ACS. The new diagrams unveil for the first timethe star formation history of SagDIG.  We identify a small population of red horizontal branch stars, which sets thefirst epoch of star formation in SagDIG back to ~9-10 Gyr ago. This shows thatSagDIG has been able to maintain a very low mean metallicity (-2.2&lt;[Fe/H]&lt;-1.9)over a 10 Gyr life time, and most importantly, that all Local Group dwarfgalaxies share an ancient (~10 Gyr) initial episode of star formation. Thus, atthe moment, IZw18 remains the only exception to this general trend, favoringthe so-called ``young galaxy'' scenario. However, a re-analysis of ACSobservations of IZw18 suggests that an older RGB population may be present alsoin IZw18.  We also compare the distribution of atomic hydrogen with that of stellarpopulations of various ages. We find little correlation between stellarpopulations older than ~1 Gyr and the HI. In particular we fail to find anygeneration of stars that preferentially lie within the large HI hole. Wetherefore exclude the possibility that the SagDIG ring-like HI distribution isthe result of multiple supernova events. Alternative scenarios have to beexplored.\",\n",
       "  'categories': ['astro-ph'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/astro-ph/0505399v1'},\n",
       " 956: {'ID': 956,\n",
       "  'title': 'Multimodal Compact Bilinear Pooling for Multimodal Neural Machine  Translation',\n",
       "  'authors': ['Stephane Dupont', 'Jean-Benoit Delbrouck'],\n",
       "  'published': '2017-03-23T14:20:52Z',\n",
       "  'updated': '2017-03-23T14:20:52Z',\n",
       "  'abstract': 'In state-of-the-art Neural Machine Translation, an attention mechanism isused during decoding to enhance the translation. At every step, the decoderuses this mechanism to focus on different parts of the source sentence togather the most useful information before outputting its target word. Recently,the effectiveness of the attention mechanism has also been explored formultimodal tasks, where it becomes possible to focus both on sentence parts andimage regions. Approaches to pool two modalities usually include element-wiseproduct, sum or concatenation. In this paper, we evaluate the more advancedMultimodal Compact Bilinear pooling method, which takes the outer product oftwo vectors to combine the attention features for the two modalities. This hasbeen previously investigated for visual question answering. We try out thisapproach for multimodal image caption translation and show improvementscompared to basic combination methods.',\n",
       "  'categories': ['cs.CL'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1703.08084v1'},\n",
       " 957: {'ID': 957,\n",
       "  'title': 'Image-Question-Answer Synergistic Network for Visual Dialog',\n",
       "  'authors': ['Dacheng Tao', 'Chang Xu', 'Dalu Guo'],\n",
       "  'published': '2019-02-26T07:30:43Z',\n",
       "  'updated': '2019-02-26T07:30:43Z',\n",
       "  'abstract': 'The image, question (combined with the history for de-referencing), and thecorresponding answer are three vital components of visual dialog. Classicalvisual dialog systems integrate the image, question, and history to search foror generate the best matched answer, and so, this approach significantlyignores the role of the answer. In this paper, we devise a novelimage-question-answer synergistic network to value the role of the answer forprecise visual dialog. We extend the traditional one-stage solution to atwo-stage solution. In the first stage, candidate answers are coarsely scoredaccording to their relevance to the image and question pair. Afterward, in thesecond stage, answers with high probability of being correct are re-ranked bysynergizing with image and question. On the Visual Dialog v1.0 dataset, theproposed synergistic network boosts the discriminative visual dialog model toachieve a new state-of-the-art of 57.88\\\\% normalized discounted cumulativegain. A generative visual dialog model equipped with the proposed techniquealso shows promising improvements.',\n",
       "  'categories': ['cs.CL', 'cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1902.09774v1'},\n",
       " 958: {'ID': 958,\n",
       "  'title': 'Factor Graph Attention',\n",
       "  'authors': ['Idan Schwartz',\n",
       "   'Seunghak Yu',\n",
       "   'Alexander Schwing',\n",
       "   'Tamir Hazan'],\n",
       "  'published': '2019-04-11T17:59:58Z',\n",
       "  'updated': '2020-03-07T23:35:13Z',\n",
       "  'abstract': 'Dialog is an effective way to exchange information, but subtle details andnuances are extremely important. While significant progress has paved a path toaddress visual dialog with algorithms, details and nuances remain a challenge.Attention mechanisms have demonstrated compelling results to extract details invisual question answering and also provide a convincing framework for visualdialog due to their interpretability and effectiveness. However, the many datautilities that accompany visual dialog challenge existing attention techniques.We address this issue and develop a general attention mechanism for visualdialog which operates on any number of data utilities. To this end, we design afactor graph based attention mechanism which combines any number of utilityrepresentations. We illustrate the applicability of the proposed approach onthe challenging and recently introduced VisDial datasets, outperforming recentstate-of-the-art methods by 1.1% for VisDial0.9 and by 2% for VisDial1.0 onMRR. Our ensemble model improved the MRR score on VisDial1.0 by more than 6%.',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.CL', 'cs.IR', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1904.05880v3'},\n",
       " 959: {'ID': 959,\n",
       "  'title': 'Exploiting Deep Generative Prior for Versatile Image Restoration and  Manipulation',\n",
       "  'authors': ['Xiaohang Zhan',\n",
       "   'Xingang Pan',\n",
       "   'Ping Luo',\n",
       "   'Dahua Lin',\n",
       "   'Bo Dai',\n",
       "   'Chen Change Loy'],\n",
       "  'published': '2020-03-30T17:45:07Z',\n",
       "  'updated': '2020-07-20T10:06:24Z',\n",
       "  'abstract': 'Learning a good image prior is a long-term goal for image restoration andmanipulation. While existing methods like deep image prior (DIP) capturelow-level image statistics, there are still gaps toward an image prior thatcaptures rich image semantics including color, spatial coherence, textures, andhigh-level concepts. This work presents an effective way to exploit the imageprior captured by a generative adversarial network (GAN) trained on large-scalenatural images. As shown in Fig.1, the deep generative prior (DGP) providescompelling results to restore missing semantics, e.g., color, patch,resolution, of various degraded images. It also enables diverse imagemanipulation including random jittering, image morphing, and category transfer.Such highly flexible restoration and manipulation are made possible throughrelaxing the assumption of existing GAN-inversion methods, which tend to fixthe generator. Notably, we allow the generator to be fine-tuned on-the-fly in aprogressive manner regularized by feature distance obtained by thediscriminator in GAN. We show that these easy-to-implement and practicalchanges help preserve the reconstruction to remain in the manifold of natureimage, and thus lead to more precise and faithful reconstruction for realimages. Code is available athttps://github.com/XingangPan/deep-generative-prior.',\n",
       "  'categories': ['eess.IV', 'cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2003.13659v4'},\n",
       " 960: {'ID': 960,\n",
       "  'title': 'Multimodal Deep Domain Adaptation',\n",
       "  'authors': ['Mohammad Reza Loghmani', 'Silvia Bucci', 'Barbara Caputo'],\n",
       "  'published': '2018-07-31T08:08:40Z',\n",
       "  'updated': '2018-07-31T08:08:40Z',\n",
       "  'abstract': 'Typically a classifier trained on a given dataset (source domain) does notperforms well if it is tested on data acquired in a different setting (targetdomain). This is the problem that domain adaptation (DA) tries to overcome and,while it is a well explored topic in computer vision, it is largely ignored inrobotic vision where usually visual classification methods are trained andtested in the same domain. Robots should be able to deal with unknownenvironments, recognize objects and use them in the correct way, so it isimportant to explore the domain adaptation scenario also in this context. Thegoal of the project is to define a benchmark and a protocol for multi-modaldomain adaptation that is valuable for the robot vision community. With thispurpose some of the state-of-the-art DA methods are selected: Deep AdaptationNetwork (DAN), Domain Adversarial Training of Neural Network (DANN), AutomaticDomain Alignment Layers (AutoDIAL) and Adversarial Discriminative DomainAdaptation (ADDA). Evaluations have been done using different data types: RGBonly, depth only and RGB-D over the following datasets, designed for therobotic community: RGB-D Object Dataset (ROD), Web Object Dataset (WOD),Autonomous Robot Indoor Dataset (ARID), Big Berkeley Instance RecognitionDataset (BigBIRD) and Active Vision Dataset. Although progresses have been madeon the formulation of effective adaptation algorithms and more realistic objectdatasets are available, the results obtained show that, training a sufficientlygood object classifier, especially in the domain adaptation scenario, is stillan unsolved problem. Also the best way to combine depth with RGB informationsto improve the performance is a point that needs to be investigated more.',\n",
       "  'categories': ['cs.LG', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1807.11697v1'},\n",
       " 961: {'ID': 961,\n",
       "  'title': 'Switchable Normalization for Learning-to-Normalize Deep Representation',\n",
       "  'authors': ['Ruimao Zhang',\n",
       "   'Zhanglin Peng',\n",
       "   'Ping Luo',\n",
       "   'Jingyu Li',\n",
       "   'Jiamin Ren'],\n",
       "  'published': '2019-07-22T17:50:31Z',\n",
       "  'updated': '2019-07-22T17:50:31Z',\n",
       "  'abstract': 'We address a learning-to-normalize problem by proposing SwitchableNormalization (SN), which learns to select different normalizers for differentnormalization layers of a deep neural network. SN employs three distinct scopesto compute statistics (means and variances) including a channel, a layer, and aminibatch. SN switches between them by learning their importance weights in anend-to-end manner. It has several good properties. First, it adapts to variousnetwork architectures and tasks. Second, it is robust to a wide range of batchsizes, maintaining high performance even when small minibatch is presented(e.g. 2 images/GPU). Third, SN does not have sensitive hyper-parameter, unlikegroup normalization that searches the number of groups as a hyper-parameter.Without bells and whistles, SN outperforms its counterparts on variouschallenging benchmarks, such as ImageNet, COCO, CityScapes, ADE20K, MegaFace,and Kinetics. Analyses of SN are also presented to answer the following threequestions: (a) Is it useful to allow each normalization layer to select its ownnormalizer? (b) What impacts the choices of normalizers? (c) Do different tasksand datasets prefer different normalizers? We hope SN will help ease the usageand understand the normalization techniques in deep learning. The code of SNhas been released at https://github.com/switchablenorms.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1907.10473v1'},\n",
       " 962: {'ID': 962,\n",
       "  'title': 'A Framework based on Deep Neural Networks to Extract Anatomy of  Mosquitoes from Images',\n",
       "  'authors': ['Pratool Bharti',\n",
       "   'Sherzod Kariev',\n",
       "   'Tanvir Bhuiyan',\n",
       "   'Mona Minakshi',\n",
       "   'Sriram Chellappan'],\n",
       "  'published': '2020-07-21T19:27:46Z',\n",
       "  'updated': '2020-07-29T14:00:13Z',\n",
       "  'abstract': 'We design a framework based on Mask Region-based Convolutional Neural Network(Mask R-CNN) to automatically detect and separately extract anatomicalcomponents of mosquitoes - thorax, wings, abdomen and legs from images. Ourtraining dataset consisted of 1500 smartphone images of nine mosquito speciestrapped in Florida. In the proposed technique, the first step is to detectanatomical components within a mosquito image. Then, we localize and classifythe extracted anatomical components, while simultaneously adding a branch inthe neural network architecture to segment pixels containing only theanatomical components. Evaluation results are favorable. To evaluategenerality, we test our architecture trained only with mosquito images onbumblebee images. We again reveal favorable results, particularly in extractingwings. Our techniques in this paper have practical applications in publichealth, taxonomy and citizen-science efforts.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2007.11052v2'},\n",
       " 963: {'ID': 963,\n",
       "  'title': 'RED: A ReRAM-based Deconvolution Accelerator',\n",
       "  'authors': ['Hai', 'Ziru Li', 'Bing Li', 'Yiran Chen', 'Li', 'Zichen Fan'],\n",
       "  'published': '2019-07-05T18:08:22Z',\n",
       "  'updated': '2019-07-05T18:08:22Z',\n",
       "  'abstract': 'Deconvolution has been widespread in neural networks. For example, it isessential for performing unsupervised learning in generative adversarialnetworks or constructing fully convolutional networks for semanticsegmentation. Resistive RAM (ReRAM)-based processing-in-memory architecture hasbeen widely explored in accelerating convolutional computation and demonstratesgood performance. Performing deconvolution on existing ReRAM-based acceleratordesigns, however, suffers from long latency and high energy consumption becausedeconvolutional computation includes not only convolution but also extra add-onoperations. To realize the more efficient execution for deconvolution, weanalyze its computation requirement and propose a ReRAM-based acceleratordesign, namely, RED. More specific, RED integrates two orthogonal methods, thepixel-wise mapping scheme for reducing redundancy caused by zero-insertingoperations and the zero-skipping data flow for increasing the computationparallelism and therefore improving performance. Experimental evaluations showthat compared to the state-of-the-art ReRAM-based accelerator, RED can speed upoperation 3.69x~1.15x and reduce 8%~88.36% energy consumption.',\n",
       "  'categories': ['cs.ET', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1907.02987v1'},\n",
       " 964: {'ID': 964,\n",
       "  'title': 'Face2Face: Real-time Face Capture and Reenactment of RGB Videos',\n",
       "  'authors': ['Marc Stamminger',\n",
       "   'Michael Zollhöfer',\n",
       "   'Christian Theobalt',\n",
       "   'Matthias Nießner',\n",
       "   'Justus Thies'],\n",
       "  'published': '2020-07-29T12:47:16Z',\n",
       "  'updated': '2020-07-29T12:47:16Z',\n",
       "  'abstract': 'We present Face2Face, a novel approach for real-time facial reenactment of amonocular target video sequence (e.g., Youtube video). The source sequence isalso a monocular video stream, captured live with a commodity webcam. Our goalis to animate the facial expressions of the target video by a source actor andre-render the manipulated output video in a photo-realistic fashion. To thisend, we first address the under-constrained problem of facial identity recoveryfrom monocular video by non-rigid model-based bundling. At run time, we trackfacial expressions of both source and target video using a dense photometricconsistency measure. Reenactment is then achieved by fast and efficientdeformation transfer between source and target. The mouth interior that bestmatches the re-targeted expression is retrieved from the target sequence andwarped to produce an accurate fit. Finally, we convincingly re-render thesynthesized target face on top of the corresponding video stream such that itseamlessly blends with the real-world illumination. We demonstrate our methodin a live setup, where Youtube videos are reenacted in real time.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2007.14808v1'},\n",
       " 965: {'ID': 965,\n",
       "  'title': 'Sensitivity of Deep Convolutional Networks to Gabor Noise',\n",
       "  'authors': ['Kenneth T. Co', 'Emil C. Lupu', 'Luis Muñoz-González'],\n",
       "  'published': '2019-06-08T13:41:38Z',\n",
       "  'updated': '2019-06-11T02:57:48Z',\n",
       "  'abstract': 'Deep Convolutional Networks (DCNs) have been shown to be sensitive toUniversal Adversarial Perturbations (UAPs): input-agnostic perturbations thatfool a model on large portions of a dataset. These UAPs exhibit interestingvisual patterns, but this phenomena is, as yet, poorly understood. Our workshows that visually similar procedural noise patterns also act as UAPs. Inparticular, we demonstrate that different DCN architectures are sensitive toGabor noise patterns. This behaviour, its causes, and implications deservefurther in-depth study.',\n",
       "  'categories': ['cs.LG', 'cs.CR', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1906.03455v2'},\n",
       " 966: {'ID': 966,\n",
       "  'title': 'Building Extraction at Scale using Convolutional Neural Network: Mapping  of the United States',\n",
       "  'authors': ['Hsiuhan Lexie Yang',\n",
       "   'Budhendra Bhaduri',\n",
       "   'Amy Rose',\n",
       "   'Dalton Lunga',\n",
       "   'Melanie Laverdiere',\n",
       "   'Jiangye Yuan'],\n",
       "  'published': '2018-05-23T03:28:05Z',\n",
       "  'updated': '2018-05-23T03:28:05Z',\n",
       "  'abstract': 'Establishing up-to-date large scale building maps is essential to understandurban dynamics, such as estimating population, urban planning and many otherapplications. Although many computer vision tasks has been successfully carriedout with deep convolutional neural networks, there is a growing need tounderstand their large scale impact on building mapping with remote sensingimagery. Taking advantage of the scalability of CNNs and using only few areaswith the abundance of building footprints, for the first time we conduct acomparative analysis of four state-of-the-art CNNs for extracting buildingfootprints across the entire continental United States. The four CNNarchitectures namely: branch-out CNN, fully convolutional neural network (FCN),conditional random field as recurrent neural network (CRFasRNN), and SegNet,support semantic pixel-wise labeling and focus on capturing texturalinformation at multi-scale. We use 1-meter resolution aerial images fromNational Agriculture Imagery Program (NAIP) as the test-bed, and compare theextraction results across the four methods. In addition, we propose to combinesigned-distance labels with SegNet, the preferred CNN architecture identifiedby our extensive evaluations, to advance building extraction results toinstance level. We further demonstrate the usefulness of fusing additional nearIR information into the building extraction framework. Large scale experimentalevaluations are conducted and reported using metrics that include: precision,recall rate, intersection over union, and the number of buildings extracted.With the improved CNN model and no requirement of further post-processing, wehave generated building maps for the United States. The quality of extractedbuildings and processing time demonstrated the proposed CNN-based frameworkfits the need of building extraction at scale.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1805.08946v1'},\n",
       " 967: {'ID': 967,\n",
       "  'title': 'Application of Yolo on Mask Detection Task',\n",
       "  'authors': ['Ziang Ren', 'Ren Liu'],\n",
       "  'published': '2021-02-10T12:34:47Z',\n",
       "  'updated': '2021-02-10T12:34:47Z',\n",
       "  'abstract': '2020 has been a year marked by the COVID-19 pandemic. This event has causeddisruptions to many aspects of normal life. An important aspect in reducing theimpact of the pandemic is to control its spread. Studies have shown that oneeffective method in reducing the transmission of COVID-19 is to wear masks.Strict mask-wearing policies have been met with not only public sensation butalso practical difficulty. We cannot hope to manually check if everyone on astreet is wearing a mask properly. Existing technology to help automate maskchecking uses deep learning models on real-time surveillance camera footages.The current dominant method to perform real-time mask detection uses Mask-RCNNwith ResNet as the backbone. While giving good detection results, this methodis computationally intensive and its efficiency in real-time face maskdetection is not ideal. Our research proposes a new approach to mask detectionby replacing Mask-R-CNN with a more efficient model \"YOLO\" to increase theprocessing speed of real-time mask detection and not compromise on accuracy.Besides, given the small volume as well as extreme imbalance of the maskdetection datasets, we adopt a latest progress made in few-shot visualclassification, simple CNAPs, to improve the classification performance.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2102.05402v1'},\n",
       " 968: {'ID': 968,\n",
       "  'title': 'Testbed for Connected Artificial Intelligence using Unmanned Aerial  Vehicles and Convolutional Pose Machines',\n",
       "  'authors': ['Neiva Linder',\n",
       "   'Carnot Braun',\n",
       "   'Flavio Brito',\n",
       "   'Kaio Forte',\n",
       "   'Andrey Silva',\n",
       "   'Diego Dantas',\n",
       "   'Silvia Lins',\n",
       "   'Aldebaro Klautau'],\n",
       "  'published': '2020-01-14T17:59:09Z',\n",
       "  'updated': '2020-01-14T17:59:09Z',\n",
       "  'abstract': 'Unmanned Aerial Vehicles (UAVs) became very popular in a vast number ofapplications in recent years, especially drones with computer vision functionsenabled by on-board cameras and embedded systems. Many of them apply objectdetection using data collected by the integrated camera. However, severalapplications of real-time object detection rely on Convolutional NeuralNetworks (CNNs) which are computationally expensive and processing CNNs on aUAV platform is challenging (due to its limited battery life and limitedprocessing power). To understand the effects of these issues, in this paper weevaluate the constraints and benefits of processing the whole data in the UAVversus in an edge computing device. We apply Convolutional Pose Machines (CPMs)known as OpenPose for the task of articulated pose estimation. We used thisinformation to detect human gestures that are used as input to send commands tocontrol the UAV. The experimental results using a real UAV indicate that theedge processing is more efficient and faster (w.r.t battery consumption and thedelay in recognizing the human pose and the command given to the drone) thanUAV processing and then could be more suitable for CNNs based applications.',\n",
       "  'categories': ['eess.SP'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2001.04944v1'},\n",
       " 969: {'ID': 969,\n",
       "  'title': 'Accelerating Training of Deep Neural Networks with a Standardization  Loss',\n",
       "  'authors': ['Jonathon Shlens', 'Jasmine Collins', 'Johannes Balle'],\n",
       "  'published': '2019-03-03T15:17:06Z',\n",
       "  'updated': '2019-03-03T15:17:06Z',\n",
       "  'abstract': 'A significant advance in accelerating neural network training has been thedevelopment of normalization methods, permitting the training of deep modelsboth faster and with better accuracy. These advances come with practicalchallenges: for instance, batch normalization ties the prediction of individualexamples with other examples within a batch, resulting in a network that isheavily dependent on batch size. Layer normalization and group normalizationare data-dependent and thus must be continually used, even at test-time. Toaddress the issues that arise from using explicit normalization techniques, wepropose to replace existing normalization methods with a simple, secondaryobjective loss that we term a standardization loss. This formulation isflexible and robust across different batch sizes and surprisingly, thissecondary objective accelerates learning on the primary training objective.Because it is a training loss, it is simply removed at test-time, and nofurther effort is needed to maintain normalized activations. We find that astandardization loss accelerates training on both small- and large-scale imageclassification experiments, works with a variety of architectures, and islargely robust to training across different batch sizes.',\n",
       "  'categories': ['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1903.00925v1'},\n",
       " 970: {'ID': 970,\n",
       "  'title': 'Obtaining Faithful Interpretations from Compositional Neural Networks',\n",
       "  'authors': ['Tomer Wolfson',\n",
       "   'Nitish Gupta',\n",
       "   'Sameer Singh',\n",
       "   'Matt Gardner',\n",
       "   'Jonathan Berant',\n",
       "   'Sanjay Subramanian',\n",
       "   'Ben Bogin'],\n",
       "  'published': '2020-05-02T06:50:35Z',\n",
       "  'updated': '2020-09-08T15:52:28Z',\n",
       "  'abstract': \"Neural module networks (NMNs) are a popular approach for modelingcompositionality: they achieve high accuracy when applied to problems inlanguage and vision, while reflecting the compositional structure of theproblem in the network architecture. However, prior work implicitly assumedthat the structure of the network modules, describing the abstract reasoningprocess, provides a faithful explanation of the model's reasoning; that is,that all modules perform their intended behaviour. In this work, we propose andconduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2and DROP, two datasets which require composing multiple reasoning steps. Wefind that the intermediate outputs differ from the expected output,illustrating that the network structure does not provide a faithful explanationof model behaviour. To remedy that, we train the model with auxiliarysupervision and propose particular choices for module architecture that yieldmuch better faithfulness, at a minimal cost to accuracy.\",\n",
       "  'categories': ['cs.CL', 'cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2005.00724v2'},\n",
       " 971: {'ID': 971,\n",
       "  'title': 'Automated Crater Detection from Co-registered Optical Images, Elevation  Maps and Slope Maps using Deep Learning',\n",
       "  'authors': ['Nitin Khanna',\n",
       "   'Vinay Verma',\n",
       "   'Atal Tewari',\n",
       "   'Vikrant Jain',\n",
       "   'Pradeep Srivastava'],\n",
       "  'published': '2020-12-30T18:52:07Z',\n",
       "  'updated': '2020-12-30T18:52:07Z',\n",
       "  'abstract': 'Impact craters are formed as a result of continuous impacts on the surface ofplanetary bodies. This paper proposes a novel way of simultaneously utilizingoptical images, digital elevation maps (DEMs), and slope maps for automaticcrater detection on the lunar surface. Mask R-CNN, tuned for the craterdetection task, is utilized in this paper. Two catalogs, namely, Head-LROC andRobbins, are used for the performance evaluation. Exhaustive analysis of thedetection results on the lunar surface has been performed with respect to bothHead-LROC and Robbins catalog. With the Head-LROC catalog, which has relativelystrict crater markings and larger possibility of missing craters, recall valueof 94.28\\\\% has been obtained as compared to 88.03\\\\% for the baseline method.However, with respect to a manually marked exhaustive crater catalog based onrelatively liberal marking, significant precision and recall values areobtained for different crater size ranges. The generalization capability of theproposed method in terms of crater detection on a different terrain withdifferent input data type is also evaluated. We show that the proposed modeltrained on the lunar surface with optical images, DEMs and corresponding slopemaps can be used to detect craters on the Martian surface even with entirelydifferent input data type, such as thermal IR images from the Martian surface.',\n",
       "  'categories': ['eess.IV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2012.15281v1'},\n",
       " 972: {'ID': 972,\n",
       "  'title': 'Semantic-driven Colorization',\n",
       "  'authors': ['Lu Zhang', 'Alexander Raake', 'Man M. Ho', 'Jinjia Zhou'],\n",
       "  'published': '2020-06-13T08:13:30Z',\n",
       "  'updated': '2020-06-18T11:32:24Z',\n",
       "  'abstract': 'Recent deep colorization works predict the semantic information implicitlywhile learning to colorize black-and-white photographic images. As aconsequence, the generated color is easier to be overflowed, and the semanticfaults are invisible. As human experience in coloring, the human firstrecognize which objects and their location in the photo, imagine which color isplausible for the objects as in real life, then colorize it. In this study, wesimulate that human-like action to firstly let our network learn to segmentwhat is in the photo, then colorize it. Therefore, our network can choose aplausible color under semantic constraint for specific objects, and givediscriminative colors between them. Moreover, the segmentation map becomesunderstandable and interactable for the user. Our models are trained onPASCAL-Context and evaluated on selected images from the public domain andCOCO-Stuff, which has several unseen categories compared to training data. Asseen from the experimental results, our colorization system can provideplausible colors for specific objects and generate harmonious colorscompetitive with state-of-the-art methods.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2006.07587v2'},\n",
       " 973: {'ID': 973,\n",
       "  'title': 'Scalable, Proposal-free Instance Segmentation Network for 3D Pixel  Clustering and Particle Trajectory Reconstruction in Liquid Argon Time  Projection Chambers',\n",
       "  'authors': ['Qing Lin',\n",
       "   'Ran Itay',\n",
       "   'Pierre Côte de Soux',\n",
       "   'Ka Vang Tsang',\n",
       "   'Kazuhiro Terao',\n",
       "   'Dae Heun Koh',\n",
       "   'François Drielsma',\n",
       "   'Tracy Usher',\n",
       "   'Laura Dominé'],\n",
       "  'published': '2020-07-06T21:37:28Z',\n",
       "  'updated': '2020-07-06T21:37:28Z',\n",
       "  'abstract': 'Liquid Argon Time Projection Chambers (LArTPCs) are high resolution particleimaging detectors, employed by accelerator-based neutrino oscillationexperiments for high precision physics measurements. While images of particletrajectories are intuitive to analyze for physicists, the development of a highquality, automated data reconstruction chain remains challenging. One of themost critical reconstruction steps is particle clustering: the task of grouping3D image pixels into different particle instances that share the same particletype. In this paper, we propose the first scalable deep learning algorithm forparticle clustering in LArTPC data using sparse convolutional neural networks(SCNN). Building on previous works on SCNNs and proposal free instancesegmentation, we build an end-to-end trainable instance segmentation networkthat learns an embedding of the image pixels to perform point cloud clusteringin a transformed space. We benchmark the performance of our algorithm onPILArNet, a public 3D particle imaging dataset, with respect to commonclustering evaluation metrics. 3D pixels were successfully clustered intoindividual particle trajectories with 90% of them having an adjusted Rand indexscore greater than 92% with a mean pixel clustering efficiency and purity above96%. This work contributes to the development of an end-to-end optimizable fulldata reconstruction chain for LArTPCs, in particular pixel-based 3D imagingdetectors including the near detector of the Deep Underground NeutrinoExperiment. Our algorithm is made available in the open access repository, andwe share our Singularity software container, which can be used to reproduce ourwork on the dataset.',\n",
       "  'categories': ['physics.ins-det', 'cs.CV', 'eess.IV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2007.03083v1'},\n",
       " 974: {'ID': 974,\n",
       "  'title': 'Dense-Captioning Events in Videos: SYSU Submission to ActivityNet  Challenge 2020',\n",
       "  'authors': ['Huicheng Zheng', 'Teng Wang', 'Mingjing Yu'],\n",
       "  'published': '2020-06-21T02:38:59Z',\n",
       "  'updated': '2020-08-12T03:44:21Z',\n",
       "  'abstract': 'This technical report presents a brief description of our submission to thedense video captioning task of ActivityNet Challenge 2020. Our approach followsa two-stage pipeline: first, we extract a set of temporal event proposals; thenwe propose a multi-event captioning model to capture the event-level temporalrelationships and effectively fuse the multi-modal information. Our approachachieves a 9.28 METEOR score on the test set.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2006.11693v2'},\n",
       " 975: {'ID': 975,\n",
       "  'title': 'SeqDialN: Sequential Visual Dialog Networks in Joint Visual-Linguistic  Representation Space',\n",
       "  'authors': ['Ming-Kuang Daniel Wu',\n",
       "   'Vicent Ying',\n",
       "   'Fanqi Meng',\n",
       "   'Liu Yang',\n",
       "   'Xianchao Xu'],\n",
       "  'published': '2020-08-02T04:57:54Z',\n",
       "  'updated': '2020-08-02T04:57:54Z',\n",
       "  'abstract': 'In this work, we formulate a visual dialog as an information flow in whicheach piece of information is encoded with the joint visual-linguisticrepresentation of a single dialog round. Based on this formulation, we considerthe visual dialog task as a sequence problem consisting of orderedvisual-linguistic vectors. For featurization, we use a Dense SymmetricCo-Attention network as a lightweight vison-language joint representationgenerator to fuse multimodal features (i.e., image and text), yielding bettercomputation and data efficiencies. For inference, we propose two SequentialDialog Networks (SeqDialN): the first uses LSTM for information propagation(IP) and the second uses a modified Transformer for multi-step reasoning (MR).Our architecture separates the complexity of multimodal feature fusion fromthat of inference, which allows simpler design of the inference engine. IPbased SeqDialN is our baseline with a simple 2-layer LSTM design that achievesdecent performance. MR based SeqDialN, on the other hand, recurrently refinesthe semantic question/history representations through the self-attention stackof Transformer and produces promising results on the visual dialog task. OnVisDial v1.0 test-std dataset, our best single generative SeqDialN achieves62.54% NDCG and 48.63% MRR; our ensemble generative SeqDialN achieves 63.78%NDCG and 49.98% MRR, which set a new state-of-the-art generative visual dialogmodel. We fine-tune discriminative SeqDialN with dense annotations and boostthe performance up to 72.41% NDCG and 55.11% MRR. In this work, we discuss theextensive experiments we have conducted to demonstrate the effectiveness of ourmodel components. We also provide visualization for the reasoning process fromthe relevant conversation rounds and discuss our fine-tuning methods. Our codeis available at https://github.com/xiaoxiaoheimei/SeqDialN',\n",
       "  'categories': ['cs.CV', 'cs.AI', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2008.00397v1'},\n",
       " 976: {'ID': 976,\n",
       "  'title': 'Learning to Learn without Forgetting by Maximizing Transfer and  Minimizing Interference',\n",
       "  'authors': ['Robert Ajemian',\n",
       "   'Ignacio Cases',\n",
       "   'Yuhai Tu',\n",
       "   'Miao Liu',\n",
       "   'Gerald Tesauro',\n",
       "   'Irina Rish',\n",
       "   'Matthew Riemer'],\n",
       "  'published': '2018-10-29T00:13:50Z',\n",
       "  'updated': '2019-05-03T03:32:40Z',\n",
       "  'abstract': 'Lack of performance when it comes to continual learning over non-stationarydistributions of data remains a major challenge in scaling neural networklearning to more human realistic settings. In this work we propose a newconceptualization of the continual learning problem in terms of a temporallysymmetric trade-off between transfer and interference that can be optimized byenforcing gradient alignment across examples. We then propose a new algorithm,Meta-Experience Replay (MER), that directly exploits this view by combiningexperience replay with optimization based meta-learning. This method learnsparameters that make interference based on future gradients less likely andtransfer based on future gradients more likely. We conduct experiments acrosscontinual lifelong supervised learning benchmarks and non-stationaryreinforcement learning environments demonstrating that our approachconsistently outperforms recently proposed baselines for continual learning.Our experiments show that the gap between the performance of MER and baselinealgorithms grows both as the environment gets more non-stationary and as thefraction of the total experiences stored gets smaller.',\n",
       "  'categories': ['cs.LG', 'cs.AI', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1810.11910v3'},\n",
       " 977: {'ID': 977,\n",
       "  'title': 'Universal Decision-Based Black-Box Perturbations: Breaking  Security-Through-Obscurity Defenses',\n",
       "  'authors': ['Bhavya Kailkhura', 'Thomas A. Hogan'],\n",
       "  'published': '2018-11-09T01:43:22Z',\n",
       "  'updated': '2018-11-13T15:56:52Z',\n",
       "  'abstract': 'We study the problem of finding a universal (image-agnostic) perturbation tofool machine learning (ML) classifiers (e.g., neural nets, decision tress) inthe hard-label black-box setting. Recent work in adversarial ML in thewhite-box setting (model parameters are known) has shown that manystate-of-the-art image classifiers are vulnerable to universal adversarialperturbations: a fixed human-imperceptible perturbation that, when added to anyimage, causes it to be misclassified with high probability Kurakin et al.[2016], Szegedy et al. [2013], Chen et al. [2017a], Carlini and Wagner [2017].This paper considers a more practical and challenging problem of finding suchuniversal perturbations in an obscure (or black-box) setting. Morespecifically, we use zeroth order optimization algorithms to find such auniversal adversarial perturbation when no model information is revealed-exceptthat the attacker can make queries to probe the classifier. We further relaxthe assumption that the output of a query is continuous valued confidencescores for all the classes and consider the case where the output is ahard-label decision. Surprisingly, we found that even in these extremelyobscure regimes, state-of-the-art ML classifiers can be fooled with a very highprobability just by adding a single human-imperceptible image perturbation toany natural image. The surprising existence of universal perturbations in ahard-label black-box setting raises serious security concerns with theexistence of a universal noise vector that adversaries can possibly exploit tobreak a classifier on most natural images.',\n",
       "  'categories': ['cs.LG', 'cs.AI', 'cs.CR', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1811.03733v2'},\n",
       " 978: {'ID': 978,\n",
       "  'title': 'CQ-VQA: Visual Question Answering on Categorized Questions',\n",
       "  'authors': ['Aakansha Mishra', 'Ashish Anand', 'Prithwijit Guha'],\n",
       "  'published': '2020-02-17T06:45:29Z',\n",
       "  'updated': '2020-02-17T06:45:29Z',\n",
       "  'abstract': 'This paper proposes CQ-VQA, a novel 2-level hierarchical but end-to-end modelto solve the task of visual question answering (VQA). The first level ofCQ-VQA, referred to as question categorizer (QC), classifies questions toreduce the potential answer search space. The QC uses attended and fusedfeatures of the input question and image. The second level, referred to asanswer predictor (AP), comprises of a set of distinct classifiers correspondingto each question category. Depending on the question category predicted by QC,only one of the classifiers of AP remains active. The loss functions of QC andAP are aggregated together to make it an end-to-end model. The proposed model(CQ-VQA) is evaluated on the TDIUC dataset and is benchmarked againststate-of-the-art approaches. Results indicate competitive or better performanceof CQ-VQA.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2002.06800v1'},\n",
       " 979: {'ID': 979,\n",
       "  'title': 'Groupes $p$-divisibles avec condition de Pappas-Rapoport et invariants  de Hasse',\n",
       "  'authors': ['Valentin Hernandez', 'Stephane Bijakowski'],\n",
       "  'published': '2016-11-30T12:06:39Z',\n",
       "  'updated': '2016-12-05T19:36:19Z',\n",
       "  'abstract': 'We study $p$-divisible groups $G$ endowed with an action of the ring ofintegers of a finite (possibly ramified) extension of $\\\\mathbb{Q}_p$ over ascheme of characteristic $p$. We suppose moreover that the $p$-divisible group$G$ satisfies the Pappas-Rapoport condition for a certain datum $\\\\mu$ ; thiscondition consists in a filtration on the sheaf of differentials $\\\\omega_G$satisfying certain properties. Over a perfect field, we define the Hodge andNewton polygons for such $p$-divisible groups, normalized with the action. Weshow that the Newton polygon lies above the Hodge polygon, itself lying above acertain polygon depending on the datum $\\\\mu$. We then construct Hasseinvariants for such $p$-divisible groups over an arbitrary base scheme ofcharacteristic $p$. We prove that the total Hasse invariant is non-zero if andonly if the $p$-divisible group is $\\\\mu$-ordinary, i.e. if its Newton polygonis minimal. Finally, we study the properties of $\\\\mu$-ordinary $p$-divisiblegroups. The construction of the Hasse invariants can in particular be appliedto special fibers of PEL Shimura varieties models as constructed by Pappas andRapoport.',\n",
       "  'categories': ['math.NT', 'math.AG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1611.10110v2'},\n",
       " 980: {'ID': 980,\n",
       "  'title': 'PeR-ViS: Person Retrieval in Video Surveillance using Semantic  Description',\n",
       "  'authors': ['Vandit Gajjar', 'Parshwa Shah', 'Arpit Garg'],\n",
       "  'published': '2020-12-04T05:11:21Z',\n",
       "  'updated': '2020-12-04T05:11:21Z',\n",
       "  'abstract': \"A person is usually characterized by descriptors like age, gender, height,cloth type, pattern, color, etc. Such descriptors are known as attributesand/or soft-biometrics. They link the semantic gap between a person'sdescription and retrieval in video surveillance. Retrieving a specific personwith the query of semantic description has an important application in videosurveillance. Using computer vision to fully automate the person retrieval taskhas been gathering interest within the research community. However, theCurrent, trend mainly focuses on retrieving persons with image-based queries,which have major limitations for practical usage. Instead of using an imagequery, in this paper, we study the problem of person retrieval in videosurveillance with a semantic description. To solve this problem, we develop adeep learning-based cascade filtering approach (PeR-ViS), which uses Mask R-CNN[14] (person detection and instance segmentation) and DenseNet-161 [16](soft-biometric classification). On the standard person retrieval dataset ofSoftBioSearch [6], we achieve 0.566 Average IoU and 0.792 %w $IoU &gt; 0.4$,surpassing the current state-of-the-art by a large margin. We hope our simple,reproducible, and effective approach will help ease future research in thedomain of person retrieval in video surveillance. The source code andpretrained weights available at https://parshwa1999.github.io/PeR-ViS/.\",\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2012.02408v1'},\n",
       " 981: {'ID': 981,\n",
       "  'title': 'SCNN: An Accelerator for Compressed-sparse Convolutional Neural Networks',\n",
       "  'authors': ['Joel Emer',\n",
       "   'Minsoo Rhu',\n",
       "   'William J. Dally',\n",
       "   'Rangharajan Venkatesan',\n",
       "   'Antonio Puglielli',\n",
       "   'Brucek Khailany',\n",
       "   'Angshuman Parashar',\n",
       "   'Stephen W. Keckler',\n",
       "   'Anurag Mukkara'],\n",
       "  'published': '2017-05-23T22:11:11Z',\n",
       "  'updated': '2017-05-23T22:11:11Z',\n",
       "  'abstract': 'Convolutional Neural Networks (CNNs) have emerged as a fundamental technologyfor machine learning. High performance and extreme energy efficiency arecritical for deployments of CNNs in a wide range of situations, especiallymobile platforms such as autonomous vehicles, cameras, and electronic personalassistants. This paper introduces the Sparse CNN (SCNN) acceleratorarchitecture, which improves performance and energy efficiency by exploitingthe zero-valued weights that stem from network pruning during training andzero-valued activations that arise from the common ReLU operator applied duringinference. Specifically, SCNN employs a novel dataflow that enables maintainingthe sparse weights and activations in a compressed encoding, which eliminatesunnecessary data transfers and reduces storage requirements. Furthermore, theSCNN dataflow facilitates efficient delivery of those weights and activationsto the multiplier array, where they are extensively reused. In addition, theaccumulation of multiplication products are performed in a novel accumulatorarray. Our results show that on contemporary neural networks, SCNN can improveboth performance and energy by a factor of 2.7x and 2.3x, respectively, over acomparably provisioned dense CNN accelerator.',\n",
       "  'categories': ['cs.NE', 'cs.AR', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1708.04485v1'},\n",
       " 982: {'ID': 982,\n",
       "  'title': 'Playing the Game of Universal Adversarial Perturbations',\n",
       "  'authors': ['Julien Perolat',\n",
       "   'Olivier Pietquin',\n",
       "   'Mateusz Malinowski',\n",
       "   'Bilal Piot'],\n",
       "  'published': '2018-09-20T18:48:36Z',\n",
       "  'updated': '2018-09-25T20:16:45Z',\n",
       "  'abstract': 'We study the problem of learning classifiers robust to universal adversarialperturbations. While prior work approaches this problem via robustoptimization, adversarial training, or input transformation, we instead phraseit as a two-player zero-sum game. In this new formulation, both playerssimultaneously play the same game, where one player chooses a classifier thatminimizes a classification loss whilst the other player creates an adversarialperturbation that increases the same loss when applied to every sample in thetraining set. By observing that performing a classification (respectivelycreating adversarial samples) is the best response to the other player, wepropose a novel extension of a game-theoretic algorithm, namely fictitiousplay, to the domain of training robust classifiers. Finally, we empiricallyshow the robustness and versatility of our approach in two defence scenarioswhere universal attacks are performed on several image classification datasets-- CIFAR10, CIFAR100 and ImageNet.',\n",
       "  'categories': ['cs.LG', 'cs.CV', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1809.07802v2'},\n",
       " 983: {'ID': 983,\n",
       "  'title': 'Fast and Full-Resolution Light Field Deblurring using a Deep Neural  Network',\n",
       "  'authors': ['Tae Hyun Kim',\n",
       "   'Ravi Ramamoorthi',\n",
       "   'Jonathan Samuel Lumentut',\n",
       "   'In Kyu Park'],\n",
       "  'published': '2019-03-31T07:36:27Z',\n",
       "  'updated': '2019-03-31T07:36:27Z',\n",
       "  'abstract': 'Restoring a sharp light field image from its blurry input has becomeessential due to the increasing popularity of parallax-based image processing.State-of-the-art blind light field deblurring methods suffer from severalissues such as slow processing, reduced spatial size, and a limited motion blurmodel. In this work, we address these challenging problems by generating acomplex blurry light field dataset and proposing a learning-based deblurringapproach. In particular, we model the full 6-degree of freedom (6-DOF) lightfield camera motion, which is used to create the blurry dataset using acombination of real light fields captured with a Lytro Illum camera, andsynthetic light field renderings of 3D scenes. Furthermore, we propose a lightfield deblurring network that is built with the capability of large receptivefields. We also introduce a simple strategy of angular sampling to train on thelarge-scale blurry light field effectively. We evaluate our method through bothquantitative and qualitative measurements and demonstrate superior performancecompared to the state-of-the-art method with a massive speedup in executiontime. Our method is about 16K times faster than Srinivasan et. al. [22] and candeblur a full-resolution light field in less than 2 seconds.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1904.00352v1'},\n",
       " 984: {'ID': 984,\n",
       "  'title': 'Phase Retrieval with Holography and Untrained Priors: Tackling the  Challenges of Low-Photon Nanoscale Imaging',\n",
       "  'authors': ['Marylou Gabrié',\n",
       "   'Michael Eickenberg',\n",
       "   'Hannah Lawrence',\n",
       "   'Henry Li',\n",
       "   'David A. Barmherzig'],\n",
       "  'published': '2020-12-14T10:15:07Z',\n",
       "  'updated': '2020-12-15T16:46:13Z',\n",
       "  'abstract': 'Phase retrieval is the inverse problem of recovering a signal frommagnitude-only Fourier measurements, and underlies numerous imaging modalities,such as Coherent Diffraction Imaging (CDI). A variant of this setup, known asholography, includes a reference object that is placed adjacent to the specimenof interest before measurements are collected. The resulting inverse problem,known as holographic phase retrieval, is well-known to have improved problemconditioning relative to the original. This innovation, i.e. Holographic CDI,becomes crucial at the nanoscale, where imaging specimens such as viruses,proteins, and crystals require low-photon measurements. This data is highlycorrupted by Poisson shot noise, and often lacks low-frequency content as well.In this work, we introduce a dataset-free deep learning framework forholographic phase retrieval adapted to these challenges. The key ingredients ofour approach are the explicit and flexible incorporation of the physicalforward model into an automatic differentiation procedure, the Poissonlog-likelihood objective function, and an optional untrained deep image prior.We perform extensive evaluation under realistic conditions. Compared tocompeting classical methods, our method recovers signal from higher noiselevels and is more resilient to suboptimal reference design, as well as tolarge missing regions of low frequencies in the observations. To the best ofour knowledge, this is the first work to consider a dataset-free machinelearning approach for holographic phase retrieval.',\n",
       "  'categories': ['cs.LG', 'cs.CV', 'physics.optics', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2012.07386v2'},\n",
       " 985: {'ID': 985,\n",
       "  'title': 'Short-time deep-learning based source separation for speech enhancement  in reverberant environments with beamforming',\n",
       "  'authors': ['Diego Pincheira',\n",
       "   'Alejandro Díaz',\n",
       "   'Rodrigo Mahu',\n",
       "   'Nestor Becerra Yoma'],\n",
       "  'published': '2020-11-03T19:18:53Z',\n",
       "  'updated': '2020-11-03T19:18:53Z',\n",
       "  'abstract': 'The source separation-based speech enhancement problem with multiplebeamforming in reverberant indoor environments is addressed in this paper. Wepropose that more generic solutions should cope with time-varying dynamicscenarios with moving microphone array or sources such as those found invoice-based human-robot interaction or smart speaker applications. Theeffectiveness of ordinary source separation methods based on statistical modelssuch as ICA and NMF depends on the analysis window size and cannot handlereverberation environments. To address these limitations, a short-term sourceseparation method based on a temporal convolutional network in combination withcompact bilinear pooling is presented. The proposed scheme is virtuallyindependent of the analysis window size and does not lose effectiveness whenthe analysis window is shortened to 1.6s, which in turn is very interesting totackle the source separation problem in time-varying scenarios. Also,improvements in WER as high as 80% were obtained when compared to ICA and NMFwith multi-condition reverberant training and testing, and with time-varyingSNR experiments to simulate a moving target speech source. Finally, theexperiment with the estimation of the clean signal employing the proposedscheme and a clean trained ASR provided a WER 13% lower than the one obtainedwith the corrupted signal and a multi-condition trained ASR. This surprisingresult contradicts the widely adopted practice of using multi-condition trainedASR systems and reinforce the use of speech enhancement methods for userprofiling in HRI environments.',\n",
       "  'categories': ['eess.AS'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2011.01965v1'},\n",
       " 986: {'ID': 986,\n",
       "  'title': 'Light Field Image Super-Resolution Using Deformable Convolution',\n",
       "  'authors': ['Wei An',\n",
       "   'Yulan Guo',\n",
       "   'Tianhao Wu',\n",
       "   'Longguang Wang',\n",
       "   'Jungang Yang',\n",
       "   'Yingqian Wang',\n",
       "   'Xinyi Ying'],\n",
       "  'published': '2020-07-07T15:07:33Z',\n",
       "  'updated': '2020-11-25T12:01:05Z',\n",
       "  'abstract': 'Light field (LF) cameras can record scenes from multiple perspectives, andthus introduce beneficial angular information for image super-resolution (SR).However, it is challenging to incorporate angular information due todisparities among LF images. In this paper, we propose a deformable convolutionnetwork (i.e., LF-DFnet) to handle the disparity problem for LF image SR.Specifically, we design an angular deformable alignment module (ADAM) forfeature-level alignment. Based on ADAM, we further propose acollect-and-distribute approach to perform bidirectional alignment between thecenter-view feature and each side-view feature. Using our approach, angularinformation can be well incorporated and encoded into features of each view,which benefits the SR reconstruction of all LF images. Moreover, we develop abaseline-adjustable LF dataset to evaluate SR performance under differentdisparity variations. Experiments on both public and our self-developeddatasets have demonstrated the superiority of our method. Our LF-DFnet cangenerate high-resolution images with more faithful details and achievestate-of-the-art reconstruction accuracy. Besides, our LF-DFnet is more robustto disparity variations, which has not been well addressed in literature.',\n",
       "  'categories': ['eess.IV', 'cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2007.03535v4'},\n",
       " 987: {'ID': 987,\n",
       "  'title': 'Chemical history of isolated dwarf galaxies of the Local Group: I.  dSphs: Cetus and Tucana',\n",
       "  'authors': ['S. L. Hidalgo', 'R. Durazo', 'N. Avila-Vergara', 'L. Carigi'],\n",
       "  'published': '2016-05-01T19:28:13Z',\n",
       "  'updated': '2016-05-01T19:28:13Z',\n",
       "  'abstract': 'For the first time, we obtain chemical evolution models (CEMs) for Tucana andCetus, two isolated dwarf spheroidal galaxies (dSphs) of the Local Group. TheCEMs have been built from the star formation histories (SFHs) and themetallicity histories, both obtained independently by the LCID project fromdeep color-magnitude dia- grams. Based on our models, we find that the chemicalhistories were complex and can be divided into different epochs and scenarios.In particular, during 75 percent of the SFH, the galaxies behaved as closedboxes and, during the remaining 25 percent, either received a lot of primordialgas by accretion or they lost metals through metal-rich winds. In order todiscriminate between these two scenarios, abundances ratios in old stars areneeded. At t approximately 4.5 Gyr, the galaxies lost most of their gas due toa short-strong, well-mixed wind. We obtain very similar CEMs for both galaxies,although Cetus is twice as massive as Tucana. We conclude that the starformation in both galaxies began with only 1.5 percent of the baryonic massfraction predicted by lambda CDM.',\n",
       "  'categories': ['astro-ph.GA'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1605.00295v1'},\n",
       " 988: {'ID': 988,\n",
       "  'title': 'Deblurring by Realistic Blurring',\n",
       "  'authors': ['Kaihao Zhang',\n",
       "   'Hongdong Li',\n",
       "   'Lin Ma',\n",
       "   'Bjorn Stenger',\n",
       "   'Yiran Zhong',\n",
       "   'Wei Liu',\n",
       "   'Wenhan Luo'],\n",
       "  'published': '2020-04-04T05:25:15Z',\n",
       "  'updated': '2020-05-07T03:34:49Z',\n",
       "  'abstract': 'Existing deep learning methods for image deblurring typically train modelsusing pairs of sharp images and their blurred counterparts. However,synthetically blurring images do not necessarily model the genuine blurringprocess in real-world scenarios with sufficient accuracy. To address thisproblem, we propose a new method which combines two GAN models, i.e., alearning-to-Blur GAN (BGAN) and learning-to-DeBlur GAN (DBGAN), in order tolearn a better model for image deblurring by primarily learning how to blurimages. The first model, BGAN, learns how to blur sharp images with unpairedsharp and blurry image sets, and then guides the second model, DBGAN, to learnhow to correctly deblur such images. In order to reduce the discrepancy betweenreal blur and synthesized blur, a relativistic blur loss is leveraged. As anadditional contribution, this paper also introduces a Real-World Blurred Image(RWBI) dataset including diverse blurry images. Our experiments show that theproposed method achieves consistently superior quantitative performance as wellas higher perceptual quality on both the newly proposed dataset and the publicGOPRO dataset.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2004.01860v2'},\n",
       " 989: {'ID': 989,\n",
       "  'title': 'Shape, Illumination, and Reflectance from Shading',\n",
       "  'authors': ['Jitendra Malik', 'Jonathan T. Barron'],\n",
       "  'published': '2020-10-07T18:14:41Z',\n",
       "  'updated': '2020-10-07T18:14:41Z',\n",
       "  'abstract': 'A fundamental problem in computer vision is that of inferring the intrinsic,3D structure of the world from flat, 2D images of that world. Traditionalmethods for recovering scene properties such as shape, reflectance, orillumination rely on multiple observations of the same scene to overconstrainthe problem. Recovering these same properties from a single image seems almostimpossible in comparison -- there are an infinite number of shapes, paint, andlights that exactly reproduce a single image. However, certain explanations aremore likely than others: surfaces tend to be smooth, paint tends to be uniform,and illumination tends to be natural. We therefore pose this problem as one ofstatistical inference, and define an optimization problem that searches for the*most likely* explanation of a single image. Our technique can be viewed as asuperset of several classic computer vision problems (shape-from-shading,intrinsic images, color constancy, illumination estimation, etc) andoutperforms all previous solutions to those constituent problems.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2010.03592v1'},\n",
       " 990: {'ID': 990,\n",
       "  'title': 'TrashCan: A Semantically-Segmented Dataset towards Visual Detection of  Marine Debris',\n",
       "  'authors': ['Junaed Sattar', 'Michael Fulton', 'Jungseok Hong'],\n",
       "  'published': '2020-07-16T04:19:06Z',\n",
       "  'updated': '2020-07-16T04:19:06Z',\n",
       "  'abstract': 'This paper presents TrashCan, a large dataset comprised of images ofunderwater trash collected from a variety of sources, annotated both usingbounding boxes and segmentation labels, for development of robust detectors ofmarine debris. The dataset has two versions, TrashCan-Material andTrashCan-Instance, corresponding to different object class configurations. Theeventual goal is to develop efficient and accurate trash detection methodssuitable for onboard robot deployment. Along with information about theconstruction and sourcing of the TrashCan dataset, we present initial resultsof instance segmentation from Mask R-CNN and object detection from FasterR-CNN. These do not represent the best possible detection results but providesan initial baseline for future work in instance segmentation and objectdetection on the TrashCan dataset.',\n",
       "  'categories': ['cs.CV', 'cs.RO'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2007.08097v1'},\n",
       " 991: {'ID': 991,\n",
       "  'title': 'Neural Architecture Search for Deep Image Prior',\n",
       "  'authors': ['John Collomosse', 'Hailin Jin', 'Kary Ho', 'Andrew Gilbert'],\n",
       "  'published': '2020-01-14T13:51:32Z',\n",
       "  'updated': '2020-01-14T13:51:32Z',\n",
       "  'abstract': 'We present a neural architecture search (NAS) technique to enhance theperformance of unsupervised image de-noising, in-painting and super-resolutionunder the recently proposed Deep Image Prior (DIP). We show that evolutionarysearch can automatically optimize the encoder-decoder (E-D) structure andmeta-parameters of the DIP network, which serves as a content-specific prior toregularize these single image restoration tasks. Our binary representationencodes the design space for an asymmetric E-D network that typically convergesto yield a content-specific DIP within 10-20 generations using a populationsize of 500. The optimized architectures consistently improve upon the visualquality of classical DIP for a diverse range of photographic and artisticcontent.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2001.04776v1'},\n",
       " 992: {'ID': 992,\n",
       "  'title': 'Style Generator Inversion for Image Enhancement and Animation',\n",
       "  'authors': ['Aviv Gabbay', 'Yedid Hoshen'],\n",
       "  'published': '2019-06-05T17:58:28Z',\n",
       "  'updated': '2019-06-05T17:58:28Z',\n",
       "  'abstract': 'One of the main motivations for training high quality image generative modelsis their potential use as tools for image manipulation. Recently, generativeadversarial networks (GANs) have been able to generate images of remarkablequality. Unfortunately, adversarially-trained unconditional generator networkshave not been successful as image priors. One of the main requirements for anetwork to act as a generative image prior, is being able to generate everypossible image from the target distribution. Adversarial learning oftenexperiences mode-collapse, which manifests in generators that cannot generatesome modes of the target distribution. Another requirement often not satisfiedis invertibility i.e. having an efficient way of finding a valid input latentcode given a required output image. In this work, we show that differently fromearlier GANs, the very recently proposed style-generators are quite easy toinvert. We use this important observation to propose style generators asgeneral purpose image priors. We show that style generators outperform otherGANs as well as Deep Image Prior as priors for image enhancement tasks. Thelatent space spanned by style-generators satisfies linear identity-poserelations. The latent space linearity, combined with invertibility, allows usto animate still facial images without supervision. Extensive experiments areperformed to support the main contributions of this paper.',\n",
       "  'categories': ['cs.CV', 'cs.LG', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1906.11880v1'},\n",
       " 993: {'ID': 993,\n",
       "  'title': 'Automatic Quantification of Settlement Damage using Deep Learning of  Satellite Images',\n",
       "  'authors': ['Lili Lu', 'Weisi Guo'],\n",
       "  'published': '2020-10-12T08:06:33Z',\n",
       "  'updated': '2020-10-12T08:06:33Z',\n",
       "  'abstract': 'Humanitarian disasters and political violence cause significant damage to ourliving space. The reparation cost to homes, infrastructure, and the ecosystemis often difficult to quantify in real-time. Real-time quantification iscritical to both informing relief operations, but also planning ahead forrebuilding. Here, we use satellite images before and after major crisis aroundthe world to train a robust baseline Residual Network (ResNet) and a disasterquantification Pyramid Scene Parsing Network (PSPNet). ResNet offers robustnessto poor image quality and can identify areas of destruction with high accuracy(92\\\\%), whereas PSPNet offers contextualised quantification of builtenvironment damage with good accuracy (84\\\\%). As there are multiple damagedimensions to consider (e.g. economic loss and fatalities), we fit amulti-linear regression model to quantify the overall damage. To validate ourcombined system of deep learning and regression modeling, we successfully matchour prediction to the ongoing recovery in the 2020 Beirut port explosion. Theseinnovations provide a better quantification of overall disaster magnitude andinform intelligent humanitarian systems of unfolding disasters.',\n",
       "  'categories': ['cs.CV', 'cs.LG'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/2010.05512v1'},\n",
       " 994: {'ID': 994,\n",
       "  'title': 'Large-scale Pretraining for Visual Dialog: A Simple State-of-the-Art  Baseline',\n",
       "  'authors': ['Vishvak Murahari',\n",
       "   'Devi Parikh',\n",
       "   'Abhishek Das',\n",
       "   'Dhruv Batra'],\n",
       "  'published': '2019-12-05T04:51:11Z',\n",
       "  'updated': '2020-03-31T03:12:26Z',\n",
       "  'abstract': 'Prior work in visual dialog has focused on training deep neural models onVisDial in isolation. Instead, we present an approach to leverage pretrainingon related vision-language datasets before transferring to visual dialog. Weadapt the recently proposed ViLBERT (Lu et al., 2019) model for multi-turnvisually-grounded conversations. Our model is pretrained on the ConceptualCaptions and Visual Question Answering datasets, and finetuned on VisDial. Ourbest single model outperforms prior published work (including model ensembles)by more than 1% absolute on NDCG and MRR. Next, we find that additionalfinetuning using \"dense\" annotations in VisDial leads to even higher NDCG --more than 10% over our base model -- but hurts MRR -- more than 17% below ourbase model! This highlights a trade-off between the two primary metrics -- NDCGand MRR -- which we find is due to dense annotations not correlating well withthe original ground-truth answers to questions.',\n",
       "  'categories': ['cs.LG', 'cs.CL', 'cs.CV', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1912.02379v2'},\n",
       " 995: {'ID': 995,\n",
       "  'title': 'Deep Direct Visual Odometry',\n",
       "  'authors': ['Athanasios V. Vasilakos',\n",
       "   'Qiyu Sun',\n",
       "   'Yang Tang',\n",
       "   'Chaoqiang Zhao'],\n",
       "  'published': '2019-12-11T03:22:03Z',\n",
       "  'updated': '2020-06-25T00:53:16Z',\n",
       "  'abstract': 'Traditional monocular direct visual odometry (DVO) is one of the most famousmethods to estimate the ego-motion of robots as well as map the environmentfrom images simultaneously. However, DVO heavily relies on high-quality imagesand accurate initial pose estimation during tracking, which means that DVO mayfail if the image quality is poor or the initial value is incorrect. With theoutstanding performance of deep learning, like image analysis and processing,previous works have shown that deep neural networks can effectively learn the6-DOF pose between frames from large volumes of image sequences in anunsupervised manner. However, these unsupervised deep learning-based frameworkscannot accurately generate the full trajectory of a long monocular videobecause of the scale-inconsistency between each pose. To tackle this problem,we take several measures to improve the scale-consistency of our network(TrajNet), including improving the previous loss function and proposing a novelscale-to-trajectory constraint. Besides, considering the lack of mapping threadin deep learning-based visual odometry (VO), a new architecture, called deepdirect sparse odometry (DDSO), is proposed to overcome the limitations of DVOas well as the mapping of deep learning-based VO by embedding our TrajNet intoDVO. Expensive experiments on the KITTI dataset show that the proposed networkachieves an outstanding performance on full trajectory prediction when comparedwith previous unsupervised monocular methods, and the integration with ourTrajNet makes the initialization and tracking of DVO more robust and accurate.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1912.05101v2'},\n",
       " 996: {'ID': 996,\n",
       "  'title': 'Instance-Level Meta Normalization',\n",
       "  'authors': ['Songhao Jia', 'Hwann-Tzong Chen', 'Ding-Jie Chen'],\n",
       "  'published': '2019-04-06T19:37:18Z',\n",
       "  'updated': '2019-04-06T19:37:18Z',\n",
       "  'abstract': 'This paper presents a normalization mechanism called Instance-Level MetaNormalization (ILM~Norm) to address a learning-to-normalize problem. ILM~Normlearns to predict the normalization parameters via both the featurefeed-forward and the gradient back-propagation paths. ILM~Norm provides a metanormalization mechanism and has several good properties. It can be easilyplugged into existing instance-level normalization schemes such as InstanceNormalization, Layer Normalization, or Group Normalization. ILM~Norm normalizeseach instance individually and therefore maintains high performance even whensmall mini-batch is used. The experimental results show that ILM~Norm welladapts to different network architectures and tasks, and it consistentlyimproves the performance of the original models. The code is available aturl{https://github.com/Gasoonjia/ILM-Norm.',\n",
       "  'categories': ['cs.LG', 'cs.CV', 'stat.ML'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1904.03516v1'},\n",
       " 997: {'ID': 997,\n",
       "  'title': 'SESR: Single Image Super Resolution with Recursive Squeeze and  Excitation Networks',\n",
       "  'authors': ['Ying Tai', 'Jian Yang', 'Xi Cheng', 'Xiang Li'],\n",
       "  'published': '2018-01-31T06:50:49Z',\n",
       "  'updated': '2018-01-31T06:50:49Z',\n",
       "  'abstract': 'Single image super resolution is a very important computer vision task, witha wide range of applications. In recent years, the depth of thesuper-resolution model has been constantly increasing, but with a smallincrease in performance, it has brought a huge amount of computation and memoryconsumption. In this work, in order to make the super resolution models moreeffective, we proposed a novel single image super resolution method viarecursive squeeze and excitation networks (SESR). By introducing the squeezeand excitation module, our SESR can model the interdependencies andrelationships between channels and that makes our model more efficiency. Inaddition, the recursive structure and progressive reconstruction method in ourmodel minimized the layers and parameters and enabled SESR to simultaneouslytrain multi-scale super resolution in a single model. After evaluating on fourbenchmark test sets, our model is proved to be above the state-of-the-artmethods in terms of speed and accuracy.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1801.10319v1'},\n",
       " 998: {'ID': 998,\n",
       "  'title': 'Learning Strict Identity Mappings in Deep Residual Networks',\n",
       "  'authors': ['Zhiding Yu', 'Xin Yu', 'Srikumar Ramalingam'],\n",
       "  'published': '2018-04-05T03:19:53Z',\n",
       "  'updated': '2019-06-16T00:03:19Z',\n",
       "  'abstract': 'A family of super deep networks, referred to as residual networks or ResNet,achieved record-beating performance in various visual tasks such as imagerecognition, object detection, and semantic segmentation. The ability to trainvery deep networks naturally pushed the researchers to use enormous resourcesto achieve the best performance. Consequently, in many applications super deepresidual networks were employed for just a marginal improvement in performance.In this paper, we propose epsilon-ResNet that allows us to automaticallydiscard redundant layers, which produces responses that are smaller than athreshold epsilon, with a marginal or no loss in performance. Theepsilon-ResNet architecture can be achieved using a few additional rectifiedlinear units in the original ResNet. Our method does not use any additionalvariables nor numerous trials like other hyper-parameter optimizationtechniques. The layer selection is achieved using a single training process andthe evaluation is performed on CIFAR-10, CIFAR-100, SVHN, and ImageNetdatasets. In some instances, we achieve about 80% reduction in the number ofparameters.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1804.01661v5'},\n",
       " 999: {'ID': 999,\n",
       "  'title': 'Differential Diagnosis for Pancreatic Cysts in CT Scans Using  Densely-Connected Convolutional Networks',\n",
       "  'authors': ['Maximilian Reichert',\n",
       "   'Ji Li',\n",
       "   'Kanru Lin',\n",
       "   'Deliang Fu',\n",
       "   'Hongwei Li',\n",
       "   'Roland Schmid',\n",
       "   'Lina Xu',\n",
       "   'Bjoern Menze',\n",
       "   'Kuangyu Shi',\n",
       "   'Rickmer Braren'],\n",
       "  'published': '2018-06-04T09:25:59Z',\n",
       "  'updated': '2018-06-19T07:38:11Z',\n",
       "  'abstract': 'The lethal nature of pancreatic ductal adenocarcinoma (PDAC) calls for earlydifferential diagnosis of pancreatic cysts, which are identified in up to 16%of normal subjects, and some of which may develop into PDAC. Previouscomputer-aided developments have achieved certain accuracy for classificationon segmented cystic lesions in CT. However, pancreatic cysts have a largevariation in size and shape, and the precise segmentation of them remainsrather challenging, which restricts the computer-aided interpretation of CTimages acquired for differential diagnosis. We propose a computer-aidedframework for early differential diagnosis of pancreatic cysts withoutpre-segmenting the lesions using densely-connected convolutional networks(Dense-Net). The Dense-Net learns high-level features from whole abnormalpancreas and builds mappings between medical imaging appearance to differentpathological types of pancreatic cysts. To enhance the clinical applicability,we integrate saliency maps in the framework to assist the physicians tounderstand the decision of the deep learning method. The test on a cohort of206 patients with 4 pathologically confirmed subtypes of pancreatic cysts hasachieved an overall accuracy of 72.8%, which is significantly higher than thebaseline accuracy of 48.1%, which strongly supports the clinical potential ofour developed method.',\n",
       "  'categories': ['cs.CV'],\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1806.01023v3'},\n",
       " ...}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-samoa",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
