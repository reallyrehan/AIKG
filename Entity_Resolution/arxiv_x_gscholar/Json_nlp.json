{"0": {"ID": 0, "title": "Multi-Source Neural Translation", "authors": ["Barret Zoph", "Kevin Knight"], "published": "2016-01-05T00:49:22Z", "updated": "2016-01-05T00:49:22Z", "abstract": "We build a multi-source machine translation model and train it to maximizethe probability of a target English string given French and German sources.Using the neural encoder-decoder framework, we explore several combinationmethods and report up to +4.8 Bleu increases on top of a very strongattention-based neural translation model.", "categories": ["cs.CL"], "journal": "HLT-NAACL, 30-34", "citations": "195", "arxiv_url": "http://arxiv.org/abs/1601.00710v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9798500345837394101&btnI=1&nossl=1&hl=en&oe=ASCII"}, "1": {"ID": 1, "title": "Deep Communicating Agents for Abstractive Summarization", "authors": ["Xiaodong He", "Asli Celikyilmaz", "Antoine Bosselut", "Yejin Choi"], "published": "2018-03-27T23:29:23Z", "updated": "2018-08-15T18:54:22Z", "abstract": "We present deep communicating agents in an encoder-decoder architecture toaddress the challenges of representing a long document for abstractivesummarization. With deep communicating agents, the task of encoding a long textis divided across multiple collaborating agents, each in charge of a subsectionof the input text. These encoders are connected to a single decoder, trainedend-to-end using reinforcement learning to generate a focused and coherentsummary. Empirical results demonstrate that multiple communicating encoderslead to a higher quality summary compared to several strong baselines,including those based on a single encoder or multiple non-communicatingencoders.", "categories": ["cs.CL"], "journal": "NAACL-HLT, 1662-1675", "citations": "146", "arxiv_url": "http://arxiv.org/abs/1803.10357v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=3657827838385959036&btnI=1&nossl=1&hl=en&oe=ASCII"}, "2": {"ID": 2, "title": "Selective Encoding for Abstractive Sentence Summarization", "authors": ["Furu Wei", "Ming Zhou", "Nan Yang", "Qingyu Zhou"], "published": "2017-04-24T07:57:37Z", "updated": "2017-04-24T07:57:37Z", "abstract": "We propose a selective encoding model to extend the sequence-to-sequenceframework for abstractive sentence summarization. It consists of a sentenceencoder, a selective gate network, and an attention equipped decoder. Thesentence encoder and decoder are built with recurrent neural networks. Theselective gate network constructs a second level sentence representation bycontrolling the information flow from encoder to decoder. The second levelrepresentation is tailored for sentence summarization task, which leads tobetter performance. We evaluate our model on the English Gigaword, DUC 2004 andMSR abstractive sentence summarization datasets. The experimental results showthat the proposed selective encoding model outperforms the state-of-the-artbaseline models.", "categories": ["cs.CL"], "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "160", "arxiv_url": "http://arxiv.org/abs/1704.07073v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8387953750036922844&btnI=1&nossl=1&hl=en&oe=ASCII"}, "3": {"ID": 3, "title": "Massive Exploration of Neural Machine Translation Architectures", "authors": ["Minh-Thang Luong", "Quoc Le", "Denny Britz", "Anna Goldie"], "published": "2017-03-11T04:17:46Z", "updated": "2017-03-21T20:34:59Z", "abstract": "Neural Machine Translation (NMT) has shown remarkable progress over the pastfew years with production systems now being deployed to end-users. One majordrawback of current architectures is that they are expensive to train,typically requiring days to weeks of GPU time to converge. This makesexhaustive hyperparameter search, as is commonly done with other neural networkarchitectures, prohibitively expensive. In this work, we present the firstlarge-scale analysis of NMT architecture hyperparameters. We report empiricalresults and variance numbers for several hundred experimental runs,corresponding to over 250,000 GPU hours on the standard WMT English to Germantranslation task. Our experiments lead to novel insights and practical advicefor building and extending NMT architectures. As part of this contribution, werelease an open-source NMT framework that enables researchers to easilyexperiment with novel techniques and reproduce state of the art results.", "categories": ["cs.CL"], "journal": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "299", "arxiv_url": "http://arxiv.org/abs/1703.03906v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=17797498583666145091&btnI=1&nossl=1&hl=en&oe=ASCII"}, "4": {"ID": 4, "title": "Self-Attention with Relative Position Representations", "authors": ["Ashish Vaswani", "Peter Shaw", "Jakob Uszkoreit"], "published": "2018-03-06T13:13:11Z", "updated": "2018-04-12T18:51:33Z", "abstract": "Relying entirely on an attention mechanism, the Transformer introduced byVaswani et al. (2017) achieves state-of-the-art results for machinetranslation. In contrast to recurrent and convolutional neural networks, itdoes not explicitly model relative or absolute position information in itsstructure. Instead, it requires adding representations of absolute positions toits inputs. In this work we present an alternative approach, extending theself-attention mechanism to efficiently consider representations of therelative positions, or distances between sequence elements. On the WMT 2014English-to-German and English-to-French translation tasks, this approach yieldsimprovements of 1.3 BLEU and 0.3 BLEU over absolute position representations,respectively. Notably, we observe that combining relative and absolute positionrepresentations yields no further improvement in translation quality. Wedescribe an efficient implementation of our method and cast it as an instanceof relation-aware self-attention mechanisms that can generalize to arbitrarygraph-labeled inputs.", "categories": ["cs.CL"], "journal": "NAACL-HLT (2), 464-468", "citations": "265", "arxiv_url": "http://arxiv.org/abs/1803.02155v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=5563767891081728261&btnI=1&nossl=1&hl=en&oe=ASCII"}, "5": {"ID": 5, "title": "SemEval-2017 Task 8: RumourEval: Determining rumour veracity and support  for rumours", "authors": ["Maria Liakata", "Leon Derczynski", "Arkaitz Zubiaga", "Kalina Bontcheva", "Rob Procter", "Geraldine Wong Sak Hoi"], "published": "2017-04-20T01:21:20Z", "updated": "2017-04-20T01:21:20Z", "abstract": "Media is full of false claims. Even Oxford Dictionaries named \"post-truth\" asthe word of 2016. This makes it more important than ever to build systems thatcan identify the veracity of a story, and the kind of discourse there is aroundit. RumourEval is a SemEval shared task that aims to identify and handlerumours and reactions to them, in text. We present an annotation scheme, alarge dataset covering multiple topics - each having their own families ofclaims and replies - and use these to pose two concrete challenges as well asthe results achieved by participants on these challenges.", "categories": ["cs.CL", "cs.AI"], "journal": "Proceedings of the 11th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "132", "arxiv_url": "http://arxiv.org/abs/1704.05972v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6061353521973070542&btnI=1&nossl=1&hl=en&oe=ASCII"}, "6": {"ID": 6, "title": "Very Deep Convolutional Networks for Text Classification", "authors": ["Holger Schwenk", "Alexis Conneau", "Yann Lecun", "Lo\u00efc Barrault"], "published": "2016-06-06T15:14:50Z", "updated": "2017-01-27T12:49:11Z", "abstract": "The dominant approach for many NLP tasks are recurrent neural networks, inparticular LSTMs, and convolutional neural networks. However, thesearchitectures are rather shallow in comparison to the deep convolutionalnetworks which have pushed the state-of-the-art in computer vision. We presenta new architecture (VDCNN) for text processing which operates directly at thecharacter level and uses only small convolutions and pooling operations. We areable to show that the performance of this model increases with depth: using upto 29 convolutional layers, we report improvements over the state-of-the-art onseveral public text classification tasks. To the best of our knowledge, this isthe first time that very deep convolutional nets have been applied to textprocessing.", "categories": ["cs.CL", "cs.LG", "cs.NE"], "journal": "EACL (1), 1107-1116", "citations": "499", "arxiv_url": "http://arxiv.org/abs/1606.01781v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10851201017229487169&btnI=1&nossl=1&hl=en&oe=ASCII"}, "7": {"ID": 7, "title": "End-to-End Relation Extraction using LSTMs on Sequences and Tree  Structures", "authors": ["Mohit Bansal", "Makoto Miwa"], "published": "2016-01-05T08:53:05Z", "updated": "2016-06-08T01:08:08Z", "abstract": "We present a novel end-to-end neural model to extract entities and relationsbetween them. Our recurrent neural network based model captures both wordsequence and dependency tree substructure information by stacking bidirectionaltree-structured LSTM-RNNs on bidirectional sequential LSTM-RNNs. This allowsour model to jointly represent both entities and relations with sharedparameters in a single model. We further encourage detection of entities duringtraining and use of entity information in relation extraction via entitypretraining and scheduled sampling. Our model improves over thestate-of-the-art feature-based model on end-to-end relation extraction,achieving 12.1% and 5.7% relative error reductions in F1-score on ACE2005 andACE2004, respectively. We also show that our LSTM-RNN based model comparesfavorably to the state-of-the-art CNN based model (in F1-score) on nominalrelation classification (SemEval-2010 Task 8). Finally, we present an extensiveablation analysis of several model components.", "categories": ["cs.CL", "cs.LG"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "487", "arxiv_url": "http://arxiv.org/abs/1601.00770v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4226785540459330499&btnI=1&nossl=1&hl=en&oe=ASCII"}, "8": {"ID": 8, "title": "Context Gates for Neural Machine Translation", "authors": ["Zhengdong Lu", "Hang Li", "Zhaopeng Tu", "Yang Liu", "Xiaohua Liu"], "published": "2016-08-22T03:19:27Z", "updated": "2017-03-08T07:14:27Z", "abstract": "In neural machine translation (NMT), generation of a target word depends onboth source and target contexts. We find that source contexts have a directimpact on the adequacy of a translation while target contexts affect thefluency. Intuitively, generation of a content word should rely more on thesource context and generation of a functional word should rely more on thetarget context. Due to the lack of effective control over the influence fromsource and target contexts, conventional NMT tends to yield fluent butinadequate translations. To address this problem, we propose context gateswhich dynamically control the ratios at which source and target contextscontribute to the generation of target words. In this way, we can enhance boththe adequacy and fluency of NMT with more careful control of the informationflow from contexts. Experiments show that our approach significantly improvesupon a standard attention-based NMT system by +2.3 BLEU points.", "categories": ["cs.CL"], "journal": "Transactions of the Association for Computational Linguistics 5, 87-99", "citations": "77", "arxiv_url": "http://arxiv.org/abs/1608.06043v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4677989351050388193&btnI=1&nossl=1&hl=en&oe=ASCII"}, "9": {"ID": 9, "title": "Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual  Transfer and Beyond", "authors": ["Mikel Artetxe", "Holger Schwenk"], "published": "2018-12-26T18:58:39Z", "updated": "2019-09-25T13:16:12Z", "abstract": "We introduce an architecture to learn joint multilingual sentencerepresentations for 93 languages, belonging to more than 30 different familiesand written in 28 different scripts. Our system uses a single BiLSTM encoderwith a shared BPE vocabulary for all languages, which is coupled with anauxiliary decoder and trained on publicly available parallel corpora. Thisenables us to learn a classifier on top of the resulting embeddings usingEnglish annotated data only, and transfer it to any of the 93 languages withoutany modification. Our experiments in cross-lingual natural language inference(XNLI dataset), cross-lingual document classification (MLDoc dataset) andparallel corpus mining (BUCC dataset) show the effectiveness of our approach.We also introduce a new test set of aligned sentences in 112 languages, andshow that our sentence embeddings obtain strong results in multilingualsimilarity search even for low-resource languages. Our implementation, thepre-trained encoder and the multilingual test set are available athttps://github.com/facebookresearch/LASER", "categories": ["cs.CL", "cs.AI", "cs.LG"], "journal": "Transactions of the Association for Computational Linguistics 7, 597-610", "citations": "138", "arxiv_url": "http://arxiv.org/abs/1812.10464v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10327946384574259834&btnI=1&nossl=1&hl=en&oe=ASCII"}, "10": {"ID": 10, "title": "A Survey on Recent Advances in Named Entity Recognition from Deep  Learning models", "authors": ["Vikas Yadav", "Steven Bethard"], "published": "2019-10-25T00:45:48Z", "updated": "2019-10-25T00:45:48Z", "abstract": "Named Entity Recognition (NER) is a key component in NLP systems for questionanswering, information retrieval, relation extraction, etc. NER systems havebeen studied and developed widely for decades, but accurate systems using deepneural networks (NN) have only been introduced in the last few years. Wepresent a comprehensive survey of deep neural network architectures for NER,and contrast them with previous approaches to NER based on feature engineeringand other supervised or semi-supervised learning algorithms. Our resultshighlight the improvements achieved by neural networks, and show howincorporating some of the lessons learned from past work on feature-based NERsystems can yield further improvements.", "categories": ["cs.CL", "cs.LG"], "journal": "COLING, 2145-2158", "citations": "126", "arxiv_url": "http://arxiv.org/abs/1910.11470v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=2301086381693107825&btnI=1&nossl=1&hl=en&oe=ASCII"}, "11": {"ID": 11, "title": "Adversarial Examples for Evaluating Reading Comprehension Systems", "authors": ["Robin Jia", "Percy Liang"], "published": "2017-07-23T18:26:29Z", "updated": "2017-07-23T18:26:29Z", "abstract": "Standard accuracy metrics indicate that reading comprehension systems aremaking rapid progress, but the extent to which these systems truly understandlanguage remains unclear. To reward systems with real language understandingabilities, we propose an adversarial evaluation scheme for the StanfordQuestion Answering Dataset (SQuAD). Our method tests whether systems can answerquestions about paragraphs that contain adversarially inserted sentences, whichare automatically generated to distract computer systems without changing thecorrect answer or misleading humans. In this adversarial setting, the accuracyof sixteen published models drops from an average of $75\\%$ F1 score to $36\\%$;when the adversary is allowed to add ungrammatical sequences of words, averageaccuracy on four models decreases further to $7\\%$. We hope our insights willmotivate the development of new models that understand language more precisely.", "categories": ["cs.CL", "cs.LG"], "journal": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "507", "arxiv_url": "http://arxiv.org/abs/1707.07328v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9214713089423419833&btnI=1&nossl=1&hl=en&oe=ASCII"}, "12": {"ID": 12, "title": "A robust self-learning method for fully unsupervised cross-lingual  mappings of word embeddings", "authors": ["Mikel Artetxe", "Gorka Labaka", "Eneko Agirre"], "published": "2018-05-16T13:23:48Z", "updated": "2018-05-17T17:21:53Z", "abstract": "Recent work has managed to learn cross-lingual word embeddings withoutparallel data by mapping monolingual embeddings to a shared space throughadversarial training. However, their evaluation has focused on favorableconditions, using comparable corpora or closely-related languages, and we showthat they often fail in more realistic scenarios. This work proposes analternative approach based on a fully unsupervised initialization thatexplicitly exploits the structural similarity of the embeddings, and a robustself-learning algorithm that iteratively improves this solution. Our methodsucceeds in all tested scenarios and obtains the best published results instandard datasets, even surpassing previous supervised systems. Ourimplementation is released as an open source project athttps://github.com/artetxem/vecmap", "categories": ["cs.CL", "cs.AI", "cs.LG"], "journal": "Proceedings of the 56th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "223", "arxiv_url": "http://arxiv.org/abs/1805.06297v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=7012967033921106213&btnI=1&nossl=1&hl=en&oe=ASCII"}, "13": {"ID": 13, "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations", "authors": ["Qizhe Xie", "Yiming Yang", "Eduard Hovy", "Hanxiao Liu", "Guokun Lai"], "published": "2017-04-15T19:31:41Z", "updated": "2017-12-05T19:36:03Z", "abstract": "We present RACE, a new dataset for benchmark evaluation of methods in thereading comprehension task. Collected from the English exams for middle andhigh school Chinese students in the age range between 12 to 18, RACE consistsof near 28,000 passages and near 100,000 questions generated by human experts(English instructors), and covers a variety of topics which are carefullydesigned for evaluating the students' ability in understanding and reasoning.In particular, the proportion of questions that requires reasoning is muchlarger in RACE than that in other benchmark datasets for reading comprehension,and there is a significant gap between the performance of the state-of-the-artmodels (43%) and the ceiling human performance (95%). We hope this new datasetcan serve as a valuable resource for research and evaluation in machinecomprehension. The dataset is freely available athttp://www.cs.cmu.edu/~glai1/data/race/ and the code is available athttps://github.com/qizhex/RACE_AR_baselines.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "journal": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "260", "arxiv_url": "http://arxiv.org/abs/1704.04683v5", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=16999749681986953093&btnI=1&nossl=1&hl=en&oe=ASCII"}, "14": {"ID": 14, "title": "Neural AMR: Sequence-to-Sequence Models for Parsing and Generation", "authors": ["Yejin Choi", "Ioannis Konstas", "Luke Zettlemoyer", "Srinivasan Iyer", "Mark Yatskar"], "published": "2017-04-26T23:53:34Z", "updated": "2017-08-18T11:28:05Z", "abstract": "Sequence-to-sequence models have shown strong performance across a broadrange of applications. However, their application to parsing and generatingtext usingAbstract Meaning Representation (AMR)has been limited, due to therelatively limited amount of labeled data and the non-sequential nature of theAMR graphs. We present a novel training procedure that can lift this limitationusing millions of unlabeled sentences and careful preprocessing of the AMRgraphs. For AMR parsing, our model achieves competitive results of 62.1SMATCH,the current best score reported without significant use of external semanticresources. For AMR generation, our model establishes a new state-of-the-artperformance of BLEU 33.8. We present extensive ablative and qualitativeanalysis including strong evidence that sequence-based AMR models are robustagainst ordering variations of graph-to-sequence conversions.", "categories": ["cs.CL"], "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "145", "arxiv_url": "http://arxiv.org/abs/1704.08381v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4497152596276591297&btnI=1&nossl=1&hl=en&oe=ASCII"}, "15": {"ID": 15, "title": "INSIGHT-1 at SemEval-2016 Task 5: Deep Learning for Multilingual  Aspect-based Sentiment Analysis", "authors": ["Parsa Ghaffari", "John G. Breslin", "Sebastian Ruder"], "published": "2016-09-09T11:23:51Z", "updated": "2016-09-22T10:04:18Z", "abstract": "This paper describes our deep learning-based approach to multilingualaspect-based sentiment analysis as part of SemEval 2016 Task 5. We use aconvolutional neural network (CNN) for both aspect extraction and aspect-basedsentiment analysis. We cast aspect extraction as a multi-label classificationproblem, outputting probabilities over aspects parameterized by a threshold. Todetermine the sentiment towards an aspect, we concatenate an aspect vector withevery word embedding and apply a convolution over it. Our constrained system(unconstrained for English) achieves competitive results across all languagesand domains, placing first or second in 5 and 7 out of 11 language-domain pairsfor aspect category detection (slot 1) and sentiment polarity (slot 3)respectively, thereby demonstrating the viability of a deep learning-basedapproach for multilingual aspect-based sentiment analysis.", "categories": ["cs.CL", "cs.LG"], "journal": "Proceedings of the 10th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "77", "arxiv_url": "http://arxiv.org/abs/1609.02748v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=11716705922398054682&btnI=1&nossl=1&hl=en&oe=ASCII"}, "16": {"ID": 16, "title": "Generating Natural Language Adversarial Examples", "authors": ["Bo-Jhang Ho", "Kai-Wei Chang", "Ahmed Elgohary", "Yash Sharma", "Mani Srivastava", "Moustafa Alzantot"], "published": "2018-04-21T17:02:20Z", "updated": "2018-09-24T20:29:35Z", "abstract": "Deep neural networks (DNNs) are vulnerable to adversarial examples,perturbations to correctly classified examples which can cause the model tomisclassify. In the image domain, these perturbations are often virtuallyindistinguishable to human perception, causing humans and state-of-the-artmodels to disagree. However, in the natural language domain, smallperturbations are clearly perceptible, and the replacement of a single word candrastically alter the semantics of the document. Given these challenges, we usea black-box population-based optimization algorithm to generate semanticallyand syntactically similar adversarial examples that fool well-trained sentimentanalysis and textual entailment models with success rates of 97% and 70%,respectively. We additionally demonstrate that 92.3% of the successfulsentiment analysis adversarial examples are classified to their original labelby 20 human annotators, and that the examples are perceptibly quite similar.Finally, we discuss an attempt to use adversarial training as a defense, butfail to yield improvement, demonstrating the strength and diversity of ouradversarial examples. We hope our findings encourage researchers to pursueimproving the robustness of DNNs in the natural language domain.", "categories": ["cs.CL"], "journal": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "134", "arxiv_url": "http://arxiv.org/abs/1804.07998v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=11405633361755931599&btnI=1&nossl=1&hl=en&oe=ASCII"}, "17": {"ID": 17, "title": "Learning to Understand Phrases by Embedding the Dictionary", "authors": ["Anna Korhonen", "Felix Hill", "Yoshua Bengio", "Kyunghyun Cho"], "published": "2015-04-02T13:30:27Z", "updated": "2016-03-22T16:30:17Z", "abstract": "Distributional models that learn rich semantic word representations are asuccess story of recent NLP research. However, developing models that learnuseful representations of phrases and sentences has proved far harder. Wepropose using the definitions found in everyday dictionaries as a means ofbridging this gap between lexical and phrasal semantics. Neural languageembedding models can be effectively trained to map dictionary definitions(phrases) to (lexical) representations of the words defined by thosedefinitions. We present two applications of these architectures: \"reversedictionaries\" that return the name of a concept given a definition ordescription and general-knowledge crossword question answerers. On both tasks,neural language embedding models trained on definitions from a handful offreely-available lexical resources perform as well or better than existingcommercial systems that rely on significant task-specific engineering. Theresults highlight the effectiveness of both neural embedding architectures anddefinition-based training for developing models that understand phrases andsentences.", "categories": ["cs.CL"], "journal": "Transactions of the Association for Computational Linguistics 4, 17-30", "citations": "111", "arxiv_url": "http://arxiv.org/abs/1504.00548v4", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=7466390602252058159&btnI=1&nossl=1&hl=en&oe=ASCII"}, "18": {"ID": 18, "title": "Diachronic word embeddings and semantic shifts: a survey", "authors": ["Lilja \u00d8vrelid", "Erik Velldal", "Terrence Szymanski", "Andrey Kutuzov"], "published": "2018-06-09T20:23:27Z", "updated": "2018-06-13T11:01:18Z", "abstract": "Recent years have witnessed a surge of publications aimed at tracing temporalchanges in lexical semantics using distributional methods, particularlyprediction-based word embedding models. However, this vein of research lacksthe cohesion, common terminology and shared practices of more established areasof natural language processing. In this paper, we survey the current state ofacademic research related to diachronic word embeddings and semantic shiftsdetection. We start with discussing the notion of semantic shifts, and thencontinue with an overview of the existing methods for tracing such time-relatedshifts with word embedding models. We propose several axes along which thesemethods can be compared, and outline the main challenges before this emergingsubfield of NLP, as well as prospects and possible applications.", "categories": ["cs.CL"], "journal": "COLING, 1384-1397", "citations": "74", "arxiv_url": "http://arxiv.org/abs/1806.03537v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=5286018749390625916&btnI=1&nossl=1&hl=en&oe=ASCII"}, "19": {"ID": 19, "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question  Answering", "authors": ["Ruslan Salakhutdinov", "Saizheng Zhang", "Zhilin Yang", "William W. Cohen", "Christopher D. Manning", "Yoshua Bengio", "Peng Qi"], "published": "2018-09-25T17:28:20Z", "updated": "2018-09-25T17:28:20Z", "abstract": "Existing question answering (QA) datasets fail to train QA systems to performcomplex reasoning and provide explanations for answers. We introduce HotpotQA,a new dataset with 113k Wikipedia-based question-answer pairs with four keyfeatures: (1) the questions require finding and reasoning over multiplesupporting documents to answer; (2) the questions are diverse and notconstrained to any pre-existing knowledge bases or knowledge schemas; (3) weprovide sentence-level supporting facts required for reasoning, allowing QAsystems to reason with strong supervision and explain the predictions; (4) weoffer a new type of factoid comparison questions to test QA systems' ability toextract relevant facts and perform necessary comparison. We show that HotpotQAis challenging for the latest QA systems, and the supporting facts enablemodels to improve performance and make explainable predictions.", "categories": ["cs.CL"], "journal": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "207", "arxiv_url": "http://arxiv.org/abs/1809.09600v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14506898183742936331&btnI=1&nossl=1&hl=en&oe=ASCII"}, "20": {"ID": 20, "title": "Attention is not Explanation", "authors": ["Sarthak Jain", "Byron C. Wallace"], "published": "2019-02-26T19:59:15Z", "updated": "2019-05-08T18:05:56Z", "abstract": "Attention mechanisms have seen wide adoption in neural NLP models. Inaddition to improving predictive performance, these are often touted asaffording transparency: models equipped with attention provide a distributionover attended-to input units, and this is often presented (at least implicitly)as communicating the relative importance of inputs. However, it is unclear whatrelationship exists between attention weights and model outputs. In this work,we perform extensive experiments across a variety of NLP tasks that aim toassess the degree to which attention weights provide meaningful `explanations'for predictions. We find that they largely do not. For example, learnedattention weights are frequently uncorrelated with gradient-based measures offeature importance, and one can identify very different attention distributionsthat nonetheless yield equivalent predictions. Our findings show that standardattention modules do not provide meaningful explanations and should not betreated as though they do. Code for all experiments is available athttps://github.com/successar/AttentionExplanation.", "categories": ["cs.CL", "cs.AI"], "journal": "NAACL-HLT (1), 3543-3556", "citations": "145", "arxiv_url": "http://arxiv.org/abs/1902.10186v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14891363743328834570&btnI=1&nossl=1&hl=en&oe=ASCII"}, "21": {"ID": 21, "title": "Tensor Fusion Network for Multimodal Sentiment Analysis", "authors": ["Soujanya Poria", "Louis-Philippe Morency", "Minghai Chen", "Amir Zadeh", "Erik Cambria"], "published": "2017-07-23T05:54:20Z", "updated": "2017-07-23T05:54:20Z", "abstract": "Multimodal sentiment analysis is an increasingly popular research area, whichextends the conventional language-based definition of sentiment analysis to amultimodal setup where other relevant modalities accompany language. In thispaper, we pose the problem of multimodal sentiment analysis as modelingintra-modality and inter-modality dynamics. We introduce a novel model, termedTensor Fusion Network, which learns both such dynamics end-to-end. The proposedapproach is tailored for the volatile nature of spoken language in onlinevideos as well as accompanying gestures and voice. In the experiments, ourmodel outperforms state-of-the-art approaches for both multimodal and unimodalsentiment analysis.", "categories": ["cs.CL"], "journal": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "199", "arxiv_url": "http://arxiv.org/abs/1707.07250v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9429518738344004114&btnI=1&nossl=1&hl=en&oe=ASCII"}, "22": {"ID": 22, "title": "Morphological Inflection Generation Using Character Sequence to Sequence  Learning", "authors": ["Chris Dyer", "Graham Neubig", "Manaal Faruqui", "Yulia Tsvetkov"], "published": "2015-12-18T20:48:26Z", "updated": "2016-03-22T01:02:01Z", "abstract": "Morphological inflection generation is the task of generating the inflectedform of a given lemma corresponding to a particular linguistic transformation.We model the problem of inflection generation as a character sequence tosequence learning problem and present a variant of the neural encoder-decodermodel for solving it. Our model is language independent and can be trained inboth supervised and semi-supervised settings. We evaluate our system on sevendatasets of morphologically rich languages and achieve either better orcomparable results to existing state-of-the-art models of inflectiongeneration.", "categories": ["cs.CL"], "journal": "HLT-NAACL, 634-643", "citations": "112", "arxiv_url": "http://arxiv.org/abs/1512.06110v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=699564305518798938&btnI=1&nossl=1&hl=en&oe=ASCII"}, "23": {"ID": 23, "title": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF", "authors": ["Eduard Hovy", "Xuezhe Ma"], "published": "2016-03-04T05:55:02Z", "updated": "2016-05-29T00:42:15Z", "abstract": "State-of-the-art sequence labeling systems traditionally require largeamounts of task-specific knowledge in the form of hand-crafted features anddata pre-processing. In this paper, we introduce a novel neutral networkarchitecture that benefits from both word- and character-level representationsautomatically, by using combination of bidirectional LSTM, CNN and CRF. Oursystem is truly end-to-end, requiring no feature engineering or datapre-processing, thus making it applicable to a wide range of sequence labelingtasks. We evaluate our system on two data sets for two sequence labeling tasks--- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003corpus for named entity recognition (NER). We obtain state-of-the-artperformance on both the two data --- 97.55\\% accuracy for POS tagging and91.21\\% F1 for NER.", "categories": ["cs.LG", "cs.CL", "stat.ML"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "1349", "arxiv_url": "http://arxiv.org/abs/1603.01354v5", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8126192794758048585&btnI=1&nossl=1&hl=en&oe=ASCII"}, "24": {"ID": 24, "title": "Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases  in Word Embeddings But do not Remove Them", "authors": ["Yoav Goldberg", "Hila Gonen"], "published": "2019-03-09T19:56:47Z", "updated": "2019-09-24T07:43:00Z", "abstract": "Word embeddings are widely used in NLP for a vast range of tasks. It wasshown that word embeddings derived from text corpora reflect gender biases insociety. This phenomenon is pervasive and consistent across different wordembedding models, causing serious concern. Several recent works tackle thisproblem, and propose methods for significantly reducing this gender bias inword embeddings, demonstrating convincing results. However, we argue that thisremoval is superficial. While the bias is indeed substantially reducedaccording to the provided bias definition, the actual effect is mostly hidingthe bias, not removing it. The gender bias information is still reflected inthe distances between \"gender-neutralized\" words in the debiased embeddings,and can be recovered from them. We present a series of experiments to supportthis claim, for two debiasing methods. We conclude that existing bias removaltechniques are insufficient, and should not be trusted for providinggender-neutral modeling.", "categories": ["cs.CL"], "journal": "NAACL-HLT (1), 609-614", "citations": "100", "arxiv_url": "http://arxiv.org/abs/1903.03862v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8954091472837868959&btnI=1&nossl=1&hl=en&oe=ASCII"}, "25": {"ID": 25, "title": "Latent Predictor Networks for Code Generation", "authors": ["Andrew Senior", "Fumin Wang", "Tom\u00e1\u0161 Ko\u010disk\u00fd", "Phil Blunsom", "Wang Ling", "Karl Moritz Hermann", "Edward Grefenstette"], "published": "2016-03-22T11:41:51Z", "updated": "2016-06-08T14:46:00Z", "abstract": "Many language generation tasks require the production of text conditioned onboth structured and unstructured inputs. We present a novel neural networkarchitecture which generates an output sequence conditioned on an arbitrarynumber of input functions. Crucially, our approach allows both the choice ofconditioning context and the granularity of generation, for example charactersor tokens, to be marginalised, thus permitting scalable and effective training.Using this framework, we address the problem of generating programming codefrom a mixed natural language and structured specification. We create two newdata sets for this paradigm derived from the collectible trading card gamesMagic the Gathering and Hearthstone. On these, and a third preexisting corpus,we demonstrate that marginalising multiple predictors allows our model tooutperform strong benchmarks.", "categories": ["cs.CL", "cs.NE"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "183", "arxiv_url": "http://arxiv.org/abs/1603.06744v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=15700819353315373066&btnI=1&nossl=1&hl=en&oe=ASCII"}, "26": {"ID": 26, "title": "Annotation Artifacts in Natural Language Inference Data", "authors": ["Suchin Gururangan", "Samuel R. Bowman", "Omer Levy", "Swabha Swayamdipta", "Noah A. Smith", "Roy Schwartz"], "published": "2018-03-06T18:23:08Z", "updated": "2018-04-16T22:14:06Z", "abstract": "Large-scale datasets for natural language inference are created by presentingcrowd workers with a sentence (premise), and asking them to generate three newsentences (hypotheses) that it entails, contradicts, or is logically neutralwith respect to. We show that, in a significant portion of such data, thisprotocol leaves clues that make it possible to identify the label by lookingonly at the hypothesis, without observing the premise. Specifically, we showthat a simple text categorization model can correctly classify the hypothesisalone in about 67% of SNLI (Bowman et. al, 2015) and 53% of MultiNLI (Williamset. al, 2017). Our analysis reveals that specific linguistic phenomena such asnegation and vagueness are highly correlated with certain inference classes.Our findings suggest that the success of natural language inference models todate has been overestimated, and that the task remains a hard open problem.", "categories": ["cs.CL", "cs.AI"], "journal": "NAACL-HLT (2), 107-112", "citations": "241", "arxiv_url": "http://arxiv.org/abs/1803.02324v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=1317984343761015094&btnI=1&nossl=1&hl=en&oe=ASCII"}, "27": {"ID": 27, "title": "Semi-Supervised Learning for Neural Machine Translation", "authors": ["Maosong Sun", "Yong Cheng", "Zhongjun He", "Hua Wu", "Wei He", "Wei Xu", "Yang Liu"], "published": "2016-06-15T00:22:27Z", "updated": "2016-12-10T20:02:52Z", "abstract": "While end-to-end neural machine translation (NMT) has made remarkableprogress recently, NMT systems only rely on parallel corpora for parameterestimation. Since parallel corpora are usually limited in quantity, quality,and coverage, especially for low-resource languages, it is appealing to exploitmonolingual corpora to improve NMT. We propose a semi-supervised approach fortraining NMT models on the concatenation of labeled (parallel corpora) andunlabeled (monolingual corpora) data. The central idea is to reconstruct themonolingual corpora using an autoencoder, in which the source-to-target andtarget-to-source translation models serve as the encoder and decoder,respectively. Our approach can not only exploit the monolingual corpora of thetarget language, but also of the source language. Experiments on theChinese-English dataset show that our approach achieves significantimprovements over state-of-the-art SMT and NMT systems.", "categories": ["cs.CL"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "142", "arxiv_url": "http://arxiv.org/abs/1606.04596v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6243971294235517477&btnI=1&nossl=1&hl=en&oe=ASCII"}, "28": {"ID": 28, "title": "Named Entity Recognition with Bidirectional LSTM-CNNs", "authors": ["Eric Nichols", "Jason P. C. Chiu"], "published": "2015-11-26T07:40:33Z", "updated": "2016-07-19T05:02:51Z", "abstract": "Named entity recognition is a challenging task that has traditionallyrequired large amounts of knowledge in the form of feature engineering andlexicons to achieve high performance. In this paper, we present a novel neuralnetwork architecture that automatically detects word- and character-levelfeatures using a hybrid bidirectional LSTM and CNN architecture, eliminatingthe need for most feature engineering. We also propose a novel method ofencoding partial lexicon matches in neural networks and compare it to existingapproaches. Extensive evaluation shows that, given only tokenized text andpublicly available word embeddings, our system is competitive on the CoNLL-2003dataset and surpasses the previously reported state of the art performance onthe OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructedfrom publicly-available sources, we establish new state of the art performancewith an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassingsystems that employ heavy feature engineering, proprietary lexicons, and richentity linking information.", "categories": ["cs.CL", "cs.LG", "cs.NE", "68T50", "I.2.7"], "journal": "Transactions of the Association for Computational Linguistics 4, 357-370", "citations": "902", "arxiv_url": "http://arxiv.org/abs/1511.08308v5", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=5826847115500522540&btnI=1&nossl=1&hl=en&oe=ASCII"}, "29": {"ID": 29, "title": "Transition-Based Dependency Parsing with Stack Long Short-Term Memory", "authors": ["Noah A. Smith", "Austin Matthews", "Wang Ling", "Chris Dyer", "Miguel Ballesteros"], "published": "2015-05-29T14:58:12Z", "updated": "2015-05-29T14:58:12Z", "abstract": "We propose a technique for learning representations of parser states intransition-based dependency parsers. Our primary innovation is a new controlstructure for sequence-to-sequence neural networks---the stack LSTM. Like theconventional stack data structures used in transition-based parsing, elementscan be pushed to or popped from the top of the stack in constant time, but, inaddition, an LSTM maintains a continuous space embedding of the stack contents.This lets us formulate an efficient parsing model that captures three facets ofa parser's state: (i) unbounded look-ahead into the buffer of incoming words,(ii) the complete history of actions taken by the parser, and (iii) thecomplete contents of the stack of partially built tree fragments, includingtheir internal structures. Standard backpropagation techniques are used fortraining and yield state-of-the-art parsing performance.", "categories": ["cs.CL", "cs.LG", "cs.NE"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "698", "arxiv_url": "http://arxiv.org/abs/1505.08075v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14180674818440394404&btnI=1&nossl=1&hl=en&oe=ASCII"}, "30": {"ID": 30, "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "authors": ["Nils Reimers", "Iryna Gurevych"], "published": "2019-08-27T08:50:17Z", "updated": "2019-08-27T08:50:17Z", "abstract": "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a newstate-of-the-art performance on sentence-pair regression tasks like semantictextual similarity (STS). However, it requires that both sentences are fed intothe network, which causes a massive computational overhead: Finding the mostsimilar pair in a collection of 10,000 sentences requires about 50 millioninference computations (~65 hours) with BERT. The construction of BERT makes itunsuitable for semantic similarity search as well as for unsupervised taskslike clustering.  In this publication, we present Sentence-BERT (SBERT), a modification of thepretrained BERT network that use siamese and triplet network structures toderive semantically meaningful sentence embeddings that can be compared usingcosine-similarity. This reduces the effort for finding the most similar pairfrom 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, whilemaintaining the accuracy from BERT.  We evaluate SBERT and SRoBERTa on common STS tasks and transfer learningtasks, where it outperforms other state-of-the-art sentence embeddings methods.", "categories": ["cs.CL"], "journal": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "125", "arxiv_url": "http://arxiv.org/abs/1908.10084v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=12599223809118664426&btnI=1&nossl=1&hl=en&oe=ASCII"}, "31": {"ID": 31, "title": "When is multitask learning effective? Semantic sequence prediction under  varying data conditions", "authors": ["Barbara Plank", "H\u00e9ctor Mart\u00ednez Alonso"], "published": "2016-12-07T14:03:15Z", "updated": "2017-01-10T18:26:26Z", "abstract": "Multitask learning has been applied successfully to a range of tasks, mostlymorphosyntactic. However, little is known on when MTL works and whether thereare data characteristics that help to determine its success. In this paper weevaluate a range of semantic sequence labeling tasks in a MTL setup. We examinedifferent auxiliary tasks, amongst which a novel setup, and correlate theirimpact to data-dependent conditions. Our results show that MTL is not alwayseffective, significant improvements are obtained only for 1 out of 5 tasks.When successful, auxiliary tasks with compact and more uniform labeldistributions are preferable.", "categories": ["cs.CL"], "journal": "EACL (1), 44-53", "citations": "119", "arxiv_url": "http://arxiv.org/abs/1612.02251v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14260579991842397906&btnI=1&nossl=1&hl=en&oe=ASCII"}, "32": {"ID": 32, "title": "Improving Coreference Resolution by Learning Entity-Level Distributed  Representations", "authors": ["Christopher D. Manning", "Kevin Clark"], "published": "2016-06-04T04:08:45Z", "updated": "2016-06-08T21:11:13Z", "abstract": "A long-standing challenge in coreference resolution has been theincorporation of entity-level information - features defined over clusters ofmentions instead of mention pairs. We present a neural network basedcoreference system that produces high-dimensional vector representations forpairs of coreference clusters. Using these representations, our system learnswhen combining clusters is desirable. We train the system with alearning-to-search algorithm that teaches it which local decisions (clustermerges) will lead to a high-scoring final coreference partition. The systemsubstantially outperforms the current state-of-the-art on the English andChinese portions of the CoNLL 2012 Shared Task dataset despite using fewhand-engineered features.", "categories": ["cs.CL"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "187", "arxiv_url": "http://arxiv.org/abs/1606.01323v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=12531502908806328230&btnI=1&nossl=1&hl=en&oe=ASCII"}, "33": {"ID": 33, "title": "Zero-Resource Translation with Multi-Lingual Neural Machine Translation", "authors": ["Orhan Firat", "Baskaran Sankaran", "Kyunghyun Cho", "Fatos T. Yarman Vural", "Yaser Al-Onaizan"], "published": "2016-06-13T22:40:33Z", "updated": "2016-06-13T22:40:33Z", "abstract": "In this paper, we propose a novel finetuning algorithm for the recentlyintroduced multi-way, mulitlingual neural machine translate that enableszero-resource machine translation. When used together with novel many-to-onetranslation strategies, we empirically show that this finetuning algorithmallows the multi-way, multilingual model to translate a zero-resource languagepair (1) as well as a single-pair neural translation model trained with up to1M direct parallel sentences of the same language pair and (2) better thanpivot-based translation strategy, while keeping only one additional copy ofattention-related parameters.", "categories": ["cs.CL"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "122", "arxiv_url": "http://arxiv.org/abs/1606.04164v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9699063558012530354&btnI=1&nossl=1&hl=en&oe=ASCII"}, "34": {"ID": 34, "title": "Sequence to Backward and Forward Sequences: A Content-Introducing  Approach to Generative Short-Text Conversation", "authors": ["Lili Mou", "Lu Zhang", "Ge Li", "Yiping Song", "Zhi Jin", "Rui Yan"], "published": "2016-07-04T17:42:52Z", "updated": "2016-10-13T07:40:37Z", "abstract": "Using neural networks to generate replies in human-computer dialogue systemsis attracting increasing attention over the past few years. However, theperformance is not satisfactory: the neural network tends to generate safe,universally relevant replies which carry little meaning. In this paper, wepropose a content-introducing approach to neural network-based generativedialogue systems. We first use pointwise mutual information (PMI) to predict anoun as a keyword, reflecting the main gist of the reply. We then proposeseq2BF, a \"sequence to backward and forward sequences\" model, which generates areply containing the given keyword. Experimental results show that our approachsignificantly outperforms traditional sequence-to-sequence models in terms ofhuman evaluation and the entropy measure, and that the predicted keyword canappear at an appropriate position in the reply.", "categories": ["cs.CL", "cs.LG"], "journal": "COLING, 3349-3358", "citations": "152", "arxiv_url": "http://arxiv.org/abs/1607.00970v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=15322306336642377359&btnI=1&nossl=1&hl=en&oe=ASCII"}, "35": {"ID": 35, "title": "Neural Approaches to Conversational AI", "authors": ["Michel Galley", "Lihong Li", "Jianfeng Gao"], "published": "2018-09-21T18:42:24Z", "updated": "2019-09-10T04:56:51Z", "abstract": "The present paper surveys neural approaches to conversational AI that havebeen developed in the last few years. We group conversational systems intothree categories: (1) question answering agents, (2) task-oriented dialogueagents, and (3) chatbots. For each category, we present a review ofstate-of-the-art neural approaches, draw the connection between them andtraditional approaches, and discuss the progress that has been made andchallenges still being faced, using specific systems and models as casestudies.", "categories": ["cs.CL"], "journal": "ACL (5), 2-7", "citations": "201", "arxiv_url": "http://arxiv.org/abs/1809.08267v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=7792233755917729361&btnI=1&nossl=1&hl=en&oe=ASCII"}, "36": {"ID": 36, "title": "Adversarial Example Generation with Syntactically Controlled Paraphrase  Networks", "authors": ["Luke Zettlemoyer", "John Wieting", "Mohit Iyyer", "Kevin Gimpel"], "published": "2018-04-17T06:12:36Z", "updated": "2018-04-17T06:12:36Z", "abstract": "We propose syntactically controlled paraphrase networks (SCPNs) and use themto generate adversarial examples. Given a sentence and a target syntactic form(e.g., a constituency parse), SCPNs are trained to produce a paraphrase of thesentence with the desired syntax. We show it is possible to create trainingdata for this task by first doing backtranslation at a very large scale, andthen using a parser to label the syntactic transformations that naturally occurduring this process. Such data allows us to train a neural encoder-decodermodel with extra inputs to specify the target syntax. A combination ofautomated and human evaluations show that SCPNs generate paraphrases thatfollow their target specifications without decreasing paraphrase quality whencompared to baseline (uncontrolled) paraphrase systems. Furthermore, they aremore capable of generating syntactically adversarial examples that both (1)\"fool\" pretrained models and (2) improve the robustness of these models tosyntactic variation when used to augment their training data.", "categories": ["cs.CL"], "journal": "NAACL-HLT, 1875-1885", "citations": "147", "arxiv_url": "http://arxiv.org/abs/1804.06059v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=790591304759926903&btnI=1&nossl=1&hl=en&oe=ASCII"}, "37": {"ID": 37, "title": "One Vector is Not Enough: Entity-Augmented Distributional Semantics for  Discourse Relations", "authors": ["Jacob Eisenstein", "Yangfeng Ji"], "published": "2014-11-25T01:25:56Z", "updated": "2014-11-25T01:25:56Z", "abstract": "Discourse relations bind smaller linguistic units into coherent texts.However, automatically identifying discourse relations is difficult, because itrequires understanding the semantics of the linked arguments. A more subtlechallenge is that it is not enough to represent the meaning of each argument ofa discourse relation, because the relation may depend on links betweenlower-level components, such as entity mentions. Our solution computesdistributional meaning representations by composition up the syntactic parsetree. A key difference from previous work on compositional distributionalsemantics is that we also compute representations for entity mentions, using anovel downward compositional pass. Discourse relations are predicted from thedistributional representations of the arguments, and also of their coreferententity mentions. The resulting system obtains substantial improvements over theprevious state-of-the-art in predicting implicit discourse relations in thePenn Discourse Treebank.", "categories": ["cs.CL", "cs.LG"], "journal": "Transactions of the Association for Computational Linguistics 3, 329-344", "citations": "99", "arxiv_url": "http://arxiv.org/abs/1411.6699v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=3432540290006020622&btnI=1&nossl=1&hl=en&oe=ASCII"}, "38": {"ID": 38, "title": "SemEval-2019 Task 6: Identifying and Categorizing Offensive Language in  Social Media (OffensEval)", "authors": ["Sara Rosenthal", "Noura Farra", "Marcos Zampieri", "Preslav Nakov", "Ritesh Kumar", "Shervin Malmasi"], "published": "2019-03-19T20:22:02Z", "updated": "2019-04-27T02:05:15Z", "abstract": "We present the results and the main findings of SemEval-2019 Task 6 onIdentifying and Categorizing Offensive Language in Social Media (OffensEval).The task was based on a new dataset, the Offensive Language IdentificationDataset (OLID), which contains over 14,000 English tweets. It featured threesub-tasks. In sub-task A, the goal was to discriminate between offensive andnon-offensive posts. In sub-task B, the focus was on the type of offensivecontent in the post. Finally, in sub-task C, systems had to detect the targetof the offensive posts. OffensEval attracted a large number of participants andit was one of the most popular tasks in SemEval-2019. In total, about 800 teamssigned up to participate in the task, and 115 of them submitted results, whichwe present and analyze in this report.", "categories": ["cs.CL"], "journal": "Proceedings of the 13th International Workshop on Semantic Evaluation, 75-86", "citations": "124", "arxiv_url": "http://arxiv.org/abs/1903.08983v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=7246495782071034011&btnI=1&nossl=1&hl=en&oe=ASCII"}, "39": {"ID": 39, "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for  Reading Comprehension", "authors": ["Luke Zettlemoyer", "Daniel S. Weld", "Mandar Joshi", "Eunsol Choi"], "published": "2017-05-09T21:35:07Z", "updated": "2017-05-13T21:12:37Z", "abstract": "We present TriviaQA, a challenging reading comprehension dataset containingover 650K question-answer-evidence triples. TriviaQA includes 95Kquestion-answer pairs authored by trivia enthusiasts and independently gatheredevidence documents, six per question on average, that provide high qualitydistant supervision for answering the questions. We show that, in comparison toother recently introduced large-scale datasets, TriviaQA (1) has relativelycomplex, compositional questions, (2) has considerable syntactic and lexicalvariability between questions and corresponding answer-evidence sentences, and(3) requires more cross sentence reasoning to find answers. We also present twobaseline algorithms: a feature-based classifier and a state-of-the-art neuralnetwork, that performs well on SQuAD reading comprehension. Neither approachcomes close to human performance (23% and 40% vs. 80%), suggesting thatTriviaQA is a challenging testbed that is worth significant future study. Dataand code available at -- http://nlp.cs.washington.edu/triviaqa/", "categories": ["cs.CL"], "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "379", "arxiv_url": "http://arxiv.org/abs/1705.03551v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=5079829668576957986&btnI=1&nossl=1&hl=en&oe=ASCII"}, "40": {"ID": 40, "title": "Automated Fact Checking: Task formulations, methods and future  directions", "authors": ["James Thorne", "Andreas Vlachos"], "published": "2018-06-20T12:13:53Z", "updated": "2018-09-05T12:47:39Z", "abstract": "The recently increased focus on misinformation has stimulated research infact checking, the task of assessing the truthfulness of a claim. Research inautomating this task has been conducted in a variety of disciplines includingnatural language processing, machine learning, knowledge representation,databases, and journalism. While there has been substantial progress, relevantpapers and articles have been published in research communities that are oftenunaware of each other and use inconsistent terminology, thus impedingunderstanding and further progress. In this paper we survey automated factchecking research stemming from natural language processing and relateddisciplines, unifying the task formulations and methodologies across papers andauthors. Furthermore, we highlight the use of evidence as an importantdistinguishing factor among them cutting across task formulations and methods.We conclude with proposing avenues for future NLP research on automated factchecking.", "categories": ["cs.CL"], "journal": "COLING, 3346-3359", "citations": "61", "arxiv_url": "http://arxiv.org/abs/1806.07687v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=11735117455517539951&btnI=1&nossl=1&hl=en&oe=ASCII"}, "41": {"ID": 41, "title": "The Web as a Knowledge-base for Answering Complex Questions", "authors": ["Jonathan Berant", "Alon Talmor"], "published": "2018-03-18T11:28:12Z", "updated": "2018-03-18T11:28:12Z", "abstract": "Answering complex questions is a time-consuming activity for humans thatrequires reasoning and integration of information. Recent work on readingcomprehension made headway in answering simple questions, but tackling complexquestions is still an ongoing research challenge. Conversely, semantic parsershave been successful at handling compositionality, but only when theinformation resides in a target knowledge-base. In this paper, we present anovel framework for answering broad and complex questions, assuming answeringsimple questions is possible using a search engine and a reading comprehensionmodel. We propose to decompose complex questions into a sequence of simplequestions, and compute the final answer from the sequence of answers. Toillustrate the viability of our approach, we create a new dataset of complexquestions, ComplexWebQuestions, and present a model that decomposes questionsand interacts with the web to compute an answer. We empirically demonstratethat question decomposition improves performance from 20.8 precision@1 to 27.5precision@1 on this new dataset.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "journal": "NAACL-HLT, 641-651", "citations": "95", "arxiv_url": "http://arxiv.org/abs/1803.06643v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=18350371657665734071&btnI=1&nossl=1&hl=en&oe=ASCII"}, "42": {"ID": 42, "title": "Fast and Accurate Entity Recognition with Iterated Dilated Convolutions", "authors": ["David Belanger", "Patrick Verga", "Andrew McCallum", "Emma Strubell"], "published": "2017-02-07T16:58:18Z", "updated": "2017-07-22T04:04:30Z", "abstract": "Today when many practitioners run basic NLP on the entire web andlarge-volume traffic, faster methods are paramount to saving time and energycosts. Recent advances in GPU hardware have led to the emergence ofbi-directional LSTMs as a standard method for obtaining per-token vectorrepresentations serving as input to labeling tasks such as NER (often followedby prediction in a linear-chain CRF). Though expressive and accurate, thesemodels fail to fully exploit GPU parallelism, limiting their computationalefficiency. This paper proposes a faster alternative to Bi-LSTMs for NER:Iterated Dilated Convolutional Neural Networks (ID-CNNs), which have bettercapacity than traditional CNNs for large context and structured prediction.Unlike LSTMs whose sequential processing on sentences of length N requires O(N)time even in the face of parallelism, ID-CNNs permit fixed-depth convolutionsto run in parallel across entire documents. We describe a distinct combinationof network structure, parameter sharing and training procedures that enabledramatic 14-20x test-time speedups while retaining accuracy comparable to theBi-LSTM-CRF. Moreover, ID-CNNs trained to aggregate context from the entiredocument are even more accurate while maintaining 8x faster test time speeds.", "categories": ["cs.CL"], "journal": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "136", "arxiv_url": "http://arxiv.org/abs/1702.02098v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=15442679972273025553&btnI=1&nossl=1&hl=en&oe=ASCII"}, "43": {"ID": 43, "title": "Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term  Memory Models and Auxiliary Loss", "authors": ["Yoav Goldberg", "Barbara Plank", "Anders S\u00f8gaard"], "published": "2016-04-19T11:53:09Z", "updated": "2016-07-21T08:17:43Z", "abstract": "Bidirectional long short-term memory (bi-LSTM) networks have recently provensuccessful for various NLP sequence modeling tasks, but little is known abouttheir reliance to input representations, target languages, data set size, andlabel noise. We address these issues and evaluate bi-LSTMs with word,character, and unicode byte embeddings for POS tagging. We compare bi-LSTMs totraditional POS taggers across languages and data sizes. We also present anovel bi-LSTM model, which combines the POS tagging loss function with anauxiliary loss function that accounts for rare words. The model obtainsstate-of-the-art performance across 22 languages, and works especially well formorphologically complex languages. Our analysis suggests that bi-LSTMs are lesssensitive to training data size and label corruptions (at small noise levels)than previously assumed.", "categories": ["cs.CL"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "285", "arxiv_url": "http://arxiv.org/abs/1604.05529v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=12369467984060743191&btnI=1&nossl=1&hl=en&oe=ASCII"}, "44": {"ID": 44, "title": "Improved Semantic Representations From Tree-Structured Long Short-Term  Memory Networks", "authors": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning"], "published": "2015-02-28T06:31:50Z", "updated": "2015-05-30T06:51:20Z", "abstract": "Because of their superior ability to preserve sequence information over time,Long Short-Term Memory (LSTM) networks, a type of recurrent neural network witha more complex computational unit, have obtained strong results on a variety ofsequence modeling tasks. The only underlying LSTM structure that has beenexplored so far is a linear chain. However, natural language exhibits syntacticproperties that would naturally combine words to phrases. We introduce theTree-LSTM, a generalization of LSTMs to tree-structured network topologies.Tree-LSTMs outperform all existing systems and strong LSTM baselines on twotasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task1) and sentiment classification (Stanford Sentiment Treebank).", "categories": ["cs.CL", "cs.AI", "cs.LG"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "1943", "arxiv_url": "http://arxiv.org/abs/1503.00075v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9111909349437359339&btnI=1&nossl=1&hl=en&oe=ASCII"}, "45": {"ID": 45, "title": "A Diversity-Promoting Objective Function for Neural Conversation Models", "authors": ["Chris Brockett", "Jianfeng Gao", "Jiwei Li", "Michel Galley", "Bill Dolan"], "published": "2015-10-11T14:04:57Z", "updated": "2016-06-10T22:03:28Z", "abstract": "Sequence-to-sequence neural network models for generation of conversationalresponses tend to generate safe, commonplace responses (e.g., \"I don't know\")regardless of the input. We suggest that the traditional objective function,i.e., the likelihood of output (response) given input (message) is unsuited toresponse generation tasks. Instead we propose using Maximum Mutual Information(MMI) as the objective function in neural models. Experimental resultsdemonstrate that the proposed MMI models produce more diverse, interesting, andappropriate responses, yielding substantive gains in BLEU scores on twoconversational datasets and in human evaluations.", "categories": ["cs.CL"], "journal": "HLT-NAACL, 110-119", "citations": "791", "arxiv_url": "http://arxiv.org/abs/1510.03055v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6301775661806157248&btnI=1&nossl=1&hl=en&oe=ASCII"}, "46": {"ID": 46, "title": "Improved Relation Classification by Deep Recurrent Neural Networks with  Data Augmentation", "authors": ["Lili Mou", "Yangyang Lu", "Ran Jia", "Yunchuan Chen", "Ge Li", "Zhi Jin", "Yan Xu"], "published": "2016-01-14T16:30:41Z", "updated": "2016-10-13T07:11:46Z", "abstract": "Nowadays, neural networks play an important role in the task of relationclassification. By designing different neural architectures, researchers haveimproved the performance to a large extent in comparison with traditionalmethods. However, existing neural networks for relation classification areusually of shallow architectures (e.g., one-layer convolutional neural networksor recurrent networks). They may fail to explore the potential representationspace in different abstraction levels. In this paper, we propose deep recurrentneural networks (DRNNs) for relation classification to tackle this challenge.Further, we propose a data augmentation method by leveraging the directionalityof relations. We evaluated our DRNNs on the SemEval-2010 Task~8, and achieve anF1-score of 86.1%, outperforming previous state-of-the-art recorded results.", "categories": ["cs.CL", "cs.LG"], "journal": "COLING, 1461-1470", "citations": "127", "arxiv_url": "http://arxiv.org/abs/1601.03651v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=16373480206076190875&btnI=1&nossl=1&hl=en&oe=ASCII"}, "47": {"ID": 47, "title": "Hypernyms under Siege: Linguistically-motivated Artillery for Hypernymy  Detection", "authors": ["Vered Shwartz", "Enrico Santus", "Dominik Schlechtweg"], "published": "2016-12-14T02:28:29Z", "updated": "2017-01-08T08:41:18Z", "abstract": "The fundamental role of hypernymy in NLP has motivated the development ofmany methods for the automatic identification of this relation, most of whichrely on word distribution. We investigate an extensive number of suchunsupervised measures, using several distributional semantic models that differby context type and feature weighting. We analyze the performance of thedifferent methods based on their linguistic motivation. Comparison to thestate-of-the-art supervised methods shows that while supervised methodsgenerally outperform the unsupervised ones, the former are sensitive to thedistribution of training instances, hurting their reliability. Being based ongeneral linguistic hypotheses and independent from training data, unsupervisedmeasures are more robust, and therefore are still useful artillery forhypernymy detection.", "categories": ["cs.CL"], "journal": "EACL (1), 65-75", "citations": "68", "arxiv_url": "http://arxiv.org/abs/1612.04460v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6364994673394436091&btnI=1&nossl=1&hl=en&oe=ASCII"}, "48": {"ID": 48, "title": "Language to Logical Form with Neural Attention", "authors": ["Mirella Lapata", "Li Dong"], "published": "2016-01-06T19:13:12Z", "updated": "2016-06-06T21:06:55Z", "abstract": "Semantic parsing aims at mapping natural language to machine interpretablemeaning representations. Traditional approaches rely on high-quality lexicons,manually-built templates, and linguistic features which are either domain- orrepresentation-specific. In this paper we present a general method based on anattention-enhanced encoder-decoder model. We encode input utterances intovector representations, and generate their logical forms by conditioning theoutput sequences or trees on the encoding vectors. Experimental results on fourdatasets show that our approach performs competitively without usinghand-engineered features and is easy to adapt across domains and meaningrepresentations.", "categories": ["cs.CL"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "355", "arxiv_url": "http://arxiv.org/abs/1601.01280v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=76161465233085418&btnI=1&nossl=1&hl=en&oe=ASCII"}, "49": {"ID": 49, "title": "An Incremental Parser for Abstract Meaning Representation", "authors": ["Marco Damonte", "Giorgio Satta", "Shay B. Cohen"], "published": "2016-08-22T10:30:18Z", "updated": "2017-04-10T14:18:14Z", "abstract": "Meaning Representation (AMR) is a semantic representation for naturallanguage that embeds annotations related to traditional tasks such as namedentity recognition, semantic role labeling, word sense disambiguation andco-reference resolution. We describe a transition-based parser for AMR thatparses sentences left-to-right, in linear time. We further propose a test-suitethat assesses specific subtasks that are helpful in comparing AMR parsers, andshow that our parser is competitive with the state of the art on the LDC2015E86dataset and that it outperforms state-of-the-art parsers for recovering namedentities and handling polarity.", "categories": ["cs.CL"], "journal": "EACL (1), 536-546", "citations": "80", "arxiv_url": "http://arxiv.org/abs/1608.06111v5", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8681101353022763612&btnI=1&nossl=1&hl=en&oe=ASCII"}, "50": {"ID": 50, "title": "Subword Regularization: Improving Neural Network Translation Models with  Multiple Subword Candidates", "authors": ["Taku Kudo"], "published": "2018-04-29T16:13:44Z", "updated": "2018-04-29T16:13:44Z", "abstract": "Subword units are an effective way to alleviate the open vocabulary problemsin neural machine translation (NMT). While sentences are usually converted intounique subword sequences, subword segmentation is potentially ambiguous andmultiple segmentations are possible even with the same vocabulary. The questionaddressed in this paper is whether it is possible to harness the segmentationambiguity as a noise to improve the robustness of NMT. We present a simpleregularization method, subword regularization, which trains the model withmultiple subword segmentations probabilistically sampled during training. Inaddition, for better subword sampling, we propose a new subword segmentationalgorithm based on a unigram language model. We experiment with multiplecorpora and report consistent improvements especially on low resource andout-of-domain settings.", "categories": ["cs.CL"], "journal": "Proceedings of the 56th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "173", "arxiv_url": "http://arxiv.org/abs/1804.10959v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10996996628614665108&btnI=1&nossl=1&hl=en&oe=ASCII"}, "51": {"ID": 51, "title": "Transfer Learning for Low-Resource Neural Machine Translation", "authors": ["Jonathan May", "Barret Zoph", "Deniz Yuret", "Kevin Knight"], "published": "2016-04-08T00:16:35Z", "updated": "2016-04-08T00:16:35Z", "abstract": "The encoder-decoder framework for neural machine translation (NMT) has beenshown effective in large data scenarios, but is much less effective forlow-resource languages. We present a transfer learning method thatsignificantly improves Bleu scores across a range of low-resource languages.Our key idea is to first train a high-resource language pair (the parentmodel), then transfer some of the learned parameters to the low-resource pair(the child model) to initialize and constrain training. Using our transferlearning method we improve baseline NMT models by an average of 5.6 Bleu onfour low-resource language pairs. Ensembling and unknown word replacement addanother 2 Bleu which brings the NMT performance on low-resource machinetranslation close to a strong syntax based machine translation (SBMT) system,exceeding its performance on one language pair. Additionally, using thetransfer learning model for re-scoring, we can improve the SBMT system by anaverage of 1.3 Bleu, improving the state-of-the-art on low-resource machinetranslation.", "categories": ["cs.CL"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "316", "arxiv_url": "http://arxiv.org/abs/1604.02201v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10126416754494258051&btnI=1&nossl=1&hl=en&oe=ASCII"}, "52": {"ID": 52, "title": "Understanding Back-Translation at Scale", "authors": ["David Grangier", "Michael Auli", "Myle Ott", "Sergey Edunov"], "published": "2018-08-28T16:05:40Z", "updated": "2018-10-03T01:42:36Z", "abstract": "An effective method to improve neural machine translation with monolingualdata is to augment the parallel training corpus with back-translations oftarget language sentences. This work broadens the understanding ofback-translation and investigates a number of methods to generate syntheticsource sentences. We find that in all but resource poor settingsback-translations obtained via sampling or noised beam outputs are mosteffective. Our analysis shows that sampling or noisy synthetic data gives amuch stronger training signal than data generated by beam or greedy search. Wealso compare how synthetic data compares to genuine bitext and study variousdomain effects. Finally, we scale to hundreds of millions of monolingualsentences and achieve a new state of the art of 35 BLEU on the WMT'14English-German test set.", "categories": ["cs.CL"], "journal": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "222", "arxiv_url": "http://arxiv.org/abs/1808.09381v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=5388849145974890035&btnI=1&nossl=1&hl=en&oe=ASCII"}, "53": {"ID": 53, "title": "Generating Natural Questions About an Image", "authors": ["Nasrin Mostafazadeh", "Lucy Vanderwende", "Ishan Misra", "Xiaodong He", "Jacob Devlin", "Margaret Mitchell"], "published": "2016-03-19T07:27:15Z", "updated": "2016-06-09T01:20:49Z", "abstract": "There has been an explosion of work in the vision &amp; language community duringthe past few years from image captioning to video transcription, and answeringquestions about images. These tasks have focused on literal descriptions of theimage. To move beyond the literal, we choose to explore how questions about animage are often directed at commonsense inference and the abstract eventsevoked by objects in the image. In this paper, we introduce the novel task ofVisual Question Generation (VQG), where the system is tasked with asking anatural and engaging question when shown an image. We provide three datasetswhich cover a variety of images from object-centric to event-centric, withconsiderably more abstract training data than provided to state-of-the-artcaptioning systems thus far. We train and test several generative and retrievalmodels to tackle the task of VQG. Evaluation results show that while suchmodels ask reasonable questions for a variety of images, there is still a widegap with human performance which motivates further work on connecting imageswith commonsense knowledge and pragmatics. Our proposed task offers a newchallenge to the community which we hope furthers interest in exploring deeperconnections between vision &amp; language.", "categories": ["cs.CL", "cs.AI", "cs.CV"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "157", "arxiv_url": "http://arxiv.org/abs/1603.06059v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4484714299067295364&btnI=1&nossl=1&hl=en&oe=ASCII"}, "54": {"ID": 54, "title": "Personalizing Dialogue Agents: I have a dog, do you have pets too?", "authors": ["Saizheng Zhang", "Emily Dinan", "Arthur Szlam", "Jack Urbanek", "Jason Weston", "Douwe Kiela"], "published": "2018-01-22T18:58:18Z", "updated": "2018-09-25T18:55:07Z", "abstract": "Chit-chat models are known to have several problems: they lack specificity,do not display a consistent personality and are often not very captivating. Inthis work we present the task of making chit-chat more engaging by conditioningon profile information. We collect data and train models to (i) condition ontheir given profile information; and (ii) information about the person they aretalking to, resulting in improved dialogues, as measured by next utteranceprediction. Since (ii) is initially unknown our model is trained to engage itspartner with personal topics, and we show the resulting dialogue can be used topredict profile information about the interlocutors.", "categories": ["cs.AI", "cs.CL"], "journal": "Proceedings of the 56th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "267", "arxiv_url": "http://arxiv.org/abs/1801.07243v5", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=3549630893257499646&btnI=1&nossl=1&hl=en&oe=ASCII"}, "55": {"ID": 55, "title": "A Strong Baseline for Learning Cross-Lingual Word Embeddings from  Sentence Alignments", "authors": ["Omer Levy", "Anders S\u00f8gaard", "Yoav Goldberg"], "published": "2016-08-18T20:27:46Z", "updated": "2017-01-09T20:49:18Z", "abstract": "While cross-lingual word embeddings have been studied extensively in recentyears, the qualitative differences between the different algorithms remainvague. We observe that whether or not an algorithm uses a particular featureset (sentence IDs) accounts for a significant performance gap among thesealgorithms. This feature set is also used by traditional alignment algorithms,such as IBM Model-1, which demonstrate similar performance to state-of-the-artembedding algorithms on a variety of benchmarks. Overall, we observe thatdifferent algorithmic approaches for utilizing the sentence ID feature spaceresult in similar performance. This paper draws both empirical and theoreticalparallels between the embedding and alignment literature, and suggests thatadding additional sources of information, which go beyond the traditionalsignal of bilingual sentence-aligned corpora, may substantially improvecross-lingual word embeddings, and that future baselines should at least takesuch features into account.", "categories": ["cs.CL"], "journal": "EACL (1), 765-774", "citations": "48", "arxiv_url": "http://arxiv.org/abs/1608.05426v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6026870361052767685&btnI=1&nossl=1&hl=en&oe=ASCII"}, "56": {"ID": 56, "title": "Compression of Neural Machine Translation Models via Pruning", "authors": ["Minh-Thang Luong", "Abigail See", "Christopher D. Manning"], "published": "2016-06-29T20:36:23Z", "updated": "2016-06-29T20:36:23Z", "abstract": "Neural Machine Translation (NMT), like many other deep learning domains,typically suffers from over-parameterization, resulting in large storage sizes.This paper examines three simple magnitude-based pruning schemes to compressNMT models, namely class-blind, class-uniform, and class-distribution, whichdiffer in terms of how pruning thresholds are computed for the differentclasses of weights in the NMT architecture. We demonstrate the efficacy ofweight pruning as a compression technique for a state-of-the-art NMT system. Weshow that an NMT model with over 200 million parameters can be pruned by 40%with very little performance loss as measured on the WMT'14 English-Germantranslation task. This sheds light on the distribution of redundancy in the NMTarchitecture. Our main result is that with retraining, we can recover and evensurpass the original performance with an 80%-pruned model.", "categories": ["cs.AI", "cs.CL", "cs.NE"], "journal": "CoNLL, 291-301", "citations": "89", "arxiv_url": "http://arxiv.org/abs/1606.09274v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=13072353668416361496&btnI=1&nossl=1&hl=en&oe=ASCII"}, "57": {"ID": 57, "title": "CoNLL-SIGMORPHON 2017 Shared Task: Universal Morphological Reinflection  in 52 Languages", "authors": ["Manaal Faruqui", "David Yarowsky", "Patrick Xia", "G\u00e9raldine Walther", "Sandra K\u00fcbler", "Jason Eisner", "Ekaterina Vylomova", "Ryan Cotterell", "Christo Kirov", "Mans Hulden", "John Sylak-Glassman"], "published": "2017-06-27T20:02:34Z", "updated": "2017-07-04T18:12:34Z", "abstract": "The CoNLL-SIGMORPHON 2017 shared task on supervised morphological generationrequired systems to be trained and tested in each of 52 typologically diverselanguages. In sub-task 1, submitted systems were asked to predict a specificinflected form of a given lemma. In sub-task 2, systems were given a lemma andsome of its specific inflected forms, and asked to complete the inflectionalparadigm by predicting all of the remaining inflected forms. Both sub-tasksincluded high, medium, and low-resource conditions. Sub-task 1 received 24system submissions, while sub-task 2 received 3 system submissions. Followingthe success of neural sequence-to-sequence models in the SIGMORPHON 2016 sharedtask, all but one of the submissions included a neural component. The resultsshow that high performance can be achieved with small training datasets, solong as models have appropriate inductive bias or make use of additionalunlabeled data or synthetic data. However, different biasing and dataaugmentation resulted in disjoint sets of inflected forms being predictedcorrectly, suggesting that there is room for future improvement.", "categories": ["cs.CL"], "journal": "CoNLL Shared Task (1), 1-30", "citations": "108", "arxiv_url": "http://arxiv.org/abs/1706.09031v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=13291277762723173318&btnI=1&nossl=1&hl=en&oe=ASCII"}, "58": {"ID": 58, "title": "Compositional Vector Space Models for Knowledge Base Completion", "authors": ["Arvind Neelakantan", "Andrew McCallum", "Benjamin Roth"], "published": "2015-04-24T23:06:10Z", "updated": "2015-05-27T21:23:45Z", "abstract": "Knowledge base (KB) completion adds new facts to a KB by making inferencesfrom existing facts, for example by inferring with high likelihoodnationality(X,Y) from bornIn(X,Y). Most previous methods infer simple one-hoprelational synonyms like this, or use as evidence a multi-hop relational pathtreated as an atomic feature, like bornIn(X,Z) -&gt; containedIn(Z,Y). This paperpresents an approach that reasons about conjunctions of multi-hop relationsnon-atomically, composing the implications of a path using a recursive neuralnetwork (RNN) that takes as inputs vector embeddings of the binary relation inthe path. Not only does this allow us to generalize to paths unseen at trainingtime, but also, with a single high-capacity RNN, to predict new relation typesnot seen when the compositional model was trained (zero-shot learning). Weassemble a new dataset of over 52M relational triples, and show that our methodimproves over a traditional classifier by 11%, and a method leveragingpre-trained embeddings by 7%.", "categories": ["cs.CL", "stat.ML"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "223", "arxiv_url": "http://arxiv.org/abs/1504.06662v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=12085078841033947995&btnI=1&nossl=1&hl=en&oe=ASCII"}, "59": {"ID": 59, "title": "Neural Symbolic Machines: Learning Semantic Parsers on Freebase with  Weak Supervision", "authors": ["Ni Lao", "Kenneth D. Forbus", "Quoc Le", "Jonathan Berant", "Chen Liang"], "published": "2016-10-31T20:07:23Z", "updated": "2017-04-23T07:16:13Z", "abstract": "Harnessing the statistical power of neural networks to perform languageunderstanding and symbolic reasoning is difficult, when it requires executingefficient discrete operations against a large knowledge-base. In this work, weintroduce a Neural Symbolic Machine, which contains (a) a neural \"programmer\",i.e., a sequence-to-sequence model that maps language utterances to programsand utilizes a key-variable memory to handle compositionality (b) a symbolic\"computer\", i.e., a Lisp interpreter that performs program execution, and helpsfind good programs by pruning the search space. We apply REINFORCE to directlyoptimize the task reward of this structured prediction problem. To train withweak supervision and improve the stability of REINFORCE, we augment it with aniterative maximum-likelihood training process. NSM outperforms thestate-of-the-art on the WebQuestionsSP dataset when trained fromquestion-answer pairs only, without requiring any feature engineering ordomain-specific knowledge.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "193", "arxiv_url": "http://arxiv.org/abs/1611.00020v4", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10752685133082009981&btnI=1&nossl=1&hl=en&oe=ASCII"}, "60": {"ID": 60, "title": "Adversarial Learning for Neural Dialogue Generation", "authors": ["Alan Ritter", "Dan Jurafsky", "S\u00e9bastien Jean", "Jiwei Li", "Tianlin Shi", "Will Monroe"], "published": "2017-01-23T18:32:27Z", "updated": "2017-09-24T01:44:39Z", "abstract": "In this paper, drawing intuition from the Turing test, we propose usingadversarial training for open-domain dialogue generation: the system is trainedto produce sequences that are indistinguishable from human-generated dialogueutterances. We cast the task as a reinforcement learning (RL) problem where wejointly train two systems, a generative model to produce response sequences,and a discriminator---analagous to the human evaluator in the Turing test--- todistinguish between the human-generated dialogues and the machine-generatedones. The outputs from the discriminator are then used as rewards for thegenerative model, pushing the system to generate dialogues that mostly resemblehuman dialogues.  In addition to adversarial training we describe a model for adversarial {\\emevaluation} that uses success in fooling an adversary as a dialogue evaluationmetric, while avoiding a number of potential pitfalls. Experimental results onseveral metrics, including adversarial evaluation, demonstrate that theadversarially-trained system generates higher-quality responses than previousbaselines.", "categories": ["cs.CL"], "journal": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "547", "arxiv_url": "http://arxiv.org/abs/1701.06547v5", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=2709411320030823497&btnI=1&nossl=1&hl=en&oe=ASCII"}, "61": {"ID": 61, "title": "Deal or No Deal? End-to-End Learning for Negotiation Dialogues", "authors": ["Denis Yarats", "Mike Lewis", "Yann N. Dauphin", "Dhruv Batra", "Devi Parikh"], "published": "2017-06-16T01:26:09Z", "updated": "2017-06-16T01:26:09Z", "abstract": "Much of human dialogue occurs in semi-cooperative settings, where agents withdifferent goals attempt to agree on common decisions. Negotiations requirecomplex communication and reasoning skills, but success is easy to measure,making this an interesting task for AI. We gather a large dataset ofhuman-human negotiations on a multi-issue bargaining task, where agents whocannot observe each other's reward functions must reach an agreement (or adeal) via natural language dialogue. For the first time, we show it is possibleto train end-to-end models for negotiation, which must learn both linguisticand reasoning skills with no annotated dialogue states. We also introducedialogue rollouts, in which the model plans ahead by simulating possiblecomplete continuations of the conversation, and find that this techniquedramatically improves performance. Our code and dataset are publicly available(https://github.com/facebookresearch/end-to-end-negotiator).", "categories": ["cs.AI", "cs.CL"], "journal": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "162", "arxiv_url": "http://arxiv.org/abs/1706.05125v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9824826429622854678&btnI=1&nossl=1&hl=en&oe=ASCII"}, "62": {"ID": 62, "title": "A Convolutional Encoder Model for Neural Machine Translation", "authors": ["Jonas Gehring", "David Grangier", "Michael Auli", "Yann N. Dauphin"], "published": "2016-11-07T23:46:45Z", "updated": "2017-07-25T01:36:14Z", "abstract": "The prevalent approach to neural machine translation relies on bi-directionalLSTMs to encode the source sentence. In this paper we present a faster andsimpler architecture based on a succession of convolutional layers. This allowsto encode the entire source sentence simultaneously compared to recurrentnetworks for which computation is constrained by temporal dependencies. OnWMT'16 English-Romanian translation we achieve competitive accuracy to thestate-of-the-art and we outperform several recently published results on theWMT'15 English-German task. Our models obtain almost the same accuracy as avery deep LSTM setup on WMT'14 English-French translation. Our convolutionalencoder speeds up CPU decoding by more than two times at the same or higheraccuracy as a strong bi-directional LSTM baseline.", "categories": ["cs.CL"], "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "229", "arxiv_url": "http://arxiv.org/abs/1611.02344v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=13078160224216368728&btnI=1&nossl=1&hl=en&oe=ASCII"}, "63": {"ID": 63, "title": "Addressing the Data Sparsity Issue in Neural AMR Parsing", "authors": ["Xiaochang Peng", "Chuan Wang", "Daniel Gildea", "Nianwen Xue"], "published": "2017-02-16T17:09:12Z", "updated": "2017-02-16T17:09:12Z", "abstract": "Neural attention models have achieved great success in different NLP tasks.How- ever, they have not fulfilled their promise on the AMR parsing task due tothe data sparsity issue. In this paper, we de- scribe a sequence-to-sequencemodel for AMR parsing and present different ways to tackle the data sparsityproblem. We show that our methods achieve significant improvement over abaseline neural atten- tion model and our results are also compet- itiveagainst state-of-the-art systems that do not use extra linguistic resources.", "categories": ["cs.CL"], "journal": "EACL (1), 366-375", "citations": "51", "arxiv_url": "http://arxiv.org/abs/1702.05053v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4779352758153500570&btnI=1&nossl=1&hl=en&oe=ASCII"}, "64": {"ID": 64, "title": "SemEval 2017 Task 10: ScienceIE - Extracting Keyphrases and Relations  from Scientific Publications", "authors": ["Lakshmi Vikraman", "Andrew McCallum", "Sebastian Riedel", "Isabelle Augenstein", "Mrinal Das"], "published": "2017-04-10T13:43:40Z", "updated": "2017-05-02T15:32:41Z", "abstract": "We describe the SemEval task of extracting keyphrases and relations betweenthem from scientific documents, which is crucial for understanding whichpublications describe which processes, tasks and materials. Although this was anew task, we had a total of 26 submissions across 3 evaluation scenarios. Weexpect the task and the findings reported in this paper to be relevant forresearchers working on understanding scientific content, as well as the broaderknowledge base population and information extraction communities.", "categories": ["cs.CL", "cs.AI", "stat.ML"], "journal": "Proceedings of the 11th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "127", "arxiv_url": "http://arxiv.org/abs/1704.02853v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14642300235422544774&btnI=1&nossl=1&hl=en&oe=ASCII"}, "65": {"ID": 65, "title": "Tree-to-Sequence Attentional Neural Machine Translation", "authors": ["Akiko Eriguchi", "Yoshimasa Tsuruoka", "Kazuma Hashimoto"], "published": "2016-03-19T10:08:40Z", "updated": "2016-06-08T08:39:11Z", "abstract": "Most of the existing Neural Machine Translation (NMT) models focus on theconversion of sequential data and do not directly use syntactic information. Wepropose a novel end-to-end syntactic NMT model, extending asequence-to-sequence model with the source-side phrase structure. Our model hasan attention mechanism that enables the decoder to generate a translated wordwhile softly aligning it with phrases as well as words of the source sentence.Experimental results on the WAT'15 English-to-Japanese dataset demonstrate thatour proposed model considerably outperforms sequence-to-sequence attentionalNMT models and compares favorably with the state-of-the-art tree-to-string SMTsystem.", "categories": ["cs.CL"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "176", "arxiv_url": "http://arxiv.org/abs/1603.06075v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10114639659174243367&btnI=1&nossl=1&hl=en&oe=ASCII"}, "66": {"ID": 66, "title": "Unsupervised Learning of Sentence Embeddings using Compositional n-Gram  Features", "authors": ["Martin Jaggi", "Prakhar Gupta", "Matteo Pagliardini"], "published": "2017-03-07T18:19:11Z", "updated": "2018-12-28T15:12:58Z", "abstract": "The recent tremendous success of unsupervised word embeddings in a multitudeof applications raises the obvious question if similar methods could be derivedto improve embeddings (i.e. semantic representations) of word sequences aswell. We present a simple but efficient unsupervised objective to traindistributed representations of sentences. Our method outperforms thestate-of-the-art unsupervised models on most benchmark tasks, highlighting therobustness of the produced general-purpose sentence embeddings.", "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2.7"], "journal": "NAACL-HLT, 528-540", "citations": "289", "arxiv_url": "http://arxiv.org/abs/1703.02507v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=11854173454943900486&btnI=1&nossl=1&hl=en&oe=ASCII"}, "67": {"ID": 67, "title": "Deep Recurrent Models with Fast-Forward Connections for Neural Machine  Translation", "authors": ["Peng Li", "Jie Zhou", "Xuguang Wang", "Wei Xu", "Ying Cao"], "published": "2016-06-14T03:53:00Z", "updated": "2016-07-23T13:14:17Z", "abstract": "Neural machine translation (NMT) aims at solving machine translation (MT)problems using neural networks and has exhibited promising results in recentyears. However, most of the existing NMT models are shallow and there is stilla performance gap between a single NMT model and the best conventional MTsystem. In this work, we introduce a new type of linear connections, namedfast-forward connections, based on deep Long Short-Term Memory (LSTM) networks,and an interleaved bi-directional architecture for stacking the LSTM layers.Fast-forward connections play an essential role in propagating the gradientsand building a deep topology of depth 16. On the WMT'14 English-to-French task,we achieve BLEU=37.7 with a single attention model, which outperforms thecorresponding single shallow model by 6.2 BLEU points. This is the first timethat a single NMT model achieves state-of-the-art performance and outperformsthe best conventional model by 0.7 BLEU points. We can still achieve BLEU=36.3even without using an attention mechanism. After special handling of unknownwords and model ensembling, we obtain the best score reported to date on thistask with BLEU=40.4. Our models are also validated on the more difficult WMT'14English-to-German task.", "categories": ["cs.CL", "cs.LG"], "journal": "Transactions of the Association for Computational Linguistics 4, 371-383", "citations": "143", "arxiv_url": "http://arxiv.org/abs/1606.04199v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=2319930273054317494&btnI=1&nossl=1&hl=en&oe=ASCII"}, "68": {"ID": 68, "title": "A Novel Embedding Model for Knowledge Base Completion Based on  Convolutional Neural Network", "authors": ["Tu Dinh Nguyen", "Dat Quoc Nguyen", "Dinh Phung", "Dai Quoc Nguyen"], "published": "2017-12-06T10:41:47Z", "updated": "2018-03-13T07:45:20Z", "abstract": "In this paper, we propose a novel embedding model, named ConvKB, forknowledge base completion. Our model ConvKB advances state-of-the-art models byemploying a convolutional neural network, so that it can capture globalrelationships and transitional characteristics between entities and relationsin knowledge bases. In ConvKB, each triple (head entity, relation, tail entity)is represented as a 3-column matrix where each column vector represents atriple element. This 3-column matrix is then fed to a convolution layer wheremultiple filters are operated on the matrix to generate different feature maps.These feature maps are then concatenated into a single feature vectorrepresenting the input triple. The feature vector is multiplied with a weightvector via a dot product to return a score. This score is then used to predictwhether the triple is valid or not. Experiments show that ConvKB achievesbetter link prediction performance than previous state-of-the-art embeddingmodels on two benchmark datasets WN18RR and FB15k-237.", "categories": ["cs.CL"], "journal": "NAACL-HLT (2), 327-333", "citations": "104", "arxiv_url": "http://arxiv.org/abs/1712.02121v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=5515928500707541108&btnI=1&nossl=1&hl=en&oe=ASCII"}, "69": {"ID": 69, "title": "When Are Tree Structures Necessary for Deep Learning of Representations?", "authors": ["Jiwei Li", "Minh-Thang Luong", "Dan Jurafsky", "Eudard Hovy"], "published": "2015-02-28T21:39:31Z", "updated": "2015-08-18T05:59:18Z", "abstract": "Recursive neural models, which use syntactic parse trees to recursivelygenerate representations bottom-up, are a popular architecture. But there havenot been rigorous evaluations showing for exactly which tasks this syntax-basedmethod is appropriate. In this paper we benchmark {\\bf recursive} neural modelsagainst sequential {\\bf recurrent} neural models (simple recurrent and LSTMmodels), enforcing apples-to-apples comparison as much as possible. Weinvestigate 4 tasks: (1) sentiment classification at the sentence level andphrase level; (2) matching questions to answer-phrases; (3) discourse parsing;(4) semantic relation extraction (e.g., {\\em component-whole} between nouns).  Our goal is to understand better when, and why, recursive models canoutperform simpler models. We find that recursive models help mainly on tasks(like semantic relation extraction) that require associating headwords across along distance, particularly on very long sequences. We then introduce a methodfor allowing recurrent models to achieve similar performance: breaking longsentences into clause-like units at punctuation and processing them separatelybefore combining. Our results thus help understand the limitations of bothclasses of models, and suggest directions for improving recurrent models.", "categories": ["cs.AI", "cs.CL"], "journal": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "168", "arxiv_url": "http://arxiv.org/abs/1503.00185v5", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8781853956244507565&btnI=1&nossl=1&hl=en&oe=ASCII"}, "70": {"ID": 70, "title": "How Grammatical is Character-level Neural Machine Translation? Assessing  MT Quality with Contrastive Translation Pairs", "authors": ["Rico Sennrich"], "published": "2016-12-14T13:45:35Z", "updated": "2017-02-13T09:59:05Z", "abstract": "Analysing translation quality in regards to specific linguistic phenomena hashistorically been difficult and time-consuming. Neural machine translation hasthe attractive property that it can produce scores for arbitrary translations,and we propose a novel method to assess how well NMT systems model specificlinguistic phenomena such as agreement over long distances, the production ofnovel words, and the faithful translation of polarity. The core idea is that wemeasure whether a reference translation is more probable under a NMT model thana contrastive translation which introduces a specific type of error. We presentLingEval97, a large-scale data set of 97000 contrastive translation pairs basedon the WMT English-&gt;German translation task, with errors automatically createdwith simple rules. We report results for a number of systems, and find thatrecently introduced character-level NMT systems perform better attransliteration than models with byte-pair encoding (BPE) segmentation, butperform more poorly at morphosyntactic agreement, and translating discontiguousunits of meaning.", "categories": ["cs.CL"], "journal": "EACL (2), 376-382", "citations": "78", "arxiv_url": "http://arxiv.org/abs/1612.04629v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14294900718072928557&btnI=1&nossl=1&hl=en&oe=ASCII"}, "71": {"ID": 71, "title": "Large-scale Analysis of Counseling Conversations: An Application of  Natural Language Processing to Mental Health", "authors": ["Jure Leskovec", "Tim Althoff", "Kevin Clark"], "published": "2016-05-14T20:02:05Z", "updated": "2016-08-14T20:45:55Z", "abstract": "Mental illness is one of the most pressing public health issues of our time.While counseling and psychotherapy can be effective treatments, our knowledgeabout how to conduct successful counseling conversations has been limited dueto lack of large-scale data with labeled outcomes of the conversations. In thispaper, we present a large-scale, quantitative study on the discourse oftext-message-based counseling conversations. We develop a set of novelcomputational discourse analysis methods to measure how various linguisticaspects of conversations are correlated with conversation outcomes. Applyingtechniques such as sequence-based conversation models, language modelcomparisons, message clustering, and psycholinguistics-inspired word frequencyanalyses, we discover actionable conversation strategies that are associatedwith better conversation outcomes.", "categories": ["cs.CL", "cs.CY", "cs.SI"], "journal": "Transactions of the Association for Computational Linguistics 4, 463-476", "citations": "90", "arxiv_url": "http://arxiv.org/abs/1605.04462v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=12360325295977698039&btnI=1&nossl=1&hl=en&oe=ASCII"}, "72": {"ID": 72, "title": "Sequential Matching Network: A New Architecture for Multi-turn Response  Selection in Retrieval-based Chatbots", "authors": ["Chen Xing", "Zhoujun Li", "Ming Zhou", "Wei Wu", "Yu Wu"], "published": "2016-12-06T01:57:39Z", "updated": "2017-05-15T01:50:55Z", "abstract": "We study response selection for multi-turn conversation in retrieval-basedchatbots. Existing work either concatenates utterances in context or matches aresponse with a highly abstract context vector finally, which may loserelationships among utterances or important contextual information. We proposea sequential matching network (SMN) to address both problems. SMN first matchesa response with each utterance in the context on multiple levels ofgranularity, and distills important matching information from each pair as avector with convolution and pooling operations. The vectors are thenaccumulated in a chronological order through a recurrent neural network (RNN)which models relationships among utterances. The final matching score iscalculated with the hidden states of the RNN. An empirical study on two publicdata sets shows that SMN can significantly outperform state-of-the-art methodsfor response selection in multi-turn conversation.", "categories": ["cs.CL"], "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "189", "arxiv_url": "http://arxiv.org/abs/1612.01627v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=16760811131286357810&btnI=1&nossl=1&hl=en&oe=ASCII"}, "73": {"ID": 73, "title": "Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change", "authors": ["Dan Jurafsky", "William L. Hamilton", "Jure Leskovec"], "published": "2016-05-30T03:54:18Z", "updated": "2018-10-25T17:34:43Z", "abstract": "Understanding how words change their meanings over time is key to models oflanguage and cultural evolution, but historical data on meaning is scarce,making theories hard to develop and test. Word embeddings show promise as adiachronic tool, but have not been carefully evaluated. We develop a robustmethodology for quantifying semantic change by evaluating word embeddings(PPMI, SVD, word2vec) against known historical changes. We then use thismethodology to reveal statistical laws of semantic evolution. Using sixhistorical corpora spanning four languages and two centuries, we propose twoquantitative laws of semantic change: (i) the law of conformity---the rate ofsemantic change scales with an inverse power-law of word frequency; (ii) thelaw of innovation---independent of frequency, words that are more polysemoushave higher rates of semantic change.", "categories": ["cs.CL"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "417", "arxiv_url": "http://arxiv.org/abs/1605.09096v6", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=902015005372841170&btnI=1&nossl=1&hl=en&oe=ASCII"}, "74": {"ID": 74, "title": "Abstractive Text Summarization Using Sequence-to-Sequence RNNs and  Beyond", "authors": ["Ramesh Nallapati", "Cicero Nogueira dos santos", "Bing Xiang", "Bowen Zhou", "Caglar Gulcehre"], "published": "2016-02-19T02:04:18Z", "updated": "2016-08-26T16:13:13Z", "abstract": "In this work, we model abstractive text summarization using AttentionalEncoder-Decoder Recurrent Neural Networks, and show that they achievestate-of-the-art performance on two different corpora. We propose several novelmodels that address critical problems in summarization that are not adequatelymodeled by the basic architecture, such as modeling key-words, capturing thehierarchy of sentence-to-word structure, and emitting words that are rare orunseen at training time. Our work shows that many of our proposed modelscontribute to further improvement in performance. We also propose a new datasetconsisting of multi-sentence summaries, and establish performance benchmarksfor further research.", "categories": ["cs.CL"], "journal": "CoNLL, 280-290", "citations": "912", "arxiv_url": "http://arxiv.org/abs/1602.06023v5", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=17503002498462524081&btnI=1&nossl=1&hl=en&oe=ASCII"}, "75": {"ID": 75, "title": "Improving Hypernymy Detection with an Integrated Path-based and  Distributional Method", "authors": ["Vered Shwartz", "Ido Dagan", "Yoav Goldberg"], "published": "2016-03-19T10:09:53Z", "updated": "2016-06-07T10:09:43Z", "abstract": "Detecting hypernymy relations is a key task in NLP, which is addressed in theliterature using two complementary approaches. Distributional methods, whosesupervised variants are the current best performers, and path-based methods,which received less research attention. We suggest an improved path-basedalgorithm, in which the dependency paths are encoded using a recurrent neuralnetwork, that achieves results comparable to distributional methods. We thenextend the approach to integrate both path-based and distributional signals,significantly improving upon the state-of-the-art on this task.", "categories": ["cs.CL"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "166", "arxiv_url": "http://arxiv.org/abs/1603.06076v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8520532206920675606&btnI=1&nossl=1&hl=en&oe=ASCII"}, "76": {"ID": 76, "title": "OpenNMT: Open-Source Toolkit for Neural Machine Translation", "authors": ["Jean Senellart", "Alexander M. Rush", "Yuntian Deng", "Yoon Kim", "Guillaume Klein"], "published": "2017-01-10T23:32:43Z", "updated": "2017-03-06T15:54:27Z", "abstract": "We describe an open-source toolkit for neural machine translation (NMT). Thetoolkit prioritizes efficiency, modularity, and extensibility with the goal ofsupporting NMT research into model architectures, feature representations, andsource modalities, while maintaining competitive performance and reasonabletraining requirements. The toolkit consists of modeling and translationsupport, as well as detailed pedagogical documentation about the underlyingtechniques.", "categories": ["cs.CL", "cs.AI", "cs.NE"], "journal": "ACL (System Demonstrations), 67-72", "citations": "899", "arxiv_url": "http://arxiv.org/abs/1701.02810v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6651054115351140376&btnI=1&nossl=1&hl=en&oe=ASCII"}, "77": {"ID": 77, "title": "Pre-Translation for Neural Machine Translation", "authors": ["Eunah Cho", "Alex Waibel", "Jan Niehues", "Thanh-Le Ha"], "published": "2016-10-17T18:14:24Z", "updated": "2016-10-17T18:14:24Z", "abstract": "Recently, the development of neural machine translation (NMT) hassignificantly improved the translation quality of automatic machinetranslation. While most sentences are more accurate and fluent thantranslations by statistical machine translation (SMT)-based systems, in somecases, the NMT system produces translations that have a completely differentmeaning. This is especially the case when rare words occur.  When using statistical machine translation, it has already been shown thatsignificant gains can be achieved by simplifying the input in a preprocessingstep. A commonly used example is the pre-reordering approach.  In this work, we used phrase-based machine translation to pre-translate theinput into the target language. Then a neural machine translation systemgenerates the final hypothesis using the pre-translation. Thereby, we useeither only the output of the phrase-based machine translation (PBMT) system ora combination of the PBMT output and the source sentence.  We evaluate the technique on the English to German translation task. Usingthis approach we are able to outperform the PBMT system as well as the baselineneural MT system by up to 2 BLEU points. We analyzed the influence of thequality of the initial system on the final result.", "categories": ["cs.CL"], "journal": "COLING, 1828-1836", "citations": "53", "arxiv_url": "http://arxiv.org/abs/1610.05243v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9288973466527219575&btnI=1&nossl=1&hl=en&oe=ASCII"}, "78": {"ID": 78, "title": "Multilingual Language Processing From Bytes", "authors": ["Cliff Brunk", "Amarnag Subramanya", "Dan Gillick", "Oriol Vinyals"], "published": "2015-12-01T00:23:44Z", "updated": "2016-04-02T16:26:23Z", "abstract": "We describe an LSTM-based model which we call Byte-to-Span (BTS) that readstext as bytes and outputs span annotations of the form [start, length, label]where start positions, lengths, and labels are separate entries in ourvocabulary. Because we operate directly on unicode bytes rather thanlanguage-specific words or characters, we can analyze text in many languageswith a single model. Due to the small vocabulary size, these multilingualmodels are very compact, but produce results similar to or better than thestate-of- the-art in Part-of-Speech tagging and Named Entity Recognition thatuse only the provided training datasets (no external data sources). Our modelsare learning \"from scratch\" in that they do not rely on any elements of thestandard pipeline in Natural Language Processing (including tokenization), andthus can run in standalone fashion on raw text.", "categories": ["cs.CL"], "journal": "HLT-NAACL, 1296-1306", "citations": "157", "arxiv_url": "http://arxiv.org/abs/1512.00103v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4801616475177522971&btnI=1&nossl=1&hl=en&oe=ASCII"}, "79": {"ID": 79, "title": "ParlAI: A Dialog Research Software Platform", "authors": ["Alexander H. Miller", "Antoine Bordes", "Dhruv Batra", "Will Feng", "Adam Fisch", "Devi Parikh", "Jason Weston", "Jiasen Lu"], "published": "2017-05-18T08:54:47Z", "updated": "2018-03-08T19:58:17Z", "abstract": "We introduce ParlAI (pronounced \"par-lay\"), an open-source software platformfor dialog research implemented in Python, available at http://parl.ai. Itsgoal is to provide a unified framework for sharing, training and testing ofdialog models, integration of Amazon Mechanical Turk for data collection, humanevaluation, and online/reinforcement learning; and a repository of machinelearning models for comparing with others' models, and improving upon existingarchitectures. Over 20 tasks are supported in the first release, includingpopular datasets such as SQuAD, bAbI tasks, MCTest, WikiQA, QACNN, QADailyMail,CBT, bAbI Dialog, Ubuntu, OpenSubtitles and VQA. Several models are integrated,including neural models such as memory networks, seq2seq and attentive LSTMs.", "categories": ["cs.CL"], "journal": "EMNLP (System Demonstrations), 79-84", "citations": "122", "arxiv_url": "http://arxiv.org/abs/1705.06476v4", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6605905411623876114&btnI=1&nossl=1&hl=en&oe=ASCII"}, "80": {"ID": 80, "title": "The Parallel Meaning Bank: Towards a Multilingual Corpus of Translations  Annotated with Compositional Meaning Representations", "authors": ["Hessel Haagsma", "Duc-Duy Nguyen", "Rik van Noord", "Pierre Ludmann", "Johan Bos", "Johannes Bjerva", "Kilian Evang", "Lasha Abzianidze"], "published": "2017-02-13T19:52:02Z", "updated": "2017-02-13T19:52:02Z", "abstract": "The Parallel Meaning Bank is a corpus of translations annotated with shared,formal meaning representations comprising over 11 million words divided overfour languages (English, German, Italian, and Dutch). Our approach is based oncross-lingual projection: automatically produced (and manually corrected)semantic annotations for English sentences are mapped onto their word-alignedtranslations, assuming that the translations are meaning-preserving. Thesemantic annotation consists of five main steps: (i) segmentation of the textin sentences and lexical items; (ii) syntactic parsing with CombinatoryCategorial Grammar; (iii) universal semantic tagging; (iv) symbolization; and(v) compositional semantic analysis based on Discourse Representation Theory.These steps are performed using statistical models trained in a semi-supervisedmanner. The employed annotation models are all language-neutral. Our firstresults are promising.", "categories": ["cs.CL"], "journal": "EACL (2), 242-247", "citations": "58", "arxiv_url": "http://arxiv.org/abs/1702.03964v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9358800795656493052&btnI=1&nossl=1&hl=en&oe=ASCII"}, "81": {"ID": 81, "title": "Enhanced LSTM for Natural Language Inference", "authors": ["Si Wei", "Qian Chen", "Hui Jiang", "Zhenhua Ling", "Diana Inkpen", "Xiaodan Zhu"], "published": "2016-09-20T06:59:31Z", "updated": "2017-04-26T17:37:13Z", "abstract": "Reasoning and inference are central to human and artificial intelligence.Modeling inference in human language is very challenging. With the availabilityof large annotated data (Bowman et al., 2015), it has recently become feasibleto train neural network based inference models, which have shown to be veryeffective. In this paper, we present a new state-of-the-art result, achievingthe accuracy of 88.6% on the Stanford Natural Language Inference Dataset.Unlike the previous top models that use very complicated network architectures,we first demonstrate that carefully designing sequential inference models basedon chain LSTMs can outperform all previous models. Based on this, we furthershow that by explicitly considering recursive architectures in both localinference modeling and inference composition, we achieve additionalimprovement. Particularly, incorporating syntactic parsing informationcontributes to our best result---it further improves the performance even whenadded to the already very strong model.", "categories": ["cs.CL"], "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "442", "arxiv_url": "http://arxiv.org/abs/1609.06038v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6032955832856638731&btnI=1&nossl=1&hl=en&oe=ASCII"}, "82": {"ID": 82, "title": "BB_twtr at SemEval-2017 Task 4: Twitter Sentiment Analysis with CNNs and  LSTMs", "authors": ["Mathieu Cliche"], "published": "2017-04-20T13:10:25Z", "updated": "2017-04-20T13:10:25Z", "abstract": "In this paper we describe our attempt at producing a state-of-the-art Twittersentiment classifier using Convolutional Neural Networks (CNNs) and Long ShortTerm Memory (LSTMs) networks. Our system leverages a large amount of unlabeleddata to pre-train word embeddings. We then use a subset of the unlabeled datato fine tune the embeddings using distant supervision. The final CNNs and LSTMsare trained on the SemEval-2017 Twitter dataset where the embeddings are finedtuned again. To boost performances we ensemble several CNNs and LSTMs together.Our approach achieved first rank on all of the five English subtasks amongst 40teams.", "categories": ["cs.CL", "stat.ML"], "journal": "Proceedings of the 11th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "110", "arxiv_url": "http://arxiv.org/abs/1704.06125v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=3873097388025856746&btnI=1&nossl=1&hl=en&oe=ASCII"}, "83": {"ID": 83, "title": "Reporting Score Distributions Makes a Difference: Performance Study of  LSTM-networks for Sequence Tagging", "authors": ["Nils Reimers", "Iryna Gurevych"], "published": "2017-07-31T14:25:24Z", "updated": "2017-07-31T14:25:24Z", "abstract": "In this paper we show that reporting a single performance score isinsufficient to compare non-deterministic approaches. We demonstrate for commonsequence tagging tasks that the seed value for the random number generator canresult in statistically significant (p &lt; 10^-4) differences forstate-of-the-art systems. For two recent systems for NER, we observe anabsolute difference of one percentage point F1-score depending on the selectedseed value, making these systems perceived either as state-of-the-art ormediocre. Instead of publishing and reporting single performance scores, wepropose to compare score distributions based on multiple executions. Based onthe evaluation of 50.000 LSTM-networks for five sequence tagging tasks, wepresent network architectures that produce both superior performance as well asare more stable with respect to the remaining hyperparameters.", "categories": ["cs.CL", "stat.ML"], "journal": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "253", "arxiv_url": "http://arxiv.org/abs/1707.09861v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8733026693541658958&btnI=1&nossl=1&hl=en&oe=ASCII"}, "84": {"ID": 84, "title": "QuAC : Question Answering in Context", "authors": ["Wen-tau Yih", "Yejin Choi", "Percy Liang", "He He", "Mohit Iyyer", "Luke Zettlemoyer", "Eunsol Choi", "Mark Yatskar"], "published": "2018-08-21T17:46:12Z", "updated": "2018-08-28T00:58:48Z", "abstract": "We present QuAC, a dataset for Question Answering in Context that contains14K information-seeking QA dialogs (100K questions in total). The dialogsinvolve two crowd workers: (1) a student who poses a sequence of freeformquestions to learn as much as possible about a hidden Wikipedia text, and (2) ateacher who answers the questions by providing short excerpts from the text.QuAC introduces challenges not found in existing machine comprehensiondatasets: its questions are often more open-ended, unanswerable, or onlymeaningful within the dialog context, as we show in a detailed qualitativeevaluation. We also report results for a number of reference models, includinga recently state-of-the-art reading comprehension architecture extended tomodel dialog context. Our best model underperforms humans by 20 F1, suggestingthat there is significant room for future work on this data. Dataset, baseline,and leaderboard available at http://quac.ai.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "journal": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "159", "arxiv_url": "http://arxiv.org/abs/1808.07036v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=1978251881583988813&btnI=1&nossl=1&hl=en&oe=ASCII"}, "85": {"ID": 85, "title": "Unsupervised Pretraining for Sequence to Sequence Learning", "authors": ["Quoc V. Le", "Peter J. Liu", "Prajit Ramachandran"], "published": "2016-11-08T20:42:26Z", "updated": "2018-02-22T01:57:27Z", "abstract": "This work presents a general unsupervised learning method to improve theaccuracy of sequence to sequence (seq2seq) models. In our method, the weightsof the encoder and decoder of a seq2seq model are initialized with thepretrained weights of two language models and then fine-tuned with labeleddata. We apply this method to challenging benchmarks in machine translation andabstractive summarization and find that it significantly improves thesubsequent supervised models. Our main result is that pretraining improves thegeneralization of seq2seq models. We achieve state-of-the art results on theWMT English$\\rightarrow$German task, surpassing a range of methods using bothphrase-based machine translation and neural machine translation. Our methodachieves a significant improvement of 1.3 BLEU from the previous best models onboth WMT'14 and WMT'15 English$\\rightarrow$German. We also conduct humanevaluations on abstractive summarization and find that our method outperforms apurely supervised learning baseline in a statistically significant manner.", "categories": ["cs.CL", "cs.LG", "cs.NE"], "journal": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "175", "arxiv_url": "http://arxiv.org/abs/1611.02683v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6622750447258456990&btnI=1&nossl=1&hl=en&oe=ASCII"}, "86": {"ID": 86, "title": "A Neural Network Approach to Context-Sensitive Generation of  Conversational Responses", "authors": ["Chris Brockett", "Jian-Yun Nie", "Jianfeng Gao", "Yangfeng Ji", "Alessandro Sordoni", "Michael Auli", "Margaret Mitchell", "Michel Galley", "Bill Dolan"], "published": "2015-06-22T18:29:03Z", "updated": "2015-06-22T18:29:03Z", "abstract": "We present a novel response generation system that can be trained end to endon large quantities of unstructured Twitter conversations. A neural networkarchitecture is used to address sparsity issues that arise when integratingcontextual information into classic statistical models, allowing the system totake into account previous dialog utterances. Our dynamic-context generativemodels show consistent gains over both context-sensitive andnon-context-sensitive Machine Translation and Information Retrieval baselines.", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "journal": "HLT-NAACL, 196-205", "citations": "662", "arxiv_url": "http://arxiv.org/abs/1506.06714v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=13228311168016753962&btnI=1&nossl=1&hl=en&oe=ASCII"}, "87": {"ID": 87, "title": "Neural Architectures for Named Entity Recognition", "authors": ["Guillaume Lample", "Kazuya Kawakami", "Chris Dyer", "Miguel Ballesteros", "Sandeep Subramanian"], "published": "2016-03-04T06:36:29Z", "updated": "2016-04-07T15:09:36Z", "abstract": "State-of-the-art named entity recognition systems rely heavily onhand-crafted features and domain-specific knowledge in order to learneffectively from the small, supervised training corpora that are available. Inthis paper, we introduce two new neural architectures---one based onbidirectional LSTMs and conditional random fields, and the other thatconstructs and labels segments using a transition-based approach inspired byshift-reduce parsers. Our models rely on two sources of information aboutwords: character-based word representations learned from the supervised corpusand unsupervised word representations learned from unannotated corpora. Ourmodels obtain state-of-the-art performance in NER in four languages withoutresorting to any language-specific knowledge or resources such as gazetteers.", "categories": ["cs.CL"], "journal": "HLT-NAACL, 260-270", "citations": "2008", "arxiv_url": "http://arxiv.org/abs/1603.01360v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=5088561272839807706&btnI=1&nossl=1&hl=en&oe=ASCII"}, "88": {"ID": 88, "title": "Marian: Fast Neural Machine Translation in C++", "authors": ["Tomasz Dwojak", "Alham Fikri Aji", "Roman Grundkiewicz", "Frank Seide", "Tom Neckermann", "Nikolay Bogoychev", "Andr\u00e9 F. T. Martins", "Alexandra Birch", "Hieu Hoang", "Marcin Junczys-Dowmunt", "Kenneth Heafield", "Ulrich Germann"], "published": "2018-04-01T20:50:57Z", "updated": "2018-04-04T15:34:17Z", "abstract": "We present Marian, an efficient and self-contained Neural Machine Translationframework with an integrated automatic differentiation engine based on dynamiccomputation graphs. Marian is written entirely in C++. We describe the designof the encoder-decoder framework and demonstrate that a research-friendlytoolkit can achieve high training and translation speed.", "categories": ["cs.CL"], "journal": "ACL (4), 116-121", "citations": "162", "arxiv_url": "http://arxiv.org/abs/1804.00344v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6156277269319504697&btnI=1&nossl=1&hl=en&oe=ASCII"}, "89": {"ID": 89, "title": "A Dependency-Based Neural Network for Relation Classification", "authors": ["Houfeng Wang", "Furu Wei", "Sujian Li", "Heng Ji", "Ming Zhou", "Yang Liu"], "published": "2015-07-16T16:43:55Z", "updated": "2015-07-16T16:43:55Z", "abstract": "Previous research on relation classification has verified the effectivenessof using dependency shortest paths or subtrees. In this paper, we furtherexplore how to make full use of the combination of these dependencyinformation. We first propose a new structure, termed augmented dependency path(ADP), which is composed of the shortest dependency path between two entitiesand the subtrees attached to the shortest path. To exploit the semanticrepresentation behind the ADP structure, we develop dependency-based neuralnetworks (DepNN): a recursive neural network designed to model the subtrees,and a convolutional neural network to capture the most important features onthe shortest path. Experiments on the SemEval-2010 dataset show that ourproposed method achieves state-of-art results.", "categories": ["cs.CL", "cs.LG", "cs.NE"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "171", "arxiv_url": "http://arxiv.org/abs/1507.04646v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10123861783018592266&btnI=1&nossl=1&hl=en&oe=ASCII"}, "90": {"ID": 90, "title": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks", "authors": ["Kazuma Hashimoto", "Richard Socher", "Yoshimasa Tsuruoka", "Caiming Xiong"], "published": "2016-11-05T01:59:29Z", "updated": "2017-07-24T14:41:16Z", "abstract": "Transfer and multi-task learning have traditionally focused on either asingle source-target pair or very few, similar tasks. Ideally, the linguisticlevels of morphology, syntax and semantics would benefit each other by beingtrained in a single model. We introduce a joint many-task model together with astrategy for successively growing its depth to solve increasingly complextasks. Higher layers include shortcut connections to lower-level taskpredictions to reflect linguistic hierarchies. We use a simple regularizationterm to allow for optimizing all model weights to improve one task's losswithout exhibiting catastrophic interference of the other tasks. Our singleend-to-end model obtains state-of-the-art or competitive results on fivedifferent tasks from tagging, parsing, relatedness, and entailment tasks.", "categories": ["cs.CL", "cs.AI"], "journal": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "303", "arxiv_url": "http://arxiv.org/abs/1611.01587v5", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=3437546579340829684&btnI=1&nossl=1&hl=en&oe=ASCII"}, "91": {"ID": 91, "title": "Sequence-to-Sequence Learning as Beam-Search Optimization", "authors": ["Sam Wiseman", "Alexander M. Rush"], "published": "2016-06-09T13:29:34Z", "updated": "2016-11-10T03:45:30Z", "abstract": "Sequence-to-Sequence (seq2seq) modeling has rapidly become an importantgeneral-purpose NLP tool that has proven effective for many text-generation andsequence-labeling tasks. Seq2seq builds on deep neural language modeling andinherits its remarkable accuracy in estimating local, next-word distributions.In this work, we introduce a model and beam-search training scheme, based onthe work of Daume III and Marcu (2005), that extends seq2seq to learn globalsequence scores. This structured approach avoids classical biases associatedwith local training and unifies the training loss with the test-time usage,while preserving the proven model architecture of seq2seq and its efficienttraining approach. We show that our system outperforms a highly-optimizedattention-based seq2seq system and other baselines on three different sequenceto sequence tasks: word ordering, parsing, and machine translation.", "categories": ["cs.CL", "cs.LG", "cs.NE", "stat.ML"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "302", "arxiv_url": "http://arxiv.org/abs/1606.02960v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8919612243620131744&btnI=1&nossl=1&hl=en&oe=ASCII"}, "92": {"ID": 92, "title": "MITRE at SemEval-2016 Task 6: Transfer Learning for Stance Detection", "authors": ["Amy Marsh", "Guido Zarrella"], "published": "2016-06-13T00:12:49Z", "updated": "2016-06-13T00:12:49Z", "abstract": "We describe MITRE's submission to the SemEval-2016 Task 6, Detecting Stancein Tweets. This effort achieved the top score in Task A on supervised stancedetection, producing an average F1 score of 67.8 when assessing whether a tweetauthor was in favor or against a topic. We employed a recurrent neural networkinitialized with features learned via distant supervision on two largeunlabeled datasets. We trained embeddings of words and phrases with theword2vec skip-gram method, then used those features to learn sentencerepresentations via a hashtag prediction auxiliary task. These sentence vectorswere then fine-tuned for stance detection on several hundred labeled examples.The result was a high performing system that used transfer learning to maximizethe value of the available training data.", "categories": ["cs.AI", "cs.CL"], "journal": "Proceedings of the 10th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "89", "arxiv_url": "http://arxiv.org/abs/1606.03784v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4647858780131029141&btnI=1&nossl=1&hl=en&oe=ASCII"}, "93": {"ID": 93, "title": "Bottom-Up Abstractive Summarization", "authors": ["Yuntian Deng", "Sebastian Gehrmann", "Alexander M. Rush"], "published": "2018-08-31T14:55:52Z", "updated": "2018-10-09T02:04:07Z", "abstract": "Neural network-based methods for abstractive summarization produce outputsthat are more fluent than other techniques, but which can be poor at contentselection. This work proposes a simple technique for addressing this issue: usea data-efficient content selector to over-determine phrases in a sourcedocument that should be part of the summary. We use this selector as abottom-up attention step to constrain the model to likely phrases. We show thatthis approach improves the ability to compress text, while still generatingfluent summaries. This two-step process is both simpler and higher performingthan other end-to-end content selection models, leading to significantimprovements on ROUGE for both the CNN-DM and NYT corpus. Furthermore, thecontent selector can be trained with as little as 1,000 sentences, making iteasy to transfer a trained summarizer to a new domain.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "journal": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "189", "arxiv_url": "http://arxiv.org/abs/1808.10792v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6702357170880053318&btnI=1&nossl=1&hl=en&oe=ASCII"}, "94": {"ID": 94, "title": "Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature  Representations", "authors": ["Yoav Goldberg", "Eliyahu Kiperwasser"], "published": "2016-03-14T17:18:27Z", "updated": "2016-07-20T15:17:29Z", "abstract": "We present a simple and effective scheme for dependency parsing which isbased on bidirectional-LSTMs (BiLSTMs). Each sentence token is associated witha BiLSTM vector representing the token in its sentential context, and featurevectors are constructed by concatenating a few BiLSTM vectors. The BiLSTM istrained jointly with the parser objective, resulting in very effective featureextractors for parsing. We demonstrate the effectiveness of the approach byapplying it to a greedy transition-based parser as well as to a globallyoptimized graph-based parser. The resulting parsers have very simplearchitectures, and match or surpass the state-of-the-art accuracies on Englishand Chinese.", "categories": ["cs.CL"], "journal": "Transactions of the Association for Computational Linguistics 4, 313-327", "citations": "417", "arxiv_url": "http://arxiv.org/abs/1603.04351v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=5993512994732130707&btnI=1&nossl=1&hl=en&oe=ASCII"}, "95": {"ID": 95, "title": "What's Cookin'? Interpreting Cooking Videos using Text, Speech and  Vision", "authors": ["Kevin Murphy", "Jonathan Malmaud", "Vivek Rathod", "Nick Johnston", "Jonathan Huang", "Andrew Rabinovich"], "published": "2015-03-05T07:07:48Z", "updated": "2015-03-13T18:55:22Z", "abstract": "We present a novel method for aligning a sequence of instructions to a videoof someone carrying out a task. In particular, we focus on the cooking domain,where the instructions correspond to the recipe. Our technique relies on an HMMto align the recipe steps to the (automatically generated) speech transcript.We then refine this alignment using a state-of-the-art visual food detector,based on a deep convolutional neural network. We show that our techniqueoutperforms simpler techniques based on keyword spotting. It also enablesinteresting applications, such as automatically illustrating recipes withkeyframes, and searching within a video for events of interest.", "categories": ["cs.CL", "cs.CV", "cs.IR"], "journal": "HLT-NAACL, 143-152", "citations": "93", "arxiv_url": "http://arxiv.org/abs/1503.01558v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14331750832735428724&btnI=1&nossl=1&hl=en&oe=ASCII"}, "96": {"ID": 96, "title": "The Role of Context Types and Dimensionality in Learning Word Embeddings", "authors": ["Mohit Bansal", "Siddharth Patwardhan", "David McClosky", "Oren Melamud"], "published": "2016-01-05T16:28:42Z", "updated": "2017-07-19T15:32:54Z", "abstract": "We provide the first extensive evaluation of how using different types ofcontext to learn skip-gram word embeddings affects performance on a wide rangeof intrinsic and extrinsic NLP tasks. Our results suggest that while intrinsictasks tend to exhibit a clear preference to particular types of contexts andhigher dimensionality, more careful tuning is required for finding the optimalsettings for most of the extrinsic tasks that we considered. Furthermore, forthese extrinsic tasks, we find that once the benefit from increasing theembedding dimensionality is mostly exhausted, simple concatenation of wordembeddings, learned with different context types, can yield further performancegains. As an additional contribution, we propose a new variant of the skip-grammodel that learns word embeddings from weighted contexts of substitute words.", "categories": ["cs.CL"], "journal": "HLT-NAACL, 1030-1040", "citations": "95", "arxiv_url": "http://arxiv.org/abs/1601.00893v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9404852206304163554&btnI=1&nossl=1&hl=en&oe=ASCII"}, "97": {"ID": 97, "title": "Semantic Relation Classification via Convolutional Neural Networks with  Simple Negative Sampling", "authors": ["Kun Xu", "Yansong Feng", "Songfang Huang", "Dongyan Zhao"], "published": "2015-06-25T07:51:55Z", "updated": "2015-06-25T07:51:55Z", "abstract": "Syntactic features play an essential role in identifying relationship in asentence. Previous neural network models often suffer from irrelevantinformation introduced when subjects and objects are in a long distance. Inthis paper, we propose to learn more robust relation representations from theshortest dependency path through a convolution neural network. We furtherpropose a straightforward negative sampling strategy to improve the assignmentof subjects and objects. Experimental results show that our method outperformsthe state-of-the-art methods on the SemEval-2010 Task 8 dataset.", "categories": ["cs.CL", "cs.LG"], "journal": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "192", "arxiv_url": "http://arxiv.org/abs/1506.07650v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=11960652150878715163&btnI=1&nossl=1&hl=en&oe=ASCII"}, "98": {"ID": 98, "title": "Linguistically-Informed Self-Attention for Semantic Role Labeling", "authors": ["Andrew McCallum", "Emma Strubell", "Patrick Verga", "David Weiss", "Daniel Andor"], "published": "2018-04-23T00:21:49Z", "updated": "2018-11-12T15:07:24Z", "abstract": "Current state-of-the-art semantic role labeling (SRL) uses a deep neuralnetwork with no explicit linguistic features. However, prior work has shownthat gold syntax trees can dramatically improve SRL decoding, suggesting thepossibility of increased accuracy from explicit modeling of syntax. In thiswork, we present linguistically-informed self-attention (LISA): a neuralnetwork model that combines multi-head self-attention with multi-task learningacross dependency parsing, part-of-speech tagging, predicate detection and SRL.Unlike previous models which require significant pre-processing to preparelinguistic features, LISA can incorporate syntax using merely raw tokens asinput, encoding the sequence only once to simultaneously perform parsing,predicate detection and role labeling for all predicates. Syntax isincorporated by training one attention head to attend to syntactic parents foreach token. Moreover, if a high-quality syntactic parse is already available,it can be beneficially injected at test time without re-training our SRL model.In experiments on CoNLL-2005 SRL, LISA achieves new state-of-the-artperformance for a model using predicted predicates and standard wordembeddings, attaining 2.5 F1 absolute higher than the previous state-of-the-arton newswire and more than 3.5 F1 on out-of-domain data, nearly 10% reduction inerror. On ConLL-2012 English SRL we also show an improvement of more than 2.5F1. LISA also out-performs the state-of-the-art with contextually-encoded(ELMo) word representations, by nearly 1.0 F1 on news and more than 2.0 F1 onout-of-domain text.", "categories": ["cs.CL"], "journal": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "156", "arxiv_url": "http://arxiv.org/abs/1804.08199v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=1929016137629003683&btnI=1&nossl=1&hl=en&oe=ASCII"}, "99": {"ID": 99, "title": "Attending to Characters in Neural Sequence Labeling Models", "authors": ["Marek Rei", "Sampo Pyysalo", "Gamal K. O. Crichton"], "published": "2016-11-14T12:36:07Z", "updated": "2016-11-14T12:36:07Z", "abstract": "Sequence labeling architectures use word embeddings for capturing similarity,but suffer when handling previously unseen or rare words. We investigatecharacter-level extensions to such models and propose a novel architecture forcombining alternative word representations. By using an attention mechanism,the model is able to dynamically decide how much information to use from aword- or character-level component. We evaluated different architectures on arange of sequence labeling datasets, and character-level extensions were foundto improve performance on every benchmark. In addition, the proposedattention-based architecture delivered the best results even with a smallernumber of trainable parameters.", "categories": ["cs.CL", "cs.LG", "cs.NE", "I.5.1; I.2.6; I.2.7"], "journal": "COLING, 309-318", "citations": "120", "arxiv_url": "http://arxiv.org/abs/1611.04361v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=3767483285761174620&btnI=1&nossl=1&hl=en&oe=ASCII"}, "100": {"ID": 100, "title": "Stance Classification in Rumours as a Sequential Task Exploiting the  Tree Structure of Social Media Conversations", "authors": ["Maria Liakata", "Michal Lukasik", "Elena Kochkina", "Arkaitz Zubiaga", "Rob Procter"], "published": "2016-09-28T18:24:12Z", "updated": "2016-10-11T11:54:36Z", "abstract": "Rumour stance classification, the task that determines if each tweet in acollection discussing a rumour is supporting, denying, questioning or simplycommenting on the rumour, has been attracting substantial interest. Here weintroduce a novel approach that makes use of the sequence of transitionsobserved in tree-structured conversation threads in Twitter. The conversationthreads are formed by harvesting users' replies to one another, which resultsin a nested tree-like structure. Previous work addressing the stanceclassification task has treated each tweet as a separate unit. Here we analysetweets by virtue of their position in a sequence and test two sequentialclassifiers, Linear-Chain CRF and Tree CRF, each of which makes differentassumptions about the conversational structure. We experiment with eightTwitter datasets, collected during breaking news, and show that exploiting thesequential structure of Twitter conversations achieves significant improvementsover the non-sequential methods. Our work is the first to model Twitterconversations as a tree structure in this manner, introducing a novel way oftackling NLP tasks on Twitter conversations.", "categories": ["cs.CL", "cs.SI"], "journal": "COLING, 2438-2448", "citations": "56", "arxiv_url": "http://arxiv.org/abs/1609.09028v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9817789726679790290&btnI=1&nossl=1&hl=en&oe=ASCII"}, "101": {"ID": 101, "title": "Effective LSTMs for Target-Dependent Sentiment Classification", "authors": ["Xiaocheng Feng", "Bing Qin", "Ting Liu", "Duyu Tang"], "published": "2015-12-03T14:54:39Z", "updated": "2016-09-29T09:40:39Z", "abstract": "Target-dependent sentiment classification remains a challenge: modeling thesemantic relatedness of a target with its context words in a sentence.Different context words have different influences on determining the sentimentpolarity of a sentence towards the target. Therefore, it is desirable tointegrate the connections between target word and context words when building alearning system. In this paper, we develop two target dependent long short-termmemory (LSTM) models, where target information is automatically taken intoaccount. We evaluate our methods on a benchmark dataset from Twitter. Empiricalresults show that modeling sentence representation with standard LSTM does notperform well. Incorporating target information into LSTM can significantlyboost the classification accuracy. The target-dependent LSTM models achievestate-of-the-art performances without using syntactic parser or externalsentiment lexicons.", "categories": ["cs.CL"], "journal": "COLING, 3298-3307", "citations": "331", "arxiv_url": "http://arxiv.org/abs/1512.01100v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=17415322636712715984&btnI=1&nossl=1&hl=en&oe=ASCII"}, "102": {"ID": 102, "title": "Stress Test Evaluation for Natural Language Inference", "authors": ["Abhilasha Ravichander", "Carolyn Rose", "Graham Neubig", "Norman Sadeh", "Aakanksha Naik"], "published": "2018-06-02T19:14:39Z", "updated": "2018-06-13T23:54:17Z", "abstract": "Natural language inference (NLI) is the task of determining if a naturallanguage hypothesis can be inferred from a given premise in a justifiablemanner. NLI was proposed as a benchmark task for natural languageunderstanding. Existing models perform well at standard datasets for NLI,achieving impressive results across different genres of text. However, theextent to which these models understand the semantic content of sentences isunclear. In this work, we propose an evaluation methodology consisting ofautomatically constructed \"stress tests\" that allow us to examine whethersystems have the ability to make real inferential decisions. Our evaluation ofsix sentence-encoder models on these stress tests reveals strengths andweaknesses of these models with respect to challenging linguistic phenomena,and suggests important directions for future work in this area.", "categories": ["cs.CL"], "journal": "COLING, 2340-2353", "citations": "67", "arxiv_url": "http://arxiv.org/abs/1806.00692v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=632890698344030156&btnI=1&nossl=1&hl=en&oe=ASCII"}, "103": {"ID": 103, "title": "Gated-Attention Readers for Text Comprehension", "authors": ["Ruslan Salakhutdinov", "Bhuwan Dhingra", "Zhilin Yang", "William W. Cohen", "Hanxiao Liu"], "published": "2016-06-05T19:30:39Z", "updated": "2017-04-21T18:50:05Z", "abstract": "In this paper we study the problem of answering cloze-style questions overdocuments. Our model, the Gated-Attention (GA) Reader, integrates a multi-hoparchitecture with a novel attention mechanism, which is based on multiplicativeinteractions between the query embedding and the intermediate states of arecurrent neural network document reader. This enables the reader to buildquery-specific representations of tokens in the document for accurate answerselection. The GA Reader obtains state-of-the-art results on three benchmarksfor this task--the CNN \\&amp; Daily Mail news stories and the Who Did What dataset.The effectiveness of multiplicative interaction is demonstrated by an ablationstudy, and by comparing to alternative compositional operators for implementingthe gated-attention. The code is available athttps://github.com/bdhingra/ga-reader.", "categories": ["cs.CL", "cs.LG"], "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "268", "arxiv_url": "http://arxiv.org/abs/1606.01549v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=3290403078094631843&btnI=1&nossl=1&hl=en&oe=ASCII"}, "104": {"ID": 104, "title": "Higher-order Coreference Resolution with Coarse-to-fine Inference", "authors": ["Luke Zettlemoyer", "Luheng He", "Kenton Lee"], "published": "2018-04-15T17:47:26Z", "updated": "2018-04-15T17:47:26Z", "abstract": "We introduce a fully differentiable approximation to higher-order inferencefor coreference resolution. Our approach uses the antecedent distribution froma span-ranking architecture as an attention mechanism to iteratively refinespan representations. This enables the model to softly consider multiple hopsin the predicted clusters. To alleviate the computational cost of thisiterative process, we introduce a coarse-to-fine approach that incorporates aless accurate but more efficient bilinear factor, enabling more aggressivepruning without hurting accuracy. Compared to the existing state-of-the-artspan-ranking approach, our model significantly improves accuracy on the EnglishOntoNotes benchmark, while being far more computationally efficient.", "categories": ["cs.CL"], "journal": "NAACL-HLT (2), 687-692", "citations": "95", "arxiv_url": "http://arxiv.org/abs/1804.05392v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8854562905498791944&btnI=1&nossl=1&hl=en&oe=ASCII"}, "105": {"ID": 105, "title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", "authors": ["Ruslan Salakhutdinov", "Quoc V. Le", "Zhilin Yang", "Zihang Dai", "Jaime Carbonell", "Yiming Yang"], "published": "2019-01-09T18:28:19Z", "updated": "2019-06-02T21:21:48Z", "abstract": "Transformers have a potential of learning longer-term dependency, but arelimited by a fixed-length context in the setting of language modeling. Wepropose a novel neural architecture Transformer-XL that enables learningdependency beyond a fixed length without disrupting temporal coherence. Itconsists of a segment-level recurrence mechanism and a novel positionalencoding scheme. Our method not only enables capturing longer-term dependency,but also resolves the context fragmentation problem. As a result,Transformer-XL learns dependency that is 80% longer than RNNs and 450% longerthan vanilla Transformers, achieves better performance on both short and longsequences, and is up to 1,800+ times faster than vanilla Transformers duringevaluation. Notably, we improve the state-of-the-art results of bpc/perplexityto 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One BillionWord, and 54.5 on Penn Treebank (without finetuning). When trained only onWikiText-103, Transformer-XL manages to generate reasonably coherent, noveltext articles with thousands of tokens. Our code, pretrained models, andhyperparameters are available in both Tensorflow and PyTorch.", "categories": ["cs.LG", "cs.CL", "stat.ML"], "journal": "Proceedings of the 57th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "445", "arxiv_url": "http://arxiv.org/abs/1901.02860v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=7150055013029036741&btnI=1&nossl=1&hl=en&oe=ASCII"}, "106": {"ID": 106, "title": "The NarrativeQA Reading Comprehension Challenge", "authors": ["Jonathan Schwarz", "Tom\u00e1\u0161 Ko\u010disk\u00fd", "G\u00e1bor Melis", "Phil Blunsom", "Chris Dyer", "Karl Moritz Hermann", "Edward Grefenstette"], "published": "2017-12-19T16:48:05Z", "updated": "2017-12-19T16:48:05Z", "abstract": "Reading comprehension (RC)---in contrast to information retrieval---requiresintegrating information and reasoning about events, entities, and theirrelations across a full document. Question answering is conventionally used toassess RC ability, in both artificial agents and children learning to read.However, existing RC datasets and tasks are dominated by questions that can besolved by selecting answers using superficial information (e.g., local contextsimilarity or global term frequency); they thus fail to test for the essentialintegrative aspect of RC. To encourage progress on deeper comprehension oflanguage, we present a new dataset and set of tasks in which the reader mustanswer questions about stories by reading entire books or movie scripts. Thesetasks are designed so that successfully answering their questions requiresunderstanding the underlying narrative rather than relying on shallow patternmatching or salience. We show that although humans solve the tasks easily,standard RC models struggle on the tasks presented here. We provide an analysisof the dataset and the challenges it presents.", "categories": ["cs.CL", "cs.AI", "cs.NE"], "journal": "Transactions of the Association for Computational Linguistics 6, 317-328", "citations": "156", "arxiv_url": "http://arxiv.org/abs/1712.07040v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=15318882709554150182&btnI=1&nossl=1&hl=en&oe=ASCII"}, "107": {"ID": 107, "title": "Siamese CBOW: Optimizing Word Embeddings for Sentence Representations", "authors": ["Alexey Borisov", "Tom Kenter", "Maarten de Rijke"], "published": "2016-06-15T04:47:43Z", "updated": "2016-06-15T04:47:43Z", "abstract": "We present the Siamese Continuous Bag of Words (Siamese CBOW) model, a neuralnetwork for efficient estimation of high-quality sentence embeddings. Averagingthe embeddings of words in a sentence has proven to be a surprisinglysuccessful and efficient way of obtaining sentence embeddings. However, wordembeddings trained with the methods currently available are not optimized forthe task of sentence representation, and, thus, likely to be suboptimal.Siamese CBOW handles this problem by training word embeddings directly for thepurpose of being averaged. The underlying neural network learns word embeddingsby predicting, from a sentence representation, its surrounding sentences. Weshow the robustness of the Siamese CBOW model by evaluating it on 20 datasetsstemming from a wide variety of sources.", "categories": ["cs.CL"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "168", "arxiv_url": "http://arxiv.org/abs/1606.04640v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=16772130221008662678&btnI=1&nossl=1&hl=en&oe=ASCII"}, "108": {"ID": 108, "title": "A Persona-Based Neural Conversation Model", "authors": ["Chris Brockett", "Jianfeng Gao", "Georgios P. Spithourakis", "Jiwei Li", "Michel Galley", "Bill Dolan"], "published": "2016-03-19T23:15:18Z", "updated": "2016-06-08T17:19:58Z", "abstract": "We present persona-based models for handling the issue of speaker consistencyin neural response generation. A speaker model encodes personas in distributedembeddings that capture individual characteristics such as backgroundinformation and speaking style. A dyadic speaker-addressee model capturesproperties of interactions between two interlocutors. Our models yieldqualitative performance improvements in both perplexity and BLEU scores overbaseline sequence-to-sequence models, with similar gains in speaker consistencyas measured by human judges.", "categories": ["cs.CL"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "597", "arxiv_url": "http://arxiv.org/abs/1603.06155v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14972915751484206058&btnI=1&nossl=1&hl=en&oe=ASCII"}, "109": {"ID": 109, "title": "Learning to Ask: Neural Question Generation for Reading Comprehension", "authors": ["Xinya Du", "Claire Cardie", "Junru Shao"], "published": "2017-04-29T01:08:48Z", "updated": "2017-04-29T01:08:48Z", "abstract": "We study automatic question generation for sentences from text passages inreading comprehension. We introduce an attention-based sequence learning modelfor the task and investigate the effect of encoding sentence- vs.paragraph-level information. In contrast to all previous work, our model doesnot rely on hand-crafted rules or a sophisticated NLP pipeline; it is insteadtrainable end-to-end via sequence-to-sequence learning. Automatic evaluationresults show that our system significantly outperforms the state-of-the-artrule-based system. In human evaluations, questions generated by our system arealso rated as being more natural (i.e., grammaticality, fluency) and as moredifficult to answer (in terms of syntactic and lexical divergence from theoriginal text and reasoning needed to answer).", "categories": ["cs.CL", "cs.AI"], "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "210", "arxiv_url": "http://arxiv.org/abs/1705.00106v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=5531417417138787871&btnI=1&nossl=1&hl=en&oe=ASCII"}, "110": {"ID": 110, "title": "Generating Sentences by Editing Prototypes", "authors": ["Tatsunori B. Hashimoto", "Yonatan Oren", "Kelvin Guu", "Percy Liang"], "published": "2017-09-26T08:11:33Z", "updated": "2018-09-07T04:57:15Z", "abstract": "We propose a new generative model of sentences that first samples a prototypesentence from the training corpus and then edits it into a new sentence.Compared to traditional models that generate from scratch either left-to-rightor by first sampling a latent sentence vector, our prototype-then-edit modelimproves perplexity on language modeling and generates higher quality outputsaccording to human evaluation. Furthermore, the model gives rise to a latentedit vector that captures interpretable semantics such as sentence similarityand sentence-level analogies.", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE", "stat.ML"], "journal": "Transactions of the Association for Computational Linguistics 6, 437-450", "citations": "140", "arxiv_url": "http://arxiv.org/abs/1709.08878v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=16553355556557056801&btnI=1&nossl=1&hl=en&oe=ASCII"}, "111": {"ID": 111, "title": "Neural Paraphrase Generation with Stacked Residual LSTM Networks", "authors": ["Ashequl Qadir", "Sadid A. Hasan", "Vivek Datla", "Kathy Lee", "Aaditya Prakash", "Joey Liu", "Oladimeji Farri"], "published": "2016-10-10T21:01:00Z", "updated": "2016-10-13T00:37:33Z", "abstract": "In this paper, we propose a novel neural approach for paraphrase generation.Conventional para- phrase generation methods either leverage hand-written rulesand thesauri-based alignments, or use statistical machine learning principles.To the best of our knowledge, this work is the first to explore deep learningmodels for paraphrase generation. Our primary contribution is a stackedresidual LSTM network, where we add residual connections between LSTM layers.This allows for efficient training of deep LSTMs. We evaluate our model andother state-of-the-art deep learning models on three different datasets: PPDB,WikiAnswers and MSCOCO. Evaluation results demonstrate that our modeloutperforms sequence to sequence, attention-based and bi- directional LSTMmodels on BLEU, METEOR, TER and an embedding-based sentence similarity metric.", "categories": ["cs.CL"], "journal": "COLING, 2923-2934", "citations": "116", "arxiv_url": "http://arxiv.org/abs/1610.03098v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10189136741870539189&btnI=1&nossl=1&hl=en&oe=ASCII"}, "112": {"ID": 112, "title": "Google's Multilingual Neural Machine Translation System: Enabling  Zero-Shot Translation", "authors": ["Melvin Johnson", "Zhifeng Chen", "Fernanda Vi\u00e9gas", "Yonghui Wu", "Macduff Hughes", "Mike Schuster", "Quoc V. Le", "Greg Corrado", "Nikhil Thorat", "Jeffrey Dean", "Maxim Krikun", "Martin Wattenberg"], "published": "2016-11-14T20:24:39Z", "updated": "2017-08-21T20:33:43Z", "abstract": "We propose a simple solution to use a single Neural Machine Translation (NMT)model to translate between multiple languages. Our solution requires no changein the model architecture from our base system but instead introduces anartificial token at the beginning of the input sentence to specify the requiredtarget language. The rest of the model, which includes encoder, decoder andattention, remains unchanged and is shared across all languages. Using a sharedwordpiece vocabulary, our approach enables Multilingual NMT using a singlemodel without any increase in parameters, which is significantly simpler thanprevious proposals for Multilingual NMT. Our method often improves thetranslation quality of all involved language pairs, even while keeping thetotal number of model parameters constant. On the WMT'14 benchmarks, a singlemultilingual model achieves comparable performance forEnglish$\\rightarrow$French and surpasses state-of-the-art results forEnglish$\\rightarrow$German. Similarly, a single multilingual model surpassesstate-of-the-art results for French$\\rightarrow$English andGerman$\\rightarrow$English on WMT'14 and WMT'15 benchmarks respectively. Onproduction corpora, multilingual models of up to twelve language pairs allowfor better translation of many individual pairs. In addition to improving thetranslation quality of language pairs that the model was trained with, ourmodels can also learn to perform implicit bridging between language pairs neverseen explicitly during training, showing that transfer learning and zero-shottranslation is possible for neural translation. Finally, we show analyses thathints at a universal interlingua representation in our models and show someinteresting examples when mixing languages.", "categories": ["cs.CL", "cs.AI"], "journal": "Transactions of the Association for Computational Linguistics 5, 339-351", "citations": "842", "arxiv_url": "http://arxiv.org/abs/1611.04558v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14714070858185645763&btnI=1&nossl=1&hl=en&oe=ASCII"}, "113": {"ID": 113, "title": "Using the Output Embedding to Improve Language Models", "authors": ["Lior Wolf", "Ofir Press"], "published": "2016-08-20T18:32:05Z", "updated": "2017-02-21T17:50:20Z", "abstract": "We study the topmost weight matrix of neural network language models. We showthat this matrix constitutes a valid word embedding. When training languagemodels, we recommend tying the input embedding and this output embedding. Weanalyze the resulting update rules and show that the tied embedding evolves ina more similar way to the output embedding than to the input embedding in theuntied model. We also offer a new method of regularizing the output embedding.Our methods lead to a significant reduction in perplexity, as we are able toshow on a variety of neural network language models. Finally, we show thatweight tying can reduce the size of neural translation models to less than halfof their original size without harming their performance.", "categories": ["cs.CL"], "journal": "EACL (2), 157-163", "citations": "366", "arxiv_url": "http://arxiv.org/abs/1608.05859v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=3142797974561089298&btnI=1&nossl=1&hl=en&oe=ASCII"}, "114": {"ID": 114, "title": "Joint Learning of the Embedding of Words and Entities for Named Entity  Disambiguation", "authors": ["Hideaki Takeda", "Ikuya Yamada", "Hiroyuki Shindo", "Yoshiyasu Takefuji"], "published": "2016-01-06T22:19:20Z", "updated": "2016-06-10T01:51:26Z", "abstract": "Named Entity Disambiguation (NED) refers to the task of resolving multiplenamed entity mentions in a document to their correct references in a knowledgebase (KB) (e.g., Wikipedia). In this paper, we propose a novel embedding methodspecifically designed for NED. The proposed method jointly maps words andentities into the same continuous vector space. We extend the skip-gram modelby using two models. The KB graph model learns the relatedness of entitiesusing the link structure of the KB, whereas the anchor context model aims toalign vectors such that similar words and entities occur close to one anotherin the vector space by leveraging KB anchors and their context words. Bycombining contexts based on the proposed embedding with standard NED features,we achieved state-of-the-art accuracy of 93.1% on the standard CoNLL datasetand 85.2% on the TAC 2010 dataset.", "categories": ["cs.CL"], "journal": "CoNLL, 250-259", "citations": "170", "arxiv_url": "http://arxiv.org/abs/1601.01343v4", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14767864336702462645&btnI=1&nossl=1&hl=en&oe=ASCII"}, "115": {"ID": 115, "title": "Learning Global Features for Coreference Resolution", "authors": ["Sam Wiseman", "Alexander M. Rush", "Stuart M. Shieber"], "published": "2016-04-11T17:15:34Z", "updated": "2016-04-11T17:15:34Z", "abstract": "There is compelling evidence that coreference prediction would benefit frommodeling global information about entity-clusters. Yet, state-of-the-artperformance can be achieved with systems treating each mention predictionindependently, which we attribute to the inherent difficulty of craftinginformative cluster-level features. We instead propose to use recurrent neuralnetworks (RNNs) to learn latent, global representations of entity clustersdirectly from their mentions. We show that such representations are especiallyuseful for the prediction of pronominal mentions, and can be incorporated intoan end-to-end coreference system that outperforms the state of the art withoutrequiring any additional search.", "categories": ["cs.CL"], "journal": "HLT-NAACL, 994-1004", "citations": "139", "arxiv_url": "http://arxiv.org/abs/1604.03035v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=12692050255746060471&btnI=1&nossl=1&hl=en&oe=ASCII"}, "116": {"ID": 116, "title": "Graph-based Neural Multi-Document Summarization", "authors": ["Rui Zhang", "Michihiro Yasunaga", "Kshitijh Meelu", "Dragomir Radev", "Krishnan Srinivasan", "Ayush Pareek"], "published": "2017-06-20T22:12:14Z", "updated": "2017-08-23T08:46:52Z", "abstract": "We propose a neural multi-document summarization (MDS) system thatincorporates sentence relation graphs. We employ a Graph Convolutional Network(GCN) on the relation graphs, with sentence embeddings obtained from RecurrentNeural Networks as input node features. Through multiple layer-wisepropagation, the GCN generates high-level hidden sentence features for salienceestimation. We then use a greedy heuristic to extract salient sentences whileavoiding redundancy. In our experiments on DUC 2004, we consider three types ofsentence relation graphs and demonstrate the advantage of combining sentencerelations in graphs with the representation power of deep neural networks. Ourmodel improves upon traditional graph-based extractive approaches and thevanilla GRU sequence model with no graph, and it achieves competitive resultsagainst other state-of-the-art multi-document summarization systems.", "categories": ["cs.CL", "cs.LG"], "journal": "Proceedings of the 21st Conference on Computational Natural Language\u00a0\u2026", "citations": "85", "arxiv_url": "http://arxiv.org/abs/1706.06681v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10202908905437352262&btnI=1&nossl=1&hl=en&oe=ASCII"}, "117": {"ID": 117, "title": "Learning to Compose Neural Networks for Question Answering", "authors": ["Dan Klein", "Marcus Rohrbach", "Trevor Darrell", "Jacob Andreas"], "published": "2016-01-07T21:21:59Z", "updated": "2016-06-07T23:25:51Z", "abstract": "We describe a question answering model that applies to both images andstructured knowledge bases. The model uses natural language strings toautomatically assemble neural networks from a collection of composable modules.Parameters for these modules are learned jointly with network-assemblyparameters via reinforcement learning, with only (world, question, answer)triples as supervision. Our approach, which we term a dynamic neural modelnetwork, achieves state-of-the-art results on benchmark datasets in both visualand structured domains.", "categories": ["cs.CL", "cs.CV", "cs.NE"], "journal": "HLT-NAACL, 1545-1554", "citations": "369", "arxiv_url": "http://arxiv.org/abs/1601.01705v4", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14872484850703865023&btnI=1&nossl=1&hl=en&oe=ASCII"}, "118": {"ID": 118, "title": "Structured prediction models for RNN based sequence labeling in clinical  text", "authors": ["Abhyuday Jagannatha", "Hong Yu"], "published": "2016-08-01T20:54:22Z", "updated": "2016-08-01T20:54:22Z", "abstract": "Sequence labeling is a widely used method for named entity recognition andinformation extraction from unstructured natural language data. In clinicaldomain one major application of sequence labeling involves extraction ofmedical entities such as medication, indication, and side-effects fromElectronic Health Record narratives. Sequence labeling in this domain, presentsits own set of challenges and objectives. In this work we experimented withvarious CRF based structured learning models with Recurrent Neural Networks. Weextend the previously studied LSTM-CRF models with explicit modeling ofpairwise potentials. We also propose an approximate version of skip-chain CRFinference with RNN potentials. We use these methodologies for structuredprediction in order to improve the exact phrase detection of various medicalentities.", "categories": ["cs.CL"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "116", "arxiv_url": "http://arxiv.org/abs/1608.00612v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10538104748245163377&btnI=1&nossl=1&hl=en&oe=ASCII"}, "119": {"ID": 119, "title": "Delete, Retrieve, Generate: A Simple Approach to Sentiment and Style  Transfer", "authors": ["Robin Jia", "Juncen Li", "He He", "Percy Liang"], "published": "2018-04-17T18:59:51Z", "updated": "2018-04-17T18:59:51Z", "abstract": "We consider the task of text attribute transfer: transforming a sentence toalter a specific attribute (e.g., sentiment) while preserving itsattribute-independent content (e.g., changing \"screen is just the right size\"to \"screen is too small\"). Our training data includes only sentences labeledwith their attribute (e.g., positive or negative), but not pairs of sentencesthat differ only in their attributes, so we must learn to disentangleattributes from attribute-independent content in an unsupervised way. Previouswork using adversarial methods has struggled to produce high-quality outputs.In this paper, we propose simpler methods motivated by the observation thattext attributes are often marked by distinctive phrases (e.g., \"too small\").Our strongest method extracts content words by deleting phrases associated withthe sentence's original attribute value, retrieves new phrases associated withthe target attribute, and uses a neural model to fluently combine these into afinal output. On human evaluation, our best method generates grammatical andappropriate responses on 22% more inputs than the best previous system,averaged over three attribute transfer datasets: altering sentiment of reviewson Yelp, altering sentiment of reviews on Amazon, and altering image captionsto be more romantic or humorous.", "categories": ["cs.CL"], "journal": "NAACL-HLT, 1865-1874", "citations": "150", "arxiv_url": "http://arxiv.org/abs/1804.06437v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6740663285538036044&btnI=1&nossl=1&hl=en&oe=ASCII"}, "120": {"ID": 120, "title": "Fast Abstractive Summarization with Reinforce-Selected Sentence  Rewriting", "authors": ["Mohit Bansal", "Yen-Chun Chen"], "published": "2018-05-28T17:49:10Z", "updated": "2018-05-28T17:49:10Z", "abstract": "Inspired by how humans summarize long documents, we propose an accurate andfast summarization model that first selects salient sentences and then rewritesthem abstractively (i.e., compresses and paraphrases) to generate a conciseoverall summary. We use a novel sentence-level policy gradient method to bridgethe non-differentiable computation between these two neural networks in ahierarchical way, while maintaining language fluency. Empirically, we achievethe new state-of-the-art on all metrics (including human evaluation) on theCNN/Daily Mail dataset, as well as significantly higher abstractiveness scores.Moreover, by first operating at the sentence-level and then the word-level, weenable parallel decoding of our neural generative model that results insubstantially faster (10-20x) inference speed as well as 4x faster trainingconvergence than previous long-paragraph encoder-decoder models. We alsodemonstrate the generalization of our model on the test-only DUC-2002 dataset,where we achieve higher scores than a state-of-the-art model.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "journal": "Proceedings of the 56th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "186", "arxiv_url": "http://arxiv.org/abs/1805.11080v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=2767929721787181726&btnI=1&nossl=1&hl=en&oe=ASCII"}, "121": {"ID": 121, "title": "Semi-supervised Word Sense Disambiguation with Neural Models", "authors": ["Dayu Yuan", "Ryan Doherty", "Eric Altendorf", "Colin Evans", "Julian Richardson"], "published": "2016-03-22T22:15:10Z", "updated": "2016-11-05T01:15:21Z", "abstract": "Determining the intended sense of words in text - word sense disambiguation(WSD) - is a long standing problem in natural language processing. Recently,researchers have shown promising results using word vectors extracted from aneural network language model as features in WSD algorithms. However, a simpleaverage or concatenation of word vectors for each word in a text loses thesequential and syntactic information of the text. In this paper, we study WSDwith a sequence learning neural net, LSTM, to better capture the sequential andsyntactic patterns of the text. To alleviate the lack of training data inall-words WSD, we employ the same LSTM in a semi-supervised label propagationclassifier. We demonstrate state-of-the-art results, especially on verbs.", "categories": ["cs.CL"], "journal": "COLING, 1374-1385", "citations": "97", "arxiv_url": "http://arxiv.org/abs/1603.07012v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=17857736891755655951&btnI=1&nossl=1&hl=en&oe=ASCII"}, "122": {"ID": 122, "title": "Translating Videos to Natural Language Using Deep Recurrent Neural  Networks", "authors": ["Marcus Rohrbach", "Huijuan Xu", "Subhashini Venugopalan", "Raymond Mooney", "Jeff Donahue", "Kate Saenko"], "published": "2014-12-15T19:21:50Z", "updated": "2015-04-30T04:22:06Z", "abstract": "Solving the visual symbol grounding problem has long been a goal ofartificial intelligence. The field appears to be advancing closer to this goalwith recent breakthroughs in deep learning for natural language grounding instatic images. In this paper, we propose to translate videos directly tosentences using a unified deep neural network with both convolutional andrecurrent structure. Described video datasets are scarce, and most existingmethods have been applied to toy domains with a small vocabulary of possiblewords. By transferring knowledge from 1.2M+ images with category labels and100,000+ images with captions, our method is able to create sentencedescriptions of open-domain videos with large vocabularies. We compare ourapproach with recent work using language generation metrics, subject, verb, andobject prediction accuracy, and a human evaluation.", "categories": ["cs.CV", "cs.CL"], "journal": "HLT-NAACL, 1494-1504", "citations": "677", "arxiv_url": "http://arxiv.org/abs/1412.4729v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=2018325242950133936&btnI=1&nossl=1&hl=en&oe=ASCII"}, "123": {"ID": 123, "title": "Long Short-Term Memory-Networks for Machine Reading", "authors": ["Mirella Lapata", "Jianpeng Cheng", "Li Dong"], "published": "2016-01-25T19:25:48Z", "updated": "2016-09-20T21:20:09Z", "abstract": "In this paper we address the question of how to render sequence-levelnetworks better at handling structured input. We propose a machine readingsimulator which processes text incrementally from left to right and performsshallow reasoning with memory and attention. The reader extends the LongShort-Term Memory architecture with a memory network in place of a singlememory cell. This enables adaptive memory usage during recurrence with neuralattention, offering a way to weakly induce relations among tokens. The systemis initially designed to process a single sequence but we also demonstrate howto integrate it with an encoder-decoder architecture. Experiments on languagemodeling, sentiment analysis, and natural language inference show that ourmodel matches or outperforms the state of the art.", "categories": ["cs.CL", "cs.NE"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "499", "arxiv_url": "http://arxiv.org/abs/1601.06733v7", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14301604565561549534&btnI=1&nossl=1&hl=en&oe=ASCII"}, "124": {"ID": 124, "title": "Classifying Relations via Long Short Term Memory Networks along Shortest  Dependency Path", "authors": ["Lili Mou", "Hao Peng", "Yunchuan Chen", "Xu Yan", "Ge Li", "Zhi Jin"], "published": "2015-08-15T11:15:32Z", "updated": "2015-08-15T11:15:32Z", "abstract": "Relation classification is an important research arena in the field ofnatural language processing (NLP). In this paper, we present SDP-LSTM, a novelneural network to classify the relation of two entities in a sentence. Ourneural architecture leverages the shortest dependency path (SDP) between twoentities; multichannel recurrent neural networks, with long short term memory(LSTM) units, pick up heterogeneous information along the SDP. Our proposedmodel has several distinct features: (1) The shortest dependency paths retainmost relevant information (to relation classification), while eliminatingirrelevant words in the sentence. (2) The multichannel LSTM networks alloweffective information integration from heterogeneous sources over thedependency paths. (3) A customized dropout strategy regularizes the neuralnetwork to alleviate overfitting. We test our model on the SemEval 2010relation classification task, and achieve an $F_1$-score of 83.7\\%, higher thancompeting methods in the literature.", "categories": ["cs.CL", "cs.LG"], "journal": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "330", "arxiv_url": "http://arxiv.org/abs/1508.03720v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=7434923510943166052&btnI=1&nossl=1&hl=en&oe=ASCII"}, "125": {"ID": 125, "title": "Multichannel Variable-Size Convolution for Sentence Classification", "authors": ["Hinrich Sch\u00fctze", "Wenpeng Yin"], "published": "2016-03-15T00:25:02Z", "updated": "2016-03-15T00:25:02Z", "abstract": "We propose MVCNN, a convolution neural network (CNN) architecture forsentence classification. It (i) combines diverse versions of pretrained wordembeddings and (ii) extracts features of multigranular phrases withvariable-size convolution filters. We also show that pretraining MVCNN iscritical for good performance. MVCNN achieves state-of-the-art performance onfour tasks: on small-scale binary, small-scale multi-class and largescaleTwitter sentiment prediction and on subjectivity classification.", "categories": ["cs.CL"], "journal": "Proceedings of the Nineteenth Conference on Computational Natural Language\u00a0\u2026", "citations": "109", "arxiv_url": "http://arxiv.org/abs/1603.04513v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=13800758588292721063&btnI=1&nossl=1&hl=en&oe=ASCII"}, "126": {"ID": 126, "title": "DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning", "authors": ["William Yang Wang", "Wenhan Xiong", "Thien Hoang"], "published": "2017-07-20T19:39:23Z", "updated": "2018-07-07T06:42:02Z", "abstract": "We study the problem of learning to reason in large scale knowledge graphs(KGs). More specifically, we describe a novel reinforcement learning frameworkfor learning multi-hop relational paths: we use a policy-based agent withcontinuous states based on knowledge graph embeddings, which reasons in a KGvector space by sampling the most promising relation to extend its path. Incontrast to prior work, our approach includes a reward function that takes theaccuracy, diversity, and efficiency into consideration. Experimentally, we showthat our proposed method outperforms a path-ranking based algorithm andknowledge graph embedding methods on Freebase and Never-Ending LanguageLearning datasets.", "categories": ["cs.CL", "cs.AI"], "journal": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "145", "arxiv_url": "http://arxiv.org/abs/1707.06690v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=13678530289575738006&btnI=1&nossl=1&hl=en&oe=ASCII"}, "127": {"ID": 127, "title": "Sentence Simplification with Deep Reinforcement Learning", "authors": ["Xingxing Zhang", "Mirella Lapata"], "published": "2017-03-31T15:05:45Z", "updated": "2017-07-16T02:28:14Z", "abstract": "Sentence simplification aims to make sentences easier to read and understand.Most recent approaches draw on insights from machine translation to learnsimplification rewrites from monolingual corpora of complex and simplesentences. We address the simplification problem with an encoder-decoder modelcoupled with a deep reinforcement learning framework. Our model, which we call{\\sc Dress} (as shorthand for {\\bf D}eep {\\bf RE}inforcement {\\bf S}entence{\\bf S}implification), explores the space of possible simplifications whilelearning to optimize a reward function that encourages outputs which aresimple, fluent, and preserve the meaning of the input. Experiments on threedatasets demonstrate that our model outperforms competitive simplificationsystems.", "categories": ["cs.CL", "cs.LG"], "journal": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "135", "arxiv_url": "http://arxiv.org/abs/1703.10931v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=17145229141243680335&btnI=1&nossl=1&hl=en&oe=ASCII"}, "128": {"ID": 128, "title": "Re-evaluating Automatic Metrics for Image Captioning", "authors": ["Erkut Erdem", "Mert Kilickaya", "Nazli Ikizler-Cinbis", "Aykut Erdem"], "published": "2016-12-22T14:00:28Z", "updated": "2016-12-22T14:00:28Z", "abstract": "The task of generating natural language descriptions from images has receiveda lot of attention in recent years. Consequently, it is becoming increasinglyimportant to evaluate such image captioning approaches in an automatic manner.In this paper, we provide an in-depth evaluation of the existing imagecaptioning metrics through a series of carefully designed experiments.Moreover, we explore the utilization of the recently proposed Word Mover'sDistance (WMD) document metric for the purpose of image captioning. Ourfindings outline the differences and/or similarities between metrics and theirrelative robustness by means of extensive correlation, accuracy and distractionbased evaluations. Our results also demonstrate that WMD provides strongadvantages over other metrics.", "categories": ["cs.CL", "cs.CV"], "journal": "EACL (1), 199-209", "citations": "74", "arxiv_url": "http://arxiv.org/abs/1612.07600v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=13697202347290523120&btnI=1&nossl=1&hl=en&oe=ASCII"}, "129": {"ID": 129, "title": "Are Emojis Predictable?", "authors": ["Miguel Ballesteros", "Horacio Saggion", "Francesco Barbieri"], "published": "2017-02-23T16:47:01Z", "updated": "2017-02-24T12:59:19Z", "abstract": "Emojis are ideograms which are naturally combined with plain text to visuallycomplement or condense the meaning of a message. Despite being widely used insocial media, their underlying semantics have received little attention from aNatural Language Processing standpoint. In this paper, we investigate therelation between words and emojis, studying the novel task of predicting whichemojis are evoked by text-based tweet messages. We train several models basedon Long Short-Term Memory networks (LSTMs) in this task. Our experimentalresults show that our neural model outperforms two baselines as well as humanssolving the same task, suggesting that computational models are able to bettercapture the underlying semantics of emojis.", "categories": ["cs.CL"], "journal": "EACL (2), 105-111", "citations": "82", "arxiv_url": "http://arxiv.org/abs/1702.07285v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8539027374179139926&btnI=1&nossl=1&hl=en&oe=ASCII"}, "130": {"ID": 130, "title": "Modeling Coverage for Neural Machine Translation", "authors": ["Zhengdong Lu", "Hang Li", "Zhaopeng Tu", "Yang Liu", "Xiaohua Liu"], "published": "2016-01-19T07:09:38Z", "updated": "2016-08-06T17:13:04Z", "abstract": "Attention mechanism has enhanced state-of-the-art Neural Machine Translation(NMT) by jointly learning to align and translate. It tends to ignore pastalignment information, however, which often leads to over-translation andunder-translation. To address this problem, we propose coverage-based NMT inthis paper. We maintain a coverage vector to keep track of the attentionhistory. The coverage vector is fed to the attention model to help adjustfuture attention, which lets NMT system to consider more about untranslatedsource words. Experiments show that the proposed approach significantlyimproves both translation quality and alignment quality over standardattention-based NMT.", "categories": ["cs.CL"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "505", "arxiv_url": "http://arxiv.org/abs/1601.04811v6", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=894656013823838967&btnI=1&nossl=1&hl=en&oe=ASCII"}, "131": {"ID": 131, "title": "Inducing Domain-Specific Sentiment Lexicons from Unlabeled Corpora", "authors": ["Jure Leskovec", "Dan Jurafsky", "William L. Hamilton", "Kevin Clark"], "published": "2016-06-09T04:28:10Z", "updated": "2016-09-24T03:12:09Z", "abstract": "A word's sentiment depends on the domain in which it is used. Computationalsocial science research thus requires sentiment lexicons that are specific tothe domains being studied. We combine domain-specific word embeddings with alabel propagation framework to induce accurate domain-specific sentimentlexicons using small sets of seed words, achieving state-of-the-art performancecompetitive with approaches that rely on hand-curated resources. Using ourframework we perform two large-scale empirical studies to quantify the extentto which sentiment varies across time and between communities. We induce andrelease historical sentiment lexicons for 150 years of English andcommunity-specific sentiment lexicons for 250 online communities from thesocial media forum Reddit. The historical lexicons show that more than 5% ofsentiment-bearing (non-neutral) English words completely switched polarityduring the last 150 years, and the community-specific lexicons highlight howsentiment varies drastically between different communities.", "categories": ["cs.CL"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "188", "arxiv_url": "http://arxiv.org/abs/1606.02820v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9450931883404078342&btnI=1&nossl=1&hl=en&oe=ASCII"}, "132": {"ID": 132, "title": "Learning Distributed Representations of Texts and Entities from  Knowledge Base", "authors": ["Hideaki Takeda", "Ikuya Yamada", "Hiroyuki Shindo", "Yoshiyasu Takefuji"], "published": "2017-05-06T15:11:30Z", "updated": "2017-11-07T15:27:55Z", "abstract": "We describe a neural network model that jointly learns distributedrepresentations of texts and knowledge base (KB) entities. Given a text in theKB, we train our proposed model to predict entities that are relevant to thetext. Our model is designed to be generic with the ability to address variousNLP tasks with ease. We train the model using a large corpus of texts and theirentity annotations extracted from Wikipedia. We evaluated the model on threeimportant NLP tasks (i.e., sentence textual similarity, entity linking, andfactoid question answering) involving both unsupervised and supervisedsettings. As a result, we achieved state-of-the-art results on all three ofthese tasks. Our code and trained models are publicly available for furtheracademic research.", "categories": ["cs.CL", "cs.NE"], "journal": "Transactions of the Association for Computational Linguistics 5, 397-411", "citations": "55", "arxiv_url": "http://arxiv.org/abs/1705.02494v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=17573737401611165276&btnI=1&nossl=1&hl=en&oe=ASCII"}, "133": {"ID": 133, "title": "Universal Neural Machine Translation for Extremely Low Resource  Languages", "authors": ["Hany Hassan", "Jiatao Gu", "Victor O. K. Li", "Jacob Devlin"], "published": "2018-02-15T00:35:08Z", "updated": "2018-04-17T02:15:20Z", "abstract": "In this paper, we propose a new universal machine translation approachfocusing on languages with a limited amount of parallel data. Our proposedapproach utilizes a transfer-learning approach to share lexical and sentencelevel representations across multiple source languages into one targetlanguage. The lexical part is shared through a Universal Lexical Representationto support multilingual word-level sharing. The sentence-level sharing isrepresented by a model of experts from all source languages that share thesource encoders with all other languages. This enables the low-resourcelanguage to utilize the lexical and sentence representations of the higherresource languages. Our approach is able to achieve 23 BLEU on Romanian-EnglishWMT2016 using a tiny parallel corpus of 6k sentences, compared to the 18 BLEUof strong baseline system which uses multilingual training andback-translation. Furthermore, we show that the proposed approach can achievealmost 20 BLEU on the same dataset through fine-tuning a pre-trainedmulti-lingual system in a zero-shot setting.", "categories": ["cs.CL"], "journal": "NAACL-HLT, 344-354", "citations": "97", "arxiv_url": "http://arxiv.org/abs/1802.05368v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=17858246967554922903&btnI=1&nossl=1&hl=en&oe=ASCII"}, "134": {"ID": 134, "title": "Many Languages, One Parser", "authors": ["Waleed Ammar", "Noah A. Smith", "George Mulcaire", "Chris Dyer", "Miguel Ballesteros"], "published": "2016-02-04T08:51:18Z", "updated": "2016-07-26T06:30:50Z", "abstract": "We train one multilingual model for dependency parsing and use it to parsesentences in several languages. The parsing model uses (i) multilingual wordclusters and embeddings; (ii) token-level language information; and (iii)language-specific features (fine-grained POS tags). This input representationenables the parser not only to parse effectively in multiple languages, butalso to generalize across languages based on linguistic universals andtypological similarities, making it more effective to learn from limitedannotations. Our parser's performance compares favorably to strong baselines ina range of data scenarios, including when the target language has a largetreebank, a small treebank, or no treebank for training.", "categories": ["cs.CL"], "journal": "Transactions of the Association for Computational Linguistics 4, 431-444", "citations": "150", "arxiv_url": "http://arxiv.org/abs/1602.01595v4", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=12564317758186692290&btnI=1&nossl=1&hl=en&oe=ASCII"}, "135": {"ID": 135, "title": "LCSTS: A Large Scale Chinese Short Text Summarization Dataset", "authors": ["Qingcai Chen", "Baotian Hu", "Fangze Zhu"], "published": "2015-06-19T02:40:42Z", "updated": "2016-02-19T16:35:35Z", "abstract": "Automatic text summarization is widely regarded as the highly difficultproblem, partially because of the lack of large text summarization data set.Due to the great challenge of constructing the large scale summaries for fulltext, in this paper, we introduce a large corpus of Chinese short textsummarization dataset constructed from the Chinese microblogging website SinaWeibo, which is released to the public{http://icrc.hitsz.edu.cn/Article/show/139.html}. This corpus consists of over2 million real Chinese short texts with short summaries given by the author ofeach text. We also manually tagged the relevance of 10,666 short summaries withtheir corresponding short texts. Based on the corpus, we introduce recurrentneural network for the summary generation and achieve promising results, whichnot only shows the usefulness of the proposed corpus for short textsummarization research, but also provides a baseline for further research onthis topic.", "categories": ["cs.CL", "cs.IR", "cs.LG"], "journal": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "145", "arxiv_url": "http://arxiv.org/abs/1506.05865v4", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=3479693408082022362&btnI=1&nossl=1&hl=en&oe=ASCII"}, "136": {"ID": 136, "title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning  Over Paragraphs", "authors": ["Dheeru Dua", "Matt Gardner", "Sameer Singh", "Gabriel Stanovsky", "Yizhong Wang", "Pradeep Dasigi"], "published": "2019-03-01T05:32:01Z", "updated": "2019-04-16T21:22:39Z", "abstract": "Reading comprehension has recently seen rapid progress, with systems matchinghumans on the most popular datasets for the task. However, a large body of workhas highlighted the brittleness of these systems, showing that there is muchwork left to be done. We introduce a new English reading comprehensionbenchmark, DROP, which requires Discrete Reasoning Over the content ofParagraphs. In this crowdsourced, adversarially-created, 96k-questionbenchmark, a system must resolve references in a question, perhaps to multipleinput positions, and perform discrete operations over them (such as addition,counting, or sorting). These operations require a much more comprehensiveunderstanding of the content of paragraphs than what was necessary for priordatasets. We apply state-of-the-art methods from both the reading comprehensionand semantic parsing literature on this dataset and show that the best systemsonly achieve 32.7% F1 on our generalized accuracy metric, while expert humanperformance is 96.0%. We additionally present a new model that combines readingcomprehension methods with simple numerical reasoning to achieve 47.0% F1.", "categories": ["cs.CL"], "journal": "NAACL-HLT (1), 2368-2378", "citations": "92", "arxiv_url": "http://arxiv.org/abs/1903.00161v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8475683595494768806&btnI=1&nossl=1&hl=en&oe=ASCII"}, "137": {"ID": 137, "title": "HotFlip: White-Box Adversarial Examples for Text Classification", "authors": ["Daniel Lowd", "Anyi Rao", "Javid Ebrahimi", "Dejing Dou"], "published": "2017-12-19T02:15:19Z", "updated": "2018-05-24T16:43:45Z", "abstract": "We propose an efficient method to generate white-box adversarial examples totrick a character-level neural classifier. We find that only a fewmanipulations are needed to greatly decrease the accuracy. Our method relies onan atomic flip operation, which swaps one token for another, based on thegradients of the one-hot input vectors. Due to efficiency of our method, we canperform adversarial training which makes the model more robust to attacks attest time. With the use of a few semantics-preserving constraints, wedemonstrate that HotFlip can be adapted to attack a word-level classifier aswell.", "categories": ["cs.CL", "cs.LG"], "journal": "Proceedings of the 56th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "177", "arxiv_url": "http://arxiv.org/abs/1712.06751v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=16462693373197108225&btnI=1&nossl=1&hl=en&oe=ASCII"}, "138": {"ID": 138, "title": "A Retrospective Analysis of the Fake News Challenge Stance Detection  Task", "authors": ["Iryna Gurevych", "Felix Caspelherr", "Andreas Hanselowski", "Benjamin Schiller", "Christian M. Meyer", "Debanjan Chaudhuri", "Avinesh PVS"], "published": "2018-06-13T15:38:09Z", "updated": "2018-06-13T15:38:09Z", "abstract": "The 2017 Fake News Challenge Stage 1 (FNC-1) shared task addressed a stanceclassification task as a crucial first step towards detecting fake news. Todate, there is no in-depth analysis paper to critically discuss FNC-1'sexperimental setup, reproduce the results, and draw conclusions fornext-generation stance classification methods. In this paper, we provide suchan in-depth analysis for the three top-performing systems. We first find thatFNC-1's proposed evaluation metric favors the majority class, which can beeasily classified, and thus overestimates the true discriminative power of themethods. Therefore, we propose a new F1-based metric yielding a changed systemranking. Next, we compare the features and architectures used, which leads to anovel feature-rich stacked LSTM model that performs on par with the bestsystems, but is superior in predicting minority classes. To understand themethods' ability to generalize, we derive a new dataset and perform bothin-domain and cross-domain experiments. Our qualitative and quantitative studyhelps interpreting the original FNC-1 scores and understand which features helpimproving performance and why. Our new dataset and all source code used duringthe reproduction study are publicly available for future research.", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.SI"], "journal": "COLING, 1859-1874", "citations": "58", "arxiv_url": "http://arxiv.org/abs/1806.05180v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6099233364910438650&btnI=1&nossl=1&hl=en&oe=ASCII"}, "139": {"ID": 139, "title": "Counter-fitting Word Vectors to Linguistic Constraints", "authors": ["David Vandyke", "Tsung-Hsien Wen", "Lina Rojas-Barahona", "Steve Young", "Pei-Hao Su", "Diarmuid \u00d3 S\u00e9aghdha", "Blaise Thomson", "Nikola Mrk\u0161i\u0107", "Milica Ga\u0161i\u0107"], "published": "2016-03-02T21:19:36Z", "updated": "2016-03-02T21:19:36Z", "abstract": "In this work, we present a novel counter-fitting method which injectsantonymy and synonymy constraints into vector space representations in order toimprove the vectors' capability for judging semantic similarity. Applying thismethod to publicly available pre-trained word vectors leads to a new state ofthe art performance on the SimLex-999 dataset. We also show how the method canbe used to tailor the word vector space for the downstream task of dialoguestate tracking, resulting in robust improvements across different dialoguedomains.", "categories": ["cs.CL", "cs.LG"], "journal": "HLT-NAACL, 142-148", "citations": "190", "arxiv_url": "http://arxiv.org/abs/1603.00892v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6505490430902838724&btnI=1&nossl=1&hl=en&oe=ASCII"}, "140": {"ID": 140, "title": "Learning Discourse-level Diversity for Neural Dialog Models using  Conditional Variational Autoencoders", "authors": ["Maxine Eskenazi", "Ran Zhao", "Tiancheng Zhao"], "published": "2017-03-31T15:55:00Z", "updated": "2017-10-21T04:58:20Z", "abstract": "While recent neural encoder-decoder models have shown great promise inmodeling open-domain conversations, they often generate dull and genericresponses. Unlike past work that has focused on diversifying the output of thedecoder at word-level to alleviate this problem, we present a novel frameworkbased on conditional variational autoencoders that captures the discourse-leveldiversity in the encoder. Our model uses latent variables to learn adistribution over potential conversational intents and generates diverseresponses using only greedy decoders. We have further developed a novel variantthat is integrated with linguistic prior knowledge for better performance.Finally, the training procedure is improved by introducing a bag-of-word loss.Our proposed models have been validated to generate significantly more diverseresponses than baseline approaches and exhibit competence in discourse-leveldecision-making.", "categories": ["cs.CL", "cs.AI"], "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "295", "arxiv_url": "http://arxiv.org/abs/1703.10960v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6713302634954347675&btnI=1&nossl=1&hl=en&oe=ASCII"}, "141": {"ID": 141, "title": "Challenges in Data-to-Document Generation", "authors": ["Sam Wiseman", "Alexander M. Rush", "Stuart M. Shieber"], "published": "2017-07-25T15:42:25Z", "updated": "2017-07-25T15:42:25Z", "abstract": "Recent neural models have shown significant progress on the problem ofgenerating short descriptive texts conditioned on a small number of databaserecords. In this work, we suggest a slightly more difficult data-to-textgeneration task, and investigate how effective current approaches are on thistask. In particular, we introduce a new, large-scale corpus of data recordspaired with descriptive documents, propose a series of extractive evaluationmethods for analyzing performance, and obtain baseline results using currentneural generation methods. Experiments show that these models produce fluenttext, but fail to convincingly approximate human-generated documents. Moreover,even templated baselines exceed the performance of these neural models on somemetrics, though copy- and reconstruction-based extensions lead to noticeableimprovements.", "categories": ["cs.CL"], "journal": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "162", "arxiv_url": "http://arxiv.org/abs/1707.08052v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8565991613457486578&btnI=1&nossl=1&hl=en&oe=ASCII"}, "142": {"ID": 142, "title": "Attention-over-Attention Neural Networks for Reading Comprehension", "authors": ["Zhipeng Chen", "Yiming Cui", "Si Wei", "Ting Liu", "Shijin Wang", "Guoping Hu"], "published": "2016-07-15T09:10:11Z", "updated": "2017-06-06T02:51:54Z", "abstract": "Cloze-style queries are representative problems in reading comprehension.Over the past few months, we have seen much progress that utilizing neuralnetwork approach to solve Cloze-style questions. In this paper, we present anovel model called attention-over-attention reader for the Cloze-style readingcomprehension task. Our model aims to place another attention mechanism overthe document-level attention, and induces \"attended attention\" for finalpredictions. Unlike the previous works, our neural network model requires lesspre-defined hyper-parameters and uses an elegant architecture for modeling.Experimental results show that the proposed attention-over-attention modelsignificantly outperforms various state-of-the-art systems by a large margin inpublic datasets, such as CNN and Children's Book Test datasets.", "categories": ["cs.CL", "cs.NE"], "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "273", "arxiv_url": "http://arxiv.org/abs/1607.04423v4", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=11046322364676774514&btnI=1&nossl=1&hl=en&oe=ASCII"}, "143": {"ID": 143, "title": "Modeling Multi-turn Conversation with Deep Utterance Aggregation", "authors": ["Pengfei Zhu", "Gongshen Liu", "Jiangtong Li", "Zhuosheng Zhang", "Hai Zhao"], "published": "2018-06-24T08:15:54Z", "updated": "2018-11-06T16:27:27Z", "abstract": "Multi-turn conversation understanding is a major challenge for buildingintelligent dialogue systems. This work focuses on retrieval-based responsematching for multi-turn conversation whose related work simply concatenates theconversation utterances, ignoring the interactions among previous utterancesfor context modeling. In this paper, we formulate previous utterances intocontext using a proposed deep utterance aggregation model to form afine-grained context representation. In detail, a self-matching attention isfirst introduced to route the vital information in each utterance. Then themodel matches a response with each refined utterance and the final matchingscore is obtained after attentive turns aggregation. Experimental results showour model outperforms the state-of-the-art methods on three multi-turnconversation benchmarks, including a newly introduced e-commerce dialoguecorpus.", "categories": ["cs.CL"], "journal": "COLING, 3740-3752", "citations": "75", "arxiv_url": "http://arxiv.org/abs/1806.09102v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=11112289968869875021&btnI=1&nossl=1&hl=en&oe=ASCII"}, "144": {"ID": 144, "title": "Chains of Reasoning over Entities, Relations, and Text using Recurrent  Neural Networks", "authors": ["Arvind Neelakantan", "Rajarshi Das", "Andrew McCallum", "David Belanger"], "published": "2016-07-05T21:59:04Z", "updated": "2017-05-01T14:10:43Z", "abstract": "Our goal is to combine the rich multistep inference of symbolic logicalreasoning with the generalization capabilities of neural networks. We areparticularly interested in complex reasoning about entities and relations intext and large-scale knowledge bases (KBs). Neelakantan et al. (2015) use RNNsto compose the distributed semantics of multi-hop paths in KBs; however formultiple reasons, the approach lacks accuracy and practicality. This paperproposes three significant modeling advances: (1) we learn to jointly reasonabout relations, entities, and entity-types; (2) we use neural attentionmodeling to incorporate multiple paths; (3) we learn to share strength in asingle RNN that represents logical composition across all relations. On alargescale Freebase+ClueWeb prediction task, we achieve 25% error reduction,and a 53% error reduction on sparse relations due to shared strength. On chainsof reasoning in WordNet we reduce error in mean quantile by 84% versus previousstate-of-the-art. The code and data are available athttps://rajarshd.github.io/ChainsofReasoning", "categories": ["cs.CL"], "journal": "EACL (1), 132-141", "citations": "125", "arxiv_url": "http://arxiv.org/abs/1607.01426v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10584306955592503684&btnI=1&nossl=1&hl=en&oe=ASCII"}, "145": {"ID": 145, "title": "Incorporating Copying Mechanism in Sequence-to-Sequence Learning", "authors": ["Jiatao Gu", "Hang Li", "Victor O. K. Li", "Zhengdong Lu"], "published": "2016-03-21T11:35:08Z", "updated": "2016-06-08T13:53:21Z", "abstract": "We address an important problem in sequence-to-sequence (Seq2Seq) learningreferred to as copying, in which certain segments in the input sequence areselectively replicated in the output sequence. A similar phenomenon isobservable in human language communication. For example, humans tend to repeatentity names or even long phrases in conversation. The challenge with regard tocopying in Seq2Seq is that new machinery is needed to decide when to performthe operation. In this paper, we incorporate copying into neural network-basedSeq2Seq learning and propose a new model called CopyNet with encoder-decoderstructure. CopyNet can nicely integrate the regular way of word generation inthe decoder with the new copying mechanism which can choose sub-sequences inthe input sequence and put them at proper places in the output sequence. Ourempirical study on both synthetic data sets and real world data setsdemonstrates the efficacy of CopyNet. For example, CopyNet can outperformregular RNN-based model with remarkable margins on text summarization tasks.", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "783", "arxiv_url": "http://arxiv.org/abs/1603.06393v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6836221883265474919&btnI=1&nossl=1&hl=en&oe=ASCII"}, "146": {"ID": 146, "title": "Neural Machine Translation with Recurrent Attention Modeling", "authors": ["Alex Smola", "Zichao Yang", "Yuntian Deng", "Zhiting Hu", "Chris Dyer"], "published": "2016-07-18T14:44:26Z", "updated": "2016-07-18T14:44:26Z", "abstract": "Knowing which words have been attended to in previous time steps whilegenerating a translation is a rich source of information for predicting whatwords will be attended to in the future. We improve upon the attention model ofBahdanau et al. (2014) by explicitly modeling the relationship between previousand subsequent attention levels for each word using one recurrent network perinput word. This architecture easily captures informative features, such asfertility and regularities in relative distortion. In experiments, we show ourparameterization of attention improves translation quality.", "categories": ["cs.NE", "cs.CL"], "journal": "EACL (2), 383-387", "citations": "48", "arxiv_url": "http://arxiv.org/abs/1607.05108v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=5621977008323303060&btnI=1&nossl=1&hl=en&oe=ASCII"}, "147": {"ID": 147, "title": "End-to-end Neural Coreference Resolution", "authors": ["Luke Zettlemoyer", "Luheng He", "Kenton Lee", "Mike Lewis"], "published": "2017-07-21T21:05:04Z", "updated": "2017-12-15T21:45:56Z", "abstract": "We introduce the first end-to-end coreference resolution model and show thatit significantly outperforms all previous work without using a syntactic parseror hand-engineered mention detector. The key idea is to directly consider allspans in a document as potential mentions and learn distributions over possibleantecedents for each. The model computes span embeddings that combinecontext-dependent boundary representations with a head-finding attentionmechanism. It is trained to maximize the marginal likelihood of gold antecedentspans from coreference clusters and is factored to enable aggressive pruning ofpotential mentions. Experiments demonstrate state-of-the-art performance, witha gain of 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5-modelensemble, despite the fact that this is the first approach to be successfullytrained with no external resources.", "categories": ["cs.CL"], "journal": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "294", "arxiv_url": "http://arxiv.org/abs/1707.07045v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=17942002369439227392&btnI=1&nossl=1&hl=en&oe=ASCII"}, "148": {"ID": 148, "title": "Modeling Relation Paths for Representation Learning of Knowledge Bases", "authors": ["Maosong Sun", "Huanbo Luan", "Song Liu", "Yankai Lin", "Zhiyuan Liu", "Siwei Rao"], "published": "2015-06-01T08:22:49Z", "updated": "2015-08-15T09:28:49Z", "abstract": "Representation learning of knowledge bases (KBs) aims to embed both entitiesand relations into a low-dimensional space. Most existing methods only considerdirect relations in representation learning. We argue that multiple-steprelation paths also contain rich inference patterns between entities, andpropose a path-based representation learning model. This model considersrelation paths as translations between entities for representation learning,and addresses two key challenges: (1) Since not all relation paths arereliable, we design a path-constraint resource allocation algorithm to measurethe reliability of relation paths. (2) We represent relation paths via semanticcomposition of relation embeddings. Experimental results on real-world datasetsshow that, as compared with baselines, our model achieves significant andconsistent improvements on knowledge base completion and relation extractionfrom text.", "categories": ["cs.CL"], "journal": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "326", "arxiv_url": "http://arxiv.org/abs/1506.00379v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4897049065397298143&btnI=1&nossl=1&hl=en&oe=ASCII"}, "149": {"ID": 149, "title": "Using millions of emoji occurrences to learn any-domain representations  for detecting sentiment, emotion and sarcasm", "authors": ["Bjarke Felbo", "Alan Mislove", "Anders S\u00f8gaard", "Iyad Rahwan", "Sune Lehmann"], "published": "2017-08-01T21:28:42Z", "updated": "2017-10-07T19:21:48Z", "abstract": "NLP tasks are often limited by scarcity of manually annotated data. In socialmedia sentiment analysis and related tasks, researchers have therefore usedbinarized emoticons and specific hashtags as forms of distant supervision. Ourpaper shows that by extending the distant supervision to a more diverse set ofnoisy labels, the models can learn richer representations. Through emojiprediction on a dataset of 1246 million tweets containing one of 64 commonemojis we obtain state-of-the-art performance on 8 benchmark datasets withinsentiment, emotion and sarcasm detection using a single pretrained model. Ouranalyses confirm that the diversity of our emotional labels yield a performanceimprovement over previous distant supervision approaches.", "categories": ["stat.ML", "cs.LG"], "journal": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "314", "arxiv_url": "http://arxiv.org/abs/1708.00524v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=18302390388674057994&btnI=1&nossl=1&hl=en&oe=ASCII"}, "150": {"ID": 150, "title": "Capturing Semantic Similarity for Entity Linking with Convolutional  Neural Networks", "authors": ["Dan Klein", "Matthew Francis-Landau", "Greg Durrett"], "published": "2016-04-04T03:58:31Z", "updated": "2016-04-04T03:58:31Z", "abstract": "A key challenge in entity linking is making effective use of contextualinformation to disambiguate mentions that might refer to different entities indifferent contexts. We present a model that uses convolutional neural networksto capture semantic correspondence between a mention's context and a proposedtarget entity. These convolutional networks operate at multiple granularitiesto exploit various kinds of topic information, and their rich parameterizationgives them the capacity to learn which n-grams characterize different topics.We combine these networks with a sparse linear model to achievestate-of-the-art performance on multiple entity linking datasets, outperformingthe prior systems of Durrett and Klein (2014) and Nguyen et al. (2014).", "categories": ["cs.CL"], "journal": "HLT-NAACL, 1256-1261", "citations": "113", "arxiv_url": "http://arxiv.org/abs/1604.00734v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=1166632434786195158&btnI=1&nossl=1&hl=en&oe=ASCII"}, "151": {"ID": 151, "title": "Ranking Sentences for Extractive Summarization with Reinforcement  Learning", "authors": ["Mirella Lapata", "Shay B. Cohen", "Shashi Narayan"], "published": "2018-02-23T16:55:31Z", "updated": "2018-04-16T14:03:41Z", "abstract": "Single document summarization is the task of producing a shorter version of adocument while preserving its principal information content. In this paper weconceptualize extractive summarization as a sentence ranking task and propose anovel training algorithm which globally optimizes the ROUGE evaluation metricthrough a reinforcement learning objective. We use our algorithm to train aneural summarization model on the CNN and DailyMail datasets and demonstrateexperimentally that it outperforms state-of-the-art extractive and abstractivesystems when evaluated automatically and by humans.", "categories": ["cs.CL"], "journal": "NAACL-HLT, 1747-1759", "citations": "178", "arxiv_url": "http://arxiv.org/abs/1802.08636v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=11274224118411029557&btnI=1&nossl=1&hl=en&oe=ASCII"}, "152": {"ID": 152, "title": "A Character-Level Decoder without Explicit Segmentation for Neural  Machine Translation", "authors": ["Yoshua Bengio", "Junyoung Chung", "Kyunghyun Cho"], "published": "2016-03-19T21:35:04Z", "updated": "2016-06-21T01:12:22Z", "abstract": "The existing machine translation systems, whether phrase-based or neural,have relied almost exclusively on word-level modelling with explicitsegmentation. In this paper, we ask a fundamental question: can neural machinetranslation generate a character sequence without any explicit segmentation? Toanswer this question, we evaluate an attention-based encoder-decoder with asubword-level encoder and a character-level decoder on four languagepairs--En-Cs, En-De, En-Ru and En-Fi-- using the parallel corpora from WMT'15.Our experiments show that the models with a character-level decoder outperformthe ones with a subword-level decoder on all of the four language pairs.Furthermore, the ensembles of neural models with a character-level decoderoutperform the state-of-the-art non-neural machine translation systems onEn-Cs, En-De and En-Fi and perform comparably on En-Ru.", "categories": ["cs.CL", "cs.LG"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "271", "arxiv_url": "http://arxiv.org/abs/1603.06147v4", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=2193535701900882329&btnI=1&nossl=1&hl=en&oe=ASCII"}, "153": {"ID": 153, "title": "Automatic Detection of Fake News", "authors": ["Bennett Kleinberg", "Ver\u00f3nica P\u00e9rez-Rosas", "Alexandra Lefevre", "Rada Mihalcea"], "published": "2017-08-23T17:12:03Z", "updated": "2017-08-23T17:12:03Z", "abstract": "The proliferation of misleading information in everyday access media outletssuch as social media feeds, news blogs, and online newspapers have made itchallenging to identify trustworthy news sources, thus increasing the need forcomputational tools able to provide insights into the reliability of onlinecontent. In this paper, we focus on the automatic identification of fakecontent in online news. Our contribution is twofold. First, we introduce twonovel datasets for the task of fake news detection, covering seven differentnews domains. We describe the collection, annotation, and validation process indetail and present several exploratory analysis on the identification oflinguistic differences in fake and legitimate news content. Second, we conducta set of learning experiments to build accurate fake news detectors. Inaddition, we provide comparative analyses of the automatic and manualidentification of fake news.", "categories": ["cs.CL"], "journal": "COLING, 3391-3401", "citations": "178", "arxiv_url": "http://arxiv.org/abs/1708.07104v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4593197943534315227&btnI=1&nossl=1&hl=en&oe=ASCII"}, "154": {"ID": 154, "title": "Improving Topic Models with Latent Feature Word Representations", "authors": ["Richard Billingsley", "Dat Quoc Nguyen", "Mark Johnson", "Lan Du"], "published": "2018-10-15T12:34:05Z", "updated": "2018-10-15T12:34:05Z", "abstract": "Probabilistic topic models are widely used to discover latent topics indocument collections, while latent feature vector representations of words havebeen used to obtain high performance in many NLP tasks. In this paper, weextend two different Dirichlet multinomial topic models by incorporating latentfeature vector representations of words trained on very large corpora toimprove the word-topic mapping learnt on a smaller corpus. Experimental resultsshow that by using information from the external corpora, our new modelsproduce significant improvements on topic coherence, document clustering anddocument classification tasks, especially on datasets with few or shortdocuments.", "categories": ["cs.CL", "cs.IR", "cs.LG"], "journal": "Transactions of the Association for Computational Linguistics 3, 299-313", "citations": "132", "arxiv_url": "http://arxiv.org/abs/1810.06306v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=15470052848054255574&btnI=1&nossl=1&hl=en&oe=ASCII"}, "155": {"ID": 155, "title": "CoQA: A Conversational Question Answering Challenge", "authors": ["Siva Reddy", "Christopher D. Manning", "Danqi Chen"], "published": "2018-08-21T17:52:02Z", "updated": "2019-03-29T20:50:21Z", "abstract": "Humans gather information by engaging in conversations involving a series ofinterconnected questions and answers. For machines to assist in informationgathering, it is therefore essential to enable them to answer conversationalquestions. We introduce CoQA, a novel dataset for building ConversationalQuestion Answering systems. Our dataset contains 127k questions with answers,obtained from 8k conversations about text passages from seven diverse domains.The questions are conversational, and the answers are free-form text with theircorresponding evidence highlighted in the passage. We analyze CoQA in depth andshow that conversational questions have challenging phenomena not present inexisting reading comprehension datasets, e.g., coreference and pragmaticreasoning. We evaluate strong conversational and reading comprehension modelson CoQA. The best system obtains an F1 score of 65.4%, which is 23.4 pointsbehind human performance (88.8%), indicating there is ample room forimprovement. We launch CoQA as a challenge to the community athttp://stanfordnlp.github.io/coqa/", "categories": ["cs.CL", "cs.AI", "cs.LG"], "journal": "Transactions of the Association for Computational Linguistics 7, 249-266", "citations": "239", "arxiv_url": "http://arxiv.org/abs/1808.07042v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=783716453669313794&btnI=1&nossl=1&hl=en&oe=ASCII"}, "156": {"ID": 156, "title": "Language Understanding for Text-based Games Using Deep Reinforcement  Learning", "authors": ["Karthik Narasimhan", "Tejas Kulkarni", "Regina Barzilay"], "published": "2015-06-30T05:51:11Z", "updated": "2015-09-11T23:16:13Z", "abstract": "In this paper, we consider the task of learning control policies fortext-based games. In these games, all interactions in the virtual world arethrough text and the underlying state is not observed. The resulting languagebarrier makes such environments challenging for automatic game players. Weemploy a deep reinforcement learning framework to jointly learn staterepresentations and action policies using game rewards as feedback. Thisframework enables us to map text descriptions into vector representations thatcapture the semantics of the game states. We evaluate our approach on two gameworlds, comparing against baselines using bag-of-words and bag-of-bigrams forstate representations. Our algorithm outperforms the baselines on both worldsdemonstrating the importance of learning expressive representations.", "categories": ["cs.CL", "cs.AI"], "journal": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "245", "arxiv_url": "http://arxiv.org/abs/1506.08941v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8068033164368262640&btnI=1&nossl=1&hl=en&oe=ASCII"}, "157": {"ID": 157, "title": "How Transferable are Neural Networks in NLP Applications?", "authors": ["Lili Mou", "Lu Zhang", "Zhao Meng", "Ge Li", "Zhi Jin", "Yan Xu", "Rui Yan"], "published": "2016-03-19T16:38:31Z", "updated": "2016-10-13T07:45:31Z", "abstract": "Transfer learning is aimed to make use of valuable knowledge in a sourcedomain to help model performance in a target domain. It is particularlyimportant to neural networks, which are very likely to be overfitting. In somefields like image processing, many studies have shown the effectiveness ofneural network-based transfer learning. For neural NLP, however, existingstudies have only casually applied transfer learning, and conclusions areinconsistent. In this paper, we conduct systematic case studies and provide anilluminating picture on the transferability of neural networks in NLP.", "categories": ["cs.CL", "cs.LG", "cs.NE"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "197", "arxiv_url": "http://arxiv.org/abs/1603.06111v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=7464971160910138225&btnI=1&nossl=1&hl=en&oe=ASCII"}, "158": {"ID": 158, "title": "Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies", "authors": ["Emmanuel Dupoux", "Tal Linzen", "Yoav Goldberg"], "published": "2016-11-04T13:36:32Z", "updated": "2016-11-04T13:36:32Z", "abstract": "The success of long short-term memory (LSTM) neural networks in languageprocessing is typically attributed to their ability to capture long-distancestatistical regularities. Linguistic regularities are often sensitive tosyntactic structure; can such dependencies be captured by LSTMs, which do nothave explicit structural representations? We begin addressing this questionusing number agreement in English subject-verb dependencies. We probe thearchitecture's grammatical competence both using training objectives with anexplicit grammatical target (number prediction, grammaticality judgments) andusing language models. In the strongly supervised settings, the LSTM achievedvery high overall accuracy (less than 1% errors), but errors increased whensequential and structural information conflicted. The frequency of such errorsrose sharply in the language-modeling setting. We conclude that LSTMs cancapture a non-trivial amount of grammatical structure given targetedsupervision, but stronger architectures may be required to further reduceerrors; furthermore, the language modeling signal is insufficient for capturingsyntax-sensitive dependencies, and should be supplemented with more directsupervision if such dependencies need to be captured.", "categories": ["cs.CL"], "journal": "Transactions of the Association for Computational Linguistics 4, 521-535", "citations": "383", "arxiv_url": "http://arxiv.org/abs/1611.01368v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=11925657964813731107&btnI=1&nossl=1&hl=en&oe=ASCII"}, "159": {"ID": 159, "title": "Neural Summarization by Extracting Sentences and Words", "authors": ["Mirella Lapata", "Jianpeng Cheng"], "published": "2016-03-23T16:05:46Z", "updated": "2016-07-01T03:16:03Z", "abstract": "Traditional approaches to extractive summarization rely heavily onhuman-engineered features. In this work we propose a data-driven approach basedon neural networks and continuous sentence features. We develop a generalframework for single-document summarization composed of a hierarchical documentencoder and an attention-based extractor. This architecture allows us todevelop different classes of summarization models which can extract sentencesor words. We train our models on large scale corpora containing hundreds ofthousands of document-summary pairs. Experimental results on two summarizationdatasets demonstrate that our models obtain results comparable to the state ofthe art without any access to linguistic annotation.", "categories": ["cs.CL"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "449", "arxiv_url": "http://arxiv.org/abs/1603.07252v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=1423563258460248878&btnI=1&nossl=1&hl=en&oe=ASCII"}, "160": {"ID": 160, "title": "Aspect-augmented Adversarial Networks for Domain Adaptation", "authors": ["Tommi Jaakkola", "Regina Barzilay", "Yuan Zhang"], "published": "2017-01-01T03:04:33Z", "updated": "2017-09-25T03:36:05Z", "abstract": "We introduce a neural method for transfer learning between two (source andtarget) classification tasks or aspects over the same domain. Rather thantraining on target labels, we use a few keywords pertaining to source andtarget aspects indicating sentence relevance instead of document class labels.Documents are encoded by learning to embed and softly select relevant sentencesin an aspect-dependent manner. A shared classifier is trained on the sourceencoded documents and labels, and applied to target encoded documents. Weensure transfer through aspect-adversarial training so that encoded documentsare, as sets, aspect-invariant. Experimental results demonstrate that ourapproach outperforms different baselines and model variants on two datasets,yielding an improvement of 27% on a pathology dataset and 5% on a reviewdataset.", "categories": ["cs.CL"], "journal": "Transactions of the Association for Computational Linguistics 5, 515-528", "citations": "55", "arxiv_url": "http://arxiv.org/abs/1701.00188v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8941641124454481053&btnI=1&nossl=1&hl=en&oe=ASCII"}, "161": {"ID": 161, "title": "Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods", "authors": ["Vicente Ordonez", "Tianlu Wang", "Kai-Wei Chang", "Mark Yatskar", "Jieyu Zhao"], "published": "2018-04-18T18:51:00Z", "updated": "2018-04-18T18:51:00Z", "abstract": "We introduce a new benchmark, WinoBias, for coreference resolution focused ongender bias. Our corpus contains Winograd-schema style sentences with entitiescorresponding to people referred by their occupation (e.g. the nurse, thedoctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and aneural coreference system all link gendered pronouns to pro-stereotypicalentities with higher accuracy than anti-stereotypical entities, by an averagedifference of 21.1 in F1 score. Finally, we demonstrate a data-augmentationapproach that, in combination with existing word-embedding debiasingtechniques, removes the bias demonstrated by these systems in WinoBias withoutsignificantly affecting their performance on existing coreference benchmarkdatasets. Our dataset and code are available at http://winobias.org.", "categories": ["cs.CL", "cs.AI"], "journal": "NAACL-HLT (2), 15-20", "citations": "112", "arxiv_url": "http://arxiv.org/abs/1804.06876v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9531484481211225588&btnI=1&nossl=1&hl=en&oe=ASCII"}, "162": {"ID": 162, "title": "SGM: Sequence Generation Model for Multi-label Classification", "authors": ["Xu Sun", "Houfeng Wang", "Pengcheng Yang", "Wei Wu", "Wei Li", "Shuming Ma"], "published": "2018-06-13T02:16:01Z", "updated": "2018-06-15T05:52:24Z", "abstract": "Multi-label classification is an important yet challenging task in naturallanguage processing. It is more complex than single-label classification inthat the labels tend to be correlated. Existing methods tend to ignore thecorrelations between labels. Besides, different parts of the text cancontribute differently for predicting different labels, which is not consideredby existing models. In this paper, we propose to view the multi-labelclassification task as a sequence generation problem, and apply a sequencegeneration model with a novel decoder structure to solve it. Extensiveexperimental results show that our proposed methods outperform previous work bya substantial margin. Further analysis of experimental results demonstratesthat the proposed methods not only capture the correlations between labels, butalso select the most informative words automatically when predicting differentlabels.", "categories": ["cs.CL"], "journal": "COLING, 3915-3926", "citations": "55", "arxiv_url": "http://arxiv.org/abs/1806.04822v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14826233978079781008&btnI=1&nossl=1&hl=en&oe=ASCII"}, "163": {"ID": 163, "title": "Deep Reinforcement Learning for Dialogue Generation", "authors": ["Alan Ritter", "Dan Jurafsky", "Jianfeng Gao", "Jiwei Li", "Will Monroe", "Michel Galley"], "published": "2016-06-05T17:59:23Z", "updated": "2016-09-29T15:02:32Z", "abstract": "Recent neural models of dialogue generation offer great promise forgenerating responses for conversational agents, but tend to be shortsighted,predicting utterances one at a time while ignoring their influence on futureoutcomes. Modeling the future direction of a dialogue is crucial to generatingcoherent, interesting dialogues, a need which led traditional NLP models ofdialogue to draw on reinforcement learning. In this paper, we show how tointegrate these goals, applying deep reinforcement learning to model futurereward in chatbot dialogue. The model simulates dialogues between two virtualagents, using policy gradient methods to reward sequences that display threeuseful conversational properties: informativity (non-repetitive turns),coherence, and ease of answering (related to forward-looking function). Weevaluate our model on diversity, length as well as with human judges, showingthat the proposed algorithm generates more interactive responses and manages tofoster a more sustained conversation in dialogue simulation. This work marks afirst step towards learning a neural conversational model based on thelong-term success of dialogues.", "categories": ["cs.CL"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "698", "arxiv_url": "http://arxiv.org/abs/1606.01541v4", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=1196225117608418887&btnI=1&nossl=1&hl=en&oe=ASCII"}, "164": {"ID": 164, "title": "Linguistic Knowledge and Transferability of Contextual Representations", "authors": ["Nelson F. Liu", "Matt Gardner", "Noah A. Smith", "Yonatan Belinkov", "Matthew E. Peters"], "published": "2019-03-21T07:19:45Z", "updated": "2019-04-25T18:35:16Z", "abstract": "Contextual word representations derived from large-scale neural languagemodels are successful across a diverse set of NLP tasks, suggesting that theyencode useful and transferable features of language. To shed light on thelinguistic knowledge they capture, we study the representations produced byseveral recent pretrained contextualizers (variants of ELMo, the OpenAItransformer language model, and BERT) with a suite of seventeen diverse probingtasks. We find that linear models trained on top of frozen contextualrepresentations are competitive with state-of-the-art task-specific models inmany cases, but fail on tasks requiring fine-grained linguistic knowledge(e.g., conjunct identification). To investigate the transferability ofcontextual word representations, we quantify differences in the transferabilityof individual layers within contextualizers, especially between recurrentneural networks (RNNs) and transformers. For instance, higher layers of RNNsare more task-specific, while transformer layers do not exhibit the samemonotonic trend. In addition, to better understand what makes contextual wordrepresentations transferable, we compare language model pretraining with elevensupervised pretraining tasks. For any given task, pretraining on a closelyrelated task yields better performance than language model pretraining (whichis better on average) when the pretraining dataset is fixed. However, languagemodel pretraining on more data gives the best results.", "categories": ["cs.CL"], "journal": "NAACL-HLT (1), 1073-1094", "citations": "124", "arxiv_url": "http://arxiv.org/abs/1903.08855v5", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=7557710226921212137&btnI=1&nossl=1&hl=en&oe=ASCII"}, "165": {"ID": 165, "title": "Neural Tree Indexers for Text Understanding", "authors": ["Hong Yu", "Tsendsuren Munkhdalai"], "published": "2016-07-15T12:59:01Z", "updated": "2017-02-28T17:10:33Z", "abstract": "Recurrent neural networks (RNNs) process input text sequentially and modelthe conditional transition between word tokens. In contrast, the advantages ofrecursive networks include that they explicitly model the compositionality andthe recursive structure of natural language. However, the current recursivearchitecture is limited by its dependence on syntactic tree. In this paper, weintroduce a robust syntactic parsing-independent tree structured model, NeuralTree Indexers (NTI) that provides a middle ground between the sequential RNNsand the syntactic treebased recursive models. NTI constructs a full n-ary treeby processing the input text with its node function in a bottom-up fashion.Attention mechanism can then be applied to both structure and node function. Weimplemented and evaluated a binarytree model of NTI, showing the model achievedthe state-of-the-art performance on three different NLP tasks: natural languageinference, answer sentence selection, and sentence classification,outperforming state-of-the-art recurrent and recursive neural networks.", "categories": ["cs.CL", "cs.LG", "stat.ML"], "journal": "EACL (1), 11-21", "citations": "88", "arxiv_url": "http://arxiv.org/abs/1607.04492v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8905746472437377800&btnI=1&nossl=1&hl=en&oe=ASCII"}, "166": {"ID": 166, "title": "Learning Natural Language Inference with LSTM", "authors": ["Jing Jiang", "Shuohang Wang"], "published": "2015-12-30T05:02:53Z", "updated": "2016-11-10T11:54:29Z", "abstract": "Natural language inference (NLI) is a fundamentally important task in naturallanguage processing that has many applications. The recently released StanfordNatural Language Inference (SNLI) corpus has made it possible to develop andevaluate learning-centered methods such as deep neural networks for naturallanguage inference (NLI). In this paper, we propose a special long short-termmemory (LSTM) architecture for NLI. Our model builds on top of a recentlyproposed neural attention model for NLI but is based on a significantlydifferent idea. Instead of deriving sentence embeddings for the premise and thehypothesis to be used for classification, our solution uses a match-LSTM toperform word-by-word matching of the hypothesis with the premise. This LSTM isable to place more emphasis on important word-level matching results. Inparticular, we observe that this LSTM remembers important mismatches that arecritical for predicting the contradiction or the neutral relationship label. Onthe SNLI corpus, our model achieves an accuracy of 86.1%, outperforming thestate of the art.", "categories": ["cs.CL", "cs.AI", "cs.NE"], "journal": "HLT-NAACL, 1442-1451", "citations": "283", "arxiv_url": "http://arxiv.org/abs/1512.08849v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=3943051717190619834&btnI=1&nossl=1&hl=en&oe=ASCII"}, "167": {"ID": 167, "title": "Toward Abstractive Summarization Using Semantic Representations", "authors": ["Jeffrey Flanigan", "Sam Thomson", "Norman Sadeh", "Noah A. Smith", "Fei Liu"], "published": "2018-05-25T23:46:11Z", "updated": "2018-05-25T23:46:11Z", "abstract": "We present a novel abstractive summarization framework that draws on therecent development of a treebank for the Abstract Meaning Representation (AMR).In this framework, the source text is parsed to a set of AMR graphs, the graphsare transformed into a summary graph, and then text is generated from thesummary graph. We focus on the graph-to-graph transformation that reduces thesource semantic graph into a summary graph, making use of an existing AMRparser and assuming the eventual availability of an AMR-to-text generator. Theframework is data-driven, trainable, and not specifically designed for aparticular domain. Experiments on gold-standard AMR annotations and systemparses show promising results. Code is available at:https://github.com/summarization", "categories": ["cs.CL"], "journal": "HLT-NAACL, 1077-1086", "citations": "181", "arxiv_url": "http://arxiv.org/abs/1805.10399v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4731361455028710225&btnI=1&nossl=1&hl=en&oe=ASCII"}, "168": {"ID": 168, "title": "Predicting the Type and Target of Offensive Posts in Social Media", "authors": ["Sara Rosenthal", "Noura Farra", "Marcos Zampieri", "Preslav Nakov", "Ritesh Kumar", "Shervin Malmasi"], "published": "2019-02-25T23:54:40Z", "updated": "2019-04-16T16:30:35Z", "abstract": "As offensive content has become pervasive in social media, there has beenmuch research in identifying potentially offensive messages. However, previouswork on this topic did not consider the problem as a whole, but rather focusedon detecting very specific types of offensive content, e.g., hate speech,cyberbulling, or cyber-aggression. In contrast, here we target severaldifferent kinds of offensive content. In particular, we model the taskhierarchically, identifying the type and the target of offensive messages insocial media. For this purpose, we complied the Offensive LanguageIdentification Dataset (OLID), a new dataset with tweets annotated foroffensive content using a fine-grained three-layer annotation scheme, which wemake publicly available. We discuss the main similarities and differencesbetween OLID and pre-existing datasets for hate speech identification,aggression detection, and similar tasks. We further experiment with and wecompare the performance of different machine learning models on OLID.", "categories": ["cs.CL"], "journal": "NAACL-HLT (1), 1415-1420", "citations": "103", "arxiv_url": "http://arxiv.org/abs/1902.09666v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=18433406301611283138&btnI=1&nossl=1&hl=en&oe=ASCII"}, "169": {"ID": 169, "title": "Learning to generate one-sentence biographies from Wikidata", "authors": ["Andrew Chisholm", "Will Radford", "Ben Hachey"], "published": "2017-02-21T01:30:59Z", "updated": "2017-02-21T01:30:59Z", "abstract": "We investigate the generation of one-sentence Wikipedia biographies fromfacts derived from Wikidata slot-value pairs. We train a recurrent neuralnetwork sequence-to-sequence model with attention to select facts and generatetextual summaries. Our model incorporates a novel secondary objective thathelps ensure it generates sentences that contain the input facts. The modelachieves a BLEU score of 41, improving significantly upon the vanillasequence-to-sequence model and scoring roughly twice that of a simple templatebaseline. Human preference evaluation suggests the model is nearly as good asthe Wikipedia reference. Manual analysis explores content selection, suggestingthe model can trade the ability to infer knowledge against the risk ofhallucinating incorrect information.", "categories": ["cs.CL"], "journal": "EACL (1), 633-642", "citations": "63", "arxiv_url": "http://arxiv.org/abs/1702.06235v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=17730514797790794660&btnI=1&nossl=1&hl=en&oe=ASCII"}, "170": {"ID": 170, "title": "Design Challenges and Misconceptions in Neural Sequence Labeling", "authors": ["Shuailong Liang", "Jie Yang", "Yue Zhang"], "published": "2018-06-12T12:43:42Z", "updated": "2018-07-12T09:31:10Z", "abstract": "We investigate the design challenges of constructing effective and efficientneural sequence labeling systems, by reproducing twelve neural sequencelabeling models, which include most of the state-of-the-art structures, andconduct a systematic model comparison on three benchmarks (i.e. NER, Chunking,and POS tagging). Misconceptions and inconsistent conclusions in existingliterature are examined and clarified under statistical experiments. In thecomparison and analysis process, we reach several practical conclusions whichcan be useful to practitioners.", "categories": ["cs.CL"], "journal": "COLING, 3879-3889", "citations": "72", "arxiv_url": "http://arxiv.org/abs/1806.04470v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=17164476548440166514&btnI=1&nossl=1&hl=en&oe=ASCII"}, "171": {"ID": 171, "title": "Dependency Parsing as Head Selection", "authors": ["Mirella Lapata", "Xingxing Zhang", "Jianpeng Cheng"], "published": "2016-06-03T21:27:03Z", "updated": "2016-12-22T15:28:34Z", "abstract": "Conventional graph-based dependency parsers guarantee a tree structure bothduring training and inference. Instead, we formalize dependency parsing as theproblem of independently selecting the head of each word in a sentence. Ourmodel which we call \\textsc{DeNSe} (as shorthand for {\\bf De}pendency {\\bfN}eural {\\bf Se}lection) produces a distribution over possible heads for eachword using features obtained from a bidirectional recurrent neural network.Without enforcing structural constraints during training, \\textsc{DeNSe}generates (at inference time) trees for the overwhelming majority of sentences,while non-tree outputs can be adjusted with a maximum spanning tree algorithm.We evaluate \\textsc{DeNSe} on four languages (English, Chinese, Czech, andGerman) with varying degrees of non-projectivity. Despite the simplicity of theapproach, our parsers are on par with the state of the art.", "categories": ["cs.CL", "cs.LG"], "journal": "EACL (1), 665-676", "citations": "56", "arxiv_url": "http://arxiv.org/abs/1606.01280v4", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=15312534315117418700&btnI=1&nossl=1&hl=en&oe=ASCII"}, "172": {"ID": 172, "title": "Abstract Syntax Networks for Code Generation and Semantic Parsing", "authors": ["Mitchell Stern", "Dan Klein", "Maxim Rabinovich"], "published": "2017-04-25T04:37:35Z", "updated": "2017-04-25T04:37:35Z", "abstract": "Tasks like code generation and semantic parsing require mapping unstructured(or partially structured) inputs to well-formed, executable outputs. Weintroduce abstract syntax networks, a modeling framework for these problems.The outputs are represented as abstract syntax trees (ASTs) and constructed bya decoder with a dynamically-determined modular structure paralleling thestructure of the output tree. On the benchmark Hearthstone dataset for codegeneration, our model obtains 79.2 BLEU and 22.7% exact match accuracy,compared to previous state-of-the-art values of 67.1 and 6.1%. Furthermore, weperform competitively on the Atis, Jobs, and Geo semantic parsing datasets withno task-specific engineering.", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "143", "arxiv_url": "http://arxiv.org/abs/1704.07535v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=193213682881631774&btnI=1&nossl=1&hl=en&oe=ASCII"}, "173": {"ID": 173, "title": "Analyzing the Behavior of Visual Question Answering Models", "authors": ["Aishwarya Agrawal", "Dhruv Batra", "Devi Parikh"], "published": "2016-06-23T16:05:16Z", "updated": "2016-09-27T19:56:22Z", "abstract": "Recently, a number of deep-learning based models have been proposed for thetask of Visual Question Answering (VQA). The performance of most models isclustered around 60-70%. In this paper we propose systematic methods to analyzethe behavior of these models as a first step towards recognizing theirstrengths and weaknesses, and identifying the most fruitful directions forprogress. We analyze two models, one each from two major classes of VQA models-- with-attention and without-attention and show the similarities anddifferences in the behavior of these models. We also analyze the winning entryof the VQA Challenge 2016.  Our behavior analysis reveals that despite recent progress, today's VQAmodels are \"myopic\" (tend to fail on sufficiently novel instances), often \"jumpto conclusions\" (converge on a predicted answer after 'listening' to just halfthe question), and are \"stubborn\" (do not change their answers across images).", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "132", "arxiv_url": "http://arxiv.org/abs/1606.07356v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=1431229789784241534&btnI=1&nossl=1&hl=en&oe=ASCII"}, "174": {"ID": 174, "title": "Query Expansion with Locally-Trained Word Embeddings", "authors": ["Fernando Diaz", "Bhaskar Mitra", "Nick Craswell"], "published": "2016-05-25T14:09:00Z", "updated": "2016-06-23T00:46:06Z", "abstract": "Continuous space word embeddings have received a great deal of attention inthe natural language processing and machine learning communities for theirability to model term similarity and other relationships. We study the use ofterm relatedness in the context of query expansion for ad hoc informationretrieval. We demonstrate that word embeddings such as word2vec and GloVe, whentrained globally, underperform corpus and query specific embeddings forretrieval tasks. These results suggest that other tasks benefiting from globalembeddings may also benefit from local embeddings.", "categories": ["cs.IR", "cs.CL"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "204", "arxiv_url": "http://arxiv.org/abs/1605.07891v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=13779683216969476205&btnI=1&nossl=1&hl=en&oe=ASCII"}, "175": {"ID": 175, "title": "Get To The Point: Summarization with Pointer-Generator Networks", "authors": ["Peter J. Liu", "Christopher D. Manning", "Abigail See"], "published": "2017-04-14T07:55:19Z", "updated": "2017-04-25T05:47:50Z", "abstract": "Neural sequence-to-sequence models have provided a viable new approach forabstractive text summarization (meaning they are not restricted to simplyselecting and rearranging passages from the original text). However, thesemodels have two shortcomings: they are liable to reproduce factual detailsinaccurately, and they tend to repeat themselves. In this work we propose anovel architecture that augments the standard sequence-to-sequence attentionalmodel in two orthogonal ways. First, we use a hybrid pointer-generator networkthat can copy words from the source text via pointing, which aids accuratereproduction of information, while retaining the ability to produce novel wordsthrough the generator. Second, we use coverage to keep track of what has beensummarized, which discourages repetition. We apply our model to the CNN / DailyMail summarization task, outperforming the current abstractive state-of-the-artby at least 2 ROUGE points.", "categories": ["cs.CL"], "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "951", "arxiv_url": "http://arxiv.org/abs/1704.04368v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=18163591669019548288&btnI=1&nossl=1&hl=en&oe=ASCII"}, "176": {"ID": 176, "title": "Linear Algebraic Structure of Word Senses, with Applications to Polysemy", "authors": ["Yingyu Liang", "Tengyu Ma", "Andrej Risteski", "Sanjeev Arora", "Yuanzhi Li"], "published": "2016-01-14T22:02:18Z", "updated": "2018-12-07T17:30:03Z", "abstract": "Word embeddings are ubiquitous in NLP and information retrieval, but it isunclear what they represent when the word is polysemous. Here it is shown thatmultiple word senses reside in linear superposition within the word embeddingand simple sparse coding can recover vectors that approximately capture thesenses. The success of our approach, which applies to several embeddingmethods, is mathematically explained using a variant of the random walk ondiscourses model (Arora et al., 2016). A novel aspect of our technique is thateach extracted word sense is accompanied by one of about 2000 \"discourse atoms\"that gives a succinct description of which other words co-occur with that wordsense. Discourse atoms can be of independent interest, and make the methodpotentially more useful. Empirical tests are used to verify and support thetheory.", "categories": ["cs.CL", "cs.LG", "stat.ML"], "journal": "Transactions of the Association for Computational Linguistics 6, 483-495", "citations": "101", "arxiv_url": "http://arxiv.org/abs/1601.03764v6", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=15248888883335695772&btnI=1&nossl=1&hl=en&oe=ASCII"}, "177": {"ID": 177, "title": "Incorporating Structural Alignment Biases into an Attentional Neural  Translation Model", "authors": ["Gholamreza Haffari", "Kaisheng Yao", "Trevor Cohn", "Chris Dyer", "Cong Duy Vu Hoang", "Ekaterina Vymolova"], "published": "2016-01-06T06:03:17Z", "updated": "2016-01-06T06:03:17Z", "abstract": "Neural encoder-decoder models of machine translation have achieved impressiveresults, rivalling traditional translation models. However their modellingformulation is overly simplistic, and omits several key inductive biases builtinto traditional models. In this paper we extend the attentional neuraltranslation model to include structural biases from word based alignmentmodels, including positional bias, Markov conditioning, fertility and agreementover translation directions. We show improvements over a baseline attentionalmodel and standard phrase-based model over several language pairs, evaluatingon difficult languages in a low resource setting.", "categories": ["cs.CL"], "journal": "HLT-NAACL, 876-885", "citations": "130", "arxiv_url": "http://arxiv.org/abs/1601.01085v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6876101136632328854&btnI=1&nossl=1&hl=en&oe=ASCII"}, "178": {"ID": 178, "title": "A Survey of Domain Adaptation for Neural Machine Translation", "authors": ["Rui Wang", "Chenhui Chu"], "published": "2018-06-01T09:54:32Z", "updated": "2018-06-01T09:54:32Z", "abstract": "Neural machine translation (NMT) is a deep learning based approach formachine translation, which yields the state-of-the-art translation performancein scenarios where large-scale parallel corpora are available. Although thehigh-quality and domain-specific translation is crucial in the real world,domain-specific corpora are usually scarce or nonexistent, and thus vanilla NMTperforms poorly in such scenarios. Domain adaptation that leverages bothout-of-domain parallel corpora as well as monolingual corpora for in-domaintranslation, is very important for domain-specific translation. In this paper,we give a comprehensive survey of the state-of-the-art domain adaptationtechniques for NMT.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "journal": "COLING, 1304-1319", "citations": "73", "arxiv_url": "http://arxiv.org/abs/1806.00258v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=12774117070156464640&btnI=1&nossl=1&hl=en&oe=ASCII"}, "179": {"ID": 179, "title": "Achieving Open Vocabulary Neural Machine Translation with Hybrid  Word-Character Models", "authors": ["Minh-Thang Luong", "Christopher D. Manning"], "published": "2016-04-04T09:30:54Z", "updated": "2016-06-23T00:50:19Z", "abstract": "Nearly all previous work on neural machine translation (NMT) has used quiterestricted vocabularies, perhaps with a subsequent method to patch in unknownwords. This paper presents a novel word-character solution to achieving openvocabulary NMT. We build hybrid systems that translate mostly at the word leveland consult the character components for rare words. Our character-levelrecurrent neural networks compute source word representations and recoverunknown target words when needed. The twofold advantage of such a hybridapproach is that it is much faster and easier to train than character-basedones; at the same time, it never produces unknown words as in the case ofword-based models. On the WMT'15 English to Czech translation task, this hybridapproach offers an addition boost of +2.1-11.4 BLEU points over models thatalready handle unknown words. Our best system achieves a new state-of-the-artresult with 20.7 BLEU score. We demonstrate that our character models cansuccessfully learn to not only generate well-formed words for Czech, ahighly-inflected language with a very complex vocabulary, but also buildcorrect representations for English source words.", "categories": ["cs.CL", "cs.LG"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "313", "arxiv_url": "http://arxiv.org/abs/1604.00788v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=7652846715026310814&btnI=1&nossl=1&hl=en&oe=ASCII"}, "180": {"ID": 180, "title": "STransE: a novel embedding model of entities and relationships in  knowledge bases", "authors": ["Dat Quoc Nguyen", "Lizhen Qu", "Kairit Sirts", "Mark Johnson"], "published": "2016-06-27T06:50:10Z", "updated": "2017-03-08T16:57:40Z", "abstract": "Knowledge bases of real-world facts about entities and their relationshipsare useful resources for a variety of natural language processing tasks.However, because knowledge bases are typically incomplete, it is useful to beable to perform link prediction or knowledge base completion, i.e., predictwhether a relationship not in the knowledge base is likely to be true. Thispaper combines insights from several previous link prediction models into a newembedding model STransE that represents each entity as a low-dimensionalvector, and each relation by two matrices and a translation vector. STransE isa simple combination of the SE and TransE models, but it obtains better linkprediction performance on two benchmark datasets than previous embeddingmodels. Thus, STransE can serve as a new baseline for the more complex modelsin the link prediction task.", "categories": ["cs.CL", "cs.AI"], "journal": "HLT-NAACL, 460-466", "citations": "108", "arxiv_url": "http://arxiv.org/abs/1606.08140v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=3989577828642116458&btnI=1&nossl=1&hl=en&oe=ASCII"}, "181": {"ID": 181, "title": "Simple and Effective Multi-Paragraph Reading Comprehension", "authors": ["Matt Gardner", "Christopher Clark"], "published": "2017-10-29T23:47:49Z", "updated": "2017-11-07T18:55:35Z", "abstract": "We consider the problem of adapting neural paragraph-level question answeringmodels to the case where entire documents are given as input. Our proposedsolution trains models to produce well calibrated confidence scores for theirresults on individual paragraphs. We sample multiple paragraphs from thedocuments during training, and use a shared-normalization training objectivethat encourages the model to produce globally correct output. We combine thismethod with a state-of-the-art pipeline for training models on document QAdata. Experiments demonstrate strong performance on several document QAdatasets. Overall, we are able to achieve a score of 71.3 F1 on the web portionof TriviaQA, a large improvement from the 56.7 F1 of the previous best system.", "categories": ["cs.CL"], "journal": "Proceedings of the 56th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "200", "arxiv_url": "http://arxiv.org/abs/1710.10723v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=3354511986633718216&btnI=1&nossl=1&hl=en&oe=ASCII"}, "182": {"ID": 182, "title": "Effective Use of Word Order for Text Categorization with Convolutional  Neural Networks", "authors": ["Rie Johnson", "Tong Zhang"], "published": "2014-12-01T16:19:51Z", "updated": "2015-03-26T12:59:35Z", "abstract": "Convolutional neural network (CNN) is a neural network that can make use ofthe internal structure of data such as the 2D structure of image data. Thispaper studies CNN on text categorization to exploit the 1D structure (namely,word order) of text data for accurate prediction. Instead of usinglow-dimensional word vectors as input as is often done, we directly apply CNNto high-dimensional text data, which leads to directly learning embedding ofsmall text regions for use in classification. In addition to a straightforwardadaptation of CNN from image to text, a simple but new variation which employsbag-of-word conversion in the convolution layer is proposed. An extension tocombine multiple convolution layers is also explored for higher accuracy. Theexperiments demonstrate the effectiveness of our approach in comparison withstate-of-the-art methods.", "categories": ["cs.CL", "cs.LG", "stat.ML"], "journal": "HLT-NAACL, 103-112", "citations": "674", "arxiv_url": "http://arxiv.org/abs/1412.1058v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=2148087133617345231&btnI=1&nossl=1&hl=en&oe=ASCII"}, "183": {"ID": 183, "title": "Towards an Automatic Turing Test: Learning to Evaluate Dialogue  Responses", "authors": ["Ryan Lowe", "Michael Noseworthy", "Iulian V. Serban", "Joelle Pineau", "Nicolas Angelard-Gontier", "Yoshua Bengio"], "published": "2017-08-23T18:56:00Z", "updated": "2018-01-16T23:29:14Z", "abstract": "Automatically evaluating the quality of dialogue responses for unstructureddomains is a challenging problem. Unfortunately, existing automatic evaluationmetrics are biased and correlate very poorly with human judgements of responsequality. Yet having an accurate automatic evaluation procedure is crucial fordialogue research, as it allows rapid prototyping and testing of new modelswith fewer expensive human evaluations. In response to this challenge, weformulate automatic dialogue evaluation as a learning problem. We present anevaluation model (ADEM) that learns to predict human-like scores to inputresponses, using a new dataset of human response scores. We show that the ADEMmodel's predictions correlate significantly, and at a level much higher thanword-overlap metrics such as BLEU, with human judgements at both the utteranceand system-level. We also show that ADEM can generalize to evaluating dialoguemodels unseen during training, an important step for automatic dialogueevaluation.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "164", "arxiv_url": "http://arxiv.org/abs/1708.07149v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=15470106720904286429&btnI=1&nossl=1&hl=en&oe=ASCII"}, "184": {"ID": 184, "title": "SentencePiece: A simple and language independent subword tokenizer and  detokenizer for Neural Text Processing", "authors": ["John Richardson", "Taku Kudo"], "published": "2018-08-19T16:49:06Z", "updated": "2018-08-19T16:49:06Z", "abstract": "This paper describes SentencePiece, a language-independent subword tokenizerand detokenizer designed for Neural-based text processing, including NeuralMachine Translation. It provides open-source C++ and Python implementations forsubword units. While existing subword segmentation tools assume that the inputis pre-tokenized into word sequences, SentencePiece can train subword modelsdirectly from raw sentences, which allows us to make a purely end-to-end andlanguage independent system. We perform a validation experiment of NMT onEnglish-Japanese machine translation, and find that it is possible to achievecomparable accuracy to direct subword training from raw sentences. We alsocompare the performance of subword training and segmentation with variousconfigurations. SentencePiece is available under the Apache 2 license athttps://github.com/google/sentencepiece.", "categories": ["cs.CL"], "journal": "EMNLP (Demonstration), 66-71", "citations": "302", "arxiv_url": "http://arxiv.org/abs/1808.06226v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=15624268342506708443&btnI=1&nossl=1&hl=en&oe=ASCII"}, "185": {"ID": 185, "title": "Recurrent Neural Network Grammars", "authors": ["Chris Dyer", "Noah A. Smith", "Miguel Ballesteros", "Adhiguna Kuncoro"], "published": "2016-02-25T02:42:58Z", "updated": "2016-10-12T04:47:45Z", "abstract": "We introduce recurrent neural network grammars, probabilistic models ofsentences with explicit phrase structure. We explain efficient inferenceprocedures that allow application to both parsing and language modeling.Experiments show that they provide better parsing in English than any singlepreviously published supervised generative model and better language modelingthan state-of-the-art sequential RNNs in English and Chinese.", "categories": ["cs.CL", "cs.NE"], "journal": "HLT-NAACL, 199-209", "citations": "325", "arxiv_url": "http://arxiv.org/abs/1602.07776v4", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=17254773880942992584&btnI=1&nossl=1&hl=en&oe=ASCII"}, "186": {"ID": 186, "title": "Data Recombination for Neural Semantic Parsing", "authors": ["Robin Jia", "Percy Liang"], "published": "2016-06-11T20:34:09Z", "updated": "2016-06-11T20:34:09Z", "abstract": "Modeling crisp logical regularities is crucial in semantic parsing, making itdifficult for neural models with no task-specific prior knowledge to achievegood results. In this paper, we introduce data recombination, a novel frameworkfor injecting such prior knowledge into a model. From the training data, weinduce a high-precision synchronous context-free grammar, which capturesimportant conditional independence properties commonly found in semanticparsing. We then train a sequence-to-sequence recurrent network (RNN) modelwith a novel attention-based copying mechanism on datapoints sampled from thisgrammar, thereby teaching the model about these structural properties. Datarecombination improves the accuracy of our RNN model on three semantic parsingdatasets, leading to new state-of-the-art performance on the standard GeoQuerydataset for models with comparable supervision.", "categories": ["cs.CL"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "220", "arxiv_url": "http://arxiv.org/abs/1606.03622v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=3763204081840224659&btnI=1&nossl=1&hl=en&oe=ASCII"}, "187": {"ID": 187, "title": "Fully Character-Level Neural Machine Translation without Explicit  Segmentation", "authors": ["Thomas Hofmann", "Jason Lee", "Kyunghyun Cho"], "published": "2016-10-10T18:19:34Z", "updated": "2017-06-13T03:32:34Z", "abstract": "Most existing machine translation systems operate at the level of words,relying on explicit segmentation to extract tokens. We introduce a neuralmachine translation (NMT) model that maps a source character sequence to atarget character sequence without any segmentation. We employ a character-levelconvolutional network with max-pooling at the encoder to reduce the length ofsource representation, allowing the model to be trained at a speed comparableto subword-level models while capturing local regularities. Ourcharacter-to-character model outperforms a recently proposed baseline with asubword-level encoder on WMT'15 DE-EN and CS-EN, and gives comparableperformance on FI-EN and RU-EN. We then demonstrate that it is possible toshare a single character-level encoder across multiple languages by training amodel on a many-to-one translation task. In this multilingual setting, thecharacter-level encoder significantly outperforms the subword-level encoder onall the language pairs. We observe that on CS-EN, FI-EN and RU-EN, the qualityof the multilingual character-level translation even surpasses the modelsspecifically trained on that language pair alone, both in terms of BLEU scoreand human judgment.", "categories": ["cs.CL", "cs.LG"], "journal": "Transactions of the Association for Computational Linguistics 5, 365-378", "citations": "258", "arxiv_url": "http://arxiv.org/abs/1610.03017v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8490618934272910643&btnI=1&nossl=1&hl=en&oe=ASCII"}, "188": {"ID": 188, "title": "Improving Neural Machine Translation Models with Monolingual Data", "authors": ["Barry Haddow", "Rico Sennrich", "Alexandra Birch"], "published": "2015-11-20T17:58:37Z", "updated": "2016-06-03T15:09:54Z", "abstract": "Neural Machine Translation (NMT) has obtained state-of-the art performancefor several language pairs, while only using parallel data for training.Target-side monolingual data plays an important role in boosting fluency forphrase-based statistical machine translation, and we investigate the use ofmonolingual data for NMT. In contrast to previous work, which combines NMTmodels with separately trained language models, we note that encoder-decoderNMT architectures already have the capacity to learn the same information as alanguage model, and we explore strategies to train with monolingual datawithout changing the neural network architecture. By pairing monolingualtraining data with an automatic back-translation, we can treat it as additionalparallel training data, and we obtain substantial improvements on the WMT 15task English&lt;-&gt;German (+2.8-3.7 BLEU), and for the low-resourced IWSLT 14 taskTurkish-&gt;English (+2.1-3.4 BLEU), obtaining new state-of-the-art results. Wealso show that fine-tuning on in-domain monolingual and parallel data givessubstantial improvements for the IWSLT 15 task English-&gt;German.", "categories": ["cs.CL"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "842", "arxiv_url": "http://arxiv.org/abs/1511.06709v4", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=16647011114557315277&btnI=1&nossl=1&hl=en&oe=ASCII"}, "189": {"ID": 189, "title": "Towards Better UD Parsing: Deep Contextualized Word Embeddings,  Ensemble, and Treebank Concatenation", "authors": ["Yijia Liu", "Yuxuan Wang", "Ting Liu", "Bo Zheng", "Wanxiang Che"], "published": "2018-07-09T13:34:16Z", "updated": "2018-07-30T06:44:34Z", "abstract": "This paper describes our system (HIT-SCIR) submitted to the CoNLL 2018 sharedtask on Multilingual Parsing from Raw Text to Universal Dependencies. We baseour submission on Stanford's winning system for the CoNLL 2017 shared task andmake two effective extensions: 1) incorporating deep contextualized wordembeddings into both the part of speech tagger and parser; 2) ensemblingparsers trained with different initialization. We also explore different waysof concatenating treebanks for further improvements. Experimental results onthe development data show the effectiveness of our methods. In the finalevaluation, our system was ranked first according to LAS (75.84%) andoutperformed the other systems by a large margin.", "categories": ["cs.CL"], "journal": "CoNLL Shared Task (2), 55-64", "citations": "91", "arxiv_url": "http://arxiv.org/abs/1807.03121v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8633151307575001639&btnI=1&nossl=1&hl=en&oe=ASCII"}, "190": {"ID": 190, "title": "FEVER: a large-scale dataset for Fact Extraction and VERification", "authors": ["James Thorne", "Arpit Mittal", "Christos Christodoulopoulos", "Andreas Vlachos"], "published": "2018-03-14T15:30:37Z", "updated": "2018-12-18T10:58:20Z", "abstract": "In this paper we introduce a new publicly available dataset for verificationagainst textual sources, FEVER: Fact Extraction and VERification. It consistsof 185,445 claims generated by altering sentences extracted from Wikipedia andsubsequently verified without knowledge of the sentence they were derived from.The claims are classified as Supported, Refuted or NotEnoughInfo by annotatorsachieving 0.6841 in Fleiss $\\kappa$. For the first two classes, the annotatorsalso recorded the sentence(s) forming the necessary evidence for theirjudgment. To characterize the challenge of the dataset presented, we develop apipeline approach and compare it to suitably designed oracles. The bestaccuracy we achieve on labeling a claim accompanied by the correct evidence is31.87%, while if we ignore the evidence we achieve 50.91%. Thus we believe thatFEVER is a challenging testbed that will help stimulate progress on claimverification against textual sources.", "categories": ["cs.CL"], "journal": "NAACL-HLT, 809-819", "citations": "164", "arxiv_url": "http://arxiv.org/abs/1803.05355v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=11697719910048500031&btnI=1&nossl=1&hl=en&oe=ASCII"}, "191": {"ID": 191, "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling", "authors": ["Sergey Edunov", "Alexei Baevski", "Nathan Ng", "Sam Gross", "Angela Fan", "Michael Auli", "David Grangier", "Myle Ott"], "published": "2019-04-01T18:05:02Z", "updated": "2019-04-01T18:05:02Z", "abstract": "fairseq is an open-source sequence modeling toolkit that allows researchersand developers to train custom models for translation, summarization, languagemodeling, and other text generation tasks. The toolkit is based on PyTorch andsupports distributed training across multiple GPUs and machines. We alsosupport fast mixed-precision training and inference on modern GPUs. A demovideo can be found at https://www.youtube.com/watch?v=OtgDdWtHvto", "categories": ["cs.CL"], "journal": "NAACL-HLT (Demonstrations), 48-53", "citations": "256", "arxiv_url": "http://arxiv.org/abs/1904.01038v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9170866963602541953&btnI=1&nossl=1&hl=en&oe=ASCII"}, "192": {"ID": 192, "title": "Multi-domain Neural Network Language Generation for Spoken Dialogue  Systems", "authors": ["David Vandyke", "Tsung-Hsien Wen", "Lina M. Rojas-Barahona", "Steve Young", "Pei-Hao Su", "Milica Gasic", "Nikola Mrksic"], "published": "2016-03-03T19:49:32Z", "updated": "2016-03-03T19:49:32Z", "abstract": "Moving from limited-domain natural language generation (NLG) to open domainis difficult because the number of semantic input combinations growsexponentially with the number of domains. Therefore, it is important toleverage existing resources and exploit similarities between domains tofacilitate domain adaptation. In this paper, we propose a procedure to trainmulti-domain, Recurrent Neural Network-based (RNN) language generators viamultiple adaptation steps. In this procedure, a model is first trained oncounterfeited data synthesised from an out-of-domain dataset, and then finetuned on a small set of in-domain utterances with a discriminative objectivefunction. Corpus-based evaluation results show that the proposed procedure canachieve competitive performance in terms of BLEU score and slot error ratewhile significantly reducing the data needed to train generators in new, unseendomains. In subjective testing, human judges confirm that the procedure greatlyimproves generator performance when only a small amount of data is available inthe domain.", "categories": ["cs.CL"], "journal": "HLT-NAACL, 120-129", "citations": "116", "arxiv_url": "http://arxiv.org/abs/1603.01232v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10592454168837533985&btnI=1&nossl=1&hl=en&oe=ASCII"}, "193": {"ID": 193, "title": "ConceptNet at SemEval-2017 Task 2: Extending Word Embeddings with  Multilingual Relational Knowledge", "authors": ["Robyn Speer", "Joanna Lowry-Duda"], "published": "2017-04-11T22:44:35Z", "updated": "2018-12-11T18:32:53Z", "abstract": "This paper describes Luminoso's participation in SemEval 2017 Task 2,\"Multilingual and Cross-lingual Semantic Word Similarity\", with a system basedon ConceptNet. ConceptNet is an open, multilingual knowledge graph that focuseson general knowledge that relates the meanings of words and phrases. Oursubmission to SemEval was an update of previous work that builds high-quality,multilingual word embeddings from a combination of ConceptNet anddistributional semantics. Our system took first place in both subtasks. Itranked first in 4 out of 5 of the separate languages, and also ranked first inall 10 of the cross-lingual language pairs.", "categories": ["cs.CL", "I.2.7"], "journal": "Proceedings of the 11th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "61", "arxiv_url": "http://arxiv.org/abs/1704.03560v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10887085465484846550&btnI=1&nossl=1&hl=en&oe=ASCII"}, "194": {"ID": 194, "title": "SemEval-2017 Task 3: Community Question Answering", "authors": ["Karin Verspoor", "Timothy Baldwin", "Hamdy Mubarak", "Doris Hoogeveen", "Llu\u00eds M\u00e0rquez", "Preslav Nakov", "Alessandro Moschitti"], "published": "2019-12-02T12:57:52Z", "updated": "2019-12-02T12:57:52Z", "abstract": "We describe SemEval-2017 Task 3 on Community Question Answering. This year,we reran the four subtasks from SemEval-2016:(A) Question-CommentSimilarity,(B) Question-Question Similarity,(C) Question-External CommentSimilarity, and (D) Rerank the correct answers for a new question in Arabic,providing all the data from 2015 and 2016 for training, and fresh data fortesting. Additionally, we added a new subtask E in order to enableexperimentation with Multi-domain Question Duplicate Detection in alarger-scale scenario, using StackExchange subforums. A total of 23 teamsparticipated in the task, and submitted a total of 85 runs (36 primary and 49contrastive) for subtasks A-D. Unfortunately, no teams participated in subtaskE. A variety of approaches and features were used by the participating systemsto address the different subtasks. The best systems achieved an official score(MAP) of 88.43, 47.22, 15.46, and 61.16 in subtasks A, B, C, and D,respectively. These scores are better than the baselines, especially forsubtasks A-C.", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "68T50", "I.2.7"], "journal": "Proceedings of the 11th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "204", "arxiv_url": "http://arxiv.org/abs/1912.00730v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9635452989109459162&btnI=1&nossl=1&hl=en&oe=ASCII"}, "195": {"ID": 195, "title": "Compositional Semantic Parsing on Semi-Structured Tables", "authors": ["Percy Liang", "Panupong Pasupat"], "published": "2015-08-03T02:53:01Z", "updated": "2015-08-03T02:53:01Z", "abstract": "Two important aspects of semantic parsing for question answering are thebreadth of the knowledge source and the depth of logical compositionality.While existing work trades off one aspect for another, this papersimultaneously makes progress on both fronts through a new task: answeringcomplex questions on semi-structured tables using question-answer pairs assupervision. The central challenge arises from two compounding factors: thebroader domain results in an open-ended set of relations, and the deepercompositionality results in a combinatorial explosion in the space of logicalforms. We propose a logical-form driven parsing algorithm guided by strongtyping constraints and show that it obtains significant improvements overnatural baselines. For evaluation, we created a new dataset of 22,033 complexquestions on Wikipedia tables, which is made publicly available.", "categories": ["cs.CL"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "227", "arxiv_url": "http://arxiv.org/abs/1508.00305v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14173136438291209898&btnI=1&nossl=1&hl=en&oe=ASCII"}, "196": {"ID": 196, "title": "Simple Question Answering by Attentive Convolutional Neural Network", "authors": ["Bing Xiang", "Wenpeng Yin", "Bowen Zhou", "Hinrich Sch\u00fctze", "Mo Yu"], "published": "2016-06-10T16:54:51Z", "updated": "2016-10-11T14:32:13Z", "abstract": "This work focuses on answering single-relation factoid questions overFreebase. Each question can acquire the answer from a single fact of form(subject, predicate, object) in Freebase. This task, simple question answering(SimpleQA), can be addressed via a two-step pipeline: entity linking and factselection. In fact selection, we match the subject entity in a fact candidatewith the entity mention in the question by a character-level convolutionalneural network (char-CNN), and match the predicate in that fact with thequestion by a word-level CNN (word-CNN). This work makes two maincontributions. (i) A simple and effective entity linker over Freebase isproposed. Our entity linker outperforms the state-of-the-art entity linker overSimpleQA task. (ii) A novel attentive maxpooling is stacked over word-CNN, sothat the predicate representation can be matched with the predicate-focusedquestion representation more effectively. Experiments show that our system setsnew state-of-the-art in this task.", "categories": ["cs.CL"], "journal": "COLING, 1746-1756", "citations": "100", "arxiv_url": "http://arxiv.org/abs/1606.03391v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=2384262381071903859&btnI=1&nossl=1&hl=en&oe=ASCII"}, "197": {"ID": 197, "title": "A Fast Unified Model for Parsing and Sentence Understanding", "authors": ["Christopher Potts", "Samuel R. Bowman", "Abhinav Rastogi", "Christopher D. Manning", "Jon Gauthier", "Raghav Gupta"], "published": "2016-03-19T00:22:20Z", "updated": "2016-07-29T18:36:15Z", "abstract": "Tree-structured neural networks exploit valuable syntactic parse informationas they interpret the meanings of sentences. However, they suffer from two keytechnical problems that make them slow and unwieldy for large-scale NLP tasks:they usually operate on parsed sentences and they do not directly supportbatched computation. We address these issues by introducing the Stack-augmentedParser-Interpreter Neural Network (SPINN), which combines parsing andinterpretation within a single tree-sequence hybrid model by integratingtree-structured sentence interpretation into the linear sequential structure ofa shift-reduce parser. Our model supports batched computation for a speedup ofup to 25 times over other tree-structured models, and its integrated parser canoperate on unparsed data with little loss in accuracy. We evaluate it on theStanford NLI entailment task and show that it significantly outperforms othersentence-encoding models.", "categories": ["cs.CL"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "260", "arxiv_url": "http://arxiv.org/abs/1603.06021v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=18084739973304650203&btnI=1&nossl=1&hl=en&oe=ASCII"}, "198": {"ID": 198, "title": "Generating Factoid Questions With Recurrent Neural Networks: The 30M  Factoid Question-Answer Corpus", "authors": ["Caglar Gulcehre", "Sungjin Ahn", "Sarath Chandar", "Iulian Vlad Serban", "Aaron Courville", "Alberto Garc\u00eda-Dur\u00e1n", "Yoshua Bengio"], "published": "2016-03-22T14:25:16Z", "updated": "2016-05-29T20:00:20Z", "abstract": "Over the past decade, large-scale supervised learning corpora have enabledmachine learning researchers to make substantial advances. However, to thisdate, there are no large-scale question-answer corpora available. In this paperwe present the 30M Factoid Question-Answer Corpus, an enormous question answerpair corpus produced by applying a novel neural network architecture on theknowledge base Freebase to transduce facts into natural language questions. Theproduced question answer pairs are evaluated both by human evaluators and usingautomatic evaluation metrics, including well-established machine translationand sentence similarity metrics. Across all evaluation criteria thequestion-generation model outperforms the competing template-based baseline.Furthermore, when presented to human evaluators, the generated questions appearcomparable in quality to real human-generated questions.", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE", "H.3.4; I.5.1; I.2.6; I.2.7"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "174", "arxiv_url": "http://arxiv.org/abs/1603.06807v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6016786167047035563&btnI=1&nossl=1&hl=en&oe=ASCII"}, "199": {"ID": 199, "title": "Making Neural QA as Simple as Possible but not Simpler", "authors": ["Dirk Weissenborn", "Laura Seiffe", "Georg Wiese"], "published": "2017-03-14T23:09:45Z", "updated": "2017-06-08T14:12:35Z", "abstract": "Recent development of large-scale question answering (QA) datasets triggereda substantial amount of research into end-to-end neural architectures for QA.Increasingly complex systems have been conceived without comparison to simplerneural baseline systems that would justify their complexity. In this work, wepropose a simple heuristic that guides the development of neural baselinesystems for the extractive QA task. We find that there are two ingredientsnecessary for building a high-performing neural QA system: first, the awarenessof question words while processing the context and second, a compositionfunction that goes beyond simple bag-of-words modeling, such as recurrentneural networks. Our results show that FastQA, a system that meets these tworequirements, can achieve very competitive performance compared with existingmodels. We argue that this surprising finding puts results of previous systemsand the complexity of recent QA datasets into perspective.", "categories": ["cs.CL", "cs.AI", "cs.NE"], "journal": "Proceedings of the 21st Conference on Computational Natural Language\u00a0\u2026", "citations": "123", "arxiv_url": "http://arxiv.org/abs/1703.04816v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=13849293522440536774&btnI=1&nossl=1&hl=en&oe=ASCII"}, "200": {"ID": 200, "title": "Interactive Attention for Neural Machine Translation", "authors": ["Qun Liu", "Hang Li", "Zhengdong Lu", "Fandong Meng"], "published": "2016-10-17T08:33:20Z", "updated": "2016-10-17T08:33:20Z", "abstract": "Conventional attention-based Neural Machine Translation (NMT) conductsdynamic alignment in generating the target sentence. By repeatedly reading therepresentation of source sentence, which keeps fixed after generated by theencoder (Bahdanau et al., 2015), the attention mechanism has greatly enhancedstate-of-the-art NMT. In this paper, we propose a new attention mechanism,called INTERACTIVE ATTENTION, which models the interaction between the decoderand the representation of source sentence during translation by both readingand writing operations. INTERACTIVE ATTENTION can keep track of the interactionhistory and therefore improve the translation performance. Experiments on NISTChinese-English translation task show that INTERACTIVE ATTENTION can achievesignificant improvements over both the previous attention-based NMT baselineand some state-of-the-art variants of attention-based NMT (i.e., coveragemodels (Tu et al., 2016)). And neural machine translator with our INTERACTIVEATTENTION can outperform the open source attention-based NMT system Groundhogby 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEUpoints averagely on multiple test sets.", "categories": ["cs.CL"], "journal": "COLING, 2174-2185", "citations": "51", "arxiv_url": "http://arxiv.org/abs/1610.05011v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=3150550352670415745&btnI=1&nossl=1&hl=en&oe=ASCII"}, "201": {"ID": 201, "title": "Classifying Relations by Ranking with Convolutional Neural Networks", "authors": ["Bowen Zhou", "Cicero Nogueira dos Santos", "Bing Xiang"], "published": "2015-04-24T17:50:33Z", "updated": "2015-05-24T13:58:05Z", "abstract": "Relation classification is an important semantic processing task for whichstate-ofthe-art systems still rely on costly handcrafted features. In this workwe tackle the relation classification task using a convolutional neural networkthat performs classification by ranking (CR-CNN). We propose a new pairwiseranking loss function that makes it easy to reduce the impact of artificialclasses. We perform experiments using the the SemEval-2010 Task 8 dataset,which is designed for the task of classifying the relationship between twonominals marked in a sentence. Using CRCNN, we outperform the state-of-the-artfor this dataset and achieve a F1 of 84.1 without using any costly handcraftedfeatures. Additionally, our experimental results show that: (1) our approach ismore effective than CNN followed by a softmax classifier; (2) omitting therepresentation of the artificial class Other improves both precision andrecall; and (3) using only word embeddings as input features is enough toachieve state-of-the-art results if we consider only the text between the twotarget nominals.", "categories": ["cs.CL", "cs.LG", "cs.NE"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "393", "arxiv_url": "http://arxiv.org/abs/1504.06580v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=1546237623627625762&btnI=1&nossl=1&hl=en&oe=ASCII"}, "202": {"ID": 202, "title": "Nematus: a Toolkit for Neural Machine Translation", "authors": ["Rico Sennrich", "Antonio Valerio Miceli Barone", "Jozef Mokry", "Samuel L\u00e4ubli", "Orhan Firat", "Maria N\u0103dejde", "Julian Hitschler", "Kyunghyun Cho", "Alexandra Birch", "Marcin Junczys-Dowmunt", "Barry Haddow"], "published": "2017-03-13T12:28:03Z", "updated": "2017-03-13T12:28:03Z", "abstract": "We present Nematus, a toolkit for Neural Machine Translation. The toolkitprioritizes high translation accuracy, usability, and extensibility. Nematushas been used to build top-performing submissions to shared translation tasksat WMT and IWSLT, and has been used to train systems for productionenvironments.", "categories": ["cs.CL"], "journal": "EACL (Software Demonstrations), 65-68", "citations": "299", "arxiv_url": "http://arxiv.org/abs/1703.04357v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9310327624786569573&btnI=1&nossl=1&hl=en&oe=ASCII"}, "203": {"ID": 203, "title": "Recursive Neural Conditional Random Fields for Aspect-based Sentiment  Analysis", "authors": ["Xiaokui Xiao", "Sinno Jialin Pan", "Wenya Wang", "Daniel Dahlmeier"], "published": "2016-03-22T05:59:00Z", "updated": "2016-09-19T14:00:43Z", "abstract": "In aspect-based sentiment analysis, extracting aspect terms along with theopinions being expressed from user-generated content is one of the mostimportant subtasks. Previous studies have shown that exploiting connectionsbetween aspect and opinion terms is promising for this task. In this paper, wepropose a novel joint model that integrates recursive neural networks andconditional random fields into a unified framework for explicit aspect andopinion terms co-extraction. The proposed model learns high-leveldiscriminative features and double propagate information between aspect andopinion terms, simultaneously. Moreover, it is flexible to incorporatehand-crafted features into the proposed model to further boost its informationextraction performance. Experimental results on the SemEval Challenge 2014dataset show the superiority of our proposed model over several baselinemethods as well as the winning systems of the challenge.", "categories": ["cs.CL", "cs.IR", "cs.LG"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "135", "arxiv_url": "http://arxiv.org/abs/1603.06679v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10118503982778372734&btnI=1&nossl=1&hl=en&oe=ASCII"}, "204": {"ID": 204, "title": "Why We Need New Evaluation Metrics for NLG", "authors": ["Amanda Cercas Curry", "Jekaterina Novikova", "Ond\u0159ej Du\u0161ek", "Verena Rieser"], "published": "2017-07-21T12:47:03Z", "updated": "2017-07-21T12:47:03Z", "abstract": "The majority of NLG evaluation relies on automatic metrics, such as BLEU . Inthis paper, we motivate the need for novel, system- and data-independentautomatic evaluation methods: We investigate a wide range of metrics, includingstate-of-the-art word-based and novel grammar-based ones, and demonstrate thatthey only weakly reflect human judgements of system outputs as generated bydata-driven, end-to-end NLG. We also show that metric performance is data- andsystem-specific. Nevertheless, our results also suggest that automatic metricsperform reliably at system-level and can support system development by findingcases where a system performs poorly.", "categories": ["cs.CL"], "journal": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "116", "arxiv_url": "http://arxiv.org/abs/1707.06875v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10624394081210897265&btnI=1&nossl=1&hl=en&oe=ASCII"}, "205": {"ID": 205, "title": "Addressing the Rare Word Problem in Neural Machine Translation", "authors": ["Wojciech Zaremba", "Minh-Thang Luong", "Oriol Vinyals", "Quoc V. Le", "Ilya Sutskever"], "published": "2014-10-30T00:20:31Z", "updated": "2015-05-30T19:57:28Z", "abstract": "Neural Machine Translation (NMT) is a new approach to machine translationthat has shown promising results that are comparable to traditional approaches.A significant weakness in conventional NMT systems is their inability tocorrectly translate very rare words: end-to-end NMTs tend to have relativelysmall vocabularies with a single unk symbol that represents every possibleout-of-vocabulary (OOV) word. In this paper, we propose and implement aneffective technique to address this problem. We train an NMT system on datathat is augmented by the output of a word alignment algorithm, allowing the NMTsystem to emit, for each OOV word in the target sentence, the position of itscorresponding word in the source sentence. This information is later utilizedin a post-processing step that translates every OOV word using a dictionary.Our experiments on the WMT14 English to French translation task show that thismethod provides a substantial improvement of up to 2.8 BLEU points over anequivalent NMT system that does not use this technique. With 37.5 BLEU points,our NMT system is the first to surpass the best result achieved on a WMT14contest task.", "categories": ["cs.CL", "cs.LG", "cs.NE"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "606", "arxiv_url": "http://arxiv.org/abs/1410.8206v4", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=1855379039969159341&btnI=1&nossl=1&hl=en&oe=ASCII"}, "206": {"ID": 206, "title": "A Neural Attention Model for Abstractive Sentence Summarization", "authors": ["Alexander M. Rush", "Jason Weston", "Sumit Chopra"], "published": "2015-09-02T13:20:40Z", "updated": "2015-09-03T19:55:45Z", "abstract": "Summarization based on text extraction is inherently limited, butgeneration-style abstractive methods have proven challenging to build. In thiswork, we propose a fully data-driven approach to abstractive sentencesummarization. Our method utilizes a local attention-based model that generateseach word of the summary conditioned on the input sentence. While the model isstructurally simple, it can easily be trained end-to-end and scales to a largeamount of training data. The model shows significant performance gains on theDUC-2004 shared task compared with several strong baselines.", "categories": ["cs.CL", "cs.AI"], "journal": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "1616", "arxiv_url": "http://arxiv.org/abs/1509.00685v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14516757443030182405&btnI=1&nossl=1&hl=en&oe=ASCII"}, "207": {"ID": 207, "title": "A Hierarchical Model of Reviews for Aspect-based Sentiment Analysis", "authors": ["Parsa Ghaffari", "John G. Breslin", "Sebastian Ruder"], "published": "2016-09-09T11:16:15Z", "updated": "2016-09-09T11:16:15Z", "abstract": "Opinion mining from customer reviews has become pervasive in recent years.Sentences in reviews, however, are usually classified independently, eventhough they form part of a review's argumentative structure. Intuitively,sentences in a review build and elaborate upon each other; knowledge of thereview structure and sentential context should thus inform the classificationof each sentence. We demonstrate this hypothesis for the task of aspect-basedsentiment analysis by modeling the interdependencies of sentences in a reviewwith a hierarchical bidirectional LSTM. We show that the hierarchical modeloutperforms two non-hierarchical baselines, obtains results competitive withthe state-of-the-art, and outperforms the state-of-the-art on fivemultilingual, multi-domain datasets without any hand-engineered features orexternal resources.", "categories": ["cs.CL", "cs.LG"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "114", "arxiv_url": "http://arxiv.org/abs/1609.02745v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=17958152432842762231&btnI=1&nossl=1&hl=en&oe=ASCII"}, "208": {"ID": 208, "title": "Variational Neural Machine Translation", "authors": ["Min Zhang", "Jinsong Su", "Hong Duan", "Deyi Xiong", "Biao Zhang"], "published": "2016-05-25T13:18:57Z", "updated": "2016-09-25T23:37:14Z", "abstract": "Models of neural machine translation are often from a discriminative familyof encoderdecoders that learn a conditional distribution of a target sentencegiven a source sentence. In this paper, we propose a variational model to learnthis conditional distribution for neural machine translation: a variationalencoderdecoder model that can be trained end-to-end. Different from the vanillaencoder-decoder model that generates target translations from hiddenrepresentations of source sentences alone, the variational model introduces acontinuous latent variable to explicitly model underlying semantics of sourcesentences and to guide the generation of target translations. In order toperform efficient posterior inference and large-scale training, we build aneural posterior approximator conditioned on both the source and the targetsides, and equip it with a reparameterization technique to estimate thevariational lower bound. Experiments on both Chinese-English and English-German translation tasks show that the proposed variational neural machinetranslation achieves significant improvements over the vanilla neural machinetranslation baselines.", "categories": ["cs.CL"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "113", "arxiv_url": "http://arxiv.org/abs/1605.07869v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=16453011540088245227&btnI=1&nossl=1&hl=en&oe=ASCII"}, "209": {"ID": 209, "title": "Multi-Task Deep Neural Networks for Natural Language Understanding", "authors": ["Weizhu Chen", "Xiaodong Liu", "Jianfeng Gao", "Pengcheng He"], "published": "2019-01-31T18:07:25Z", "updated": "2019-05-30T00:01:20Z", "abstract": "In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) forlearning representations across multiple natural language understanding (NLU)tasks. MT-DNN not only leverages large amounts of cross-task data, but alsobenefits from a regularization effect that leads to more generalrepresentations in order to adapt to new tasks and domains. MT-DNN extends themodel proposed in Liu et al. (2015) by incorporating a pre-trainedbidirectional transformer language model, known as BERT (Devlin et al., 2018).MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI,SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7%(2.2% absolute improvement). We also demonstrate using the SNLI and SciTaildatasets that the representations learned by MT-DNN allow domain adaptationwith substantially fewer in-domain labels than the pre-trained BERTrepresentations. The code and pre-trained models are publicly available athttps://github.com/namisan/mt-dnn.", "categories": ["cs.CL"], "journal": "Proceedings of the 57th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "236", "arxiv_url": "http://arxiv.org/abs/1901.11504v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=7022248125389648461&btnI=1&nossl=1&hl=en&oe=ASCII"}, "210": {"ID": 210, "title": "On Using Very Large Target Vocabulary for Neural Machine Translation", "authors": ["Roland Memisevic", "Kyunghyun Cho", "Yoshua Bengio", "S\u00e9bastien Jean"], "published": "2014-12-05T14:26:27Z", "updated": "2015-03-18T19:41:42Z", "abstract": "Neural machine translation, a recently proposed approach to machinetranslation based purely on neural networks, has shown promising resultscompared to the existing approaches such as phrase-based statistical machinetranslation. Despite its recent success, neural machine translation has itslimitation in handling a larger vocabulary, as training complexity as well asdecoding complexity increase proportionally to the number of target words. Inthis paper, we propose a method that allows us to use a very large targetvocabulary without increasing training complexity, based on importancesampling. We show that decoding can be efficiently done even with the modelhaving a very large target vocabulary by selecting only a small subset of thewhole target vocabulary. The models trained by the proposed approach areempirically found to outperform the baseline models with a small vocabulary aswell as the LSTM-based neural machine translation models. Furthermore, when weuse the ensemble of a few models with very large target vocabularies, weachieve the state-of-the-art translation performance (measured by BLEU) on theEnglish-&gt;German translation and almost as high performance as state-of-the-artEnglish-&gt;French translation system.", "categories": ["cs.CL"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "758", "arxiv_url": "http://arxiv.org/abs/1412.2007v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=13222564911222792417&btnI=1&nossl=1&hl=en&oe=ASCII"}, "211": {"ID": 211, "title": "Distant Supervision for Relation Extraction beyond the Sentence Boundary", "authors": ["Hoifung Poon", "Chris Quirk"], "published": "2016-09-15T22:01:26Z", "updated": "2017-08-14T23:54:49Z", "abstract": "The growing demand for structured knowledge has led to great interest inrelation extraction, especially in cases with limited supervision. However,existing distance supervision approaches only extract relations expressed insingle sentences. In general, cross-sentence relation extraction isunder-explored, even in the supervised-learning setting. In this paper, wepropose the first approach for applying distant supervision to cross- sentencerelation extraction. At the core of our approach is a graph representation thatcan incorporate both standard dependencies and discourse relations, thusproviding a unifying way to model relations within and across sentences. Weextract features from multiple paths in this graph, increasing accuracy androbustness when confronted with linguistic variation and analysis error.Experiments on an important extraction task for precision medicine show thatour approach can learn an accurate cross-sentence extractor, using only a smallexisting knowledge base and unlabeled text from biomedical research articles.Compared to the existing distant supervision paradigm, our approach extractedtwice as many relations at similar precision, thus demonstrating the prevalenceof cross-sentence relations and the promise of our approach.", "categories": ["cs.CL"], "journal": "EACL (1), 1171-1182", "citations": "73", "arxiv_url": "http://arxiv.org/abs/1609.04873v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4674138985865691115&btnI=1&nossl=1&hl=en&oe=ASCII"}, "212": {"ID": 212, "title": "Identifying beneficial task relations for multi-task learning in deep  neural networks", "authors": ["Anders S\u00f8gaard", "Joachim Bingel"], "published": "2017-02-27T14:37:21Z", "updated": "2017-02-27T14:37:21Z", "abstract": "Multi-task learning (MTL) in deep neural networks for NLP has recentlyreceived increasing interest due to some compelling benefits, including itspotential to efficiently regularize models and to reduce the need for labeleddata. While it has brought significant improvements in a number of NLP tasks,mixed results have been reported, and little is known about the conditionsunder which MTL leads to gains in NLP. This paper sheds light on the specifictask relations that can lead to gains from MTL models over single-task setups.", "categories": ["cs.CL", "I.2.7"], "journal": "EACL (2), 164-169", "citations": "144", "arxiv_url": "http://arxiv.org/abs/1702.08303v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=498901342272492774&btnI=1&nossl=1&hl=en&oe=ASCII"}, "213": {"ID": 213, "title": "A Hierarchical Neural Autoencoder for Paragraphs and Documents", "authors": ["Jiwei Li", "Minh-Thang Luong", "Dan Jurafsky"], "published": "2015-06-02T20:53:53Z", "updated": "2015-06-06T01:47:34Z", "abstract": "Natural language generation of coherent long texts like paragraphs or longerdocuments is a challenging problem for recurrent networks models. In thispaper, we explore an important step toward this generation task: training anLSTM (Long-short term memory) auto-encoder to preserve and reconstructmulti-sentence paragraphs. We introduce an LSTM model that hierarchicallybuilds an embedding for a paragraph from embeddings for sentences and words,then decodes this embedding to reconstruct the original paragraph. We evaluatethe reconstructed paragraph using standard metrics like ROUGE and Entity Grid,showing that neural models are able to encode texts in a way that preservesyntactic, semantic, and discourse coherence. While only a first step towardgenerating coherent text units from neural models, our work has the potentialto significantly impact natural language generation andsummarization\\footnote{Code for the three models described in this paper can befound at www.stanford.edu/~jiweil/ .", "categories": ["cs.CL"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "468", "arxiv_url": "http://arxiv.org/abs/1506.01057v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8614969202104458050&btnI=1&nossl=1&hl=en&oe=ASCII"}, "214": {"ID": 214, "title": "Semantic Tagging with Deep Residual Networks", "authors": ["Barbara Plank", "Johan Bos", "Johannes Bjerva"], "published": "2016-09-22T16:34:00Z", "updated": "2016-10-31T18:33:13Z", "abstract": "We propose a novel semantic tagging task, sem-tagging, tailored for thepurpose of multilingual semantic parsing, and present the first tagger usingdeep residual networks (ResNets). Our tagger uses both word and characterrepresentations and includes a novel residual bypass architecture. We evaluatethe tagset both intrinsically on the new task of semantic tagging, as well ason Part-of-Speech (POS) tagging. Our system, consisting of a ResNet and anauxiliary loss function predicting our semantic tags, significantly outperformsprior results on English Universal Dependencies POS tagging (95.71% accuracy onUD v1.2 and 95.67% accuracy on UD v1.3).", "categories": ["cs.CL"], "journal": "COLING, 3531-3541", "citations": "57", "arxiv_url": "http://arxiv.org/abs/1609.07053v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8842746976087283952&btnI=1&nossl=1&hl=en&oe=ASCII"}, "215": {"ID": 215, "title": "Dependency Sensitive Convolutional Neural Networks for Modeling  Sentences and Documents", "authors": ["Rui Zhang", "Honglak Lee", "Dragomir Radev"], "published": "2016-11-08T01:48:15Z", "updated": "2016-11-08T01:48:15Z", "abstract": "The goal of sentence and document modeling is to accurately represent themeaning of sentences and documents for various Natural Language Processingtasks. In this work, we present Dependency Sensitive Convolutional NeuralNetworks (DSCNN) as a general-purpose classification system for both sentencesand documents. DSCNN hierarchically builds textual representations byprocessing pretrained word embeddings via Long Short-Term Memory networks andsubsequently extracting features with convolution operators. Compared withexisting recursive neural models with tree structures, DSCNN does not rely onparsers and expensive phrase labeling, and thus is not restricted tosentence-level tasks. Moreover, unlike other CNN-based models that analyzesentences locally by sliding windows, our system captures both the dependencyinformation within each sentence and relationships across sentences in the samedocument. Experiment results demonstrate that our approach is achievingstate-of-the-art performance on several tasks, including sentiment analysis,question type classification, and subjectivity classification.", "categories": ["cs.CL"], "journal": "HLT-NAACL, 1512-1521", "citations": "93", "arxiv_url": "http://arxiv.org/abs/1611.02361v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=13516257814976233883&btnI=1&nossl=1&hl=en&oe=ASCII"}, "216": {"ID": 216, "title": "Improving Neural Machine Translation with Conditional Sequence  Generative Adversarial Nets", "authors": ["Zhen Yang", "Wei Chen", "Feng Wang", "Bo Xu"], "published": "2017-03-15T02:26:25Z", "updated": "2018-04-08T08:23:43Z", "abstract": "This paper proposes an approach for applying GANs to NMT. We build aconditional sequence generative adversarial net which comprises of twoadversarial sub models, a generator and a discriminator. The generator aims togenerate sentences which are hard to be discriminated from human-translatedsentences (i.e., the golden target sentences), And the discriminator makesefforts to discriminate the machine-generated sentences from human-translatedones. The two sub models play a mini-max game and achieve the win-win situationwhen they reach a Nash Equilibrium. Additionally, the static sentence-levelBLEU is utilized as the reinforced objective for the generator, which biasesthe generation towards high BLEU points. During training, both the dynamicdiscriminator and the static BLEU objective are employed to evaluate thegenerated sentences and feedback the evaluations to guide the learning of thegenerator. Experimental results show that the proposed model consistentlyoutperforms the traditional RNNSearch and the newly emerged state-of-the-artTransformer on English-German and Chinese-English translation tasks.", "categories": ["cs.CL"], "journal": "NAACL-HLT, 1346-1355", "citations": "104", "arxiv_url": "http://arxiv.org/abs/1703.04887v4", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14312548252804187966&btnI=1&nossl=1&hl=en&oe=ASCII"}, "217": {"ID": 217, "title": "Towards End-to-End Reinforcement Learning of Dialogue Agents for  Information Access", "authors": ["Jianfeng Gao", "Bhuwan Dhingra", "Li Deng", "Lihong Li", "Faisal Ahmed", "Xiujun Li", "Yun-Nung Chen"], "published": "2016-09-03T01:02:51Z", "updated": "2017-04-20T17:26:35Z", "abstract": "This paper proposes KB-InfoBot -- a multi-turn dialogue agent which helpsusers search Knowledge Bases (KBs) without composing complicated queries. Suchgoal-oriented dialogue agents typically need to interact with an externaldatabase to access real-world knowledge. Previous systems achieved this byissuing a symbolic query to the KB to retrieve entries based on theirattributes. However, such symbolic operations break the differentiability ofthe system and prevent end-to-end training of neural dialogue agents. In thispaper, we address this limitation by replacing symbolic queries with an induced\"soft\" posterior distribution over the KB that indicates which entities theuser is interested in. Integrating the soft retrieval process with areinforcement learner leads to higher task success rate and reward in bothsimulations and against real users. We also present a fully neural end-to-endagent, trained entirely from user feedback, and discuss its application towardspersonalized dialogue agents. The source code is available athttps://github.com/MiuLab/KB-InfoBot.", "categories": ["cs.CL", "cs.LG"], "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "190", "arxiv_url": "http://arxiv.org/abs/1609.00777v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10519366996413471387&btnI=1&nossl=1&hl=en&oe=ASCII"}, "218": {"ID": 218, "title": "Analysis Methods in Neural Language Processing: A Survey", "authors": ["James Glass", "Yonatan Belinkov"], "published": "2018-12-21T05:13:03Z", "updated": "2019-01-14T16:38:50Z", "abstract": "The field of natural language processing has seen impressive progress inrecent years, with neural network models replacing many of the traditionalsystems. A plethora of new models have been proposed, many of which are thoughtto be opaque compared to their feature-rich counterparts. This has ledresearchers to analyze, interpret, and evaluate neural networks in novel andmore fine-grained ways. In this survey paper, we review analysis methods inneural language processing, categorize them according to prominent researchtrends, highlight existing limitations, and point to potential directions forfuture work.", "categories": ["cs.CL", "cs.LG", "cs.NE", "68T50", "I.2.7"], "journal": "Transactions of the Association for Computational Linguistics 7, 49-72", "citations": "89", "arxiv_url": "http://arxiv.org/abs/1812.08951v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=18189996871733265233&btnI=1&nossl=1&hl=en&oe=ASCII"}, "219": {"ID": 219, "title": "Explainable Prediction of Medical Codes from Clinical Text", "authors": ["Jacob Eisenstein", "Sarah Wiegreffe", "Jimeng Sun", "Jon Duke", "James Mullenbach"], "published": "2018-02-15T18:25:32Z", "updated": "2018-04-16T21:45:35Z", "abstract": "Clinical notes are text documents that are created by clinicians for eachpatient encounter. They are typically accompanied by medical codes, whichdescribe the diagnosis and treatment. Annotating these codes is labor intensiveand error prone; furthermore, the connection between the codes and the text isnot annotated, obscuring the reasons and details behind specific diagnoses andtreatments. We present an attentional convolutional network that predictsmedical codes from clinical text. Our method aggregates information across thedocument using a convolutional neural network, and uses an attention mechanismto select the most relevant segments for each of the thousands of possiblecodes. The method is accurate, achieving precision@8 of 0.71 and a Micro-F1 of0.54, which are both better than the prior state of the art. Furthermore,through an interpretability evaluation by a physician, we show that theattention mechanism identifies meaningful explanations for each code assignment", "categories": ["cs.CL", "cs.LG", "stat.ML"], "journal": "NAACL-HLT, 1101-1111", "citations": "97", "arxiv_url": "http://arxiv.org/abs/1802.05695v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=7588660628347197715&btnI=1&nossl=1&hl=en&oe=ASCII"}, "220": {"ID": 220, "title": "Encoding Sentences with Graph Convolutional Networks for Semantic Role  Labeling", "authors": ["Ivan Titov", "Diego Marcheggiani"], "published": "2017-03-14T23:25:34Z", "updated": "2017-07-30T17:24:38Z", "abstract": "Semantic role labeling (SRL) is the task of identifying thepredicate-argument structure of a sentence. It is typically regarded as animportant step in the standard NLP pipeline. As the semantic representationsare closely related to syntactic ones, we exploit syntactic information in ourmodel. We propose a version of graph convolutional networks (GCNs), a recentclass of neural networks operating on graphs, suited to model syntacticdependency graphs. GCNs over syntactic dependency trees are used as sentenceencoders, producing latent feature representations of words in a sentence. Weobserve that GCN layers are complementary to LSTM ones: when we stack both GCNand LSTM layers, we obtain a substantial improvement over an alreadystate-of-the-art LSTM SRL model, resulting in the best reported scores on thestandard benchmark (CoNLL-2009) both for Chinese and English.", "categories": ["cs.CL", "cs.LG"], "journal": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "283", "arxiv_url": "http://arxiv.org/abs/1703.04826v4", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4909578531156646395&btnI=1&nossl=1&hl=en&oe=ASCII"}, "221": {"ID": 221, "title": "A large annotated corpus for learning natural language inference", "authors": ["Christopher Potts", "Christopher D. Manning", "Gabor Angeli", "Samuel R. Bowman"], "published": "2015-08-21T16:17:01Z", "updated": "2015-08-21T16:17:01Z", "abstract": "Understanding entailment and contradiction is fundamental to understandingnatural language, and inference about entailment and contradiction is avaluable testing ground for the development of semantic representations.However, machine learning research in this area has been dramatically limitedby the lack of large-scale resources. To address this, we introduce theStanford Natural Language Inference corpus, a new, freely available collectionof labeled sentence pairs, written by humans doing a novel grounded task basedon image captioning. At 570K pairs, it is two orders of magnitude larger thanall other resources of its type. This increase in scale allows lexicalizedclassifiers to outperform some sophisticated existing entailment models, and itallows a neural network-based model to perform competitively on naturallanguage inference benchmarks for the first time.", "categories": ["cs.CL"], "journal": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "1350", "arxiv_url": "http://arxiv.org/abs/1508.05326v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=11107072381442434916&btnI=1&nossl=1&hl=en&oe=ASCII"}, "222": {"ID": 222, "title": "Chinese Poetry Generation with Planning based Neural Network", "authors": ["Haifeng Wang", "Haiyang Wu", "Hua Wu", "Wei He", "Zhe Wang", "Enhong Chen", "Wei Li"], "published": "2016-10-31T12:16:39Z", "updated": "2016-12-07T03:56:19Z", "abstract": "Chinese poetry generation is a very challenging task in natural languageprocessing. In this paper, we propose a novel two-stage poetry generatingmethod which first plans the sub-topics of the poem according to the user'swriting intent, and then generates each line of the poem sequentially, using amodified recurrent neural network encoder-decoder framework. The proposedplanning-based method can ensure that the generated poem is coherent andsemantically consistent with the user's intent. A comprehensive evaluation withhuman judgments demonstrates that our proposed approach outperforms thestate-of-the-art poetry generating methods and the poem quality is somehowcomparable to human poets.", "categories": ["cs.CL", "cs.AI"], "journal": "COLING, 1051-1060", "citations": "78", "arxiv_url": "http://arxiv.org/abs/1610.09889v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=5542866896344725179&btnI=1&nossl=1&hl=en&oe=ASCII"}, "223": {"ID": 223, "title": "Neural Belief Tracker: Data-Driven Dialogue State Tracking", "authors": ["Tsung-Hsien Wen", "Steve Young", "Diarmuid \u00d3 S\u00e9aghdha", "Blaise Thomson", "Nikola Mrk\u0161i\u0107"], "published": "2016-06-12T22:59:14Z", "updated": "2017-04-21T15:15:03Z", "abstract": "One of the core components of modern spoken dialogue systems is the belieftracker, which estimates the user's goal at every step of the dialogue.However, most current approaches have difficulty scaling to larger, morecomplex dialogue domains. This is due to their dependency on either: a) SpokenLanguage Understanding models that require large amounts of annotated trainingdata; or b) hand-crafted lexicons for capturing some of the linguisticvariation in users' language. We propose a novel Neural Belief Tracking (NBT)framework which overcomes these problems by building on recent advances inrepresentation learning. NBT models reason over pre-trained word vectors,learning to compose them into distributed representations of user utterancesand dialogue context. Our evaluation on two datasets shows that this approachsurpasses past limitations, matching the performance of state-of-the-art modelswhich rely on hand-crafted semantic lexicons and outperforming them when suchlexicons are not provided.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "210", "arxiv_url": "http://arxiv.org/abs/1606.03777v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9063714191770032851&btnI=1&nossl=1&hl=en&oe=ASCII"}, "224": {"ID": 224, "title": "Retrofitting Word Vectors to Semantic Lexicons", "authors": ["Manaal Faruqui", "Eduard Hovy", "Sujay K. Jauhar", "Noah A. Smith", "Chris Dyer", "Jesse Dodge"], "published": "2014-11-15T17:34:20Z", "updated": "2015-03-22T17:55:20Z", "abstract": "Vector space word representations are learned from distributional informationof words in large corpora. Although such statistics are semanticallyinformative, they disregard the valuable information that is contained insemantic lexicons such as WordNet, FrameNet, and the Paraphrase Database. Thispaper proposes a method for refining vector space representations usingrelational information from semantic lexicons by encouraging linked words tohave similar vector representations, and it makes no assumptions about how theinput vectors were constructed. Evaluated on a battery of standard lexicalsemantic evaluation tasks in several languages, we obtain substantialimprovements starting with a variety of word vector models. Our refinementmethod outperforms prior techniques for incorporating semantic lexicons intothe word vector training algorithms.", "categories": ["cs.CL"], "journal": "HLT-NAACL, 1606-1615", "citations": "680", "arxiv_url": "http://arxiv.org/abs/1411.4166v4", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=621044119810582816&btnI=1&nossl=1&hl=en&oe=ASCII"}, "225": {"ID": 225, "title": "Learning Structured Text Representations", "authors": ["Yang Liu", "Mirella Lapata"], "published": "2017-05-25T14:54:07Z", "updated": "2018-02-03T13:31:40Z", "abstract": "In this paper, we focus on learning structure-aware document representationsfrom data without recourse to a discourse parser or additional annotations.Drawing inspiration from recent efforts to empower neural networks with astructural bias, we propose a model that can encode a document whileautomatically inducing rich structural dependencies. Specifically, we embed adifferentiable non-projective parsing algorithm into a neural model and useattention mechanisms to incorporate the structural biases. Experimentalevaluation across different tasks and datasets shows that the proposed modelachieves state-of-the-art results on document modeling tasks while inducingintermediate structures which are both interpretable and meaningful.", "categories": ["cs.CL", "cs.AI"], "journal": "Transactions of the Association for Computational Linguistics 6, 63-75", "citations": "72", "arxiv_url": "http://arxiv.org/abs/1705.09207v4", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6754950803684629168&btnI=1&nossl=1&hl=en&oe=ASCII"}, "226": {"ID": 226, "title": "Improved Transition-Based Parsing by Modeling Characters instead of  Words with LSTMs", "authors": ["Chris Dyer", "Noah A. Smith", "Miguel Ballesteros"], "published": "2015-08-04T04:36:36Z", "updated": "2015-08-11T17:33:47Z", "abstract": "We present extensions to a continuous-state dependency parsing method thatmakes it applicable to morphologically rich languages. Starting with ahigh-performance transition-based parser that uses long short-term memory(LSTM) recurrent neural networks to learn representations of the parser state,we replace lookup-based word representations with representations constructedfrom the orthographic representations of the words, also using LSTMs. Thisallows statistical sharing across word forms that are similar on the surface.Experiments for morphologically rich languages show that the parsing modelbenefits from incorporating the character-based encodings of words.", "categories": ["cs.CL"], "journal": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "262", "arxiv_url": "http://arxiv.org/abs/1508.00657v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14400794765925169201&btnI=1&nossl=1&hl=en&oe=ASCII"}, "227": {"ID": 227, "title": "A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task", "authors": ["Danqi Chen", "Christopher D. Manning", "Jason Bolton"], "published": "2016-06-09T08:19:16Z", "updated": "2016-08-08T21:21:19Z", "abstract": "Enabling a computer to understand a document so that it can answercomprehension questions is a central, yet unsolved goal of NLP. A key factorimpeding its solution by machine learned systems is the limited availability ofhuman-annotated data. Hermann et al. (2015) seek to solve this problem bycreating over a million training examples by pairing CNN and Daily Mail newsarticles with their summarized bullet points, and show that a neural networkcan then be trained to give good performance on this task. In this paper, weconduct a thorough examination of this new reading comprehension task. Ourprimary aim is to understand what depth of language understanding is requiredto do well on this task. We approach this from one side by doing a carefulhand-analysis of a small subset of the problems and from the other by showingthat simple, carefully designed systems can obtain accuracies of 73.6% and76.6% on these two datasets, exceeding current state-of-the-art results by7-10% and approaching what we believe is the ceiling for performance on thistask.", "categories": ["cs.CL", "cs.AI"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "401", "arxiv_url": "http://arxiv.org/abs/1606.02858v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=18197180518917268018&btnI=1&nossl=1&hl=en&oe=ASCII"}, "228": {"ID": 228, "title": "Language as a Latent Variable: Discrete Generative Models for Sentence  Compression", "authors": ["Phil Blunsom", "Yishu Miao"], "published": "2016-09-23T11:25:41Z", "updated": "2016-10-14T00:21:00Z", "abstract": "In this work we explore deep generative models of text in which the latentrepresentation of a document is itself drawn from a discrete language modeldistribution. We formulate a variational auto-encoder for inference in thismodel and apply it to the task of compressing sentences. In this applicationthe generative model first draws a latent summary sentence from a backgroundlanguage model, and then subsequently draws the observed sentence conditionedon this latent summary. In our empirical evaluation we show that generativeformulations of both abstractive and extractive compression yieldstate-of-the-art results when trained on a large amount of supervised data.Further, we explore semi-supervised compression scenarios where we show that itis possible to achieve performance competitive with previously proposedsupervised models while training on a fraction of the supervised data.", "categories": ["cs.CL", "cs.AI"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "136", "arxiv_url": "http://arxiv.org/abs/1609.07317v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=2822078281007934295&btnI=1&nossl=1&hl=en&oe=ASCII"}, "229": {"ID": 229, "title": "A Deeper Look into Sarcastic Tweets Using Deep Convolutional Neural  Networks", "authors": ["Soujanya Poria", "Devamanyu Hazarika", "Erik Cambria", "Prateek Vij"], "published": "2016-10-27T14:50:43Z", "updated": "2017-07-27T02:38:59Z", "abstract": "Sarcasm detection is a key task for many natural language processing tasks.In sentiment analysis, for example, sarcasm can flip the polarity of an\"apparently positive\" sentence and, hence, negatively affect polarity detectionperformance. To date, most approaches to sarcasm detection have treated thetask primarily as a text categorization problem. Sarcasm, however, can beexpressed in very subtle ways and requires a deeper understanding of naturallanguage that standard text categorization techniques cannot grasp. In thiswork, we develop models based on a pre-trained convolutional neural network forextracting sentiment, emotion and personality features for sarcasm detection.Such features, along with the network's baseline features, allow the proposedmodels to outperform the state of the art on benchmark datasets. We alsoaddress the often ignored generalizability issue of classifying data that havenot been seen by the models at learning phase.", "categories": ["cs.CL"], "journal": "COLING, 1601-1612", "citations": "175", "arxiv_url": "http://arxiv.org/abs/1610.08815v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=11024431644312350592&btnI=1&nossl=1&hl=en&oe=ASCII"}, "230": {"ID": 230, "title": "Neural Text Generation from Structured Data with Application to the  Biography Domain", "authors": ["David Grangier", "Remi Lebret", "Michael Auli"], "published": "2016-03-24T22:40:00Z", "updated": "2016-09-23T15:16:46Z", "abstract": "This paper introduces a neural model for concept-to-text generation thatscales to large, rich domains. We experiment with a new dataset of biographiesfrom Wikipedia that is an order of magnitude larger than existing resourceswith over 700k samples. The dataset is also vastly more diverse with a 400kvocabulary, compared to a few hundred words for Weathergov or Robocup. Ourmodel builds upon recent work on conditional neural language model for textgeneration. To deal with the large vocabulary, we extend these models to mix afixed vocabulary with copy actions that transfer sample-specific words from theinput database to the generated output sentence. Our neural model significantlyout-performs a classical Kneser-Ney language model adapted to this task bynearly 15 BLEU.", "categories": ["cs.CL"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "167", "arxiv_url": "http://arxiv.org/abs/1603.07771v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=7894338485021394364&btnI=1&nossl=1&hl=en&oe=ASCII"}, "231": {"ID": 231, "title": "Language Models for Image Captioning: The Quirks and What Works", "authors": ["Hao Cheng", "Li Deng", "Geoffrey Zweig", "Saurabh Gupta", "Xiaodong He", "Jacob Devlin", "Margaret Mitchell", "Hao Fang"], "published": "2015-05-07T18:36:14Z", "updated": "2015-10-14T22:03:40Z", "abstract": "Two recent approaches have achieved state-of-the-art results in imagecaptioning. The first uses a pipelined process where a set of candidate wordsis generated by a convolutional neural network (CNN) trained on images, andthen a maximum entropy (ME) language model is used to arrange these words intoa coherent sentence. The second uses the penultimate activation layer of theCNN as input to a recurrent neural network (RNN) that then generates thecaption sequence. In this paper, we compare the merits of these differentlanguage modeling approaches for the first time by using the samestate-of-the-art CNN as input. We examine issues in the different approaches,including linguistic irregularities, caption repetition, and data set overlap.By combining key aspects of the ME and RNN methods, we achieve a new recordperformance over previously published results on the benchmark COCO dataset.However, the gains we see in BLEU do not translate to human judgments.", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "208", "arxiv_url": "http://arxiv.org/abs/1505.01809v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4462970380624797236&btnI=1&nossl=1&hl=en&oe=ASCII"}, "232": {"ID": 232, "title": "A Decomposable Attention Model for Natural Language Inference", "authors": ["Oscar T\u00e4ckstr\u00f6m", "Dipanjan Das", "Ankur P. Parikh", "Jakob Uszkoreit"], "published": "2016-06-06T20:30:57Z", "updated": "2016-09-25T23:52:45Z", "abstract": "We propose a simple neural architecture for natural language inference. Ourapproach uses attention to decompose the problem into subproblems that can besolved separately, thus making it trivially parallelizable. On the StanfordNatural Language Inference (SNLI) dataset, we obtain state-of-the-art resultswith almost an order of magnitude fewer parameters than previous work andwithout relying on any word-order information. Adding intra-sentence attentionthat takes a minimum amount of order into account yields further improvements.", "categories": ["cs.CL"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "687", "arxiv_url": "http://arxiv.org/abs/1606.01933v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=5396586952287396836&btnI=1&nossl=1&hl=en&oe=ASCII"}, "233": {"ID": 233, "title": "Semi-supervised Multitask Learning for Sequence Labeling", "authors": ["Marek Rei"], "published": "2017-04-24T11:47:06Z", "updated": "2017-04-24T11:47:06Z", "abstract": "We propose a sequence labeling framework with a secondary training objective,learning to predict surrounding words for every word in the dataset. Thislanguage modeling objective incentivises the system to learn general-purposepatterns of semantic and syntactic composition, which are also useful forimproving accuracy on different sequence labeling tasks. The architecture wasevaluated on a range of datasets, covering the tasks of error detection inlearner texts, named entity recognition, chunking and POS-tagging. The novellanguage modeling objective provided consistent performance improvements onevery benchmark, without requiring any additional annotated or unannotateddata.", "categories": ["cs.CL", "cs.LG", "cs.NE", "I.5.1; I.2.6; I.2.7"], "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "155", "arxiv_url": "http://arxiv.org/abs/1704.07156v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10895736556454976520&btnI=1&nossl=1&hl=en&oe=ASCII"}, "234": {"ID": 234, "title": "Learning Distributed Representations of Sentences from Unlabelled Data", "authors": ["Anna Korhonen", "Felix Hill", "Kyunghyun Cho"], "published": "2016-02-10T18:49:58Z", "updated": "2016-02-10T18:49:58Z", "abstract": "Unsupervised methods for learning distributed representations of words areubiquitous in today's NLP research, but far less is known about the best waysto learn distributed phrase or sentence representations from unlabelled data.This paper is a systematic comparison of models that learn suchrepresentations. We find that the optimal approach depends critically on theintended application. Deeper, more complex models are preferable forrepresentations to be used in supervised systems, but shallow log-linear modelswork best for building representation spaces that can be decoded with simplespatial distance metrics. We also propose two new unsupervisedrepresentation-learning objectives designed to optimise the trade-off betweentraining time, domain portability and performance.", "categories": ["cs.CL", "cs.LG"], "journal": "HLT-NAACL, 1367-1377", "citations": "353", "arxiv_url": "http://arxiv.org/abs/1602.03483v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=2003172603055187072&btnI=1&nossl=1&hl=en&oe=ASCII"}, "235": {"ID": 235, "title": "AutoExtend: Extending Word Embeddings to Embeddings for Synsets and  Lexemes", "authors": ["Hinrich Sch\u00fctze", "Sascha Rothe"], "published": "2015-07-04T16:59:30Z", "updated": "2015-07-04T16:59:30Z", "abstract": "We present \\textit{AutoExtend}, a system to learn embeddings for synsets andlexemes. It is flexible in that it can take any word embeddings as input anddoes not need an additional training corpus. The synset/lexeme embeddingsobtained live in the same vector space as the word embeddings. A sparse tensorformalization guarantees efficiency and parallelizability. We use WordNet as alexical resource, but AutoExtend can be easily applied to other resources likeFreebase. AutoExtend achieves state-of-the-art performance on word similarityand word sense disambiguation tasks.", "categories": ["cs.CL"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "246", "arxiv_url": "http://arxiv.org/abs/1507.01127v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4572079392398523224&btnI=1&nossl=1&hl=en&oe=ASCII"}, "236": {"ID": 236, "title": "Hybrid Code Networks: practical and efficient end-to-end dialog control  with supervised and reinforcement learning", "authors": ["Jason D. Williams", "Kavosh Asadi", "Geoffrey Zweig"], "published": "2017-02-10T18:24:13Z", "updated": "2017-04-24T14:39:27Z", "abstract": "End-to-end learning of recurrent neural networks (RNNs) is an attractivesolution for dialog systems; however, current techniques are data-intensive andrequire thousands of dialogs to learn simple behaviors. We introduce HybridCode Networks (HCNs), which combine an RNN with domain-specific knowledgeencoded as software and system action templates. Compared to existingend-to-end approaches, HCNs considerably reduce the amount of training datarequired, while retaining the key benefit of inferring a latent representationof dialog state. In addition, HCNs can be optimized with supervised learning,reinforcement learning, or a mixture of both. HCNs attain state-of-the-artperformance on the bAbI dialog dataset, and outperform two commerciallydeployed customer-facing dialog systems.", "categories": ["cs.AI", "cs.CL"], "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "216", "arxiv_url": "http://arxiv.org/abs/1702.03274v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=11966127855011058367&btnI=1&nossl=1&hl=en&oe=ASCII"}, "237": {"ID": 237, "title": "Universal Dependency Parsing from Scratch", "authors": ["Timothy Dozat", "Christopher D. Manning", "Peng Qi", "Yuhao Zhang"], "published": "2019-01-29T18:58:29Z", "updated": "2019-01-29T18:58:29Z", "abstract": "This paper describes Stanford's system at the CoNLL 2018 UD Shared Task. Weintroduce a complete neural pipeline system that takes raw text as input, andperforms all tasks required by the shared task, ranging from tokenization andsentence segmentation, to POS tagging and dependency parsing. Our single systemsubmission achieved very competitive performance on big treebanks. Moreover,after fixing an unfortunate bug, our corrected system would have placed the2nd, 1st, and 3rd on the official evaluation metrics LAS,MLAS, and BLEX, andwould have outperformed all submission systems on low-resource treebankcategories on all metrics by a large margin. We further show the effectivenessof different model components through extensive ablation studies.", "categories": ["cs.CL"], "journal": "CoNLL Shared Task (2), 160-170", "citations": "76", "arxiv_url": "http://arxiv.org/abs/1901.10457v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=2126685086695913159&btnI=1&nossl=1&hl=en&oe=ASCII"}, "238": {"ID": 238, "title": "What do Neural Machine Translation Models Learn about Morphology?", "authors": ["Hassan Sajjad", "Fahim Dalvi", "Nadir Durrani", "Yonatan Belinkov", "James Glass"], "published": "2017-04-11T18:01:07Z", "updated": "2018-10-22T14:27:40Z", "abstract": "Neural machine translation (MT) models obtain state-of-the-art performancewhile maintaining a simple, end-to-end architecture. However, little is knownabout what these models learn about source and target languages during thetraining process. In this work, we analyze the representations learned byneural MT models at various levels of granularity and empirically evaluate thequality of the representations for learning morphology through extrinsicpart-of-speech and morphological tagging tasks. We conduct a thoroughinvestigation along several parameters: word-based vs. character-basedrepresentations, depth of the encoding layer, the identity of the targetlanguage, and encoder vs. decoder representations. Our data-driven,quantitative evaluation sheds light on important aspects in the neural MTsystem and its ability to capture word structure.", "categories": ["cs.CL", "I.2.7"], "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "164", "arxiv_url": "http://arxiv.org/abs/1704.03471v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=3142186338143493642&btnI=1&nossl=1&hl=en&oe=ASCII"}, "239": {"ID": 239, "title": "Men Also Like Shopping: Reducing Gender Bias Amplification using  Corpus-level Constraints", "authors": ["Vicente Ordonez", "Tianlu Wang", "Kai-Wei Chang", "Mark Yatskar", "Jieyu Zhao"], "published": "2017-07-29T03:38:32Z", "updated": "2017-07-29T03:38:32Z", "abstract": "Language is increasingly being used to define rich visual recognitionproblems with supporting image collections sourced from the web. Structuredprediction models are used in these tasks to take advantage of correlationsbetween co-occurring labels and visual input but risk inadvertently encodingsocial biases found in web corpora. In this work, we study data and modelsassociated with multilabel object classification and visual semantic rolelabeling. We find that (a) datasets for these tasks contain significant genderbias and (b) models trained on these datasets further amplify existing bias.For example, the activity cooking is over 33% more likely to involve femalesthan males in a training set, and a trained model further amplifies thedisparity to 68% at test time. We propose to inject corpus-level constraintsfor calibrating existing structured prediction models and design an algorithmbased on Lagrangian relaxation for collective inference. Our method results inalmost no performance loss for the underlying recognition task but decreasesthe magnitude of bias amplification by 47.5% and 40.5% for multilabelclassification and visual semantic role labeling, respectively.", "categories": ["cs.AI", "cs.CL", "cs.CV", "stat.ML"], "journal": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "272", "arxiv_url": "http://arxiv.org/abs/1707.09457v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=2445652374493118307&btnI=1&nossl=1&hl=en&oe=ASCII"}, "240": {"ID": 240, "title": "Zero-Shot Relation Extraction via Reading Comprehension", "authors": ["Omer Levy", "Eunsol Choi", "Luke Zettlemoyer", "Minjoon Seo"], "published": "2017-06-13T15:17:42Z", "updated": "2017-06-13T15:17:42Z", "abstract": "We show that relation extraction can be reduced to answering simple readingcomprehension questions, by associating one or more natural-language questionswith each relation slot. This reduction has several advantages: we can (1)learn relation-extraction models by extending recent neuralreading-comprehension techniques, (2) build very large training sets for thosemodels by combining relation-specific crowd-sourced questions with distantsupervision, and even (3) do zero-shot learning by extracting new relationtypes that are only specified at test-time, for which we have no labeledtraining examples. Experiments on a Wikipedia slot-filling task demonstratethat the approach can generalize to new questions for known relation types withhigh accuracy, and that zero-shot generalization to unseen relation types ispossible, at lower accuracy levels, setting the bar for future work on thistask.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "journal": "Proceedings of the 21st Conference on Computational Natural Language\u00a0\u2026", "citations": "100", "arxiv_url": "http://arxiv.org/abs/1706.04115v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=1839997200582981079&btnI=1&nossl=1&hl=en&oe=ASCII"}, "241": {"ID": 241, "title": "Sentence Similarity Learning by Lexical Decomposition and Composition", "authors": ["Zhiguo Wang", "Haitao Mi", "Abraham Ittycheriah"], "published": "2016-02-23T03:08:50Z", "updated": "2017-07-14T19:51:10Z", "abstract": "Most conventional sentence similarity methods only focus on similar parts oftwo input sentences, and simply ignore the dissimilar parts, which usually giveus some clues and semantic meanings about the sentences. In this work, wepropose a model to take into account both the similarities and dissimilaritiesby decomposing and composing lexical semantics over sentences. The modelrepresents each word as a vector, and calculates a semantic matching vector foreach word based on all words in the other sentence. Then, each word vector isdecomposed into a similar component and a dissimilar component based on thesemantic matching vector. After this, a two-channel CNN model is employed tocapture features by composing the similar and dissimilar components. Finally, asimilarity score is estimated over the composed feature vectors. Experimentalresults show that our model gets the state-of-the-art performance on the answersentence selection task, and achieves a comparable result on the paraphraseidentification task.", "categories": ["cs.CL"], "journal": "COLING, 1340-1349", "citations": "140", "arxiv_url": "http://arxiv.org/abs/1602.07019v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=279666867462337074&btnI=1&nossl=1&hl=en&oe=ASCII"}, "242": {"ID": 242, "title": "A Hybrid Convolutional Variational Autoencoder for Text Generation", "authors": ["Aliaksei Severyn", "Stanislau Semeniuta", "Erhardt Barth"], "published": "2017-02-08T12:11:41Z", "updated": "2017-02-08T12:11:41Z", "abstract": "In this paper we explore the effect of architectural choices on learning aVariational Autoencoder (VAE) for text generation. In contrast to thepreviously introduced VAE model for text where both the encoder and decoder areRNNs, we propose a novel hybrid architecture that blends fully feed-forwardconvolutional and deconvolutional components with a recurrent language model.Our architecture exhibits several attractive properties such as faster run timeand convergence, ability to better handle long sequences and, more importantly,it helps to avoid some of the major difficulties posed by training VAE modelson textual data.", "categories": ["cs.CL"], "journal": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "132", "arxiv_url": "http://arxiv.org/abs/1702.02390v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=22686147435504388&btnI=1&nossl=1&hl=en&oe=ASCII"}, "243": {"ID": 243, "title": "SimVerb-3500: A Large-Scale Evaluation Set of Verb Similarity", "authors": ["Ivan Vuli\u0107", "Felix Hill", "Roi Reichart", "Daniela Gerz", "Anna Korhonen"], "published": "2016-08-02T15:35:12Z", "updated": "2016-09-20T14:35:14Z", "abstract": "Verbs play a critical role in the meaning of sentences, but these ubiquitouswords have received little attention in recent distributional semanticsresearch. We introduce SimVerb-3500, an evaluation resource that provides humanratings for the similarity of 3,500 verb pairs. SimVerb-3500 covers all normedverb types from the USF free-association database, providing at least threeexamples for every VerbNet class. This broad coverage facilitates detailedanalyses of how syntactic and semantic phenomena together influence humanunderstanding of verb meaning. Further, with significantly larger developmentand test sets than existing benchmarks, SimVerb-3500 enables more robustevaluation of representation learning architectures and promotes thedevelopment of methods tailored to verbs. We hope that SimVerb-3500 will enablea richer understanding of the diversity and complexity of verb semantics andguide the development of systems that can effectively represent and interpretthis meaning.", "categories": ["cs.CL"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "141", "arxiv_url": "http://arxiv.org/abs/1608.00869v4", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4986650878938590900&btnI=1&nossl=1&hl=en&oe=ASCII"}, "244": {"ID": 244, "title": "Semi-supervised sequence tagging with bidirectional language models", "authors": ["Russell Power", "Waleed Ammar", "Chandra Bhagavatula", "Matthew E. Peters"], "published": "2017-04-29T01:13:04Z", "updated": "2017-04-29T01:13:04Z", "abstract": "Pre-trained word embeddings learned from unlabeled text have become astandard component of neural network architectures for NLP tasks. However, inmost cases, the recurrent network that operates on word-level representationsto produce context sensitive representations is trained on relatively littlelabeled data. In this paper, we demonstrate a general semi-supervised approachfor adding pre- trained context embeddings from bidirectional language modelsto NLP systems and apply it to sequence labeling tasks. We evaluate our modelon two standard datasets for named entity recognition (NER) and chunking, andin both cases achieve state of the art results, surpassing previous systemsthat use other forms of transfer or joint learning with additional labeled dataand task specific gazetteers.", "categories": ["cs.CL"], "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "304", "arxiv_url": "http://arxiv.org/abs/1705.00108v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=1748888201451410574&btnI=1&nossl=1&hl=en&oe=ASCII"}, "245": {"ID": 245, "title": "Deep contextualized word representations", "authors": ["Kenton Lee", "Matt Gardner", "Mark Neumann", "Christopher Clark", "Mohit Iyyer", "Luke Zettlemoyer", "Matthew E. Peters"], "published": "2018-02-15T00:05:11Z", "updated": "2018-03-22T21:59:40Z", "abstract": "We introduce a new type of deep contextualized word representation thatmodels both (1) complex characteristics of word use (e.g., syntax andsemantics), and (2) how these uses vary across linguistic contexts (i.e., tomodel polysemy). Our word vectors are learned functions of the internal statesof a deep bidirectional language model (biLM), which is pre-trained on a largetext corpus. We show that these representations can be easily added to existingmodels and significantly improve the state of the art across six challengingNLP problems, including question answering, textual entailment and sentimentanalysis. We also present an analysis showing that exposing the deep internalsof the pre-trained network is crucial, allowing downstream models to mixdifferent types of semi-supervision signals.", "categories": ["cs.CL"], "journal": "NAACL-HLT, 2227-2237", "citations": "3820", "arxiv_url": "http://arxiv.org/abs/1802.05365v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14181983828043963745&btnI=1&nossl=1&hl=en&oe=ASCII"}, "246": {"ID": 246, "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through  Inference", "authors": ["Nikita Nangia", "Adina Williams", "Samuel R. Bowman"], "published": "2017-04-18T17:10:13Z", "updated": "2018-02-19T19:19:51Z", "abstract": "This paper introduces the Multi-Genre Natural Language Inference (MultiNLI)corpus, a dataset designed for use in the development and evaluation of machinelearning models for sentence understanding. In addition to being one of thelargest corpora available for the task of NLI, at 433k examples, this corpusimproves upon available resources in its coverage: it offers data from tendistinct genres of written and spoken English--making it possible to evaluatesystems on nearly the full complexity of the language--and it offers anexplicit setting for the evaluation of cross-genre domain adaptation.", "categories": ["cs.CL"], "journal": "NAACL-HLT, 1112-1122", "citations": "639", "arxiv_url": "http://arxiv.org/abs/1704.05426v4", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4521178278304992412&btnI=1&nossl=1&hl=en&oe=ASCII"}, "247": {"ID": 247, "title": "Latent Variable Dialogue Models and their Diversity", "authors": ["Kris Cao", "Stephen Clark"], "published": "2017-02-20T13:36:23Z", "updated": "2017-02-20T13:36:23Z", "abstract": "We present a dialogue generation model that directly captures the variabilityin possible responses to a given input, which reduces the `boring output' issueof deterministic dialogue models. Experiments show that our model generatesmore diverse outputs than baseline models, and also generates more consistentlyacceptable output than sampling from a deterministic encoder-decoder model.", "categories": ["cs.CL"], "journal": "EACL (2), 182-187", "citations": "51", "arxiv_url": "http://arxiv.org/abs/1702.05962v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8016969718736858778&btnI=1&nossl=1&hl=en&oe=ASCII"}, "248": {"ID": 248, "title": "From Paraphrase Database to Compositional Paraphrase Model and Back", "authors": ["Mohit Bansal", "Dan Roth", "Karen Livescu", "John Wieting", "Kevin Gimpel"], "published": "2015-06-10T21:29:28Z", "updated": "2015-08-26T21:18:00Z", "abstract": "The Paraphrase Database (PPDB; Ganitkevitch et al., 2013) is an extensivesemantic resource, consisting of a list of phrase pairs with (heuristic)confidence estimates. However, it is still unclear how it can best be used, dueto the heuristic nature of the confidences and its necessarily incompletecoverage. We propose models to leverage the phrase pairs from the PPDB to buildparametric paraphrase models that score paraphrase pairs more accurately thanthe PPDB's internal scores while simultaneously improving its coverage. Theyallow for learning phrase embeddings as well as improved word embeddings.Moreover, we introduce two new, manually annotated datasets to evaluateshort-phrase paraphrasing models. Using our paraphrase model trained usingPPDB, we achieve state-of-the-art results on standard word and bigramsimilarity tasks and beat strong baselines on our new short phrase paraphrasetasks.", "categories": ["cs.CL"], "journal": "Transactions of the Association for Computational Linguistics 3, 345-358", "citations": "211", "arxiv_url": "http://arxiv.org/abs/1506.03487v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=15463675364142986201&btnI=1&nossl=1&hl=en&oe=ASCII"}, "249": {"ID": 249, "title": "F-Score Driven Max Margin Neural Network for Named Entity Recognition in  Chinese Social Media", "authors": ["Hangfeng He", "Xu Sun"], "published": "2016-11-14T02:50:33Z", "updated": "2017-04-11T10:57:34Z", "abstract": "We focus on named entity recognition (NER) for Chinese social media. Withmassive unlabeled text and quite limited labelled corpus, we propose asemi-supervised learning model based on B-LSTM neural network. To takeadvantage of traditional methods in NER such as CRF, we combine transitionprobability with deep learning in our model. To bridge the gap between labelaccuracy and F-score of NER, we construct a model which can be directly trainedon F-score. When considering the instability of F-score driven method andmeaningful information provided by label accuracy, we propose an integratedmethod to train on both F-score and label accuracy. Our integrated model yields7.44\\% improvement over previous state-of-the-art result.", "categories": ["cs.CL"], "journal": "EACL (2), 713-718", "citations": "47", "arxiv_url": "http://arxiv.org/abs/1611.04234v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=5877339938447980746&btnI=1&nossl=1&hl=en&oe=ASCII"}, "250": {"ID": 250, "title": "Minimum Risk Training for Neural Machine Translation", "authors": ["Maosong Sun", "Shiqi Shen", "Yong Cheng", "Zhongjun He", "Hua Wu", "Wei He", "Yang Liu"], "published": "2015-12-08T12:42:00Z", "updated": "2016-06-15T00:07:05Z", "abstract": "We propose minimum risk training for end-to-end neural machine translation.Unlike conventional maximum likelihood estimation, minimum risk training iscapable of optimizing model parameters directly with respect to arbitraryevaluation metrics, which are not necessarily differentiable. Experiments showthat our approach achieves significant improvements over maximum likelihoodestimation on a state-of-the-art neural machine translation system acrossvarious languages pairs. Transparent to architectures, our approach can beapplied to more neural networks and potentially benefit more NLP tasks.", "categories": ["cs.CL"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "292", "arxiv_url": "http://arxiv.org/abs/1512.02433v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=13568140432319924245&btnI=1&nossl=1&hl=en&oe=ASCII"}, "251": {"ID": 251, "title": "JFLEG: A Fluency Corpus and Benchmark for Grammatical Error Correction", "authors": ["Courtney Napoles", "Joel Tetreault", "Keisuke Sakaguchi"], "published": "2017-02-14T03:47:34Z", "updated": "2017-02-14T03:47:34Z", "abstract": "We present a new parallel corpus, JHU FLuency-Extended GUG corpus (JFLEG) fordeveloping and evaluating grammatical error correction (GEC). Unlike othercorpora, it represents a broad range of language proficiency levels and usesholistic fluency edits to not only correct grammatical errors but also make theoriginal text more native sounding. We describe the types of corrections madeand benchmark four leading GEC systems on this corpus, identifying specificareas in which they do well and how they can improve. JFLEG fulfills the needfor a new gold standard to properly assess the current state of GEC.", "categories": ["cs.CL"], "journal": "EACL (2), 229-234", "citations": "61", "arxiv_url": "http://arxiv.org/abs/1702.04066v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14012499899893589917&btnI=1&nossl=1&hl=en&oe=ASCII"}, "252": {"ID": 252, "title": "Adversarial Multi-task Learning for Text Classification", "authors": ["Xipeng Qiu", "Pengfei Liu", "Xuanjing Huang"], "published": "2017-04-19T14:17:25Z", "updated": "2017-04-19T14:17:25Z", "abstract": "Neural network models have shown their promising opportunities for multi-tasklearning, which focus on learning the shared layers to extract the common andtask-invariant features. However, in most existing approaches, the extractedshared features are prone to be contaminated by task-specific features or thenoise brought by other tasks. In this paper, we propose an adversarialmulti-task learning framework, alleviating the shared and private latentfeature spaces from interfering with each other. We conduct extensiveexperiments on 16 different text classification tasks, which demonstrates thebenefits of our approach. Besides, we show that the shared knowledge learned byour proposed model can be regarded as off-the-shelf knowledge and easilytransferred to new tasks. The datasets of all 16 tasks are publicly availableat \\url{http://nlp.fudan.edu.cn/data/}", "categories": ["cs.CL"], "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "276", "arxiv_url": "http://arxiv.org/abs/1704.05742v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6202310749502426295&btnI=1&nossl=1&hl=en&oe=ASCII"}, "253": {"ID": 253, "title": "Multi-Way, Multilingual Neural Machine Translation with a Shared  Attention Mechanism", "authors": ["Orhan Firat", "Yoshua Bengio", "Kyunghyun Cho"], "published": "2016-01-06T04:00:50Z", "updated": "2016-01-06T04:00:50Z", "abstract": "We propose multi-way, multilingual neural machine translation. The proposedapproach enables a single neural translation model to translate betweenmultiple languages, with a number of parameters that grows only linearly withthe number of languages. This is made possible by having a single attentionmechanism that is shared across all language pairs. We train the proposedmulti-way, multilingual model on ten language pairs from WMT'15 simultaneouslyand observe clear performance improvements over models trained on only onelanguage pair. In particular, we observe that the proposed model significantlyimproves the translation quality of low-resource language pairs.", "categories": ["cs.CL", "stat.ML"], "journal": "HLT-NAACL, 866-875", "citations": "313", "arxiv_url": "http://arxiv.org/abs/1601.01073v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=1297298716616390295&btnI=1&nossl=1&hl=en&oe=ASCII"}, "254": {"ID": 254, "title": "Learning to Remember Translation History with a Continuous Cache", "authors": ["Yang Liu", "Zhaopeng Tu", "Tong Zhang", "Shuming Shi"], "published": "2017-11-26T10:44:55Z", "updated": "2017-11-26T10:44:55Z", "abstract": "Existing neural machine translation (NMT) models generally translatesentences in isolation, missing the opportunity to take advantage ofdocument-level information. In this work, we propose to augment NMT models witha very light-weight cache-like memory network, which stores recent hiddenrepresentations as translation history. The probability distribution overgenerated words is updated online depending on the translation historyretrieved from the memory, endowing NMT models with the capability todynamically adapt over time. Experiments on multiple domains with differenttopics and styles show the effectiveness of the proposed approach withnegligible impact on the computational cost.", "categories": ["cs.CL"], "journal": "Transactions of the Association for Computational Linguistics 6, 407-420", "citations": "62", "arxiv_url": "http://arxiv.org/abs/1711.09367v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4417021458385953863&btnI=1&nossl=1&hl=en&oe=ASCII"}, "255": {"ID": 255, "title": "Do latent tree learning models identify meaningful structure in  sentences?", "authors": ["Adina Williams", "Andrew Drozdov", "Samuel R. Bowman"], "published": "2017-09-04T19:05:39Z", "updated": "2018-02-26T15:59:38Z", "abstract": "Recent work on the problem of latent tree learning has made it possible totrain neural networks that learn to both parse a sentence and use the resultingparse to interpret the sentence, all without exposure to ground-truth parsetrees at training time. Surprisingly, these models often perform better atsentence understanding tasks than models that use parse trees from conventionalparsers. This paper aims to investigate what these latent tree learning modelslearn. We replicate two such models in a shared codebase and find that (i) onlyone of these models outperforms conventional tree-structured models on sentenceclassification, (ii) its parsing strategies are not especially consistentacross random restarts, (iii) the parses it produces tend to be shallower thanstandard Penn Treebank (PTB) parses, and (iv) they do not resemble those of PTBor any other semantic or syntactic formalism that the authors are aware of.", "categories": ["cs.CL"], "journal": "Transactions of the Association for Computational Linguistics 6, 253-267", "citations": "67", "arxiv_url": "http://arxiv.org/abs/1709.01121v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=12549377047222009803&btnI=1&nossl=1&hl=en&oe=ASCII"}, "256": {"ID": 256, "title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of  Unsupervised Evaluation Metrics for Dialogue Response Generation", "authors": ["Ryan Lowe", "Laurent Charlin", "Michael Noseworthy", "Iulian V. Serban", "Joelle Pineau", "Chia-Wei Liu"], "published": "2016-03-25T20:32:21Z", "updated": "2017-01-03T18:28:32Z", "abstract": "We investigate evaluation metrics for dialogue response generation systemswhere supervised labels, such as task completion, are not available. Recentworks in response generation have adopted metrics from machine translation tocompare a model's generated response to a single target response. We show thatthese metrics correlate very weakly with human judgements in the non-technicalTwitter domain, and not at all in the technical Ubuntu domain. We providequantitative and qualitative results highlighting specific weaknesses inexisting metrics, and provide recommendations for future development of betterautomatic evaluation metrics for dialogue systems.", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "606", "arxiv_url": "http://arxiv.org/abs/1603.08023v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9769435644141907733&btnI=1&nossl=1&hl=en&oe=ASCII"}, "257": {"ID": 257, "title": "Neural versus Phrase-Based Machine Translation Quality: a Case Study", "authors": ["Mauro Cettolo", "Luisa Bentivogli", "Marcello Federico", "Arianna Bisazza"], "published": "2016-08-16T15:04:18Z", "updated": "2016-10-09T09:20:08Z", "abstract": "Within the field of Statistical Machine Translation (SMT), the neuralapproach (NMT) has recently emerged as the first technology able to challengethe long-standing dominance of phrase-based approaches (PBMT). In particular,at the IWSLT 2015 evaluation campaign, NMT outperformed well establishedstate-of-the-art PBMT systems on English-German, a language pair known to beparticularly hard because of morphology and syntactic differences. Tounderstand in what respects NMT provides better translation quality than PBMT,we perform a detailed analysis of neural versus phrase-based SMT outputs,leveraging high quality post-edits performed by professional translators on theIWSLT data. For the first time, our analysis provides useful insights on whatlinguistic phenomena are best modeled by neural models -- such as thereordering of verbs -- while pointing out other aspects that remain to beimproved.", "categories": ["cs.CL"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "234", "arxiv_url": "http://arxiv.org/abs/1608.04631v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8474999711586923966&btnI=1&nossl=1&hl=en&oe=ASCII"}, "258": {"ID": 258, "title": "AttSum: Joint Learning of Focusing and Summarization with Neural  Attention", "authors": ["Furu Wei", "Ziqiang Cao", "Wenjie Li", "Sujian Li", "Yanran Li"], "published": "2016-04-01T04:18:39Z", "updated": "2016-09-27T02:22:33Z", "abstract": "Query relevance ranking and sentence saliency ranking are the two main tasksin extractive query-focused summarization. Previous supervised summarizationsystems often perform the two tasks in isolation. However, since referencesummaries are the trade-off between relevance and saliency, using them assupervision, neither of the two rankers could be trained well. This paperproposes a novel summarization system called AttSum, which tackles the twotasks jointly. It automatically learns distributed representations forsentences as well as the document cluster. Meanwhile, it applies the attentionmechanism to simulate the attentive reading of human behavior when a query isgiven. Extensive experiments are conducted on DUC query-focused summarizationbenchmark datasets. Without using any hand-crafted features, AttSum achievescompetitive performance. It is also observed that the sentences recognized tofocus on the query indeed meet the query need.", "categories": ["cs.IR", "cs.CL"], "journal": "COLING, 547-556", "citations": "77", "arxiv_url": "http://arxiv.org/abs/1604.00125v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4866352286283723842&btnI=1&nossl=1&hl=en&oe=ASCII"}, "259": {"ID": 259, "title": "A Network-based End-to-End Trainable Task-oriented Dialogue System", "authors": ["David Vandyke", "Tsung-Hsien Wen", "Stefan Ultes", "Lina M. Rojas-Barahona", "Steve Young", "Pei-Hao Su", "Milica Gasic", "Nikola Mrksic"], "published": "2016-04-15T16:40:49Z", "updated": "2017-04-24T10:55:12Z", "abstract": "Teaching machines to accomplish tasks by conversing naturally with humans ischallenging. Currently, developing task-oriented dialogue systems requirescreating multiple components and typically this involves either a large amountof handcrafting, or acquiring costly labelled datasets to solve a statisticallearning problem for each component. In this work we introduce a neuralnetwork-based text-in, text-out end-to-end trainable goal-oriented dialoguesystem along with a new way of collecting dialogue data based on a novelpipe-lined Wizard-of-Oz framework. This approach allows us to develop dialoguesystems easily and without making too many assumptions about the task at hand.The results show that the model can converse with human subjects naturallywhilst helping them to accomplish tasks in a restaurant search domain.", "categories": ["cs.CL", "cs.AI", "cs.NE", "stat.ML"], "journal": "EACL (1), 438-449", "citations": "533", "arxiv_url": "http://arxiv.org/abs/1604.04562v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=5172833917634651324&btnI=1&nossl=1&hl=en&oe=ASCII"}, "260": {"ID": 260, "title": "SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense  Inference", "authors": ["Yonatan Bisk", "Rowan Zellers", "Roy Schwartz", "Yejin Choi"], "published": "2018-08-16T02:21:01Z", "updated": "2018-08-16T02:21:01Z", "abstract": "Given a partial description like \"she opened the hood of the car,\" humans canreason about the situation and anticipate what might come next (\"then, sheexamined the engine\"). In this paper, we introduce the task of groundedcommonsense inference, unifying natural language inference and commonsensereasoning.  We present SWAG, a new dataset with 113k multiple choice questions about arich spectrum of grounded situations. To address the recurring challenges ofthe annotation artifacts and human biases found in many existing datasets, wepropose Adversarial Filtering (AF), a novel procedure that constructs ade-biased dataset by iteratively training an ensemble of stylistic classifiers,and using them to filter the data. To account for the aggressive adversarialfiltering, we use state-of-the-art language models to massively oversample adiverse set of potential counterfactuals. Empirical results demonstrate thatwhile humans can solve the resulting inference problems with high accuracy(88%), various competitive models struggle on our task. We providecomprehensive analysis that indicates significant opportunities for futureresearch.", "categories": ["cs.CL"], "journal": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "166", "arxiv_url": "http://arxiv.org/abs/1808.05326v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=5467106153402339872&btnI=1&nossl=1&hl=en&oe=ASCII"}, "261": {"ID": 261, "title": "XNLI: Evaluating Cross-lingual Sentence Representations", "authors": ["Guillaume Lample", "Ruty Rinott", "Adina Williams", "Samuel R. Bowman", "Holger Schwenk", "Veselin Stoyanov", "Alexis Conneau"], "published": "2018-09-13T16:39:53Z", "updated": "2018-09-13T16:39:53Z", "abstract": "State-of-the-art natural language processing systems rely on supervision inthe form of annotated data to learn competent models. These models aregenerally trained on data in a single language (usually English), and cannot bedirectly used beyond that language. Since collecting data in every language isnot realistic, there has been a growing interest in cross-lingual languageunderstanding (XLU) and low-resource cross-language transfer. In this work, weconstruct an evaluation set for XLU by extending the development and test setsof the Multi-Genre Natural Language Inference Corpus (MultiNLI) to 15languages, including low-resource languages such as Swahili and Urdu. We hopethat our dataset, dubbed XNLI, will catalyze research in cross-lingual sentenceunderstanding by providing an informative standard evaluation task. Inaddition, we provide several baselines for multilingual sentence understanding,including two based on machine translation systems, and two that use paralleldata to train aligned multilingual bag-of-words and LSTM encoders. We find thatXNLI represents a practical and challenging evaluation suite, and that directlytranslating the test data yields the best performance among availablebaselines.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "journal": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "159", "arxiv_url": "http://arxiv.org/abs/1809.05053v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=15041461338388299895&btnI=1&nossl=1&hl=en&oe=ASCII"}, "262": {"ID": 262, "title": "Newsroom: A Dataset of 1.3 Million Summaries with Diverse Extractive  Strategies", "authors": ["Mor Naaman", "Max Grusky", "Yoav Artzi"], "published": "2018-04-30T15:53:05Z", "updated": "2020-05-17T19:10:09Z", "abstract": "We present NEWSROOM, a summarization dataset of 1.3 million articles andsummaries written by authors and editors in newsrooms of 38 major newspublications. Extracted from search and social media metadata between 1998 and2017, these high-quality summaries demonstrate high diversity of summarizationstyles. In particular, the summaries combine abstractive and extractivestrategies, borrowing words and phrases from articles at varying rates. Weanalyze the extraction strategies used in NEWSROOM summaries against otherdatasets to quantify the diversity and difficulty of our new data, and trainexisting methods on the data to evaluate its utility and challenges.", "categories": ["cs.CL"], "journal": "NAACL-HLT, 708-719", "citations": "91", "arxiv_url": "http://arxiv.org/abs/1804.11283v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8707860707234422251&btnI=1&nossl=1&hl=en&oe=ASCII"}, "263": {"ID": 263, "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text", "authors": ["Konstantin Lopyrev", "Pranav Rajpurkar", "Jian Zhang", "Percy Liang"], "published": "2016-06-16T16:36:00Z", "updated": "2016-10-11T02:42:36Z", "abstract": "We present the Stanford Question Answering Dataset (SQuAD), a new readingcomprehension dataset consisting of 100,000+ questions posed by crowdworkers ona set of Wikipedia articles, where the answer to each question is a segment oftext from the corresponding reading passage. We analyze the dataset tounderstand the types of reasoning required to answer the questions, leaningheavily on dependency and constituency trees. We build a strong logisticregression model, which achieves an F1 score of 51.0%, a significantimprovement over a simple baseline (20%). However, human performance (86.8%) ismuch higher, indicating that the dataset presents a good challenge problem forfuture research.  The dataset is freely available at https://stanford-qa.com", "categories": ["cs.CL"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "1874", "arxiv_url": "http://arxiv.org/abs/1606.05250v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=11037830585227643418&btnI=1&nossl=1&hl=en&oe=ASCII"}, "264": {"ID": 264, "title": "A Copy-Augmented Sequence-to-Sequence Architecture Gives Good  Performance on Task-Oriented Dialogue", "authors": ["Christopher D. Manning", "Mihail Eric"], "published": "2017-01-15T10:38:17Z", "updated": "2017-08-14T22:18:38Z", "abstract": "Task-oriented dialogue focuses on conversational agents that participate inuser-initiated dialogues on domain-specific topics. In contrast to chatbots,which simply seek to sustain open-ended meaningful discourse, existingtask-oriented agents usually explicitly model user intent and belief states.This paper examines bypassing such an explicit representation by depending on alatent neural embedding of state and learning selective attention to dialoguehistory together with copying to incorporate relevant prior context. Wecomplement recent work by showing the effectiveness of simplesequence-to-sequence neural architectures with a copy mechanism. Our modeloutperforms more complex memory-augmented models by 7% in per-responsegeneration and is on par with the current state-of-the-art on DSTC2.", "categories": ["cs.CL", "cs.AI"], "journal": "EACL (2), 468-473", "citations": "89", "arxiv_url": "http://arxiv.org/abs/1701.04024v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10888975453259817923&btnI=1&nossl=1&hl=en&oe=ASCII"}, "265": {"ID": 265, "title": "ABCNN: Attention-Based Convolutional Neural Network for Modeling  Sentence Pairs", "authors": ["Bowen Zhou", "Hinrich Sch\u00fctze", "Bing Xiang", "Wenpeng Yin"], "published": "2015-12-16T14:55:17Z", "updated": "2018-06-25T13:31:07Z", "abstract": "How to model a pair of sentences is a critical issue in many NLP tasks suchas answer selection (AS), paraphrase identification (PI) and textual entailment(TE). Most prior work (i) deals with one individual task by fine-tuning aspecific system; (ii) models each sentence's representation separately, rarelyconsidering the impact of the other sentence; or (iii) relies fully on manuallydesigned, task-specific linguistic features. This work presents a generalAttention Based Convolutional Neural Network (ABCNN) for modeling a pair ofsentences. We make three contributions. (i) ABCNN can be applied to a widevariety of tasks that require modeling of sentence pairs. (ii) We propose threeattention schemes that integrate mutual influence between sentences into CNN;thus, the representation of each sentence takes into consideration itscounterpart. These interdependent sentence pair representations are morepowerful than isolated sentence representations. (iii) ABCNN achievesstate-of-the-art performance on AS, PI and TE tasks.", "categories": ["cs.CL"], "journal": "Transactions of the Association for Computational Linguistics 4, 259-272", "citations": "564", "arxiv_url": "http://arxiv.org/abs/1512.05193v4", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=577630522001776015&btnI=1&nossl=1&hl=en&oe=ASCII"}, "266": {"ID": 266, "title": "Universal Language Model Fine-tuning for Text Classification", "authors": ["Jeremy Howard", "Sebastian Ruder"], "published": "2018-01-18T17:54:52Z", "updated": "2018-05-23T09:23:47Z", "abstract": "Inductive transfer learning has greatly impacted computer vision, butexisting approaches in NLP still require task-specific modifications andtraining from scratch. We propose Universal Language Model Fine-tuning(ULMFiT), an effective transfer learning method that can be applied to any taskin NLP, and introduce techniques that are key for fine-tuning a language model.Our method significantly outperforms the state-of-the-art on six textclassification tasks, reducing the error by 18-24% on the majority of datasets.Furthermore, with only 100 labeled examples, it matches the performance oftraining from scratch on 100x more data. We open-source our pretrained modelsand code.", "categories": ["cs.CL", "cs.LG", "stat.ML"], "journal": "Proceedings of the 56th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "998", "arxiv_url": "http://arxiv.org/abs/1801.06146v5", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=2986760879834934707&btnI=1&nossl=1&hl=en&oe=ASCII"}, "267": {"ID": 267, "title": "Do Multi-Sense Embeddings Improve Natural Language Understanding?", "authors": ["Jiwei Li", "Dan Jurafsky"], "published": "2015-06-02T21:30:21Z", "updated": "2015-11-24T18:29:40Z", "abstract": "Learning a distinct representation for each sense of an ambiguous word couldlead to more powerful and fine-grained models of vector-space representations.Yet while `multi-sense' methods have been proposed and tested on artificialword-similarity tasks, we don't know if they improve real natural languageunderstanding tasks. In this paper we introduce a multi-sense embedding modelbased on Chinese Restaurant Processes that achieves state of the artperformance on matching human word similarity judgments, and propose apipelined architecture for incorporating multi-sense embeddings into languageunderstanding.  We then test the performance of our model on part-of-speech tagging, namedentity recognition, sentiment analysis, semantic relation identification andsemantic relatedness, controlling for embedding dimensionality. We find thatmulti-sense embeddings do improve performance on some tasks (part-of-speechtagging, semantic relation identification, semantic relatedness) but not onothers (named entity recognition, various forms of sentiment analysis). Wediscuss how these differences may be caused by the different role of word senseinformation in each of the tasks. The results highlight the importance oftesting embedding models in real applications.", "categories": ["cs.CL"], "journal": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "194", "arxiv_url": "http://arxiv.org/abs/1506.01070v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14140267304199748634&btnI=1&nossl=1&hl=en&oe=ASCII"}, "268": {"ID": 268, "title": "Visualizing and Understanding Neural Models in NLP", "authors": ["Eduard Hovy", "Jiwei Li", "Xinlei Chen", "Dan Jurafsky"], "published": "2015-06-02T21:17:31Z", "updated": "2016-01-08T18:10:22Z", "abstract": "While neural networks have been successfully applied to many NLP tasks theresulting vector-based models are very difficult to interpret. For example it'snot clear how they achieve {\\em compositionality}, building sentence meaningfrom the meanings of words and phrases. In this paper we describe fourstrategies for visualizing compositionality in neural models for NLP, inspiredby similar work in computer vision. We first plot unit values to visualizecompositionality of negation, intensification, and concessive clauses, allow usto see well-known markedness asymmetries in negation. We then introduce threesimple and straightforward methods for visualizing a unit's {\\em salience}, theamount it contributes to the final composed meaning: (1) gradientback-propagation, (2) the variance of a token from the average word node, (3)LSTM-style gates that measure information flow. We test our methods onsentiment using simple recurrent nets and LSTMs. Our general-purpose methodsmay have wide applications for understanding compositionality and othersemantic properties of deep networks , and also shed light on why LSTMsoutperform simple recurrent nets,", "categories": ["cs.CL"], "journal": "HLT-NAACL, 681-691", "citations": "356", "arxiv_url": "http://arxiv.org/abs/1506.01066v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=310028312991444814&btnI=1&nossl=1&hl=en&oe=ASCII"}, "269": {"ID": 269, "title": "Human Attention in Visual Question Answering: Do Humans and Deep  Networks Look at the Same Regions?", "authors": ["Harsh Agrawal", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh", "Abhishek Das"], "published": "2016-06-17T17:00:02Z", "updated": "2016-06-17T17:00:02Z", "abstract": "We conduct large-scale studies on `human attention' in Visual QuestionAnswering (VQA) to understand where humans choose to look to answer questionsabout images. We design and test multiple game-inspired novelattention-annotation interfaces that require the subject to sharpen regions ofa blurred image to answer a question. Thus, we introduce the VQA-HAT (HumanATtention) dataset. We evaluate attention maps generated by state-of-the-artVQA models against human attention both qualitatively (via visualizations) andquantitatively (via rank-order correlation). Overall, our experiments show thatcurrent attention models in VQA do not seem to be looking at the same regionsas humans.", "categories": ["stat.ML", "cs.CV"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "222", "arxiv_url": "http://arxiv.org/abs/1606.05589v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14473824043461236815&btnI=1&nossl=1&hl=en&oe=ASCII"}, "270": {"ID": 270, "title": "Reading Wikipedia to Answer Open-Domain Questions", "authors": ["Danqi Chen", "Jason Weston", "Antoine Bordes", "Adam Fisch"], "published": "2017-03-31T20:39:10Z", "updated": "2017-04-28T03:53:14Z", "abstract": "This paper proposes to tackle open- domain question answering using Wikipediaas the unique knowledge source: the answer to any factoid question is a textspan in a Wikipedia article. This task of machine reading at scale combines thechallenges of document retrieval (finding the relevant articles) with that ofmachine comprehension of text (identifying the answer spans from thosearticles). Our approach combines a search component based on bigram hashing andTF-IDF matching with a multi-layer recurrent neural network model trained todetect answers in Wikipedia paragraphs. Our experiments on multiple existing QAdatasets indicate that (1) both modules are highly competitive with respect toexisting counterparts and (2) multitask learning using distant supervision ontheir combination is an effective complete system on this challenging task.", "categories": ["cs.CL"], "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "626", "arxiv_url": "http://arxiv.org/abs/1704.00051v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=11430863989102948348&btnI=1&nossl=1&hl=en&oe=ASCII"}, "271": {"ID": 271, "title": "Style Transfer Through Back-Translation", "authors": ["Alan W Black", "Ruslan Salakhutdinov", "Shrimai Prabhumoye", "Yulia Tsvetkov"], "published": "2018-04-24T12:58:45Z", "updated": "2018-05-24T17:12:43Z", "abstract": "Style transfer is the task of rephrasing the text to contain specificstylistic properties without changing the intent or affect within the context.This paper introduces a new method for automatic style transfer. We first learna latent representation of the input sentence which is grounded in a languagetranslation model in order to better preserve the meaning of the sentence whilereducing stylistic properties. Then adversarial generation techniques are usedto make the output match the desired style. We evaluate this technique on threedifferent style transformations: sentiment, gender and political slant.Compared to two state-of-the-art style transfer modeling techniques we showimprovements both in automatic evaluation of style transfer and in manualevaluation of meaning preservation and fluency.", "categories": ["cs.CL"], "journal": "Proceedings of the 56th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "154", "arxiv_url": "http://arxiv.org/abs/1804.09000v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9688974258985905206&btnI=1&nossl=1&hl=en&oe=ASCII"}, "272": {"ID": 272, "title": "Graph Convolution over Pruned Dependency Trees Improves Relation  Extraction", "authors": ["Christopher D. Manning", "Peng Qi", "Yuhao Zhang"], "published": "2018-09-26T18:49:07Z", "updated": "2018-09-26T18:49:07Z", "abstract": "Dependency trees help relation extraction models capture long-range relationsbetween words. However, existing dependency-based models either neglect crucialinformation (e.g., negation) by pruning the dependency trees too aggressively,or are computationally inefficient because it is difficult to parallelize overdifferent tree structures. We propose an extension of graph convolutionalnetworks that is tailored for relation extraction, which pools information overarbitrary dependency structures efficiently in parallel. To incorporaterelevant information while maximally removing irrelevant content, we furtherapply a novel pruning strategy to the input trees by keeping words immediatelyaround the shortest path between the two entities among which a relation mighthold. The resulting model achieves state-of-the-art performance on thelarge-scale TACRED dataset, outperforming existing sequence anddependency-based neural models. We also show through detailed analysis thatthis model has complementary strengths to sequence models, and combining themfurther improves the state of the art.", "categories": ["cs.CL"], "journal": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "152", "arxiv_url": "http://arxiv.org/abs/1809.10185v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14475807300726215311&btnI=1&nossl=1&hl=en&oe=ASCII"}, "273": {"ID": 273, "title": "Constructing Datasets for Multi-hop Reading Comprehension Across  Documents", "authors": ["Sebastian Riedel", "Johannes Welbl", "Pontus Stenetorp"], "published": "2017-10-17T19:35:07Z", "updated": "2018-06-11T17:08:20Z", "abstract": "Most Reading Comprehension methods limit themselves to queries which can beanswered using a single sentence, paragraph, or document. Enabling models tocombine disjoint pieces of textual evidence would extend the scope of machinecomprehension methods, but currently there exist no resources to train and testthis capability. We propose a novel task to encourage the development of modelsfor text understanding across multiple documents and to investigate the limitsof existing methods. In our task, a model learns to seek and combine evidence -effectively performing multi-hop (alias multi-step) inference. We devise amethodology to produce datasets for this task, given a collection ofquery-answer pairs and thematically linked documents. Two datasets fromdifferent domains are induced, and we identify potential pitfalls and devisecircumvention strategies. We evaluate two previously proposed competitivemodels and find that one can integrate information across documents. However,both models struggle to select relevant information, as providing documentsguaranteed to be relevant greatly improves their performance. While the modelsoutperform several strong baselines, their best accuracy reaches 42.9% comparedto human performance at 74.0% - leaving ample room for improvement.", "categories": ["cs.CL", "cs.AI"], "journal": "Transactions of the Association for Computational Linguistics 6, 287-302", "citations": "157", "arxiv_url": "http://arxiv.org/abs/1710.06481v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=12333326429053188403&btnI=1&nossl=1&hl=en&oe=ASCII"}, "274": {"ID": 274, "title": "Globally Normalized Transition-Based Neural Networks", "authors": ["Daniel Andor", "Kuzman Ganchev", "Michael Collins", "Aliaksei Severyn", "David Weiss", "Slav Petrov", "Chris Alberti", "Alessandro Presta"], "published": "2016-03-19T03:56:03Z", "updated": "2016-06-08T13:43:30Z", "abstract": "We introduce a globally normalized transition-based neural network model thatachieves state-of-the-art part-of-speech tagging, dependency parsing andsentence compression results. Our model is a simple feed-forward neural networkthat operates on a task-specific transition system, yet achieves comparable orbetter accuracies than recurrent models. We discuss the importance of global asopposed to local normalization: a key insight is that the label bias problemimplies that globally normalized models can be strictly more expressive thanlocally normalized models.", "categories": ["cs.CL", "cs.LG", "cs.NE"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "495", "arxiv_url": "http://arxiv.org/abs/1603.06042v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10973467127305341223&btnI=1&nossl=1&hl=en&oe=ASCII"}, "275": {"ID": 275, "title": "Structured Training for Neural Network Transition-Based Parsing", "authors": ["Slav Petrov", "Michael Collins", "David Weiss", "Chris Alberti"], "published": "2015-06-19T21:05:01Z", "updated": "2015-06-19T21:05:01Z", "abstract": "We present structured perceptron training for neural network transition-baseddependency parsing. We learn the neural network representation using a goldcorpus augmented by a large number of automatically parsed sentences. Giventhis fixed network representation, we learn a final layer using the structuredperceptron with beam-search decoding. On the Penn Treebank, our parser reaches94.26% unlabeled and 92.41% labeled attachment accuracy, which to our knowledgeis the best accuracy on Stanford Dependencies to date. We also provide in-depthablative analysis to determine which aspects of our model provide the largestgains in accuracy.", "categories": ["cs.CL"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "219", "arxiv_url": "http://arxiv.org/abs/1506.06158v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=7890929338984410638&btnI=1&nossl=1&hl=en&oe=ASCII"}, "276": {"ID": 276, "title": "When and Why are Pre-trained Word Embeddings Useful for Neural Machine  Translation?", "authors": ["Graham Neubig", "Ye Qi", "Matthieu Felix", "Sarguna Janani Padmanabhan", "Devendra Singh Sachan"], "published": "2018-04-17T15:34:07Z", "updated": "2018-04-18T07:03:57Z", "abstract": "The performance of Neural Machine Translation (NMT) systems often suffers inlow-resource scenarios where sufficiently large-scale parallel corpora cannotbe obtained. Pre-trained word embeddings have proven to be invaluable forimproving performance in natural language analysis tasks, which often sufferfrom paucity of data. However, their utility for NMT has not been extensivelyexplored. In this work, we perform five sets of experiments that analyze whenwe can expect pre-trained word embeddings to help in NMT tasks. We show thatsuch embeddings can be surprisingly effective in some cases -- providing gainsof up to 20 BLEU points in the most favorable setting.", "categories": ["cs.CL"], "journal": "NAACL-HLT (2), 529-535", "citations": "93", "arxiv_url": "http://arxiv.org/abs/1804.06323v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6166308028416584239&btnI=1&nossl=1&hl=en&oe=ASCII"}, "277": {"ID": 277, "title": "Colorless green recurrent networks dream hierarchically", "authors": ["Edouard Grave", "Piotr Bojanowski", "Kristina Gulordava", "Tal Linzen", "Marco Baroni"], "published": "2018-03-29T16:27:36Z", "updated": "2018-03-29T16:27:36Z", "abstract": "Recurrent neural networks (RNNs) have achieved impressive results in avariety of linguistic processing tasks, suggesting that they can inducenon-trivial properties of language. We investigate here to what extent RNNslearn to track abstract hierarchical syntactic structure. We test whether RNNstrained with a generic language modeling objective in four languages (Italian,English, Hebrew, Russian) can predict long-distance number agreement in variousconstructions. We include in our evaluation nonsensical sentences where RNNscannot rely on semantic or lexical cues (\"The colorless green ideas I ate withthe chair sleep furiously\"), and, for Italian, we compare model performance tohuman intuitions. Our language-model-trained RNNs make reliable predictionsabout long-distance agreement, and do not lag much behind human performance. Wethus bring support to the hypothesis that RNNs are not just shallow-patternextractors, but they also acquire deeper grammatical competence.", "categories": ["cs.CL"], "journal": "NAACL-HLT, 1195-1205", "citations": "190", "arxiv_url": "http://arxiv.org/abs/1803.11138v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=1004519771441063838&btnI=1&nossl=1&hl=en&oe=ASCII"}, "278": {"ID": 278, "title": "Cross-Sentence N-ary Relation Extraction with Graph LSTMs", "authors": ["Wen-tau Yih", "Nanyun Peng", "Chris Quirk", "Kristina Toutanova", "Hoifung Poon"], "published": "2017-08-12T04:33:52Z", "updated": "2017-08-12T04:33:52Z", "abstract": "Past work in relation extraction has focused on binary relations in singlesentences. Recent NLP inroads in high-value domains have sparked interest inthe more general setting of extracting n-ary relations that span multiplesentences. In this paper, we explore a general relation extraction frameworkbased on graph long short-term memory networks (graph LSTMs) that can be easilyextended to cross-sentence n-ary relation extraction. The graph formulationprovides a unified way of exploring different LSTM approaches and incorporatingvarious intra-sentential and inter-sentential dependencies, such as sequential,syntactic, and discourse relations. A robust contextual representation islearned for the entities, which serves as input to the relation classifier.This simplifies handling of relations with arbitrary arity, and enablesmulti-task learning with related relations. We evaluate this framework in twoimportant precision medicine settings, demonstrating its effectiveness withboth conventional supervised learning and distant supervision. Cross-sentenceextraction produced larger knowledge bases. and multi-task learningsignificantly improved extraction accuracy. A thorough analysis of various LSTMapproaches yielded useful insight the impact of linguistic analysis onextraction accuracy.", "categories": ["cs.CL"], "journal": "Transactions of the Association for Computational Linguistics 5, 101-115", "citations": "180", "arxiv_url": "http://arxiv.org/abs/1708.03743v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=7996820128579384769&btnI=1&nossl=1&hl=en&oe=ASCII"}, "279": {"ID": 279, "title": "What Do Recurrent Neural Network Grammars Learn About Syntax?", "authors": ["Lingpeng Kong", "Graham Neubig", "Noah A. Smith", "Chris Dyer", "Miguel Ballesteros", "Adhiguna Kuncoro"], "published": "2016-11-17T16:41:41Z", "updated": "2017-01-10T19:15:08Z", "abstract": "Recurrent neural network grammars (RNNG) are a recently proposedprobabilistic generative modeling family for natural language. They showstate-of-the-art language modeling and parsing performance. We investigate whatinformation they learn, from a linguistic perspective, through variousablations to the model and the data, and by augmenting the model with anattention mechanism (GA-RNNG) to enable closer inspection. We find thatexplicit modeling of composition is crucial for achieving the best performance.Through the attention mechanism, we find that headedness plays a central rolein phrasal representation (with the model's latent attention largely agreeingwith predictions made by hand-crafted head rules, albeit with some importantdifferences). By training grammars without nonterminal labels, we find thatphrasal representations depend minimally on nonterminals, providing support forthe endocentricity hypothesis.", "categories": ["cs.CL"], "journal": "EACL (1), 1249-1258", "citations": "91", "arxiv_url": "http://arxiv.org/abs/1611.05774v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10539779986608577431&btnI=1&nossl=1&hl=en&oe=ASCII"}, "280": {"ID": 280, "title": "Pointing the Unknown Words", "authors": ["Ramesh Nallapati", "Sungjin Ahn", "Bowen Zhou", "Caglar Gulcehre", "Yoshua Bengio"], "published": "2016-03-26T22:31:57Z", "updated": "2016-08-21T20:03:39Z", "abstract": "The problem of rare and unknown words is an important issue that canpotentially influence the performance of many NLP systems, including both thetraditional count-based and the deep learning models. We propose a novel way todeal with the rare and unseen words for the neural network models usingattention. Our model uses two softmax layers in order to predict the next wordin conditional language models: one predicts the location of a word in thesource sentence, and the other predicts a word in the shortlist vocabulary. Ateach time-step, the decision of which softmax layer to use choose adaptivelymade by an MLP which is conditioned on the context.~We motivate our work from apsychological evidence that humans naturally have a tendency to point towardsobjects in the context or the environment when the name of an object is notknown.~We observe improvements on two tasks, neural machine translation on theEuroparl English to French parallel corpora and text summarization on theGigaword dataset using our proposed model.", "categories": ["cs.CL", "cs.LG", "cs.NE"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "359", "arxiv_url": "http://arxiv.org/abs/1603.08148v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8939912433163056499&btnI=1&nossl=1&hl=en&oe=ASCII"}, "281": {"ID": 281, "title": "Supervised Learning of Universal Sentence Representations from Natural  Language Inference Data", "authors": ["Loic Barrault", "Douwe Kiela", "Antoine Bordes", "Alexis Conneau", "Holger Schwenk"], "published": "2017-05-05T18:54:39Z", "updated": "2018-07-08T21:22:11Z", "abstract": "Many modern NLP systems rely on word embeddings, previously trained in anunsupervised manner on large corpora, as base features. Efforts to obtainembeddings for larger chunks of text, such as sentences, have however not beenso successful. Several attempts at learning unsupervised representations ofsentences have not reached satisfactory enough performance to be widelyadopted. In this paper, we show how universal sentence representations trainedusing the supervised data of the Stanford Natural Language Inference datasetscan consistently outperform unsupervised methods like SkipThought vectors on awide range of transfer tasks. Much like how computer vision uses ImageNet toobtain features, which can then be transferred to other tasks, our work tendsto indicate the suitability of natural language inference for transfer learningto other NLP tasks. Our encoder is publicly available.", "categories": ["cs.CL"], "journal": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "939", "arxiv_url": "http://arxiv.org/abs/1705.02364v5", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=7032922865683861275&btnI=1&nossl=1&hl=en&oe=ASCII"}, "282": {"ID": 282, "title": "Rationalizing Neural Predictions", "authors": ["Tommi Jaakkola", "Regina Barzilay", "Tao Lei"], "published": "2016-06-13T22:10:23Z", "updated": "2016-11-02T20:26:20Z", "abstract": "Prediction without justification has limited applicability. As a remedy, welearn to extract pieces of input text as justifications -- rationales -- thatare tailored to be short and coherent, yet sufficient for making the sameprediction. Our approach combines two modular components, generator andencoder, which are trained to operate well together. The generator specifies adistribution over text fragments as candidate rationales and these are passedthrough the encoder for prediction. Rationales are never given during training.Instead, the model is regularized by desiderata for rationales. We evaluate theapproach on multi-aspect sentiment analysis against manually annotated testcases. Our approach outperforms attention-based baseline by a significantmargin. We also successfully illustrate the method on the question retrievaltask.", "categories": ["cs.CL", "cs.NE"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "338", "arxiv_url": "http://arxiv.org/abs/1606.04155v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4433915093406114937&btnI=1&nossl=1&hl=en&oe=ASCII"}, "283": {"ID": 283, "title": "Natural Language Inference by Tree-Based Convolution and Heuristic  Matching", "authors": ["Lili Mou", "Ge Li", "Rui Yan", "Rui Men", "Zhi Jin", "Yan Xu", "Lu Zhang"], "published": "2015-12-28T14:28:21Z", "updated": "2016-05-13T16:24:56Z", "abstract": "In this paper, we propose the TBCNN-pair model to recognize entailment andcontradiction between two sentences. In our model, a tree-based convolutionalneural network (TBCNN) captures sentence-level semantics; then heuristicmatching layers like concatenation, element-wise product/difference combine theinformation in individual sentences. Experimental results show that our modeloutperforms existing sentence encoding-based approaches by a large margin.", "categories": ["cs.CL", "cs.LG"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "211", "arxiv_url": "http://arxiv.org/abs/1512.08422v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=125294644878662580&btnI=1&nossl=1&hl=en&oe=ASCII"}, "284": {"ID": 284, "title": "Hierarchical Neural Story Generation", "authors": ["Angela Fan", "Yann Dauphin", "Mike Lewis"], "published": "2018-05-13T07:07:08Z", "updated": "2018-05-13T07:07:08Z", "abstract": "We explore story generation: creative systems that can build coherent andfluent passages of text about a topic. We collect a large dataset of 300Khuman-written stories paired with writing prompts from an online forum. Ourdataset enables hierarchical story generation, where the model first generatesa premise, and then transforms it into a passage of text. We gain furtherimprovements with a novel form of model fusion that improves the relevance ofthe story to the prompt, and adding a new gated multi-scale self-attentionmechanism to model long-range context. Experiments show large improvements overstrong baselines on both automated and human evaluations. Human judges preferstories generated by our approach to those from a strong non-hierarchical modelby a factor of two to one.", "categories": ["cs.CL"], "journal": "Proceedings of the 56th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "200", "arxiv_url": "http://arxiv.org/abs/1805.04833v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6764929710004991388&btnI=1&nossl=1&hl=en&oe=ASCII"}, "285": {"ID": 285, "title": "A Syntactic Neural Model for General-Purpose Code Generation", "authors": ["Graham Neubig", "Pengcheng Yin"], "published": "2017-04-06T03:13:46Z", "updated": "2017-04-06T03:13:46Z", "abstract": "We consider the problem of parsing natural language descriptions into sourcecode written in a general-purpose programming language like Python. Existingdata-driven methods treat this problem as a language generation task withoutconsidering the underlying syntax of the target programming language. Informedby previous work in semantic parsing, in this paper we propose a novel neuralarchitecture powered by a grammar model to explicitly capture the target syntaxas prior knowledge. Experiments find this an effective way to scale up togeneration of complex programs from natural language descriptions, achievingstate-of-the-art results that well outperform previous code generation andsemantic parsing approaches.", "categories": ["cs.CL", "cs.PL", "cs.SE"], "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "217", "arxiv_url": "http://arxiv.org/abs/1704.01696v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=2092433724770544936&btnI=1&nossl=1&hl=en&oe=ASCII"}, "286": {"ID": 286, "title": "Neural Machine Translation with Supervised Attention", "authors": ["Masao Utiyama", "Andrew Finch", "Lemao Liu", "Eiichiro Sumita"], "published": "2016-09-14T09:31:40Z", "updated": "2016-09-14T09:31:40Z", "abstract": "The attention mechanisim is appealing for neural machine translation, sinceit is able to dynam- ically encode a source sentence by generating a alignmentbetween a target word and source words. Unfortunately, it has been proved to beworse than conventional alignment models in aligment accuracy. In this paper,we analyze and explain this issue from the point view of re- ordering, andpropose a supervised attention which is learned with guidance from conventionalalignment models. Experiments on two Chinese-to-English translation tasks showthat the super- vised attention mechanism yields better alignments leading tosubstantial gains over the standard attention based NMT.", "categories": ["cs.CL"], "journal": "COLING, 3093-3102", "citations": "81", "arxiv_url": "http://arxiv.org/abs/1609.04186v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14159649791483644220&btnI=1&nossl=1&hl=en&oe=ASCII"}, "287": {"ID": 287, "title": "Semantically Conditioned LSTM-based Natural Language Generation for  Spoken Dialogue Systems", "authors": ["David Vandyke", "Tsung-Hsien Wen", "Steve Young", "Pei-Hao Su", "Milica Gasic", "Nikola Mrksic"], "published": "2015-08-07T16:16:44Z", "updated": "2015-08-26T17:16:25Z", "abstract": "Natural language generation (NLG) is a critical component of spoken dialogueand it has a significant impact both on usability and perceived quality. MostNLG systems in common use employ rules and heuristics and tend to generaterigid and stylised responses without the natural variation of human language.They are also not easily scaled to systems covering multiple domains andlanguages. This paper presents a statistical language generator based on asemantically controlled Long Short-term Memory (LSTM) structure. The LSTMgenerator can learn from unaligned data by jointly optimising sentence planningand surface realisation using a simple cross entropy training criterion, andlanguage variation can be easily achieved by sampling from output candidates.With fewer heuristics, an objective evaluation in two differing test domainsshowed the proposed method improved performance compared to previous methods.Human judges scored the LSTM system higher on informativeness and naturalnessand overall preferred it to the other systems.", "categories": ["cs.CL"], "journal": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "583", "arxiv_url": "http://arxiv.org/abs/1508.01745v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=12898523265884814660&btnI=1&nossl=1&hl=en&oe=ASCII"}, "288": {"ID": 288, "title": "Text Classification Improved by Integrating Bidirectional LSTM with  Two-dimensional Max Pooling", "authors": ["Bo Xu", "Suncong Zheng", "Zhenyu Qi", "Hongyun Bao", "Jiaming Xu", "Peng Zhou"], "published": "2016-11-21T03:26:29Z", "updated": "2016-11-21T03:26:29Z", "abstract": "Recurrent Neural Network (RNN) is one of the most popular architectures usedin Natural Language Processsing (NLP) tasks because its recurrent structure isvery suitable to process variable-length text. RNN can utilize distributedrepresentations of words by first converting the tokens comprising each textinto vectors, which form a matrix. And this matrix includes two dimensions: thetime-step dimension and the feature vector dimension. Then most existing modelsusually utilize one-dimensional (1D) max pooling operation or attention-basedoperation only on the time-step dimension to obtain a fixed-length vector.However, the features on the feature vector dimension are not mutuallyindependent, and simply applying 1D pooling operation over the time-stepdimension independently may destroy the structure of the featurerepresentation. On the other hand, applying two-dimensional (2D) poolingoperation over the two dimensions may sample more meaningful features forsequence modeling tasks. To integrate the features on both dimensions of thematrix, this paper explores applying 2D max pooling operation to obtain afixed-length representation of the text. This paper also utilizes 2Dconvolution to sample more meaningful information of the matrix. Experimentsare conducted on six text classification tasks, including sentiment analysis,question classification, subjectivity classification and newsgroupclassification. Compared with the state-of-the-art models, the proposed modelsachieve excellent performance on 4 out of 6 tasks. Specifically, one of theproposed models achieves highest accuracy on Stanford Sentiment Treebank binaryclassification and fine-grained classification tasks.", "categories": ["cs.CL"], "journal": "COLING, 3485-3495", "citations": "219", "arxiv_url": "http://arxiv.org/abs/1611.06639v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10822944950744933208&btnI=1&nossl=1&hl=en&oe=ASCII"}, "289": {"ID": 289, "title": "Modelling Context with User Embeddings for Sarcasm Detection in Social  Media", "authors": ["Silvio Amir", "Paula Carvalho M\u00e1rio J. Silva", "Hao Lyu", "Byron C. Wallace"], "published": "2016-07-04T18:04:18Z", "updated": "2016-07-05T02:27:41Z", "abstract": "We introduce a deep neural network for automated sarcasm detection. Recentwork has emphasized the need for models to capitalize on contextual features,beyond lexical and syntactic cues present in utterances. For example, differentspeakers will tend to employ sarcasm regarding different subjects and, thus,sarcasm detection models ought to encode such speaker information. Currentmethods have achieved this by way of laborious feature engineering. Bycontrast, we propose to automatically learn and then exploit user embeddings,to be used in concert with lexical signals to recognize sarcasm. Our approachdoes not require elaborate feature engineering (and concomitant data scraping);fitting user embeddings requires only the text from their previous posts. Theexperimental results show that our model outperforms a state-of-the-artapproach leveraging an extensive set of carefully crafted features.", "categories": ["cs.CL", "cs.AI"], "journal": "CoNLL, 167-177", "citations": "117", "arxiv_url": "http://arxiv.org/abs/1607.00976v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=13147435607300322087&btnI=1&nossl=1&hl=en&oe=ASCII"}, "290": {"ID": 290, "title": "Traversing Knowledge Graphs in Vector Space", "authors": ["John Miller", "Kelvin Guu", "Percy Liang"], "published": "2015-06-03T00:38:25Z", "updated": "2015-08-19T05:16:24Z", "abstract": "Path queries on a knowledge graph can be used to answer compositionalquestions such as \"What languages are spoken by people living in Lisbon?\".However, knowledge graphs often have missing facts (edges) which disrupts pathqueries. Recent models for knowledge base completion impute missing facts byembedding knowledge graphs in vector spaces. We show that these models can berecursively applied to answer path queries, but that they suffer from cascadingerrors. This motivates a new \"compositional\" training objective, whichdramatically improves all models' ability to answer path queries, in some casesmore than doubling accuracy. On a standard knowledge base completion task, wealso demonstrate that compositional training acts as a novel form of structuralregularization, reliably improving performance across all base models (reducingerrors by up to 43%) and achieving new state-of-the-art results.", "categories": ["cs.CL", "cs.AI", "cs.DB", "stat.ML"], "journal": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "224", "arxiv_url": "http://arxiv.org/abs/1506.01094v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=2340872828944672390&btnI=1&nossl=1&hl=en&oe=ASCII"}, "291": {"ID": 291, "title": "Recurrent Dropout without Memory Loss", "authors": ["Aliaksei Severyn", "Stanislau Semeniuta", "Erhardt Barth"], "published": "2016-03-16T14:33:47Z", "updated": "2016-08-05T09:59:25Z", "abstract": "This paper presents a novel approach to recurrent neural network (RNN)regularization. Differently from the widely adopted dropout method, which isapplied to \\textit{forward} connections of feed-forward architectures or RNNs,we propose to drop neurons directly in \\textit{recurrent} connections in a waythat does not cause loss of long-term memory. Our approach is as easy toimplement and apply as the regular feed-forward dropout and we demonstrate itseffectiveness for Long Short-Term Memory network, the most popular type of RNNcells. Our experiments on NLP benchmarks show consistent improvements even whencombined with conventional feed-forward dropout.", "categories": ["cs.CL"], "journal": "COLING, 1757-1766", "citations": "147", "arxiv_url": "http://arxiv.org/abs/1603.05118v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=16998373299318301099&btnI=1&nossl=1&hl=en&oe=ASCII"}, "292": {"ID": 292, "title": "Learning to Translate in Real-time with Neural Machine Translation", "authors": ["Jiatao Gu", "Victor O. K. Li", "Graham Neubig", "Kyunghyun Cho"], "published": "2016-10-03T02:11:03Z", "updated": "2017-01-10T21:07:56Z", "abstract": "Translating in real-time, a.k.a. simultaneous translation, outputstranslation words before the input sentence ends, which is a challengingproblem for conventional machine translation methods. We propose a neuralmachine translation (NMT) framework for simultaneous translation in which anagent learns to make decisions on when to translate from the interaction with apre-trained NMT environment. To trade off quality and delay, we extensivelyexplore various targets for delay and design a method for beam-searchapplicable in the simultaneous MT setting. Experiments against state-of-the-artbaselines on two language pairs demonstrate the efficacy of the proposedframework both quantitatively and qualitatively.", "categories": ["cs.CL", "cs.LG"], "journal": "EACL (1), 1053-1062", "citations": "60", "arxiv_url": "http://arxiv.org/abs/1610.00388v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14299891671990230013&btnI=1&nossl=1&hl=en&oe=ASCII"}, "293": {"ID": 293, "title": "All-in-one: Multi-task Learning for Rumour Verification", "authors": ["Elena Kochkina", "Maria Liakata", "Arkaitz Zubiaga"], "published": "2018-06-10T19:46:17Z", "updated": "2018-06-10T19:46:17Z", "abstract": "Automatic resolution of rumours is a challenging task that can be broken downinto smaller components that make up a pipeline, including rumour detection,rumour tracking and stance classification, leading to the final outcome ofdetermining the veracity of a rumour. In previous work, these steps in theprocess of rumour verification have been developed as separate components wherethe output of one feeds into the next. We propose a multi-task learningapproach that allows joint training of the main and auxiliary tasks, improvingthe performance of rumour verification. We examine the connection between thedataset properties and the outcomes of the multi-task learning models used.", "categories": ["cs.CL"], "journal": "COLING, 3402-3413", "citations": "50", "arxiv_url": "http://arxiv.org/abs/1806.03713v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=5345112942964425407&btnI=1&nossl=1&hl=en&oe=ASCII"}, "294": {"ID": 294, "title": "Cross-lingual Models of Word Embeddings: An Empirical Comparison", "authors": ["Chris Dyer", "Dan Roth", "Manaal Faruqui", "Shyam Upadhyay"], "published": "2016-04-01T22:18:51Z", "updated": "2016-06-08T03:14:08Z", "abstract": "Despite interest in using cross-lingual knowledge to learn word embeddingsfor various tasks, a systematic comparison of the possible approaches islacking in the literature. We perform an extensive evaluation of four popularapproaches of inducing cross-lingual embeddings, each requiring a differentform of supervision, on four typographically different language pairs. Ourevaluation setup spans four different tasks, including intrinsic evaluation onmono-lingual and cross-lingual similarity, and extrinsic evaluation ondownstream semantic and syntactic applications. We show that models whichrequire expensive cross-lingual knowledge almost always perform better, butcheaply supervised models often prove competitive on certain tasks.", "categories": ["cs.CL"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "151", "arxiv_url": "http://arxiv.org/abs/1604.00425v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9561056950331565667&btnI=1&nossl=1&hl=en&oe=ASCII"}, "295": {"ID": 295, "title": "\"Liar, Liar Pants on Fire\": A New Benchmark Dataset for Fake News  Detection", "authors": ["William Yang Wang"], "published": "2017-05-01T18:20:47Z", "updated": "2017-05-01T18:20:47Z", "abstract": "Automatic fake news detection is a challenging problem in deceptiondetection, and it has tremendous real-world political and social impacts.However, statistical approaches to combating fake news has been dramaticallylimited by the lack of labeled benchmark datasets. In this paper, we presentliar: a new, publicly available dataset for fake news detection. We collected adecade-long, 12.8K manually labeled short statements in various contexts fromPolitiFact.com, which provides detailed analysis report and links to sourcedocuments for each case. This dataset can be used for fact-checking research aswell. Notably, this new dataset is an order of magnitude larger than previouslylargest public fake news datasets of similar type. Empirically, we investigateautomatic fake news detection based on surface-level linguistic patterns. Wehave designed a novel, hybrid convolutional neural network to integratemeta-data with text. We show that this hybrid approach can improve a text-onlydeep learning model.", "categories": ["cs.CL", "cs.CY"], "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "370", "arxiv_url": "http://arxiv.org/abs/1705.00648v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14489560950621704885&btnI=1&nossl=1&hl=en&oe=ASCII"}, "296": {"ID": 296, "title": "Neural Architectures for Fine-grained Entity Type Classification", "authors": ["Sebastian Riedel", "Sonse Shimaoka", "Pontus Stenetorp", "Kentaro Inui"], "published": "2016-06-04T07:52:22Z", "updated": "2017-02-21T06:49:42Z", "abstract": "In this work, we investigate several neural network architectures forfine-grained entity type classification. Particularly, we consider extensionsto a recently proposed attentive neural architecture and make three keycontributions. Previous work on attentive neural architectures do not considerhand-crafted features, we combine learnt and hand-crafted features and observethat they complement each other. Additionally, through quantitative analysis weestablish that the attention mechanism is capable of learning to attend oversyntactic heads and the phrase containing the mention, where both are knownstrong hand-crafted features for our task. We enable parameter sharing througha hierarchical label encoding method, that in low-dimensional projections showclear clusters for each type hierarchy. Lastly, despite using the sameevaluation dataset, the literature frequently compare models trained usingdifferent data. We establish that the choice of training data has a drasticimpact on performance, with decreases by as much as 9.85% loose micro F1 scorefor a previously proposed method. Despite this, our best model achievesstate-of-the-art results with 75.36% loose micro F1 score on the well-established FIGER (GOLD) dataset.", "categories": ["cs.CL"], "journal": "EACL (1), 1271-1280", "citations": "68", "arxiv_url": "http://arxiv.org/abs/1606.01341v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=1554341579692960830&btnI=1&nossl=1&hl=en&oe=ASCII"}, "297": {"ID": 297, "title": "Energy and Policy Considerations for Deep Learning in NLP", "authors": ["Andrew McCallum", "Ananya Ganesh", "Emma Strubell"], "published": "2019-06-05T18:40:53Z", "updated": "2019-06-05T18:40:53Z", "abstract": "Recent progress in hardware and methodology for training neural networks hasushered in a new generation of large networks trained on abundant data. Thesemodels have obtained notable gains in accuracy across many NLP tasks. However,these accuracy improvements depend on the availability of exceptionally largecomputational resources that necessitate similarly substantial energyconsumption. As a result these models are costly to train and develop, bothfinancially, due to the cost of hardware and electricity or cloud compute time,and environmentally, due to the carbon footprint required to fuel modern tensorprocessing hardware. In this paper we bring this issue to the attention of NLPresearchers by quantifying the approximate financial and environmental costs oftraining a variety of recently successful neural network models for NLP. Basedon these findings, we propose actionable recommendations to reduce costs andimprove equity in NLP research and practice.", "categories": ["cs.CL"], "journal": "Proceedings of the 57th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "218", "arxiv_url": "http://arxiv.org/abs/1906.02243v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=1758095683412713863&btnI=1&nossl=1&hl=en&oe=ASCII"}, "298": {"ID": 298, "title": "Sparse Communication for Distributed Gradient Descent", "authors": ["Alham Fikri Aji", "Kenneth Heafield"], "published": "2017-04-17T16:32:02Z", "updated": "2017-07-24T21:47:51Z", "abstract": "We make distributed stochastic gradient descent faster by exchanging sparseupdates instead of dense updates. Gradient updates are positively skewed asmost updates are near zero, so we map the 99% smallest updates (by absolutevalue) to zero then exchange sparse matrices. This method can be combined withquantization to further improve the compression. We explore differentconfigurations and apply them to neural machine translation and MNIST imageclassification tasks. Most configurations work on MNIST, whereas differentconfigurations reduce convergence rate on the more complex translation task.Our experiments show that we can achieve up to 49% speed up on MNIST and 22% onNMT without damaging the final accuracy or BLEU.", "categories": ["cs.CL", "cs.DC", "cs.LG"], "journal": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "166", "arxiv_url": "http://arxiv.org/abs/1704.05021v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=11261881325311048998&btnI=1&nossl=1&hl=en&oe=ASCII"}, "299": {"ID": 299, "title": "Neural Semantic Encoders", "authors": ["Hong Yu", "Tsendsuren Munkhdalai"], "published": "2016-07-14T20:58:26Z", "updated": "2017-01-05T15:41:13Z", "abstract": "We present a memory augmented neural network for natural languageunderstanding: Neural Semantic Encoders. NSE is equipped with a novel memoryupdate rule and has a variable sized encoding memory that evolves over time andmaintains the understanding of input sequences through read}, compose and writeoperations. NSE can also access multiple and shared memories. In this paper, wedemonstrated the effectiveness and the flexibility of NSE on five differentnatural language tasks: natural language inference, question answering,sentence classification, document sentiment analysis and machine translationwhere NSE achieved state-of-the-art performance when evaluated on publicallyavailable benchmarks. For example, our shared-memory model showed anencouraging result on neural machine translation, improving an attention-basedbaseline by approximately 1.0 BLEU.", "categories": ["cs.LG", "cs.CL", "stat.ML"], "journal": "EACL (1), 397-407", "citations": "105", "arxiv_url": "http://arxiv.org/abs/1607.04315v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6549774526077427636&btnI=1&nossl=1&hl=en&oe=ASCII"}, "300": {"ID": 300, "title": "Combining Language and Vision with a Multimodal Skip-gram Model", "authors": ["Nghia The Pham", "Marco Baroni", "Angeliki Lazaridou"], "published": "2015-01-12T10:48:32Z", "updated": "2015-03-12T09:47:33Z", "abstract": "We extend the SKIP-GRAM model of Mikolov et al. (2013a) by taking visualinformation into account. Like SKIP-GRAM, our multimodal models (MMSKIP-GRAM)build vector-based word representations by learning to predict linguisticcontexts in text corpora. However, for a restricted set of words, the modelsare also exposed to visual representations of the objects they denote(extracted from natural images), and must predict linguistic and visualfeatures jointly. The MMSKIP-GRAM models achieve good performance on a varietyof semantic benchmarks. Moreover, since they propagate visual information toall words, we use them to improve image labeling and retrieval in the zero-shotsetup, where the test concepts are never seen during model training. Finally,the MMSKIP-GRAM models discover intriguing visual properties of abstract words,paving the way to realistic implementations of embodied theories of meaning.", "categories": ["cs.CL", "cs.CV", "cs.LG"], "journal": "HLT-NAACL, 153-163", "citations": "193", "arxiv_url": "http://arxiv.org/abs/1501.02598v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=810317863434591306&btnI=1&nossl=1&hl=en&oe=ASCII"}, "301": {"ID": 301, "title": "Aspect Level Sentiment Classification with Deep Memory Network", "authors": ["Bing Qin", "Ting Liu", "Duyu Tang"], "published": "2016-05-28T14:47:49Z", "updated": "2016-09-24T06:04:15Z", "abstract": "We introduce a deep memory network for aspect level sentiment classification.Unlike feature-based SVM and sequential neural models such as LSTM, thisapproach explicitly captures the importance of each context word when inferringthe sentiment polarity of an aspect. Such importance degree and textrepresentation are calculated with multiple computational layers, each of whichis a neural attention model over an external memory. Experiments on laptop andrestaurant datasets demonstrate that our approach performs comparable tostate-of-art feature based SVM system, and substantially better than LSTM andattention-based LSTM architectures. On both datasets we show that multiplecomputational layers could improve the performance. Moreover, our approach isalso fast. The deep memory network with 9 layers is 15 times faster than LSTMwith a CPU implementation.", "categories": ["cs.CL"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "361", "arxiv_url": "http://arxiv.org/abs/1605.08900v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=7883668420001560230&btnI=1&nossl=1&hl=en&oe=ASCII"}, "302": {"ID": 302, "title": "Neural Responding Machine for Short-Text Conversation", "authors": ["Hang Li", "Zhengdong Lu", "Lifeng Shang"], "published": "2015-03-09T02:54:29Z", "updated": "2015-04-27T02:28:58Z", "abstract": "We propose Neural Responding Machine (NRM), a neural network-based responsegenerator for Short-Text Conversation. NRM takes the general encoder-decoderframework: it formalizes the generation of response as a decoding process basedon the latent representation of the input text, while both encoding anddecoding are realized with recurrent neural networks (RNN). The NRM is trainedwith a large amount of one-round conversation data collected from amicroblogging service. Empirical study shows that NRM can generategrammatically correct and content-wise appropriate responses to over 75% of theinput text, outperforming state-of-the-arts in the same setting, includingretrieval-based and SMT-based models.", "categories": ["cs.CL", "cs.AI", "cs.NE"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "753", "arxiv_url": "http://arxiv.org/abs/1503.02364v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=5059354282047966578&btnI=1&nossl=1&hl=en&oe=ASCII"}, "303": {"ID": 303, "title": "Sequence-Level Knowledge Distillation", "authors": ["Alexander M. Rush", "Yoon Kim"], "published": "2016-06-25T18:16:39Z", "updated": "2016-09-22T01:17:12Z", "abstract": "Neural machine translation (NMT) offers a novel alternative formulation oftranslation that is potentially simpler than statistical approaches. However toreach competitive performance, NMT models need to be exceedingly large. In thispaper we consider applying knowledge distillation approaches (Bucila et al.,2006; Hinton et al., 2015) that have proven successful for reducing the size ofneural models in other domains to the problem of NMT. We demonstrate thatstandard knowledge distillation applied to word-level prediction can beeffective for NMT, and also introduce two novel sequence-level versions ofknowledge distillation that further improve performance, and somewhatsurprisingly, seem to eliminate the need for beam search (even when applied onthe original teacher model). Our best student model runs 10 times faster thanits state-of-the-art teacher with little loss in performance. It is alsosignificantly better than a baseline model trained without knowledgedistillation: by 4.2/1.7 BLEU with greedy decoding/beam search. Applying weightpruning on top of knowledge distillation results in a student model that has 13times fewer parameters than the original teacher model, with a decrease of 0.4BLEU.", "categories": ["cs.CL", "cs.LG", "cs.NE"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "267", "arxiv_url": "http://arxiv.org/abs/1606.07947v4", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=7576178314368830775&btnI=1&nossl=1&hl=en&oe=ASCII"}, "304": {"ID": 304, "title": "A Multifaceted Evaluation of Neural versus Phrase-Based Machine  Translation for 9 Language Directions", "authors": ["V\u00edctor M. S\u00e1nchez-Cartagena", "Antonio Toral"], "published": "2017-01-11T09:32:47Z", "updated": "2017-01-11T09:32:47Z", "abstract": "We aim to shed light on the strengths and weaknesses of the newly introducedneural machine translation paradigm. To that end, we conduct a multifacetedevaluation in which we compare outputs produced by state-of-the-art neuralmachine translation and phrase-based machine translation systems for 9 languagedirections across a number of dimensions. Specifically, we measure thesimilarity of the outputs, their fluency and amount of reordering, the effectof sentence length and performance across different error categories. We findout that translations produced by neural machine translation systems areconsiderably different, more fluent and more accurate in terms of word ordercompared to those produced by phrase-based systems. Neural machine translationsystems are also more accurate at producing inflected forms, but they performpoorly when translating very long sentences.", "categories": ["cs.CL"], "journal": "EACL (1), 1063-1073", "citations": "92", "arxiv_url": "http://arxiv.org/abs/1701.02901v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6969425796814323485&btnI=1&nossl=1&hl=en&oe=ASCII"}, "305": {"ID": 305, "title": "OpenNMT: Open-source Toolkit for Neural Machine Translation", "authors": ["Jean Senellart", "Alexander M. Rush", "Yuntian Deng", "Yoon Kim", "Josep Crego", "Guillaume Klein"], "published": "2017-09-12T12:58:07Z", "updated": "2017-09-12T12:58:07Z", "abstract": "We introduce an open-source toolkit for neural machine translation (NMT) tosupport research into model architectures, feature representations, and sourcemodalities, while maintaining competitive performance, modularity andreasonable training requirements.", "categories": ["cs.CL"], "journal": "ACL (System Demonstrations), 67-72", "citations": "899", "arxiv_url": "http://arxiv.org/abs/1709.03815v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6651054115351140376&btnI=1&nossl=1&hl=en&oe=ASCII"}, "306": {"ID": 306, "title": "Question Answering on Freebase via Relation Extraction and Textual  Evidence", "authors": ["Siva Reddy", "Kun Xu", "Songfang Huang", "Dongyan Zhao", "Yansong Feng"], "published": "2016-03-03T03:22:01Z", "updated": "2016-06-09T15:12:19Z", "abstract": "Existing knowledge-based question answering systems often rely on smallannotated training data. While shallow methods like relation extraction arerobust to data scarcity, they are less expressive than the deep meaningrepresentation methods like semantic parsing, thereby failing at answeringquestions involving multiple constraints. Here we alleviate this problem byempowering a relation extraction method with additional evidence fromWikipedia. We first present a neural network based relation extractor toretrieve the candidate answers from Freebase, and then infer over Wikipedia tovalidate these answers. Experiments on the WebQuestions question answeringdataset show that our method achieves an F_1 of 53.3%, a substantialimprovement over the state-of-the-art.", "categories": ["cs.CL"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "151", "arxiv_url": "http://arxiv.org/abs/1603.00957v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=17139515614838673727&btnI=1&nossl=1&hl=en&oe=ASCII"}, "307": {"ID": 307, "title": "The Best of Both Worlds: Combining Recent Advances in Neural Machine  Translation", "authors": ["Ankur Bapna", "Melvin Johnson", "Zhifeng Chen", "Yonghui Wu", "Macduff Hughes", "Orhan Firat", "Mike Schuster", "Niki Parmar", "Mia Xu Chen", "Llion Jones", "George Foster", "Wolfgang Macherey"], "published": "2018-04-26T01:24:39Z", "updated": "2018-04-27T02:31:16Z", "abstract": "The past year has witnessed rapid advances in sequence-to-sequence (seq2seq)modeling for Machine Translation (MT). The classic RNN-based approaches to MTwere first out-performed by the convolutional seq2seq model, which was thenout-performed by the more recent Transformer model. Each of these newapproaches consists of a fundamental architecture accompanied by a set ofmodeling and training techniques that are in principle applicable to otherseq2seq architectures. In this paper, we tease apart the new architectures andtheir accompanying techniques in two ways. First, we identify several keymodeling and training techniques, and apply them to the RNN architecture,yielding a new RNMT+ model that outperforms all of the three fundamentalarchitectures on the benchmark WMT'14 English to French and English to Germantasks. Second, we analyze the properties of each fundamental seq2seqarchitecture and devise new hybrid architectures intended to combine theirstrengths. Our hybrid models obtain further improvements, outperforming theRNMT+ model on both benchmark datasets.", "categories": ["cs.CL", "cs.AI"], "journal": "Proceedings of the 56th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "198", "arxiv_url": "http://arxiv.org/abs/1804.09849v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=1960239321427735403&btnI=1&nossl=1&hl=en&oe=ASCII"}, "308": {"ID": 308, "title": "Key-Value Memory Networks for Directly Reading Documents", "authors": ["Amir-Hossein Karimi", "Antoine Bordes", "Adam Fisch", "Alexander Miller", "Jason Weston", "Jesse Dodge"], "published": "2016-06-09T21:33:55Z", "updated": "2016-10-10T20:14:10Z", "abstract": "Directly reading documents and being able to answer questions from them is anunsolved challenge. To avoid its inherent difficulty, question answering (QA)has been directed towards using Knowledge Bases (KBs) instead, which has proveneffective. Unfortunately KBs often suffer from being too restrictive, as theschema cannot support certain types of answers, and too sparse, e.g. Wikipediacontains much more information than Freebase. In this work we introduce a newmethod, Key-Value Memory Networks, that makes reading documents more viable byutilizing different encodings in the addressing and output stages of the memoryread operation. To compare using KBs, information extraction or Wikipediadocuments directly in a single framework we construct an analysis tool,WikiMovies, a QA dataset that contains raw text alongside a preprocessed KB, inthe domain of movies. Our method reduces the gap between all three settings. Italso achieves state-of-the-art results on the existing WikiQA benchmark.", "categories": ["cs.CL"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "399", "arxiv_url": "http://arxiv.org/abs/1606.03126v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4280486537333639695&btnI=1&nossl=1&hl=en&oe=ASCII"}, "309": {"ID": 309, "title": "Text Understanding with the Attention Sum Reader Network", "authors": ["Ondrej Bajgar", "Rudolf Kadlec", "Martin Schmid", "Jan Kleindienst"], "published": "2016-03-04T17:32:42Z", "updated": "2016-06-24T13:04:47Z", "abstract": "Several large cloze-style context-question-answer datasets have beenintroduced recently: the CNN and Daily Mail news data and the Children's BookTest. Thanks to the size of these datasets, the associated text comprehensiontask is well suited for deep-learning techniques that currently seem tooutperform all alternative approaches. We present a new, simple model that usesattention to directly pick the answer from the context as opposed to computingthe answer using a blended representation of words in the document as is usualin similar models. This makes the model particularly suitable forquestion-answering problems where the answer is a single word from thedocument. Ensemble of our models sets new state of the art on all evaluateddatasets.", "categories": ["cs.CL"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "233", "arxiv_url": "http://arxiv.org/abs/1603.01547v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=13344834090487552365&btnI=1&nossl=1&hl=en&oe=ASCII"}, "310": {"ID": 310, "title": "Multimodal Compact Bilinear Pooling for Visual Question Answering and  Visual Grounding", "authors": ["Akira Fukui", "Marcus Rohrbach", "Trevor Darrell", "Dong Huk Park", "Daylen Yang", "Anna Rohrbach"], "published": "2016-06-06T17:59:56Z", "updated": "2016-09-24T01:58:59Z", "abstract": "Modeling textual or visual information with vector representations trainedfrom large language or visual datasets has been successfully explored in recentyears. However, tasks such as visual question answering require combining thesevector representations with each other. Approaches to multimodal poolinginclude element-wise product or sum, as well as concatenation of the visual andtextual representations. We hypothesize that these methods are not asexpressive as an outer product of the visual and textual vectors. As the outerproduct is typically infeasible due to its high dimensionality, we insteadpropose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently andexpressively combine multimodal features. We extensively evaluate MCB on thevisual question answering and grounding tasks. We consistently show the benefitof MCB over ablations without MCB. For visual question answering, we present anarchitecture which uses MCB twice, once for predicting attention over spatialfeatures and again to combine the attended representation with the questionrepresentation. This model outperforms the state-of-the-art on the Visual7Wdataset and the VQA challenge.", "categories": ["cs.CV", "cs.AI", "cs.CL"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "738", "arxiv_url": "http://arxiv.org/abs/1606.01847v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4520651819365506698&btnI=1&nossl=1&hl=en&oe=ASCII"}, "311": {"ID": 311, "title": "Sequential Short-Text Classification with Recurrent and Convolutional  Neural Networks", "authors": ["Franck Dernoncourt", "Ji Young Lee"], "published": "2016-03-12T00:02:51Z", "updated": "2016-03-12T00:02:51Z", "abstract": "Recent approaches based on artificial neural networks (ANNs) have shownpromising results for short-text classification. However, many short textsoccur in sequences (e.g., sentences in a document or utterances in a dialog),and most existing ANN-based systems do not leverage the preceding short textswhen classifying a subsequent one. In this work, we present a model based onrecurrent neural networks and convolutional neural networks that incorporatesthe preceding short texts. Our model achieves state-of-the-art results on threedifferent datasets for dialog act prediction.", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE", "stat.ML"], "journal": "HLT-NAACL, 515-520", "citations": "271", "arxiv_url": "http://arxiv.org/abs/1603.03827v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=15204618355692725322&btnI=1&nossl=1&hl=en&oe=ASCII"}, "312": {"ID": 312, "title": "BERT Rediscovers the Classical NLP Pipeline", "authors": ["Dipanjan Das", "Ellie Pavlick", "Ian Tenney"], "published": "2019-05-15T05:47:23Z", "updated": "2019-08-09T15:51:47Z", "abstract": "Pre-trained text encoders have rapidly advanced the state of the art on manyNLP tasks. We focus on one such model, BERT, and aim to quantify wherelinguistic information is captured within the network. We find that the modelrepresents the steps of the traditional NLP pipeline in an interpretable andlocalizable way, and that the regions responsible for each step appear in theexpected sequence: POS tagging, parsing, NER, semantic roles, then coreference.Qualitative analysis reveals that the model can and often does adjust thispipeline dynamically, revising lower-level decisions on the basis ofdisambiguating information from higher-level representations.", "categories": ["cs.CL"], "journal": "Proceedings of the 57th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "142", "arxiv_url": "http://arxiv.org/abs/1905.05950v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=249564336075756665&btnI=1&nossl=1&hl=en&oe=ASCII"}, "313": {"ID": 313, "title": "Graph Convolutional Encoders for Syntax-aware Neural Machine Translation", "authors": ["Jasmijn Bastings", "Ivan Titov", "Wilker Aziz", "Diego Marcheggiani", "Khalil Sima'an"], "published": "2017-04-15T19:04:59Z", "updated": "2020-06-18T19:29:11Z", "abstract": "We present a simple and effective approach to incorporating syntacticstructure into neural attention-based encoder-decoder models for machinetranslation. We rely on graph-convolutional networks (GCNs), a recent class ofneural networks developed for modeling graph-structured data. Our GCNs usepredicted syntactic dependency trees of source sentences to producerepresentations of words (i.e. hidden states of the encoder) that are sensitiveto their syntactic neighborhoods. GCNs take word representations as input andproduce word representations as output, so they can easily be incorporated aslayers into standard encoders (e.g., on top of bidirectional RNNs orconvolutional neural networks). We evaluate their effectiveness withEnglish-German and English-Czech translation experiments for different types ofencoders and observe substantial improvements over their syntax-agnosticversions in all the considered setups.", "categories": ["cs.CL"], "journal": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "179", "arxiv_url": "http://arxiv.org/abs/1704.04675v4", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4876389727678322394&btnI=1&nossl=1&hl=en&oe=ASCII"}, "314": {"ID": 314, "title": "What to talk about and how? Selective Generation using LSTMs with  Coarse-to-Fine Alignment", "authors": ["Hongyuan Mei", "Mohit Bansal", "Matthew R. Walter"], "published": "2015-09-02T19:52:56Z", "updated": "2016-01-08T23:07:32Z", "abstract": "We propose an end-to-end, domain-independent neural encoder-aligner-decodermodel for selective generation, i.e., the joint task of content selection andsurface realization. Our model first encodes a full set of over-determineddatabase event records via an LSTM-based recurrent neural network, thenutilizes a novel coarse-to-fine aligner to identify the small subset of salientrecords to talk about, and finally employs a decoder to generate free-formdescriptions of the aligned, selected records. Our model achieves the bestselection and generation results reported to-date (with 59% relativeimprovement in generation) on the benchmark WeatherGov dataset, despite usingno specialized features or linguistic resources. Using an improved k-nearestneighbor beam filter helps further. We also perform a series of ablations andvisualizations to elucidate the contributions of our key model components.Lastly, we evaluate the generalizability of our model on the RoboCup dataset,and get results that are competitive with or better than the state-of-the-art,despite being severely data-starved.", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "journal": "HLT-NAACL, 720-730", "citations": "188", "arxiv_url": "http://arxiv.org/abs/1509.00838v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=15975005819124169962&btnI=1&nossl=1&hl=en&oe=ASCII"}, "315": {"ID": 315, "title": "Neural Network Acceptability Judgments", "authors": ["Alex Warstadt", "Samuel R. Bowman", "Amanpreet Singh"], "published": "2018-05-31T13:52:06Z", "updated": "2019-10-01T18:41:05Z", "abstract": "This paper investigates the ability of artificial neural networks to judgethe grammatical acceptability of a sentence, with the goal of testing theirlinguistic competence. We introduce the Corpus of Linguistic Acceptability(CoLA), a set of 10,657 English sentences labeled as grammatical orungrammatical from published linguistics literature. As baselines, we trainseveral recurrent neural network models on acceptability classification, andfind that our models outperform unsupervised models by Lau et al (2016) onCoLA. Error-analysis on specific grammatical phenomena reveals that both Lau etal.'s models and ours learn systematic generalizations like subject-verb-objectorder. However, all models we test perform far below human level on a widerange of grammatical constructions.", "categories": ["cs.CL"], "journal": "Transactions of the Association for Computational Linguistics 7, 625-641", "citations": "112", "arxiv_url": "http://arxiv.org/abs/1805.12471v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=2014979748342165365&btnI=1&nossl=1&hl=en&oe=ASCII"}, "316": {"ID": 316, "title": "Deep Reinforcement Learning for Mention-Ranking Coreference Models", "authors": ["Christopher D. Manning", "Kevin Clark"], "published": "2016-09-27T21:00:26Z", "updated": "2016-10-31T20:30:15Z", "abstract": "Coreference resolution systems are typically trained with heuristic lossfunctions that require careful tuning. In this paper we instead applyreinforcement learning to directly optimize a neural mention-ranking model forcoreference evaluation metrics. We experiment with two approaches: theREINFORCE policy gradient algorithm and a reward-rescaled max-margin objective.We find the latter to be more effective, resulting in significant improvementsover the current state-of-the-art on the English and Chinese portions of theCoNLL 2012 Shared Task.", "categories": ["cs.CL"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "184", "arxiv_url": "http://arxiv.org/abs/1609.08667v3", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=3849814247631700770&btnI=1&nossl=1&hl=en&oe=ASCII"}, "317": {"ID": 317, "title": "Combining Recurrent and Convolutional Neural Networks for Relation  Classification", "authors": ["Ngoc Thang Vu", "Hinrich Sch\u00fctze", "Heike Adel", "Pankaj Gupta"], "published": "2016-05-24T08:20:12Z", "updated": "2016-05-24T08:20:12Z", "abstract": "This paper investigates two different neural architectures for the task ofrelation classification: convolutional neural networks and recurrent neuralnetworks. For both models, we demonstrate the effect of different architecturalchoices. We present a new context representation for convolutional neuralnetworks for relation classification (extended middle context). Furthermore, wepropose connectionist bi-directional recurrent neural networks and introduceranking loss for their optimization. Finally, we show that combiningconvolutional and recurrent neural networks using a simple voting scheme isaccurate enough to improve results. Our neural models achieve state-of-the-artresults on the SemEval 2010 relation classification task.", "categories": ["cs.CL"], "journal": "HLT-NAACL, 534-539", "citations": "114", "arxiv_url": "http://arxiv.org/abs/1605.07333v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=16920949698266666753&btnI=1&nossl=1&hl=en&oe=ASCII"}, "318": {"ID": 318, "title": "Adversarial Deep Averaging Networks for Cross-Lingual Sentiment  Classification", "authors": ["Xilun Chen", "Claire Cardie", "Kilian Weinberger", "Ben Athiwaratkun", "Yu Sun"], "published": "2016-06-06T05:04:23Z", "updated": "2018-08-18T21:41:42Z", "abstract": "In recent years great success has been achieved in sentiment classificationfor English, thanks in part to the availability of copious annotated resources.Unfortunately, most languages do not enjoy such an abundance of labeled data.To tackle the sentiment classification problem in low-resource languageswithout adequate annotated data, we propose an Adversarial Deep AveragingNetwork (ADAN) to transfer the knowledge learned from labeled data on aresource-rich source language to low-resource languages where only unlabeleddata exists. ADAN has two discriminative branches: a sentiment classifier andan adversarial language discriminator. Both branches take input from a sharedfeature extractor to learn hidden representations that are simultaneouslyindicative for the classification task and invariant across languages.Experiments on Chinese and Arabic sentiment classification demonstrate thatADAN significantly outperforms state-of-the-art systems.", "categories": ["cs.CL"], "journal": "Transactions of the Association for Computational Linguistics 6, 557-570", "citations": "128", "arxiv_url": "http://arxiv.org/abs/1606.01614v5", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8202333963441251200&btnI=1&nossl=1&hl=en&oe=ASCII"}, "319": {"ID": 319, "title": "Charagram: Embedding Words and Sentences via Character n-grams", "authors": ["Mohit Bansal", "John Wieting", "Karen Livescu", "Kevin Gimpel"], "published": "2016-07-10T21:59:19Z", "updated": "2016-07-10T21:59:19Z", "abstract": "We present Charagram embeddings, a simple approach for learningcharacter-based compositional models to embed textual sequences. A word orsentence is represented using a character n-gram count vector, followed by asingle nonlinear transformation to yield a low-dimensional embedding. We usethree tasks for evaluation: word similarity, sentence similarity, andpart-of-speech tagging. We demonstrate that Charagram embeddings outperformmore complex architectures based on character-level recurrent and convolutionalneural networks, achieving new state-of-the-art performance on severalsimilarity tasks.", "categories": ["cs.CL"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "142", "arxiv_url": "http://arxiv.org/abs/1607.02789v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=12174908863693179515&btnI=1&nossl=1&hl=en&oe=ASCII"}, "320": {"ID": 320, "title": "Stance Detection with Bidirectional Conditional Encoding", "authors": ["Andreas Vlachos", "Tim Rockt\u00e4schel", "Isabelle Augenstein", "Kalina Bontcheva"], "published": "2016-06-17T09:39:47Z", "updated": "2016-09-26T20:49:16Z", "abstract": "Stance detection is the task of classifying the attitude expressed in a texttowards a target such as Hillary Clinton to be \"positive\", negative\" or\"neutral\". Previous work has assumed that either the target is mentioned in thetext or that training data for every target is given. This paper considers themore challenging version of this task, where targets are not always mentionedand no training data is available for the test targets. We experiment withconditional LSTM encoding, which builds a representation of the tweet that isdependent on the target, and demonstrate that it outperforms encoding the tweetand the target independently. Performance is improved further when theconditional model is augmented with bidirectional encoding. We evaluate ourapproach on the SemEval 2016 Task 6 Twitter Stance Detection corpus achievingperformance second best only to a system trained on semi-automatically labelledtweets for the test target. When such weak supervision is added, our approachachieves state-of-the-art results.", "categories": ["cs.CL", "cs.LG", "cs.NE", "68T50", "I.2.7"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "154", "arxiv_url": "http://arxiv.org/abs/1606.05464v2", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=3691062047577945178&btnI=1&nossl=1&hl=en&oe=ASCII"}, "321": {"ID": 321, "title": "A Stylometric Inquiry into Hyperpartisan and Fake News", "authors": ["Benno Stein", "Martin Potthast", "Kevin Reinartz", "Johannes Kiesel", "Janek Bevendorff"], "published": "2017-02-18T18:10:04Z", "updated": "2017-02-18T18:10:04Z", "abstract": "This paper reports on a writing style analysis of hyperpartisan (i.e.,extremely one-sided) news in connection to fake news. It presents a largecorpus of 1,627 articles that were manually fact-checked by professionaljournalists from BuzzFeed. The articles originated from 9 well-known politicalpublishers, 3 each from the mainstream, the hyperpartisan left-wing, and thehyperpartisan right-wing. In sum, the corpus contains 299 fake news, 97% ofwhich originated from hyperpartisan publishers.  We propose and demonstrate a new way of assessing style similarity betweentext categories via Unmasking---a meta-learning approach originally devised forauthorship verification---, revealing that the style of left-wing andright-wing news have a lot more in common than any of the two have with themainstream. Furthermore, we show that hyperpartisan news can be discriminatedwell by its style from the mainstream (F1=0.78), as can be satire from both(F1=0.81). Unsurprisingly, style-based fake news detection does not live up toscratch (F1=0.46). Nevertheless, the former results are important to implementpre-screening for fake news detectors.", "categories": ["cs.CL"], "journal": "Proceedings of the 56th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "193", "arxiv_url": "http://arxiv.org/abs/1702.05638v1", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=2058347447961423495&btnI=1&nossl=1&hl=en&oe=ASCII"}, "322": {"ID": 322, "title": "A Latent Variable Model Approach to PMI-based Word Embeddings", "authors": ["Yingyu Liang", "Tengyu Ma", "Andrej Risteski", "Sanjeev Arora", "Yuanzhi Li"], "published": "2015-02-12T02:50:08Z", "updated": "2019-06-19T21:54:20Z", "abstract": "Semantic word embeddings represent the meaning of a word via a vector, andare created by diverse methods. Many use nonlinear operations on co-occurrencestatistics, and have hand-tuned hyperparameters and reweighting methods.  This paper proposes a new generative model, a dynamic version of thelog-linear topic model of~\\citet{mnih2007three}. The methodological novelty isto use the prior to compute closed form expressions for word statistics. Thisprovides a theoretical justification for nonlinear models like PMI, word2vec,and GloVe, as well as some hyperparameter choices. It also helps explain whylow-dimensional semantic embeddings contain linear algebraic structure thatallows solution of word analogies, as shown by~\\citet{mikolov2013efficient} andmany subsequent papers.  Experimental support is provided for the generative model assumptions, themost important of which is that latent word vectors are fairly uniformlydispersed in space.", "categories": ["cs.LG", "cs.CL", "stat.ML"], "arxiv_url": "http://arxiv.org/abs/1502.03520v8"}, "323": {"ID": 323, "title": "Learning to Parse and Translate Improves Neural Machine Translation", "authors": ["Akiko Eriguchi", "Yoshimasa Tsuruoka", "Kyunghyun Cho"], "published": "2017-02-12T13:19:03Z", "updated": "2017-04-23T16:52:03Z", "abstract": "There has been relatively little attention to incorporating linguistic priorto neural machine translation. Much of the previous work was furtherconstrained to considering linguistic prior on the source side. In this paper,we propose a hybrid model, called NMT+RNNG, that learns to parse and translateby combining the recurrent neural network grammar into the attention-basedneural machine translation. Our approach encourages the neural machinetranslation model to incorporate linguistic prior during training, and lets ittranslate on its own afterward. Extensive experiments with four language pairsshow the effectiveness of the proposed NMT+RNNG.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1702.03525v2"}, "324": {"ID": 324, "title": "A Robust Self-Learning Method for Fully Unsupervised Cross-Lingual  Mappings of Word Embeddings: Making the Method Robustly Reproducible as Well", "authors": ["Nicolas Garneau", "Mathieu Godbout", "Audrey Durand", "David Beauchemin", "Luc Lamontagne"], "published": "2019-12-03T22:07:47Z", "updated": "2020-03-03T14:30:50Z", "abstract": "In this paper, we reproduce the experiments of Artetxe et al. (2018b)regarding the robust self-learning method for fully unsupervised cross-lingualmappings of word embeddings. We show that the reproduction of their method isindeed feasible with some minor assumptions. We further investigate therobustness of their model by introducing four new languages that are lesssimilar to English than the ones proposed by the original paper. In order toassess the stability of their model, we also conduct a grid search oversensible hyperparameters. We then propose key recommendations applicable to anyresearch project in order to deliver fully reproducible research.", "categories": ["cs.LG", "cs.CL", "stat.ML"], "arxiv_url": "http://arxiv.org/abs/1912.01706v2"}, "325": {"ID": 325, "title": "Smile, Be Happy :) Emoji Embedding for Visual Sentiment Analysis", "authors": ["Ziad Al-Halah", "Jose Caballero", "Andrew Aitken", "Wenzhe Shi"], "published": "2019-07-14T03:29:02Z", "updated": "2020-08-09T02:48:03Z", "abstract": "Due to the lack of large-scale datasets, the prevailing approach in visualsentiment analysis is to leverage models trained for object classification inlarge datasets like ImageNet. However, objects are sentiment neutral whichhinders the expected gain of transfer learning for such tasks. In this work, wepropose to overcome this problem by learning a novel sentiment-aligned imageembedding that is better suited for subsequent visual sentiment analysis. Ourembedding leverages the intricate relation between emojis and images inlarge-scale and readily available data from social media. Emojis arelanguage-agnostic, consistent, and carry a clear sentiment signal which makethem an excellent proxy to learn a sentiment aligned embedding. Hence, weconstruct a novel dataset of 4 million images collected from Twitter with theirassociated emojis. We train a deep neural model for image embedding using emojiprediction task as a proxy. Our evaluation demonstrates that the proposedembedding outperforms the popular object-based counterpart consistently acrossseveral sentiment analysis benchmarks. Furthermore, without bell and whistles,our compact, effective and simple embedding outperforms the more elaborate andcustomized state-of-the-art deep models on these public benchmarks.Additionally, we introduce a novel emoji representation based on their visualemotional response which supports a deeper understanding of the emoji modalityand their usage on social media.", "categories": ["cs.CV"], "arxiv_url": "http://arxiv.org/abs/1907.06160v3"}, "326": {"ID": 326, "title": "Exploiting Out-of-Domain Parallel Data through Multilingual Transfer  Learning for Low-Resource Neural Machine Translation", "authors": ["Raj Dabre", "Aizhan Imankulova", "Kenji Imamura", "Atsushi Fujita"], "published": "2019-07-06T02:14:30Z", "updated": "2019-07-06T02:14:30Z", "abstract": "This paper proposes a novel multilingual multistage fine-tuning approach forlow-resource neural machine translation (NMT), taking a challengingJapanese--Russian pair for benchmarking. Although there are many solutions forlow-resource scenarios, such as multilingual NMT and back-translation, we haveempirically confirmed their limited success when restricted to in-domain data.We therefore propose to exploit out-of-domain data through transfer learning,by using it to first train a multilingual NMT model followed by multistagefine-tuning on in-domain parallel and back-translated pseudo-parallel data. Ourapproach, which combines domain adaptation, multilingualism, andback-translation, helps improve the translation quality by more than 3.7 BLEUpoints, over a strong baseline, for this extremely low-resource scenario.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1907.03060v1"}, "327": {"ID": 327, "title": "Limiting the Spread of Fake News on Social Media Platforms by Evaluating  Users' Trustworthiness", "authors": ["Matej Pavlovic", "Oana Balmau", "Alexandre Maurer", "Willy Zwaenepoel", "Rachid Guerraoui", "Anne-Marie Kermarrec"], "published": "2018-08-29T16:50:16Z", "updated": "2018-08-29T16:50:16Z", "abstract": "Today's social media platforms enable to spread both authentic and fake newsvery quickly. Some approaches have been proposed to automatically detect such\"fake\" news based on their content, but it is difficult to agree on universalcriteria of authenticity (which can be bypassed by adversaries once known).Besides, it is obviously impossible to have each news item checked by a human.  In this paper, we a mechanism to limit the spread of fake news which is notbased on content. It can be implemented as a plugin on a social media platform.The principle is as follows: a team of fact-checkers reviews a small number ofnews items (the most popular ones), which enables to have an estimation of eachuser's inclination to share fake news items. Then, using a Bayesian approach,we estimate the trustworthiness of future news items, and treat accordinglythose of them that pass a certain \"untrustworthiness\" threshold.  We then evaluate the effectiveness and overhead of this technique on a largeTwitter graph. We show that having a few thousands users exposed to one givennews item enables to reach a very precise estimation of its reliability. Wethus identify more than 99% of fake news items with no false positives. Theperformance impact is very small: the induced overhead on the 90th percentilelatency is less than 3%, and less than 8% on the throughput of user operations.", "categories": ["cs.SI"], "arxiv_url": "http://arxiv.org/abs/1808.09922v1"}, "328": {"ID": 328, "title": "Deep contextualized word representations for detecting sarcasm and irony", "authors": ["Yutaka Matsuo", "Suzana Ili\u0107", "Jorge A. Balazs", "Edison Marrese-Taylor"], "published": "2018-09-26T03:54:22Z", "updated": "2018-09-26T03:54:22Z", "abstract": "Predicting context-dependent and non-literal utterances like sarcastic andironic expressions still remains a challenging task in NLP, as it goes beyondlinguistic patterns, encompassing common sense and shared knowledge as crucialcomponents. To capture complex morpho-syntactic features that can usually serveas indicators for irony or sarcasm across dynamic contexts, we propose a modelthat uses character-level vector representations of words, based on ELMo. Wetest our model on 7 different datasets derived from 3 different data sources,providing state-of-the-art performance in 6 of them, and otherwise offeringcompetitive results.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1809.09795v1"}, "329": {"ID": 329, "title": "The Microsoft Toolkit of Multi-Task Deep Neural Networks for Natural  Language Understanding", "authors": ["Hao Cheng", "Jianfeng Gao", "Xueyun Zhu", "Jianshu Ji", "Pengcheng He", "Weizhu Chen", "Xiaodong Liu", "Guihong Cao", "Emmanuel Awa", "Hoifung Poon", "Yu Wang"], "published": "2020-02-19T03:05:28Z", "updated": "2020-05-15T21:47:31Z", "abstract": "We present MT-DNN, an open-source natural language understanding (NLU)toolkit that makes it easy for researchers and developers to train customizeddeep learning models. Built upon PyTorch and Transformers, MT-DNN is designedto facilitate rapid customization for a broad spectrum of NLU tasks, using avariety of objectives (classification, regression, structured prediction) andtext encoders (e.g., RNNs, BERT, RoBERTa, UniLM). A unique feature of MT-DNN isits built-in support for robust and transferable learning using the adversarialmulti-task learning paradigm. To enable efficient production deployment, MT-DNNsupports multi-task knowledge distillation, which can substantially compress adeep neural model without significant performance drop. We demonstrate theeffectiveness of MT-DNN on a wide range of NLU applications across general andbiomedical domains. The software and pre-trained models will be publiclyavailable at https://github.com/namisan/mt-dnn.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/2002.07972v2"}, "330": {"ID": 330, "title": "Harnessing Deep Neural Networks with Logic Rules", "authors": ["Eduard Hovy", "Xuezhe Ma", "Zhiting Hu", "Eric Xing", "Zhengzhong Liu"], "published": "2016-03-21T03:33:20Z", "updated": "2020-08-08T07:38:00Z", "abstract": "Combining deep neural networks with structured logic rules is desirable toharness flexibility and reduce uninterpretability of the neural models. Wepropose a general framework capable of enhancing various types of neuralnetworks (e.g., CNNs and RNNs) with declarative first-order logic rules.Specifically, we develop an iterative distillation method that transfers thestructured information of logic rules into the weights of neural networks. Wedeploy the framework on a CNN for sentiment analysis, and an RNN for namedentity recognition. With a few highly intuitive rules, we obtain substantialimprovements and achieve state-of-the-art or comparable results to previousbest-performing systems.", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "arxiv_url": "http://arxiv.org/abs/1603.06318v6"}, "331": {"ID": 331, "title": "NTUA-SLP at SemEval-2018 Task 2: Predicting Emojis using RNNs with  Context-aware Attention", "authors": ["Athanasia Kolovou", "Nikolaos Ellinas", "Nikos Athanasiou", "Alexandros Potamianos", "Georgios Paraskevopoulos", "Christos Baziotis"], "published": "2018-04-18T11:30:57Z", "updated": "2018-04-18T11:30:57Z", "abstract": "In this paper we present a deep-learning model that competed at SemEval-2018Task 2 \"Multilingual Emoji Prediction\". We participated in subtask A, in whichwe are called to predict the most likely associated emoji in English tweets.The proposed architecture relies on a Long Short-Term Memory network, augmentedwith an attention mechanism, that conditions the weight of each word, on a\"context vector\" which is taken as the aggregation of a tweet's meaning.Moreover, we initialize the embedding layer of our model, with word2vec wordembeddings, pretrained on a dataset of 550 million English tweets. Finally, ourmodel does not rely on hand-crafted features or lexicons and is trainedend-to-end with back-propagation. We ranked 2nd out of 48 teams.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1804.06657v1"}, "332": {"ID": 332, "title": "Read, Highlight and Summarize: A Hierarchical Neural Semantic  Encoder-based Approach", "authors": ["Prasenjit Mitra", "Rajeev Bhatt Ambati", "Saptarashmi Bandyopadhyay"], "published": "2019-10-08T02:36:02Z", "updated": "2019-11-01T04:54:54Z", "abstract": "Traditional sequence-to-sequence (seq2seq) models and other variations of theattention-mechanism such as hierarchical attention have been applied to thetext summarization problem. Though there is a hierarchy in the way humans uselanguage by forming paragraphs from sentences and sentences from words,hierarchical models have usually not worked that much better than theirtraditional seq2seq counterparts. This effect is mainly because either thehierarchical attention mechanisms are too sparse using hard attention or noisyusing soft attention. In this paper, we propose a method based on extractingthe highlights of a document; a key concept that is conveyed in a fewsentences. In a typical text summarization dataset consisting of documents thatare 800 tokens in length (average), capturing long-term dependencies is veryimportant, e.g., the last sentence can be grouped with the first sentence of adocument to form a summary. LSTMs (Long Short-Term Memory) proved useful formachine translation. However, they often fail to capture long-term dependencieswhile modeling long sequences. To address these issues, we have adapted NeuralSemantic Encoders (NSE) to text summarization, a class of memory-augmentedneural networks by improving its functionalities and proposed a novelhierarchical NSE that outperforms similar previous models significantly. Thequality of summarization was improved by augmenting linguistic factors, namelylemma, and Part-of-Speech (PoS) tags, to each word in the dataset for improvedvocabulary coverage and generalization. The hierarchical NSE model on factoreddataset outperformed the state-of-the-art by nearly 4 ROUGE points. We furtherdesigned and used the first GPU-based self-critical Reinforcement Learningmodel.", "categories": ["cs.LG", "cs.CL", "stat.ML"], "arxiv_url": "http://arxiv.org/abs/1910.03177v2"}, "333": {"ID": 333, "title": "Duluth UROP at SemEval-2018 Task 2: Multilingual Emoji Prediction with  Ensemble Learning and Oversampling", "authors": ["Ted Pedersen", "Shuning Jin"], "published": "2018-05-25T17:36:51Z", "updated": "2018-05-25T17:36:51Z", "abstract": "This paper describes the Duluth UROP systems that participated inSemEval--2018 Task 2, Multilingual Emoji Prediction. We relied on a variety ofensembles made up of classifiers using Naive Bayes, Logistic Regression, andRandom Forests. We used unigram and bigram features and tried to offset theskewness of the data through the use of oversampling. Our task evaluationresults place us 19th of 48 systems in the English evaluation, and 5th of 21 inthe Spanish. After the evaluation we realized that some simple changes topreprocessing could significantly improve our results. After making thesechanges we attained results that would have placed us sixth in the Englishevaluation, and second in the Spanish.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1805.10267v1"}, "334": {"ID": 334, "title": "Exploring Emoji Usage and Prediction Through a Temporal Variation Lens", "authors": ["Luis Marujo", "Francesco Barbieri", "William Brendel", "Pradeep Karuturi", "Horacio Saggion"], "published": "2018-05-02T11:03:52Z", "updated": "2018-05-02T11:03:52Z", "abstract": "The frequent use of Emojis on social media platforms has created a new formof multimodal social interaction. Developing methods for the study andrepresentation of emoji semantics helps to improve future multimodalcommunication systems. In this paper, we explore the usage and semantics ofemojis over time. We compare emoji embeddings trained on a corpus of differentseasons and show that some emojis are used differently depending on the time ofthe year. Moreover, we propose a method to take into account the timeinformation for emoji prediction systems, outperforming state-of-the-artsystems. We show that, using the time information, the accuracy of some emojiscan be significantly improved.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1805.00731v1"}, "335": {"ID": 335, "title": "An Attentive Neural Architecture for Fine-grained Entity Type  Classification", "authors": ["Sebastian Riedel", "Sonse Shimaoka", "Pontus Stenetorp", "Kentaro Inui"], "published": "2016-04-19T11:39:53Z", "updated": "2016-04-19T11:39:53Z", "abstract": "In this work we propose a novel attention-based neural network model for thetask of fine-grained entity type classification that unlike previously proposedmodels recursively composes representations of entity mention contexts. Ourmodel achieves state-of-the-art performance with 74.94% loose micro F1-score onthe well-established FIGER dataset, a relative improvement of 2.59%. We alsoinvestigate the behavior of the attention mechanism of our model and observethat it can learn contextual linguistic expressions that indicate thefine-grained category memberships of an entity.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1604.05525v1"}, "336": {"ID": 336, "title": "SCELMo: Source Code Embeddings from Language Models", "authors": ["Rafael - Michael Karampatsis", "Charles Sutton"], "published": "2020-04-28T00:06:25Z", "updated": "2020-04-28T00:06:25Z", "abstract": "Continuous embeddings of tokens in computer programs have been used tosupport a variety of software development tools, including readability, codesearch, and program repair. Contextual embeddings are common in naturallanguage processing but have not been previously applied in softwareengineering. We introduce a new set of deep contextualized word representationsfor computer programs based on language models. We train a set of embeddingsusing the ELMo (embeddings from language models) framework of Peters et al(2018). We investigate whether these embeddings are effective when fine-tunedfor the downstream task of bug detection. We show that even a low-dimensionalembedding trained on a relatively small corpus of programs can improve astate-of-the-art machine learning system for bug detection.", "categories": ["cs.SE", "cs.LG"], "arxiv_url": "http://arxiv.org/abs/2004.13214v1"}, "337": {"ID": 337, "title": "UG18 at SemEval-2018 Task 1: Generating Additional Training Data for  Predicting Emotion Intensity in Spanish", "authors": ["Marloes Kuijper", "Mike van Lenthe", "Rik van Noord"], "published": "2018-05-28T09:02:18Z", "updated": "2018-05-28T09:02:18Z", "abstract": "The present study describes our submission to SemEval 2018 Task 1: Affect inTweets. Our Spanish-only approach aimed to demonstrate that it is beneficial toautomatically generate additional training data by (i) translating trainingdata from other languages and (ii) applying a semi-supervised learning method.We find strong support for both approaches, with those models outperforming ourregular models in all subtasks. However, creating a stepwise ensemble ofdifferent models as opposed to simply averaging did not result in an increasein performance. We placed second (EI-Reg), second (EI-Oc), fourth (V-Reg) andfifth (V-Oc) in the four Spanish subtasks we participated in.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1805.10824v1"}, "338": {"ID": 338, "title": "Generating Natural Language Adversarial Examples on a Large Scale with  Generative Models", "authors": ["Siliang Tang", "Yuan Qi", "Jun Zhou", "Yankun Ren", "Xiang Ren", "Shuang Yang", "Jianbin Lin"], "published": "2020-03-10T03:21:35Z", "updated": "2020-03-10T03:21:35Z", "abstract": "Today text classification models have been widely used. However, theseclassifiers are found to be easily fooled by adversarial examples. Fortunately,standard attacking methods generate adversarial texts in a pair-wise way, thatis, an adversarial text can only be created from a real-world text by replacinga few words. In many applications, these texts are limited in numbers,therefore their corresponding adversarial examples are often not diverse enoughand sometimes hard to read, thus can be easily detected by humans and cannotcreate chaos at a large scale. In this paper, we propose an end to end solutionto efficiently generate adversarial texts from scratch using generative models,which are not restricted to perturbing the given texts. We call it unrestrictedadversarial text generation. Specifically, we train a conditional variationalautoencoder (VAE) with an additional adversarial loss to guide the generationof adversarial examples. Moreover, to improve the validity of adversarialtexts, we utilize discrimators and the training framework of generativeadversarial networks (GANs) to make adversarial texts consistent with realdata. Experimental results on sentiment analysis demonstrate the scalabilityand efficiency of our method. It can attack text classification models with ahigher success rate than existing methods, and provide acceptable quality forhumans in the meantime.", "categories": ["cs.CL", "cs.LG", "stat.ML"], "arxiv_url": "http://arxiv.org/abs/2003.10388v1"}, "339": {"ID": 339, "title": "Attention is not not Explanation", "authors": ["Yuval Pinter", "Sarah Wiegreffe"], "published": "2019-08-13T13:15:04Z", "updated": "2019-09-05T14:11:27Z", "abstract": "Attention mechanisms play a central role in NLP systems, especially withinrecurrent neural network (RNN) models. Recently, there has been increasinginterest in whether or not the intermediate representations offered by thesemodules may be used to explain the reasoning for a model's prediction, andconsequently reach insights regarding the model's decision-making process. Arecent paper claims that `Attention is not Explanation' (Jain and Wallace,2019). We challenge many of the assumptions underlying this work, arguing thatsuch a claim depends on one's definition of explanation, and that testing itneeds to take into account all elements of the model, using a rigorousexperimental design. We propose four alternative tests to determinewhen/whether attention can be used as explanation: a simple uniform-weightsbaseline; a variance calibration based on multiple random seed runs; adiagnostic framework using frozen weights from pretrained models; and anend-to-end adversarial attention training protocol. Each allows for meaningfulinterpretation of attention mechanisms in RNN models. We show that even whenreliable adversarial distributions can be found, they don't perform well on thesimple diagnostic, indicating that prior work does not disprove the usefulnessof attention mechanisms for explainability.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1908.04626v2"}, "340": {"ID": 340, "title": "Variational Neural Machine Translation with Normalizing Flows", "authors": ["Matthias Sperber", "Matthias Paulik", "Udhay Nallasamy", "Hendra Setiawan"], "published": "2020-05-28T13:30:53Z", "updated": "2020-05-28T13:30:53Z", "abstract": "Variational Neural Machine Translation (VNMT) is an attractive framework formodeling the generation of target translations, conditioned not only on thesource sentence but also on some latent random variables. The latent variablemodeling may introduce useful statistical dependencies that can improvetranslation accuracy. Unfortunately, learning informative latent variables isnon-trivial, as the latent space can be prohibitively large, and the latentcodes are prone to be ignored by many translation models at training time.Previous works impose strong assumptions on the distribution of the latent codeand limit the choice of the NMT architecture. In this paper, we propose toapply the VNMT framework to the state-of-the-art Transformer and introduce amore flexible approximate posterior based on normalizing flows. We demonstratethe efficacy of our proposal under both in-domain and out-of-domain conditions,significantly outperforming strong baselines.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/2005.13978v1"}, "341": {"ID": 341, "title": "Sentence Simplification with Memory-Augmented Neural Networks", "authors": ["Hong Yu", "Baotian Hu", "Tsendsuren Munkhdalai", "Tu Vu"], "published": "2018-04-20T03:52:20Z", "updated": "2018-04-20T03:52:20Z", "abstract": "Sentence simplification aims to simplify the content and structure of complexsentences, and thus make them easier to interpret for human readers, and easierto process for downstream NLP applications. Recent advances in neural machinetranslation have paved the way for novel approaches to the task. In this paper,we adapt an architecture with augmented memory capacities called NeuralSemantic Encoders (Munkhdalai and Yu, 2017) for sentence simplification. Ourexperiments demonstrate the effectiveness of our approach on differentsimplification datasets, both in terms of automatic evaluation measures andhuman judgments.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1804.07445v1"}, "342": {"ID": 342, "title": "SeerNet at SemEval-2018 Task 1: Domain Adaptation for Affect in Tweets", "authors": ["Venkatesh Duppada", "Sushant Hiray", "Royal Jain"], "published": "2018-04-17T09:50:01Z", "updated": "2018-04-17T09:50:01Z", "abstract": "The paper describes the best performing system for the SemEval-2018 Affect inTweets (English) sub-tasks. The system focuses on the ordinal classificationand regression sub-tasks for valence and emotion. For ordinal classificationvalence is classified into 7 different classes ranging from -3 to 3 whereasemotion is classified into 4 different classes 0 to 3 separately for eachemotion namely anger, fear, joy and sadness. The regression sub-tasks estimatethe intensity of valence and each emotion. The system performs domainadaptation of 4 different models and creates an ensemble to give the finalprediction. The proposed system achieved 1st position out of 75 teams whichparticipated in the fore-mentioned sub-tasks. We outperform the baseline modelby margins ranging from 49.2% to 76.4%, thus, pushing the state-of-the-artsignificantly.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1804.06137v1"}, "343": {"ID": 343, "title": "TransPose: Towards Explainable Human Pose Estimation by Transformer", "authors": ["Mu Nie", "Wankou Yang", "Sen Yang", "Zhibin Quan"], "published": "2020-12-28T12:33:52Z", "updated": "2020-12-31T07:15:16Z", "abstract": "Deep Convolutional Neural Networks (CNNs) have made remarkable progress onhuman pose estimation task. However, there is no explicit understanding of howthe locations of body keypoints are predicted by CNN, and it is also unknownwhat spatial dependency relationships between structural variables are learnedin the model. To explore these questions, we construct an explainable modelnamed TransPose based on Transformer architecture and low-level convolutionalblocks. Given an image, the attention layers built in Transformer can capturelong-range spatial relationships between keypoints and explain whatdependencies the predicted keypoints locations highly rely on. We analyze therationality of using attention as the explanation to reveal the spatialdependencies in this task. The revealed dependencies are image-specific andvariable for different keypoint types, layer depths, or trained models. Theexperiments show that TransPose can accurately predict the positions ofkeypoints. It achieves state-of-the-art performance on COCO dataset, whilebeing more interpretable, lightweight, and efficient than mainstream fullyconvolutional architectures.", "categories": ["cs.CV"], "arxiv_url": "http://arxiv.org/abs/2012.14214v2"}, "344": {"ID": 344, "title": "Computational linking theory", "authors": ["Drew Reisinger", "Benjamin Van Durme", "Aaron Steven White", "Rachel Rudinger", "Kyle Rawlins"], "published": "2016-10-08T15:24:50Z", "updated": "2016-10-08T15:24:50Z", "abstract": "A linking theory explains how verbs' semantic arguments are mapped to theirsyntactic arguments---the inverse of the Semantic Role Labeling task from theshallow semantic parsing literature. In this paper, we develop theComputational Linking Theory framework as a method for implementing and testinglinking theories proposed in the theoretical literature. We deploy thisframework to assess two cross-cutting types of linking theory: local v. globalmodels and categorical v. featural models. To further investigate the behaviorof these models, we develop a measurement model in the spirit of previous workin semantic role induction: the Semantic Proto-Role Linking Model. We use thismodel, which implements a generalization of Dowty's seminal Proto-Role Theory,to induce semantic proto-roles, which we compare to those Dowty proposes.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1610.02544v1"}, "345": {"ID": 345, "title": "A non-projective greedy dependency parser with bidirectional LSTMs", "authors": ["Carlos G\u00f3mez-Rodr\u00edguez", "David Vilares"], "published": "2017-07-11T11:44:31Z", "updated": "2017-07-11T11:44:31Z", "abstract": "The LyS-FASTPARSE team presents BIST-COVINGTON, a neural implementation ofthe Covington (2001) algorithm for non-projective dependency parsing. Thebidirectional LSTM approach by Kipperwasser and Goldberg (2016) is used totrain a greedy parser with a dynamic oracle to mitigate error propagation. Themodel participated in the CoNLL 2017 UD Shared Task. In spite of not using anyensemble methods and using the baseline segmentation and PoS tagging, theparser obtained good results on both macro-average LAS and UAS in the bigtreebanks category (55 languages), ranking 7th out of 33 teams. In the alltreebanks category (LAS and UAS) we ranked 16th and 12th. The gap between theall and big categories is mainly due to the poor performance on four parallelPUD treebanks, suggesting that some `suffixed' treebanks (e.g. Spanish-AnCora)perform poorly on cross-treebank settings, which does not occur with thecorresponding `unsuffixed' treebank (e.g. Spanish). By changing that, we obtainthe 11th best LAS among all runs (official and unofficial). The code is madeavailable at https://github.com/CoNLL-UD-2017/LyS-FASTPARSE", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1707.03228v1"}, "346": {"ID": 346, "title": "Improving Text Generation Evaluation with Batch Centering and Tempered  Word Mover Distance", "authors": ["Radu Soricut", "Xi Chen", "Tomer Levinboim", "Nan Ding"], "published": "2020-10-13T03:46:25Z", "updated": "2020-10-13T03:46:25Z", "abstract": "Recent advances in automatic evaluation metrics for text have shown that deepcontextualized word representations, such as those generated by BERT encoders,are helpful for designing metrics that correlate well with human judgements. Atthe same time, it has been argued that contextualized word representationsexhibit sub-optimal statistical properties for encoding the true similaritybetween words or sentences. In this paper, we present two techniques forimproving encoding representations for similarity metrics: a batch-meancentering strategy that improves statistical properties; and a computationallyefficient tempered Word Mover Distance, for better fusion of the information inthe contextualized word representations. We conduct numerical experiments thatdemonstrate the robustness of our techniques, reporting results over variousBERT-backbone learned metrics and achieving state of the art correlation withhuman ratings on several benchmarks.", "categories": ["cs.CL", "cs.LG"], "arxiv_url": "http://arxiv.org/abs/2010.06150v1"}, "347": {"ID": 347, "title": "IIIDYT at SemEval-2018 Task 3: Irony detection in English tweets", "authors": ["Yutaka Matsuo", "Suzana Ilic", "Helmut Prendinger", "Jorge A. Balazs", "Edison Marrese-Taylor"], "published": "2018-04-22T11:01:08Z", "updated": "2018-04-22T11:01:08Z", "abstract": "In this paper we introduce our system for the task of Irony detection inEnglish tweets, a part of SemEval 2018. We propose representation learningapproach that relies on a multi-layered bidirectional LSTM, without usingexternal features that provide additional semantic information. Although ourmodel is able to outperform the baseline in the validation set, our resultsshow limited generalization power over the test set. Given the limited size ofthe dataset, we think the usage of more pre-training schemes would greatlyimprove the obtained results.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1804.08094v1"}, "348": {"ID": 348, "title": "ETH-DS3Lab at SemEval-2018 Task 7: Effectively Combining Recurrent and  Convolutional Neural Networks for Relation Classification and Extraction", "authors": ["Nora Hollenstein", "Ce Zhang", "Jonathan Rotsztejn"], "published": "2018-04-05T20:01:48Z", "updated": "2018-04-05T20:01:48Z", "abstract": "Reliably detecting relevant relations between entities in unstructured textis a valuable resource for knowledge extraction, which is why it has awakensignificant interest in the field of Natural Language Processing. In thispaper, we present a system for relation classification and extraction based onan ensemble of convolutional and recurrent neural networks that ranked first in3 out of the 4 subtasks at SemEval 2018 Task 7. We provide detailedexplanations and grounds for the design choices behind the most relevantfeatures and analyze their importance.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1804.02042v1"}, "349": {"ID": 349, "title": "Challenges in Building Intelligent Open-domain Dialog Systems", "authors": ["Minlie Huang", "Jianfeng Gao", "Xiaoyan Zhu"], "published": "2019-05-13T02:46:28Z", "updated": "2020-02-28T09:16:35Z", "abstract": "There is a resurgent interest in developing intelligent open-domain dialogsystems due to the availability of large amounts of conversational data and therecent progress on neural approaches to conversational AI. Unlike traditionaltask-oriented bots, an open-domain dialog system aims to establish long-termconnections with users by satisfying the human need for communication,affection, and social belonging. This paper reviews the recent works on neuralapproaches that are devoted to addressing three challenges in developing suchsystems: semantics, consistency, and interactiveness. Semantics requires adialog system to not only understand the content of the dialog but alsoidentify user's social needs during the conversation. Consistency requires thesystem to demonstrate a consistent personality to win users trust and gaintheir long-term confidence. Interactiveness refers to the system's ability togenerate interpersonal responses to achieve particular social goals such asentertainment, conforming, and task completion. The works we select to presenthere is based on our unique views and are by no means complete. Nevertheless,we hope that the discussion will inspire new research in developing moreintelligent dialog systems.", "categories": ["cs.CL", "cs.AI"], "arxiv_url": "http://arxiv.org/abs/1905.05709v3"}, "350": {"ID": 350, "title": "SemEval-2015 Task 3: Answer Selection in Community Question Answering", "authors": ["Llu\u00eds M\u00e0rquez", "Preslav Nakov", "James Glass", "Alessandro Moschitti", "Walid Magdy", "Bilal Randeree"], "published": "2019-11-26T08:40:49Z", "updated": "2019-11-26T08:40:49Z", "abstract": "Community Question Answering (cQA) provides new interesting researchdirections to the traditional Question Answering (QA) field, e.g., theexploitation of the interaction between users and the structure of relatedposts. In this context, we organized SemEval-2015 Task 3 on \"Answer Selectionin cQA\", which included two subtasks: (a) classifying answers as \"good\", \"bad\",or \"potentially relevant\" with respect to the question, and (b) answering aYES/NO question with \"yes\", \"no\", or \"unsure\", based on the list of allanswers. We set subtask A for Arabic and English on two relatively differentcQA domains, i.e., the Qatar Living website for English, and a Quran-relatedwebsite for Arabic. We used crowdsourcing on Amazon Mechanical Turk to label alarge English training dataset, which we released to the research community.Thirteen teams participated in the challenge with a total of 61 submissions: 24primary and 37 contrastive. The best systems achieved an official score(macro-averaged F1) of 57.19 and 63.7 for the English subtasks A and B, and78.55 for the Arabic subtask A.", "categories": ["cs.CL", "cs.AI", "cs.IR", "68T50", "I.2.7"], "arxiv_url": "http://arxiv.org/abs/1911.11403v1"}, "351": {"ID": 351, "title": "Yuanfudao at SemEval-2018 Task 11: Three-way Attention and Relational  Knowledge for Commonsense Machine Comprehension", "authors": ["Meng Sun", "Kewei Shen", "Jingming Liu", "Liang Wang", "Wei Zhao"], "published": "2018-03-01T03:23:55Z", "updated": "2018-05-15T06:21:02Z", "abstract": "This paper describes our system for SemEval-2018 Task 11: MachineComprehension using Commonsense Knowledge. We use Three-way Attentive Networks(TriAN) to model interactions between the passage, question and answers. Toincorporate commonsense knowledge, we augment the input with relation embeddingfrom the graph of general knowledge ConceptNet (Speer et al., 2017). As aresult, our system achieves state-of-the-art performance with 83.95% accuracyon the official test data. Code is publicly available athttps://github.com/intfloat/commonsense-rc", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1803.00191v5"}, "352": {"ID": 352, "title": "Neural Symbolic Machines: Learning Semantic Parsers on Freebase with  Weak Supervision (Short Version)", "authors": ["Ni Lao", "Kenneth D. Forbus", "Quoc Le", "Jonathan Berant", "Chen Liang"], "published": "2016-12-04T22:29:32Z", "updated": "2016-12-04T22:29:32Z", "abstract": "Extending the success of deep neural networks to natural languageunderstanding and symbolic reasoning requires complex operations and externalmemory. Recent neural program induction approaches have attempted to addressthis problem, but are typically limited to differentiable memory, andconsequently cannot scale beyond small synthetic tasks. In this work, wepropose the Manager-Programmer-Computer framework, which integrates neuralnetworks with non-differentiable memory to support abstract, scalable andprecise operations through a friendly neural computer interface. Specifically,we introduce a Neural Symbolic Machine, which contains a sequence-to-sequenceneural \"programmer\", and a non-differentiable \"computer\" that is a Lispinterpreter with code assist. To successfully apply REINFORCE for training, weaugment it with approximate gold programs found by an iterative maximumlikelihood training process. NSM is able to learn a semantic parser from weaksupervision over a large knowledge base. It achieves new state-of-the-artperformance on WebQuestionsSP, a challenging semantic parsing dataset, withweak supervision. Compared to previous approaches, NSM is end-to-end, thereforedoes not rely on feature engineering or domain specific knowledge.", "categories": ["cs.CL", "cs.AI", "cs.LG"], "arxiv_url": "http://arxiv.org/abs/1612.01197v1"}, "353": {"ID": 353, "title": "Gated End-to-End Memory Networks", "authors": ["Julien Perez", "Fei Liu"], "published": "2016-10-13T19:38:03Z", "updated": "2016-11-17T15:09:29Z", "abstract": "Machine reading using differentiable reasoning models has recently shownremarkable progress. In this context, End-to-End trainable Memory Networks,MemN2N, have demonstrated promising performance on simple natural languagebased reasoning tasks such as factual reasoning and basic deduction. However,other tasks, namely multi-fact question-answering, positional reasoning ordialog related tasks, remain challenging particularly due to the necessity ofmore complex interactions between the memory and controller modules composingthis family of models. In this paper, we introduce a novel end-to-end memoryaccess regulation mechanism inspired by the current progress on the connectionshort-cutting principle in the field of computer vision. Concretely, we developa Gated End-to-End trainable Memory Network architecture, GMemN2N. From themachine learning perspective, this new capability is learned in an end-to-endfashion without the use of any additional supervision signal which is, as faras our knowledge goes, the first of its kind. Our experiments show significantimprovements on the most challenging tasks in the 20 bAbI dataset, without theuse of any domain knowledge. Then, we show improvements on the dialog bAbItasks including the real human-bot conversion-based Dialog State TrackingChallenge (DSTC-2) dataset. On these two datasets, our model sets the new stateof the art.", "categories": ["cs.CL", "stat.ML"], "arxiv_url": "http://arxiv.org/abs/1610.04211v2"}, "354": {"ID": 354, "title": "Affect in Tweets Using Experts Model", "authors": ["Mounika Marreddy", "Radhika Mamidi", "Subba Reddy Oota", "Adithya Avvaru"], "published": "2019-03-20T11:10:29Z", "updated": "2019-03-20T11:10:29Z", "abstract": "Estimating the intensity of emotion has gained significance as modern textualinputs in potential applications like social media, e-retail markets,psychology, advertisements etc., carry a lot of emotions, feelings, expressionsalong with its meaning. However, the approaches of traditional sentimentanalysis primarily focuses on classifying the sentiment in general (positive ornegative) or at an aspect level(very positive, low negative, etc.) and cannotexploit the intensity information. Moreover, automatically identifying emotionslike anger, fear, joy, sadness, disgust etc., from text introduces challengingscenarios where single tweet may contain multiple emotions with differentintensities and some emotions may even co-occur in some of the tweets. In thispaper, we propose an architecture, Experts Model, inspired from the standardMixture of Experts (MoE) model. The key idea here is each expert learnsdifferent sets of features from the feature vector which helps in betteremotion detection from the tweet. We compared the results of our Experts Modelwith both baseline results and top five performers of SemEval-2018 Task-1,Affect in Tweets (AIT). The experimental results show that our proposedapproach deals with the emotion detection problem and stands at top-5 results.", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "arxiv_url": "http://arxiv.org/abs/1904.00762v1"}, "355": {"ID": 355, "title": "Consensus Attention-based Neural Networks for Chinese Reading  Comprehension", "authors": ["Zhipeng Chen", "Yiming Cui", "Shijin Wang", "Ting Liu", "Guoping Hu"], "published": "2016-07-08T06:46:48Z", "updated": "2018-03-15T09:21:09Z", "abstract": "Reading comprehension has embraced a booming in recent NLP research. Severalinstitutes have released the Cloze-style reading comprehension data, and thesehave greatly accelerated the research of machine comprehension. In this work,we firstly present Chinese reading comprehension datasets, which consist ofPeople Daily news dataset and Children's Fairy Tale (CFT) dataset. Also, wepropose a consensus attention-based neural network architecture to tackle theCloze-style reading comprehension problem, which aims to induce a consensusattention over every words in the query. Experimental results show that theproposed neural network significantly outperforms the state-of-the-artbaselines in several public datasets. Furthermore, we setup a baseline forChinese reading comprehension task, and hopefully this would speed up theprocess for future research.", "categories": ["cs.CL", "cs.NE"], "arxiv_url": "http://arxiv.org/abs/1607.02250v3"}, "356": {"ID": 356, "title": "Distill, Adapt, Distill: Training Small, In-Domain Models for Neural  Machine Translation", "authors": ["Kevin Duh", "Mitchell A. Gordon"], "published": "2020-03-05T19:14:33Z", "updated": "2020-06-23T17:21:56Z", "abstract": "We explore best practices for training small, memory efficient machinetranslation models with sequence-level knowledge distillation in the domainadaptation setting. While both domain adaptation and knowledge distillation arewidely-used, their interaction remains little understood. Our large-scaleempirical results in machine translation (on three language pairs with threedomains each) suggest distilling twice for best performance: once usinggeneral-domain data and again using in-domain data with an adapted teacher.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/2003.02877v3"}, "357": {"ID": 357, "title": "Subformer: Exploring Weight Sharing for Parameter Efficiency in  Generative Transformers", "authors": ["Machel Reid", "Yutaka Matsuo", "Edison Marrese-Taylor"], "published": "2021-01-01T13:53:22Z", "updated": "2021-01-01T13:53:22Z", "abstract": "The advent of the Transformer can arguably be described as a driving forcebehind many of the recent advances in natural language processing. However,despite their sizeable performance improvements, as recently shown, the modelis severely over-parameterized, being parameter inefficient and computationallyexpensive to train. Inspired by the success of parameter-sharing in pretraineddeep contextualized word representation encoders, we explore parameter-sharingmethods in Transformers, with a specific focus on encoder-decoder models forsequence-to-sequence tasks such as neural machine translation. We perform ananalysis of different parameter sharing/reduction methods and develop theSubformer, a parameter efficient Transformer-based model which combines thenewly proposed Sandwich-style parameter sharing technique - designed toovercome the deficiencies in naive cross-layer parameter sharing for generativemodels - and self-attentive embedding factorization (SAFE). Experiments onmachine translation, abstractive summarization, and language modeling show thatthe Subformer can outperform the Transformer even when using significantlyfewer parameters.", "categories": ["cs.CL", "cs.LG"], "arxiv_url": "http://arxiv.org/abs/2101.00234v1"}, "358": {"ID": 358, "title": "The CLaC Discourse Parser at CoNLL-2015", "authors": ["Majid Laali", "Leila Kosseim", "Elnaz Davoodi"], "published": "2017-08-19T14:43:50Z", "updated": "2017-08-19T14:43:50Z", "abstract": "This paper describes our submission (kosseim15) to the CoNLL-2015 shared taskon shallow discourse parsing. We used the UIMA framework to develop our parserand used ClearTK to add machine learning functionality to the UIMA framework.Overall, our parser achieves a result of 17.3 F1 on the identification ofdiscourse relations on the blind CoNLL-2015 test set, ranking in sixth place.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1708.05857v1"}, "359": {"ID": 359, "title": "A Geometry-Inspired Attack for Generating Natural Language Adversarial  Examples", "authors": ["Zhao Meng", "Roger Wattenhofer"], "published": "2020-10-03T12:58:47Z", "updated": "2020-10-03T12:58:47Z", "abstract": "Generating adversarial examples for natural language is hard, as naturallanguage consists of discrete symbols, and examples are often of variablelengths. In this paper, we propose a geometry-inspired attack for generatingnatural language adversarial examples. Our attack generates adversarialexamples by iteratively approximating the decision boundary of Deep NeuralNetworks (DNNs). Experiments on two datasets with two different models showthat our attack fools natural language models with high success rates, whileonly replacing a few words. Human evaluation shows that adversarial examplesgenerated by our attack are hard for humans to recognize. Further experimentsshow that adversarial training can improve model robustness against our attack.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/2010.01345v1"}, "360": {"ID": 360, "title": "Multilingual Hierarchical Attention Networks for Document Classification", "authors": ["Nikolaos Pappas", "Andrei Popescu-Belis"], "published": "2017-07-04T10:28:04Z", "updated": "2017-09-15T10:47:26Z", "abstract": "Hierarchical attention networks have recently achieved remarkable performancefor document classification in a given language. However, when multilingualdocument collections are considered, training such models separately for eachlanguage entails linear parameter growth and lack of cross-language transfer.Learning a single multilingual model with fewer parameters is therefore achallenging but potentially beneficial objective. To this end, we proposemultilingual hierarchical attention networks for learning document structures,with shared encoders and/or shared attention mechanisms across languages, usingmulti-task learning and an aligned semantic space as input. We evaluate theproposed models on multilingual document classification with disjoint labelsets, on a large dataset which we provide, with 600k news documents in 8languages, and 5k labels. The multilingual models outperform monolingual onesin low-resource as well as full-resource settings, and use fewer parameters,thus confirming their computational efficiency and the utility ofcross-language transfer.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1707.00896v4"}, "361": {"ID": 361, "title": "Deformable Stacked Structure for Named Entity Recognition", "authors": ["Xipeng Qiu", "Shuyang Cao", "Xuanjing Huang"], "published": "2018-09-24T02:42:40Z", "updated": "2018-09-28T07:37:12Z", "abstract": "Neural architecture for named entity recognition has achieved great successin the field of natural language processing. Currently, the dominatingarchitecture consists of a bi-directional recurrent neural network (RNN) as theencoder and a conditional random field (CRF) as the decoder. In this paper, wepropose a deformable stacked structure for named entity recognition, in whichthe connections between two adjacent layers are dynamically established. Weevaluate the deformable stacked structure by adapting it to different layers.Our model achieves the state-of-the-art performances on the OntoNotes dataset.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1809.08730v2"}, "362": {"ID": 362, "title": "Extracting Temporal and Causal Relations between Events", "authors": ["Paramita Mirza"], "published": "2016-04-27T15:52:18Z", "updated": "2016-04-27T15:52:18Z", "abstract": "Structured information resulting from temporal information processing iscrucial for a variety of natural language processing tasks, for instance togenerate timeline summarization of events from news documents, or to answertemporal/causal-related questions about some events. In this thesis we presenta framework for an integrated temporal and causal relation extraction system.We first develop a robust extraction component for each type of relations, i.e.temporal order and causality. We then combine the two extraction componentsinto an integrated relation extraction system, CATENA---CAusal and Temporalrelation Extraction from NAtural language texts---, by utilizing thepresumption about event precedence in causality, that causing events musthappened BEFORE resulting events. Several resources and techniques to improveour relation extraction systems are also discussed, including word embeddingsand training data expansion. Finally, we report our adaptation efforts oftemporal information processing for languages other than English, namelyItalian and Indonesian.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1604.08120v1"}, "363": {"ID": 363, "title": "Frame-Based Continuous Lexical Semantics through Exponential Family  Tensor Factorization and Semantic Proto-Roles", "authors": ["Francis Ferraro", "Ryan Cotterell", "Adam Poliak", "Benjamin Van Durme"], "published": "2017-06-29T03:19:39Z", "updated": "2017-06-29T03:19:39Z", "abstract": "We study how different frame annotations complement one another when learningcontinuous lexical semantics. We learn the representations from a tensorizedskip-gram model that consistently encodes syntactic-semantic content better,with multiple 10% gains over baselines.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1706.09562v1"}, "364": {"ID": 364, "title": "Reasoning with Memory Augmented Neural Networks for Language  Comprehension", "authors": ["Hong Yu", "Tsendsuren Munkhdalai"], "published": "2016-10-20T15:17:04Z", "updated": "2017-02-28T17:06:17Z", "abstract": "Hypothesis testing is an important cognitive process that supports humanreasoning. In this paper, we introduce a computational hypothesis testingapproach based on memory augmented neural networks. Our approach involves ahypothesis testing loop that reconsiders and progressively refines a previouslyformed hypothesis in order to generate new hypotheses to test. We apply theproposed approach to language comprehension task by using Neural SemanticEncoders (NSE). Our NSE models achieve the state-of-the-art results showing anabsolute improvement of 1.2% to 2.6% accuracy over previous results obtained bysingle and ensemble systems on standard machine comprehension benchmarks suchas the Children's Book Test (CBT) and Who-Did-What (WDW) news article datasets.", "categories": ["cs.CL", "cs.AI", "cs.NE", "stat.ML"], "arxiv_url": "http://arxiv.org/abs/1610.06454v2"}, "365": {"ID": 365, "title": "Explaining Sequence-Level Knowledge Distillation as Data-Augmentation  for Neural Machine Translation", "authors": ["Kevin Duh", "Mitchell A. Gordon"], "published": "2019-12-06T20:27:38Z", "updated": "2019-12-06T20:27:38Z", "abstract": "Sequence-level knowledge distillation (SLKD) is a model compression techniquethat leverages large, accurate teacher models to train smaller,under-parameterized student models. Why does pre-processing MT data with SLKDhelp us train smaller models? We test the common hypothesis that SLKD addressesa capacity deficiency in students by \"simplifying\" noisy data points and findit unlikely in our case. Models trained on concatenations of original and\"simplified\" datasets generalize just as well as baseline SLKD. We then proposean alternative hypothesis under the lens of data augmentation andregularization. We try various augmentation strategies and observe that dropoutregularization can become unnecessary. Our methods achieve BLEU gains of0.7-1.2 on TED Talks.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1912.03334v1"}, "366": {"ID": 366, "title": "NTUA-SLP at SemEval-2018 Task 3: Tracking Ironic Tweets using Ensembles  of Word and Character Level Attentive RNNs", "authors": ["Athanasia Kolovou", "Nikos Athanasiou", "Nikolaos Ellinas", "Alexandros Potamianos", "Georgios Paraskevopoulos", "Pinelopi Papalampidi", "Christos Baziotis"], "published": "2018-04-18T11:35:56Z", "updated": "2018-04-18T11:35:56Z", "abstract": "In this paper we present two deep-learning systems that competed atSemEval-2018 Task 3 \"Irony detection in English tweets\". We design and ensembletwo independent models, based on recurrent neural networks (Bi-LSTM), whichoperate at the word and character level, in order to capture both the semanticand syntactic information in tweets. Our models are augmented with aself-attention mechanism, in order to identify the most informative words. Theembedding layer of our word-level model is initialized with word2vec wordembeddings, pretrained on a collection of 550 million English tweets. We didnot utilize any handcrafted features, lexicons or external datasets as priorinformation and our models are trained end-to-end using back propagation onconstrained data. Furthermore, we provide visualizations of tweets withannotations for the salient tokens of the attention layer that can help tointerpret the inner workings of the proposed models. We ranked 2nd out of 42teams in Subtask A and 2nd out of 31 teams in Subtask B. However,post-task-completion enhancements of our models achieve state-of-the-artresults ranking 1st for both subtasks.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1804.06659v1"}, "367": {"ID": 367, "title": "Emoji-Powered Representation Learning for Cross-Lingual Sentiment  Classification", "authors": ["Sheng Shen", "Ziniu Hu", "Qiaozhu Mei", "Zhenpeng Chen", "Xuanzhe Liu", "Xuan Lu"], "published": "2018-06-07T08:23:11Z", "updated": "2019-03-25T07:16:15Z", "abstract": "Sentiment classification typically relies on a large amount of labeled data.In practice, the availability of labels is highly imbalanced among differentlanguages, e.g., more English texts are labeled than texts in any otherlanguages, which creates a considerable inequality in the quality of relatedinformation services received by users speaking different languages. To tacklethis problem, cross-lingual sentiment classification approaches aim to transferknowledge learned from one language that has abundant labeled examples (i.e.,the source language, usually English) to another language with fewer labels(i.e., the target language). The source and the target languages are usuallybridged through off-the-shelf machine translation tools. Through such achannel, cross-language sentiment patterns can be successfully learned fromEnglish and transferred into the target languages. This approach, however,often fails to capture sentiment knowledge specific to the target language, andthus compromises the accuracy of the downstream classification task. In thispaper, we employ emojis, which are widely available in many languages, as a newchannel to learn both the cross-language and the language-specific sentimentpatterns. We propose a novel representation learning method that uses emojiprediction as an instrument to learn respective sentiment-aware representationsfor each language. The learned representations are then integrated tofacilitate cross-lingual sentiment classification. The proposed methoddemonstrates state-of-the-art performance on benchmark datasets, which issustained even when sentiment labels are scarce.", "categories": ["cs.IR"], "arxiv_url": "http://arxiv.org/abs/1806.02557v2"}, "368": {"ID": 368, "title": "AdvExpander: Generating Natural Language Adversarial Examples by  Expanding Text", "authors": ["Jiyong Zhang", "Zhihong Shao", "Zhongqin Wu", "Minlie Huang", "Zitao Liu"], "published": "2020-12-18T13:50:17Z", "updated": "2020-12-18T13:50:17Z", "abstract": "Adversarial examples are vital to expose the vulnerability of machinelearning models. Despite the success of the most popular substitution-basedmethods which substitutes some characters or words in the original examples,only substitution is insufficient to uncover all robustness issues of models.In this paper, we present AdvExpander, a method that crafts new adversarialexamples by expanding text, which is complementary to previoussubstitution-based methods. We first utilize linguistic rules to determinewhich constituents to expand and what types of modifiers to expand with. Wethen expand each constituent by inserting an adversarial modifier searched froma CVAE-based generative model which is pre-trained on a large scale corpus. Tosearch adversarial modifiers, we directly search adversarial latent codes inthe latent space without tuning the pre-trained parameters. To ensure that ouradversarial examples are label-preserving for text matching, we also constrainthe modifications with a heuristic rule. Experiments on three classificationtasks verify the effectiveness of AdvExpander and the validity of ouradversarial examples. AdvExpander crafts a new type of adversarial examples bytext expansion, thereby promising to reveal new robustness issues.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/2012.10235v1"}, "369": {"ID": 369, "title": "DeepStance at SemEval-2016 Task 6: Detecting Stance in Tweets Using  Character and Word-Level CNNs", "authors": ["Deb Roy", "Ivan Sysoev", "Prashanth Vijayaraghavan", "Soroush Vosoughi"], "published": "2016-06-17T22:32:50Z", "updated": "2016-06-17T22:32:50Z", "abstract": "This paper describes our approach for the Detecting Stance in Tweets task(SemEval-2016 Task 6). We utilized recent advances in short text categorizationusing deep learning to create word-level and character-level models. The choicebetween word-level and character-level models in each particular case wasinformed through validation performance. Our final system is a combination ofclassifiers using word-level or character-level models. We also employed noveldata augmentation techniques to expand and diversify our training dataset, thusmaking our system more robust. Our system achieved a macro-average precision,recall and F1-scores of 0.67, 0.61 and 0.635 respectively.", "categories": ["cs.CL", "cs.SI"], "arxiv_url": "http://arxiv.org/abs/1606.05694v1"}, "370": {"ID": 370, "title": "Finding Function in Form: Compositional Character Models for Open  Vocabulary Word Representation", "authors": ["Silvio Amir", "Lu\u00eds Marujo", "Isabel Trancoso", "Alan W. Black", "Ram\u00f3n Fernandez Astudillo", "Wang Ling", "Chris Dyer", "Tiago Lu\u00eds"], "published": "2015-08-09T23:41:38Z", "updated": "2016-05-23T20:57:19Z", "abstract": "We introduce a model for constructing vector representations of words bycomposing characters using bidirectional LSTMs. Relative to traditional wordrepresentation models that have independent vectors for each word type, ourmodel requires only a single vector per character type and a fixed set ofparameters for the compositional model. Despite the compactness of this modeland, more importantly, the arbitrary nature of the form-function relationshipin language, our \"composed\" word representations yield state-of-the-art resultsin language modeling and part-of-speech tagging. Benefits over traditionalbaselines are particularly pronounced in morphologically rich languages (e.g.,Turkish).", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1508.02096v2"}, "371": {"ID": 371, "title": "Towards Neural Machine Translation with Latent Tree Attention", "authors": ["Richard Socher", "James Bradbury"], "published": "2017-09-06T17:44:53Z", "updated": "2017-09-06T17:44:53Z", "abstract": "Building models that take advantage of the hierarchical structure of languagewithout a priori annotation is a longstanding goal in natural languageprocessing. We introduce such a model for the task of machine translation,pairing a recurrent neural network grammar encoder with a novel attentionalRNNG decoder and applying policy gradient reinforcement learning to induceunsupervised tree structures on both the source and target. When trained oncharacter-level datasets with no explicit segmentation or parse annotation, themodel learns a plausible segmentation and shallow parse, obtaining performanceclose to an attentional baseline.", "categories": ["cs.CL", "cs.AI"], "arxiv_url": "http://arxiv.org/abs/1709.01915v1"}, "372": {"ID": 372, "title": "FEUP at SemEval-2017 Task 5: Predicting Sentiment Polarity and Intensity  with Financial Word Embeddings", "authors": ["Eduarda Mendes Rodrigues", "Eug\u00e9nio Oliveira", "Carlos Soares", "Pedro Saleiro"], "published": "2017-04-17T18:48:00Z", "updated": "2017-04-17T18:48:00Z", "abstract": "This paper presents the approach developed at the Faculty of Engineering ofUniversity of Porto, to participate in SemEval 2017, Task 5: Fine-grainedSentiment Analysis on Financial Microblogs and News. The task consisted inpredicting a real continuous variable from -1.0 to +1.0 representing thepolarity and intensity of sentiment concerning companies/stocks mentioned inshort texts. We modeled the task as a regression analysis problem and combinedtraditional techniques such as pre-processing short texts, bag-of-wordsrepresentations and lexical-based features with enhanced financial specificbag-of-embeddings. We used an external collection of tweets and news headlinesmentioning companies/stocks from S\\&amp;P 500 to create financial word embeddingswhich are able to capture domain-specific syntactic and semantic similarities.The resulting approach obtained a cosine similarity score of 0.69 in sub-task5.1 - Microblogs and 0.68 in sub-task 5.2 - News Headlines.", "categories": ["cs.CL", "cs.IR"], "arxiv_url": "http://arxiv.org/abs/1704.05091v1"}, "373": {"ID": 373, "title": "Trivial Transfer Learning for Low-Resource Neural Machine Translation", "authors": ["Ond\u0159ej Bojar", "Tom Kocmi"], "published": "2018-09-02T15:24:15Z", "updated": "2018-09-02T15:24:15Z", "abstract": "Transfer learning has been proven as an effective technique for neuralmachine translation under low-resource conditions. Existing methods require acommon target language, language relatedness, or specific training tricks andregimes. We present a simple transfer learning method, where we first train a\"parent\" model for a high-resource language pair and then continue the trainingon a lowresource pair only by replacing the training corpus. This \"child\" modelperforms significantly better than the baseline trained for lowresource paironly. We are the first to show this for targeting different languages, and weobserve the improvements even for unrelated languages with different alphabets.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1809.00357v1"}, "374": {"ID": 374, "title": "Learning the grammar of prescription: recurrent neural network grammars  for medication information extraction in clinical texts", "authors": ["Antoine Neuraz", "Ivan Lerner", "Jordan Jouffroy", "Anita Burgun"], "published": "2020-04-24T09:43:14Z", "updated": "2020-04-24T09:43:14Z", "abstract": "In this study, we evaluated the RNNG, a neural top-down transition basedparser, for medication information extraction in clinical texts. We evaluatedthis model on a French clinical corpus. The task was to extract the name of adrug (or class of drug), as well as fields informing its administration:frequency, dosage, duration, condition and route of administration. We comparedthe RNNG model that jointly identify entities and their relations with separateBiLSTMs models for entities and relations as baselines. We call seq-BiLSTMs thebaseline models for relations extraction that takes as extra-input the outputof the BiLSTMs for entities. RNNG outperforms seq-BiLSTM for identifyingrelations, with on average 88.5% [87.2-89.8] versus 84.6 [83.1-86.1] F-measure.However, RNNG is weaker than the baseline BiLSTM on detecting entities, with onaverage 82.4 [80.8-83.8] versus 84.1 [82.7-85.6] % F- measure. RNNG trainedonly for detecting relations is weaker than RNNG with the joint modellingobjective, 87.4 [85.8-88.8] versus 88.5% [87.2-89.8]. The performance of RNNGon relations can be explained both by the model architecture, which providesshortcut between distant parts of the sentence, and the joint modellingobjective which allow the RNNG to learn richer representations. RNNG isefficient for modeling relations between entities in medical texts and itsperformances are close to those of a BiLSTM for entity detection.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/2004.11622v1"}, "375": {"ID": 375, "title": "Disorder-robust high-field superconducting phase of FeSe single crystals", "authors": ["Y. Meng", "X. L. Yi", "Y. F. Zhang", "C. Y. Xi", "Y. Q. Pan", "L. Pi", "T. Tamegai", "Nan Zhou", "Yue Sun", "Z. S. Wang", "J. J. Feng", "C. Q. Xu", "Zhixiang Shi", "Xiangzhuo Xing"], "published": "2021-02-04T00:57:15Z", "updated": "2021-02-04T00:57:15Z", "abstract": "When exposed to high magnetic fields, certain materials manifest an exoticsuperconducting (SC) phase that attracts considerable attention. A proposedexplanation of the origin of the high-field phase is theFulde-Ferrel-Larkin-Ovchinnikov (FFLO) state. This state is characterized byinhomogeneous superconductivity, where the Cooper pairs have finitecenter-of-mass momenta. Recently, the high-field phase has been observed inFeSe, and it was deemed to originate from the FFLO state. Here, we synthesizedFeSe single crystals with different levels of disorders. The level of disorderis expressed by the ratio of the mean free path to the coherence length andranges between 35 and 1.2. The upper critical field $B_{\\rm{c}2}$ wassystematically studied over a wide range of temperatures, which went as low as$\\sim$ 0.5 K, and magnetic fields, which went up to $\\sim$ 38 T along the $c$axis and in the $ab$ plane. In the high-field region parallel to the $ab$plane, an unusual SC phase was confirmed in all the crystals, and the phase wasfound to be robust to disorders. This result suggests that the high-filed SCstate in FeSe may not be a FFLO state, which should be sensitive to disorders.", "categories": ["cond-mat.supr-con", "cond-mat.str-el"], "arxiv_url": "http://arxiv.org/abs/2102.02353v1"}, "376": {"ID": 376, "title": "Fake News Detection System using XLNet model with Topic Distributions:  CONSTRAINT@AAAI2021 Shared Task", "authors": ["Venktesh V", "Sarah Masud", "Akansha Gautam"], "published": "2021-01-12T16:23:24Z", "updated": "2021-01-12T16:23:24Z", "abstract": "With the ease of access to information, and its rapid dissemination over theinternet (both velocity and volume), it has become challenging to filter outtruthful information from fake ones. The research community is now faced withthe task of automatic detection of fake news, which carries real-worldsocio-political impact. One such research contribution came in the form of theConstraint@AAA12021 Shared Task on COVID19 Fake News Detection in English. Inthis paper, we shed light on a novel method we proposed as a part of thisshared task. Our team introduced an approach to combine topical distributionsfrom Latent Dirichlet Allocation (LDA) with contextualized representations fromXLNet. We also compared our method with existing baselines to show that XLNet +Topic Distributions outperforms other approaches by attaining an F1-score of0.967.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/2101.11425v1"}, "377": {"ID": 377, "title": "A Deep Learning Approach for Automatic Detection of Fake News", "authors": ["Asif Ekbal", "Tanik Saikh", "Arkadipta De", "Pushpak Bhattacharyya"], "published": "2020-05-11T09:07:46Z", "updated": "2020-05-11T09:07:46Z", "abstract": "Fake news detection is a very prominent and essential task in the field ofjournalism. This challenging problem is seen so far in the field of politics,but it could be even more challenging when it is to be determined in themulti-domain platform. In this paper, we propose two effective models based ondeep learning for solving fake news detection problem in online news contentsof multiple domains. We evaluate our techniques on the two recently releaseddatasets, namely FakeNews AMT and Celebrity for fake news detection. Theproposed systems yield encouraging performance, outperforming the currenthandcrafted feature engineering based state-of-the-art system with asignificant margin of 3.08% and 9.3% by the two models, respectively. In orderto exploit the datasets, available for the related tasks, we performcross-domain analysis (i.e. model trained on FakeNews AMT and tested onCelebrity and vice versa) to explore the applicability of our systems acrossthe domains.", "categories": ["cs.CL", "cs.LG"], "arxiv_url": "http://arxiv.org/abs/2005.04938v1"}, "378": {"ID": 378, "title": "Finding Syntax in Human Encephalography with Beam Search", "authors": ["Chris Dyer", "John Hale", "Jonathan R. Brennan", "Adhiguna Kuncoro"], "published": "2018-06-11T17:51:23Z", "updated": "2018-06-11T17:51:23Z", "abstract": "Recurrent neural network grammars (RNNGs) are generative models of(tree,string) pairs that rely on neural networks to evaluate derivationalchoices. Parsing with them using beam search yields a variety of incrementalcomplexity metrics such as word surprisal and parser action count. When used asregressors against human electrophysiological responses to naturalistic text,they derive two amplitude effects: an early peak and a P600-like later peak. Bycontrast, a non-syntactic neural language model yields no reliable effects.Model comparisons attribute the early peak to syntactic composition within theRNNG. This pattern of results recommends the RNNG+beam search combination as amechanistic model of the syntactic processing that occurs during normal humanlanguage comprehension.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1806.04127v1"}, "379": {"ID": 379, "title": "Lexicons and Minimum Risk Training for Neural Machine Translation:  NAIST-CMU at WAT2016", "authors": ["Graham Neubig"], "published": "2016-10-20T19:10:09Z", "updated": "2016-10-20T19:10:09Z", "abstract": "This year, the Nara Institute of Science and Technology (NAIST)/CarnegieMellon University (CMU) submission to the Japanese-English translation track ofthe 2016 Workshop on Asian Translation was based on attentional neural machinetranslation (NMT) models. In addition to the standard NMT model, we make anumber of improvements, most notably the use of discrete translation lexiconsto improve probability estimates, and the use of minimum risk training tooptimize the MT system for BLEU score. As a result, our system achieved thehighest translation evaluation scores for the task.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1610.06542v1"}, "380": {"ID": 380, "title": "SemEval-2015 Task 10: Sentiment Analysis in Twitter", "authors": ["Alan Ritter", "Sara Rosenthal", "Svetlana Kiritchenko", "Preslav Nakov", "Veselin Stoyanov", "Saif M Mohammad"], "published": "2019-12-05T05:08:36Z", "updated": "2019-12-05T05:08:36Z", "abstract": "In this paper, we describe the 2015 iteration of the SemEval shared task onSentiment Analysis in Twitter. This was the most popular sentiment analysisshared task to date with more than 40 teams participating in each of the lastthree years. This year's shared task competition consisted of five sentimentprediction subtasks. Two were reruns from previous years: (A) sentimentexpressed by a phrase in the context of a tweet, and (B) overall sentiment of atweet. We further included three new subtasks asking to predict (C) thesentiment towards a topic in a single tweet, (D) the overall sentiment towardsa topic in a set of tweets, and (E) the degree of prior polarity of a phrase.", "categories": ["cs.CL", "cs.IR", "cs.LG", "68T50", "I.2.7"], "arxiv_url": "http://arxiv.org/abs/1912.02387v1"}, "381": {"ID": 381, "title": "Non-Markovian Control with Gated End-to-End Memory Policy Networks", "authors": ["Julien Perez", "Tomi Silander"], "published": "2017-05-31T09:00:44Z", "updated": "2017-05-31T09:00:44Z", "abstract": "Partially observable environments present an important open challenge in thedomain of sequential control learning with delayed rewards. Despite numerousattempts during the two last decades, the majority of reinforcement learningalgorithms and associated approximate models, applied to this context, stillassume Markovian state transitions. In this paper, we explore the use of arecently proposed attention-based model, the Gated End-to-End Memory Network,for sequential control. We call the resulting model the Gated End-to-End MemoryPolicy Network. More precisely, we use a model-free value-based algorithm tolearn policies for partially observed domains using this memory-enhanced neuralnetwork. This model is end-to-end learnable and it features unbounded memory.Indeed, because of its attention mechanism and associated non-parametricmemory, the proposed model allows us to define an attention mechanism over theobservation stream unlike recurrent models. We show encouraging results thatillustrate the capability of our attention-based model in the context of thecontinuous-state non-stationary control problem of stock trading. We alsopresent an OpenAI Gym environment for simulated stock exchange and explain itsrelevance as a benchmark for the field of non-Markovian decision processlearning.", "categories": ["stat.ML", "cs.AI", "cs.LG", "cs.NE"], "arxiv_url": "http://arxiv.org/abs/1705.10993v1"}, "382": {"ID": 382, "title": "An Argument-Marker Model for Syntax-Agnostic Proto-Role Labeling", "authors": ["Anette Frank", "Juri Opitz"], "published": "2019-02-04T18:11:36Z", "updated": "2019-04-12T07:47:44Z", "abstract": "Semantic proto-role labeling (SPRL) is an alternative to semantic rolelabeling (SRL) that moves beyond a categorical definition of roles, followingDowty's feature-based view of proto-roles. This theory determines agenthood vs.patienthood based on a participant's instantiation of more or less typicalagent vs. patient properties, such as, for example, volition in an event. Toperform SPRL, we develop an ensemble of hierarchical models with self-attentionand concurrently learned predicate-argument-markers. Our method is competitivewith the state-of-the art, overall outperforming previous work in twoformulations of the task (multi-label and multi-variate Likert scaleprediction). In contrast to previous work, our results do not depend on goldargument heads derived from supplementary gold tree banks.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1902.01349v2"}, "383": {"ID": 383, "title": "Sequence-Level Knowledge Distillation for Model Compression of  Attention-based Sequence-to-Sequence Speech Recognition", "authors": ["Koichi Shinoda", "Raden Mu'az Mun'im", "Nakamasa Inoue"], "published": "2018-11-12T02:55:55Z", "updated": "2018-11-12T02:55:55Z", "abstract": "We investigate the feasibility of sequence-level knowledge distillation ofSequence-to-Sequence (Seq2Seq) models for Large Vocabulary Continuous SpeechRecognition (LVSCR). We first use a pre-trained larger teacher model togenerate multiple hypotheses per utterance with beam search. With the sameinput, we then train the student model using these hypotheses generated fromthe teacher as pseudo labels in place of the original ground truth labels. Weevaluate our proposed method using Wall Street Journal (WSJ) corpus. Itachieved up to $ 9.8 \\times$ parameter reduction with accuracy loss of up to7.0\\% word-error rate (WER) increase", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1811.04531v1"}, "384": {"ID": 384, "title": "PlusEmo2Vec at SemEval-2018 Task 1: Exploiting emotion knowledge from  emoji and #hashtags", "authors": ["Pascale Fung", "Peng Xu", "Ji Ho Park"], "published": "2018-04-23T08:30:46Z", "updated": "2018-04-23T08:30:46Z", "abstract": "This paper describes our system that has been submitted to SemEval-2018 Task1: Affect in Tweets (AIT) to solve five subtasks. We focus on modeling bothsentence and word level representations of emotion inside texts through largedistantly labeled corpora with emojis and hashtags. We transfer the emotionalknowledge by exploiting neural network models as feature extractors and usethese representations for traditional machine learning models such as supportvector regression (SVR) and logistic regression to solve the competition tasks.Our system is placed among the Top3 for all subtasks we participated.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1804.08280v1"}, "385": {"ID": 385, "title": "Binarizer at SemEval-2018 Task 3: Parsing dependency and deep learning  for irony detection", "authors": ["Muktabh Mayank Srivastava", "Nishant Nikhil"], "published": "2018-05-03T04:53:06Z", "updated": "2018-05-03T04:53:06Z", "abstract": "In this paper, we describe the system submitted for the SemEval 2018 Task 3(Irony detection in English tweets) Subtask A by the team Binarizer. Ironydetection is a key task for many natural language processing works. Our methodtreats ironical tweets to consist of smaller parts containing differentemotions. We break down tweets into separate phrases using a dependency parser.We then embed those phrases using an LSTM-based neural network model which ispre-trained to predict emoticons for tweets. Finally, we train afully-connected network to achieve classification.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1805.01112v1"}, "386": {"ID": 386, "title": "Multimodal Emoji Prediction", "authors": ["Horacio Saggion", "Miguel Ballesteros", "Francesco Ronzano", "Francesco Barbieri"], "published": "2018-03-06T19:23:24Z", "updated": "2018-04-17T14:02:19Z", "abstract": "Emojis are small images that are commonly included in social media textmessages. The combination of visual and textual content in the same messagebuilds up a modern way of communication, that automatic systems are not used todeal with. In this paper we extend recent advances in emoji prediction byputting forward a multimodal approach that is able to predict emojis inInstagram posts. Instagram posts are composed of pictures together with textswhich sometimes include emojis. We show that these emojis can be predicted byusing the text, but also using the picture. Our main finding is thatincorporating the two synergistic modalities, in a combined model, improvesaccuracy in an emoji prediction task. This result demonstrates that these twomodalities (text and images) encode different information on the use of emojisand therefore can complement each other.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1803.02392v2"}, "387": {"ID": 387, "title": "HFL-RC System at SemEval-2018 Task 11: Hybrid Multi-Aspects Model for  Commonsense Reading Comprehension", "authors": ["Zhipeng Chen", "Wentao Ma", "Yiming Cui", "Ting Liu", "Shijin Wang", "Guoping Hu"], "published": "2018-03-15T09:30:12Z", "updated": "2018-03-15T09:30:12Z", "abstract": "This paper describes the system which got the state-of-the-art results atSemEval-2018 Task 11: Machine Comprehension using Commonsense Knowledge. Inthis paper, we present a neural network called Hybrid Multi-Aspects (HMA)model, which mimic the human's intuitions on dealing with the multiple-choicereading comprehension. In this model, we aim to produce the predictions inmultiple aspects by calculating attention among the text, question and choices,and combine these results for final predictions. Experimental results show thatour HMA model could give substantial improvements over the baseline system andgot the first place on the final test set leaderboard with the accuracy of84.13%.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1803.05655v1"}, "388": {"ID": 388, "title": "Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems", "authors": ["Svetlana Kiritchenko", "Saif M. Mohammad"], "published": "2018-05-11T17:57:40Z", "updated": "2018-05-11T17:57:40Z", "abstract": "Automatic machine learning systems can inadvertently accentuate andperpetuate inappropriate human biases. Past work on examining inappropriatebiases has largely focused on just individual systems. Further, there is nobenchmark dataset for examining inappropriate biases in systems. Here for thefirst time, we present the Equity Evaluation Corpus (EEC), which consists of8,640 English sentences carefully chosen to tease out biases towards certainraces and genders. We use the dataset to examine 219 automatic sentimentanalysis systems that took part in a recent shared task, SemEval-2018 Task 1'Affect in Tweets'. We find that several of the systems show statisticallysignificant bias; that is, they consistently provide slightly higher sentimentintensity predictions for one race or one gender. We make the EEC freelyavailable.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1805.04508v1"}, "389": {"ID": 389, "title": "UMDSub at SemEval-2018 Task 2: Multilingual Emoji Prediction  Multi-channel Convolutional Neural Network on Subword Embedding", "authors": ["Ted Pedersen", "Zhenduo Wang"], "published": "2018-05-25T17:48:20Z", "updated": "2018-05-25T17:48:20Z", "abstract": "This paper describes the UMDSub system that participated in Task 2 ofSemEval-2018. We developed a system that predicts an emoji given the raw textin a English tweet. The system is a Multi-channel Convolutional Neural Networkbased on subword embeddings for the representation of tweets. This modelimproves on character or word based methods by about 2\\%. Our system placed21st of 48 participating systems in the official evaluation.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1805.10274v1"}, "390": {"ID": 390, "title": "Brundlefly at SemEval-2016 Task 12: Recurrent Neural Networks vs. Joint  Inference for Clinical Temporal Information Extraction", "authors": ["Jason Alan Fries"], "published": "2016-06-04T23:22:41Z", "updated": "2016-06-04T23:22:41Z", "abstract": "We submitted two systems to the SemEval-2016 Task 12: Clinical TempEvalchallenge, participating in Phase 1, where we identified text spans of time andevent expressions in clinical notes and Phase 2, where we predicted a relationbetween an event and its parent document creation time.  For temporal entity extraction, we find that a joint inference-based approachusing structured prediction outperforms a vanilla recurrent neural network thatincorporates word embeddings trained on a variety of large clinical documentsets. For document creation time relations, we find that a combination of datecanonicalization and distant supervision rules for predicting relations on bothevents and time expressions improves classification, though gains are limited,likely due to the small scale of training data.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1606.01433v1"}, "391": {"ID": 391, "title": "Are Interpretations Fairly Evaluated? A Definition Driven Pipeline for  Post-Hoc Interpretability", "authors": ["Xia Hu", "Tie Wang", "Yunsong Meng", "Ninghao Liu", "Bo Long"], "published": "2020-09-16T06:38:03Z", "updated": "2020-09-16T06:38:03Z", "abstract": "Recent years have witnessed an increasing number of interpretation methodsbeing developed for improving transparency of NLP models. Meanwhile,researchers also try to answer the question that whether the obtainedinterpretation is faithful in explaining mechanisms behind model prediction?Specifically, (Jain and Wallace, 2019) proposes that \"attention is notexplanation\" by comparing attention interpretation with gradient alternatives.However, it raises a new question that can we safely pick one interpretationmethod as the ground-truth? If not, on what basis can we compare differentinterpretation methods? In this work, we propose that it is crucial to have aconcrete definition of interpretation before we could evaluate faithfulness ofan interpretation. The definition will affect both the algorithm to obtaininterpretation and, more importantly, the metric used in evaluation. Throughboth theoretical and experimental analysis, we find that althoughinterpretation methods perform differently under a certain evaluation metric,such a difference may not result from interpretation quality or faithfulness,but rather the inherent bias of the evaluation metric.", "categories": ["cs.CL", "cs.LG"], "arxiv_url": "http://arxiv.org/abs/2009.07494v1"}, "392": {"ID": 392, "title": "NIHRIO at SemEval-2018 Task 3: A Simple and Accurate Neural Network  Model for Irony Detection in Twitter", "authors": ["Dat Quoc Nguyen", "Xuan-Son Vu", "Thanh Vu", "Michael Trenell", "Michael Catt", "Dai Quoc Nguyen"], "published": "2018-04-02T14:09:01Z", "updated": "2018-04-08T17:34:25Z", "abstract": "This paper describes our NIHRIO system for SemEval-2018 Task 3 \"Ironydetection in English tweets\". We propose to use a simple neural networkarchitecture of Multilayer Perceptron with various types of input featuresincluding: lexical, syntactic, semantic and polarity features. Our systemachieves very high performance in both subtasks of binary and multi-class ironydetection in tweets. In particular, we rank third using the accuracy metric andfifth using the F1 metric. Our code is available athttps://github.com/NIHRIO/IronyDetectionInTwitter", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1804.00520v2"}, "393": {"ID": 393, "title": "Autoregressive Knowledge Distillation through Imitation Learning", "authors": ["Howard Chen", "Alexander Lin", "Jeremy Wohlwend", "Tao Lei"], "published": "2020-09-15T17:43:02Z", "updated": "2020-10-29T00:40:45Z", "abstract": "The performance of autoregressive models on natural language generation taskshas dramatically improved due to the adoption of deep, self-attentivearchitectures. However, these gains have come at the cost of hinderinginference speed, making state-of-the-art models cumbersome to deploy inreal-world, time-sensitive settings. We develop a compression technique forautoregressive models that is driven by an imitation learning perspective onknowledge distillation. The algorithm is designed to address the exposure biasproblem. On prototypical language generation tasks such as translation andsummarization, our method consistently outperforms other distillationalgorithms, such as sequence-level knowledge distillation. Student modelstrained with our method attain 1.4 to 4.8 BLEU/ROUGE points higher than thosetrained from scratch, while increasing inference speed by up to 14 times incomparison to the teacher model.", "categories": ["cs.CL", "cs.LG"], "arxiv_url": "http://arxiv.org/abs/2009.07253v2"}, "394": {"ID": 394, "title": "LightRel SemEval-2018 Task 7: Lightweight and Fast Relation  Classification", "authors": ["G\u00fcnter Neumann", "Tyler Renslow"], "published": "2018-04-19T09:42:01Z", "updated": "2018-04-19T09:42:01Z", "abstract": "We present LightRel, a lightweight and fast relation classifier. Our goal isto develop a high baseline for different relation extraction tasks. By definingonly very few data-internal, word-level features and external knowledge sourcesin the form of word clusters and word embeddings, we train a fast and simplelinear classifier.", "categories": ["cs.CL", "cs.AI"], "arxiv_url": "http://arxiv.org/abs/1804.08426v1"}, "395": {"ID": 395, "title": "UTFPR at SemEval-2019 Task 5: Hate Speech Identification with Recurrent  Neural Networks", "authors": ["Gustavo Henrique Paetzold", "Shervin Malmasi", "Marcos Zampieri"], "published": "2019-04-16T17:41:49Z", "updated": "2019-04-16T17:41:49Z", "abstract": "In this paper we revisit the problem of automatically identifying hate speechin posts from social media. We approach the task using a system based onminimalistic compositional Recurrent Neural Networks (RNN). We tested ourapproach on the SemEval-2019 Task 5: Multilingual Detection of Hate SpeechAgainst Immigrants and Women in Twitter (HatEval) shared task dataset. Thedataset made available by the HatEval organizers contained English and Spanishposts retrieved from Twitter annotated with respect to the presence of hatefulcontent and its target. In this paper we present the results obtained by oursystem in comparison to the other entries in the shared task. Our systemachieved competitive performance ranking 7th in sub-task A out of 62 systems inthe English track.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1904.07839v1"}, "396": {"ID": 396, "title": "Emoji Prediction: Extensions and Benchmarking", "authors": ["Soroush Vosoughi", "Weicheng Ma", "Lili Wang", "Ruibo Liu"], "published": "2020-07-14T22:41:20Z", "updated": "2020-07-14T22:41:20Z", "abstract": "Emojis are a succinct form of language which can express concrete meanings,emotions, and intentions. Emojis also carry signals that can be used to betterunderstand communicative intent. They have become a ubiquitous part of ourdaily lives, making them an important part of understanding user-generatedcontent. The emoji prediction task aims at predicting the proper set of emojisassociated with a piece of text. Through emoji prediction, models can learnrich representations of the communicative intent of the written text. Whileexisting research on the emoji prediction task focus on a small subset of emojitypes closely related to certain emotions, this setting oversimplifies the taskand wastes the expressive power of emojis. In this paper, we extend theexisting setting of the emoji prediction task to include a richer set of emojisand to allow multi-label classification on the task. We propose novel modelsfor multi-class and multi-label emoji prediction based on Transformer networks.We also construct multiple emoji prediction datasets from Twitter usingheuristics. The BERT models achieve state-of-the-art performances on all ourdatasets under all the settings, with relative improvements of 27.21% to236.36% in accuracy, 2.01% to 88.28% in top-5 accuracy and 65.19% to 346.79% inF-1 score, compared to the prior state-of-the-art. Our results demonstrate theefficacy of deep Transformer-based models on the emoji prediction task. We alsorelease our datasets athttps://github.com/hikari-NYU/Emoji_Prediction_Datasets_MMS for futureresearchers.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/2007.07389v1"}, "397": {"ID": 397, "title": "Structural Supervision Improves Learning of Non-Local Grammatical  Dependencies", "authors": ["Ethan Wilcox", "Richard Futrell", "Peng Qian", "Roger Levy", "Miguel Ballesteros"], "published": "2019-03-03T17:08:00Z", "updated": "2019-04-06T17:48:38Z", "abstract": "State-of-the-art LSTM language models trained on large corpora learnsequential contingencies in impressive detail and have been shown to acquire anumber of non-local grammatical dependencies with some success. Here weinvestigate whether supervision with hierarchical structure enhances learningof a range of grammatical dependencies, a question that has previously beenaddressed only for subject-verb agreement. Using controlled experimentalmethods from psycholinguistics, we compare the performance of word-based LSTMmodels versus two models that represent hierarchical structure and deploy it inleft-to-right processing: Recurrent Neural Network Grammars (RNNGs) (Dyer etal., 2016) and a incrementalized version of the Parsing-as-Language-Modelingconfiguration from Chariak et al., (2016). Models are tested on a diverse rangeof configurations for two classes of non-local grammatical dependencies inEnglish---Negative Polarity licensing and Filler--Gap Dependencies. Using thesame training data across models, we find that structurally-supervised modelsoutperform the LSTM, with the RNNG demonstrating best results on both types ofgrammatical dependencies and even learning many of the Island Constraints onthe filler--gap dependency. Structural supervision thus provides dataefficiency advantages over purely string-based training of neural languagemodels in acquiring human-like generalizations about non-local grammaticaldependencies.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1903.00943v2"}, "398": {"ID": 398, "title": "Non-universal Z' from SO(10) GUTs with vector-like family and the origin  of neutrino masses", "authors": ["Vasja Susic", "Steve F. King", "Christian Hohl", "Stefan Antusch"], "published": "2017-12-14T17:52:46Z", "updated": "2018-08-15T09:57:29Z", "abstract": "A $Z'$ gauge boson with mass around the (few) TeV scale is a popular exampleof physics beyond the Standard Model (SM) and can be a fascinating remnant of aGrand Unified Theory (GUT). Recently, $Z'$ models with non-universal couplingsto the SM fermions due to extra vector-like states have received attention aspotential explanations of the present $R_K$, $R_{K^{\\ast}}$ anomalies; thisincludes GUT model proposals based on the $\\mathrm{SO}(10)$ group. In thispaper we further develop GUT models with a flavour non-universal low scale $Z'$and clarify several outstanding issues within them. First, we successfullyincorporate a realistic neutrino sector (with linear and/or inverse low scaleseesaw mechanism), which was so far a missing ingredient. Second, weinvestigate in detail their compatibility with the $R_K$, $R_{K^{\\ast}}$anomalies; we find that the anomalies do not have a consistent explanationwithin such models. Third, we demonstrate that these models have othercompelling phenomenological features; we study the correlations between theflavour violating processes of $\\mu\\to 3e$ and $\\mu$-$e$ conversion in a muonicatom, showing how a GUT imprint could manifest itself in experiments.", "categories": ["hep-ph"], "arxiv_url": "http://arxiv.org/abs/1712.05366v2"}, "399": {"ID": 399, "title": "An Electron Fixed Target Experiment to Search for a New Vector Boson A'  Decaying to e+e-", "authors": ["Rouven Essig", "Natalia Toro", "Philip Schuster", "Bogdan Wojtsekhowski"], "published": "2010-01-15T17:45:21Z", "updated": "2010-01-15T17:45:21Z", "abstract": "We describe an experiment to search for a new vector boson A' with weakcoupling alpha' &gt; 6 x 10^{-8} alpha to electrons (alpha=e^2/4pi) in the massrange 65 MeV &lt; m_A' &lt; 550 MeV. New vector bosons with such small couplingsarise naturally from a small kinetic mixing of the \"dark photon\" A' with thephoton -- one of the very few ways in which new forces can couple to theStandard Model -- and have received considerable attention as an explanation ofvarious dark matter related anomalies. A' bosons are produced by radiation offan electron beam, and could appear as narrow resonances with small productioncross-section in the trident e+e- spectrum. We summarize the experimentalapproach described in a proposal submitted to Jefferson Laboratory's PAC35,PR-10-009. This experiment, the A' Experiment (APEX), uses the electron beam ofthe Continuous Electron Beam Accelerator Facility at Jefferson Laboratory(CEBAF) at energies of ~1-4 GeV incident on 0.5-10% radiation length Tungstenwire mesh targets, and measures the resulting e+e- pairs to search for the A'using the High Resolution Spectrometer and the septum magnet in Hall A. With a~1 month run, APEX will achieve very good sensitivity because the statistics ofe+e- pairs will be ~10,000 times larger in the explored mass range than anyprevious search for the A' boson. These statistics and the excellent massresolution of the spectrometers allow sensitivity to alpha'/alpha one to threeorders of magnitude below current limits, in a region of parameter space ofgreat theoretical and phenomenological interest. Similar experiments could alsobe performed at other facilities, such as the Mainz Microtron.", "categories": ["hep-ph"], "arxiv_url": "http://arxiv.org/abs/1001.2557v1"}, "400": {"ID": 400, "title": "Readout of field induced magnetic anisotropy in a magnetoactive  elastomer", "authors": ["Alexander Brunhuber", "Viktor M. Kalita", "Sergey M. Ryabchenko", "Albert F. Lozenko", "Andrei A. Snarskii", "Andrii V. Bodnaruk", "Mykola M. Kulyk", "Mikhail Shamonin"], "published": "2018-06-28T14:42:49Z", "updated": "2018-06-28T14:42:49Z", "abstract": "It is shown that in external magnetic fields, a uniaxial magnetic anisotropycomes into being in a magnetoactive elastomer (MAE). The magnitude of theinduced uniaxial anisotropy grows with the increasing external magnetic field.The filler particles are immobilized in the matrix if the MAE sample is cooledbelow 220 K, where the anisotropy can be read out. The cooling of the sample isconsidered as an alternative methodological approach to the experimentalinvestigation of the magnetized state of MAEs. The appearance of magneticanisotropy in MAE is associated with restructuring of the filler duringmagnetization, which leads to an additional effective field felt by themagnetization. It is found that the magnitude of the effective magneticanisotropy constant of the MAE is approximately two times larger than itseffective shear modulus in the absence of magnetic field. It is proposed thatthe experimentally observed large (about 40) ratio of the magnetic anisotropyconstant of the filler to the shear modulus of the matrix deserves attentionfor the explanation of magnetic and magnetoelastic properties of MAEs. It maylead to additional rigidity of the elastic subsystem increasing the shearmodulus of the composite material through the magnetomechanical coupling.", "categories": ["cond-mat.mtrl-sci"], "arxiv_url": "http://arxiv.org/abs/1806.11014v1"}, "401": {"ID": 401, "title": "Cross-Lingual Task-Specific Representation Learning for Text  Classification in Resource Poor Languages", "authors": ["Manish Shrivastava", "Nurendra Choudhary", "Rajat Singh"], "published": "2018-06-10T06:09:57Z", "updated": "2018-06-10T06:09:57Z", "abstract": "Neural network models have shown promising results for text classification.However, these solutions are limited by their dependence on the availability ofannotated data.  The prospect of leveraging resource-rich languages to enhance the textclassification of resource-poor languages is fascinating. The performance onresource-poor languages can significantly improve if the resource availabilityconstraints can be offset. To this end, we present a twin Bidirectional LongShort Term Memory (Bi-LSTM) network with shared parameters consolidated by acontrastive loss function (based on a similarity metric). The model learns therepresentation of resource-poor and resource-rich sentences in a common spaceby using the similarity between their assigned annotation tags. Hence, themodel projects sentences with similar tags closer and those with different tagsfarther from each other. We evaluated our model on the classification tasks ofsentiment analysis and emoji prediction for resource-poor languages - Hindi andTelugu and resource-rich languages - English and Spanish. Our modelsignificantly outperforms the state-of-the-art approaches in both the tasksacross all metrics.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1806.03590v1"}, "402": {"ID": 402, "title": "Integer-Programming Ensemble of Temporal-Relations Classifiers", "authors": ["Terri Hoare", "Paula Carroll", "Jakub Marecek", "Catherine Kerr"], "published": "2014-12-05T00:30:09Z", "updated": "2018-07-30T22:15:58Z", "abstract": "The extraction and understanding of temporal events and their relations aremajor challenges in natural language processing. Processing text on asentence-by-sentence or expression-by-expression basis often fails, in part dueto the challenge of capturing the global consistency of the text. We present anensemble method, which reconciles the outputs of multiple classifiers oftemporal expressions across the text using integer programming. Computationalexperiments show that the ensemble improves upon the best individual resultsfrom two recent challenges, SemEval-2013 TempEval-3 (Temporal Annotation) andSemEval-2016 Task 12 (Clinical TempEval).", "categories": ["cs.CL", "cs.LG", "math.OC"], "arxiv_url": "http://arxiv.org/abs/1412.1866v4"}, "403": {"ID": 403, "title": "TweetEval: Unified Benchmark and Comparative Evaluation for Tweet  Classification", "authors": ["Leonardo Neves", "Jose Camacho-Collados", "Luis Espinosa-Anke", "Francesco Barbieri"], "published": "2020-10-23T14:11:04Z", "updated": "2020-10-26T09:14:54Z", "abstract": "The experimental landscape in natural language processing for social media istoo fragmented. Each year, new shared tasks and datasets are proposed, rangingfrom classics like sentiment analysis to irony detection or emoji prediction.Therefore, it is unclear what the current state of the art is, as there is nostandardized evaluation protocol, neither a strong set of baselines trained onsuch domain-specific data. In this paper, we propose a new evaluation framework(TweetEval) consisting of seven heterogeneous Twitter-specific classificationtasks. We also provide a strong set of baselines as starting point, and comparedifferent language modeling pre-training strategies. Our initial experimentsshow the effectiveness of starting off with existing pre-trained genericlanguage models, and continue training them on Twitter corpora.", "categories": ["cs.CL", "cs.SI"], "arxiv_url": "http://arxiv.org/abs/2010.12421v2"}, "404": {"ID": 404, "title": "Unsupervised Recurrent Neural Network Grammars", "authors": ["Alexander M. Rush", "Yoon Kim", "G\u00e1bor Melis", "Chris Dyer", "Lei Yu", "Adhiguna Kuncoro"], "published": "2019-04-07T21:14:43Z", "updated": "2019-08-05T01:21:15Z", "abstract": "Recurrent neural network grammars (RNNG) are generative models of languagewhich jointly model syntax and surface structure by incrementally generating asyntax tree and sentence in a top-down, left-to-right order. Supervised RNNGsachieve strong language modeling and parsing performance, but require anannotated corpus of parse trees. In this work, we experiment with unsupervisedlearning of RNNGs. Since directly marginalizing over the space of latent treesis intractable, we instead apply amortized variational inference. To maximizethe evidence lower bound, we develop an inference network parameterized as aneural CRF constituency parser. On language modeling, unsupervised RNNGsperform as well their supervised counterparts on benchmarks in English andChinese. On constituency grammar induction, they are competitive with recentneural language models that induce tree structures from words through attentionmechanisms.", "categories": ["cs.CL", "stat.ML"], "arxiv_url": "http://arxiv.org/abs/1904.03746v6"}, "405": {"ID": 405, "title": "The Nernst effect from fluctuating pairs in the pseudogap phase", "authors": ["A. Levchenko", "A. A. Varlamov", "M. R. Norman"], "published": "2010-09-12T03:16:04Z", "updated": "2010-09-12T03:16:04Z", "abstract": "The observation of a large Nernst signal in cuprates above thesuperconducting transition temperature has attracted much attention. Apotential explanation is that it originates from superconducting fluctuations.Although the Nernst signal is indeed consistent with gaussian fluctuations foroverdoped cuprates, gaussian theory fails to describe the temperaturedependence seen for underdoped cuprates. Here, we consider the vertexcorrection to gaussian theory resulting from the pseudogap. This yields aNernst signal in good agreement with the data.", "categories": ["cond-mat.supr-con"], "arxiv_url": "http://arxiv.org/abs/1009.2213v1"}, "406": {"ID": 406, "title": "Duluth at SemEval-2019 Task 6: Lexical Approaches to Identify and  Categorize Offensive Tweets", "authors": ["Ted Pedersen"], "published": "2020-07-25T14:56:10Z", "updated": "2020-07-25T14:56:10Z", "abstract": "This paper describes the Duluth systems that participated in SemEval--2019Task 6, Identifying and Categorizing Offensive Language in Social Media(OffensEval). For the most part these systems took traditional Machine Learningapproaches that built classifiers from lexical features found in manuallylabeled training data. However, our most successful system for classifying atweet as offensive (or not) was a rule-based black--list approach, and we alsoexperimented with combining the training data from two different but relatedSemEval tasks. Our best systems in each of the three OffensEval tasks placed inthe middle of the comparative evaluation, ranking 57th of 103 in task A, 39thof 75 in task B, and 44th of 65 in task C.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/2007.12949v1"}, "407": {"ID": 407, "title": "Auto-Encoding Variational Neural Machine Translation", "authors": ["Bryan Eikema", "Wilker Aziz"], "published": "2018-07-27T13:03:06Z", "updated": "2019-05-31T14:00:00Z", "abstract": "We present a deep generative model of bilingual sentence pairs for machinetranslation. The model generates source and target sentences jointly from ashared latent representation and is parameterised by neural networks. Weperform efficient training using amortised variational inference andreparameterised gradients. Additionally, we discuss the statisticalimplications of joint modelling and propose an efficient approximation tomaximum a posteriori decoding for fast test-time predictions. We demonstratethe effectiveness of our model in three machine translation scenarios:in-domain training, mixed-domain training, and learning from a mix ofgold-standard and synthetic data. Our experiments show consistently that ourjoint formulation outperforms conditional modelling (i.e. standard neuralmachine translation) in all such scenarios.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1807.10564v4"}, "408": {"ID": 408, "title": "Sieving Fake News From Genuine: A Synopsis", "authors": ["Abdulaziz Ravshanbekov", "Shahid Alam"], "published": "2019-11-19T19:24:07Z", "updated": "2019-11-19T19:24:07Z", "abstract": "With the rise of social media, it has become easier to disseminate fake newsfaster and cheaper, compared to traditional news media, such as television andnewspapers. Recently this phenomenon has attracted lot of public attention,because it is causing significant social and financial impacts on their livesand businesses. Fake news are responsible for creating false, deceptive,misleading, and suspicious information that can greatly effect the outcome ofan event. This paper presents a synopsis that explains what are fake news withexamples and also discusses some of the current machine learning techniques,specifically natural language processing (NLP) and deep learning, forautomatically predicting and detecting fake news. Based on this synopsis, werecommend that there is a potential of using NLP and deep learning to improveautomatic detection of fake news, but with the right set of data and features.", "categories": ["cs.CR", "cs.CY"], "arxiv_url": "http://arxiv.org/abs/1911.08516v1"}, "409": {"ID": 409, "title": "A Study on Multimodal and Interactive Explanations for Visual Question  Answering", "authors": ["Giedrius Burachas", "Yi Yao", "Jurgen P. Schulze", "Kamran Alipour", "Avi Ziskind"], "published": "2020-03-01T07:54:01Z", "updated": "2020-03-01T07:54:01Z", "abstract": "Explainability and interpretability of AI models is an essential factoraffecting the safety of AI. While various explainable AI (XAI) approaches aimat mitigating the lack of transparency in deep networks, the evidence of theeffectiveness of these approaches in improving usability, trust, andunderstanding of AI systems are still missing. We evaluate multimodalexplanations in the setting of a Visual Question Answering (VQA) task, byasking users to predict the response accuracy of a VQA agent with and withoutexplanations. We use between-subjects and within-subjects experiments to probeexplanation effectiveness in terms of improving user prediction accuracy,confidence, and reliance, among other factors. The results indicate that theexplanations help improve human prediction accuracy, especially in trials whenthe VQA system's answer is inaccurate. Furthermore, we introduce activeattention, a novel method for evaluating causal attentional effects throughintervention by editing attention maps. User explanation ratings are stronglycorrelated with human prediction accuracy and suggest the efficacy of theseexplanations in human-machine AI collaboration tasks.", "categories": ["cs.AI"], "arxiv_url": "http://arxiv.org/abs/2003.00431v1"}, "410": {"ID": 410, "title": "Contrastive Learning of Emoji-based Representations for Resource-Poor  Languages", "authors": ["Manish Shrivastava", "Ishita Bindlish", "Nurendra Choudhary", "Rajat Singh"], "published": "2018-04-03T03:19:45Z", "updated": "2018-04-03T03:19:45Z", "abstract": "The introduction of emojis (or emoticons) in social media platforms has giventhe users an increased potential for expression. We propose a novel methodcalled Classification of Emojis using Siamese Network Architecture (CESNA) tolearn emoji-based representations of resource-poor languages by jointlytraining them with resource-rich languages using a siamese network.  CESNA model consists of twin Bi-directional Long Short-Term Memory RecurrentNeural Networks (Bi-LSTM RNN) with shared parameters joined by a contrastiveloss function based on a similarity metric. The model learns therepresentations of resource-poor and resource-rich language in a common emojispace by using a similarity metric based on the emojis present in sentencesfrom both languages. The model, hence, projects sentences with similar emojiscloser to each other and the sentences with different emojis farther from oneanother. Experiments on large-scale Twitter datasets of resource-rich languages- English and Spanish and resource-poor languages - Hindi and Telugu revealthat CESNA outperforms the state-of-the-art emoji prediction approaches basedon distributional semantics, semantic rules, lexicon lists and deep neuralnetwork representations without shared parameters.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1804.01855v1"}, "411": {"ID": 411, "title": "Syntax-Enhanced Self-Attention-Based Semantic Role Labeling", "authors": ["Luo Si", "Rui Wang", "Yue Zhang"], "published": "2019-10-24T15:05:01Z", "updated": "2019-10-24T15:05:01Z", "abstract": "As a fundamental NLP task, semantic role labeling (SRL) aims to discover thesemantic roles for each predicate within one sentence. This paper investigateshow to incorporate syntactic knowledge into the SRL task effectively. Wepresent different approaches of encoding the syntactic information derived fromdependency trees of different quality and representations; we propose asyntax-enhanced self-attention model and compare it with other two strongbaseline methods; and we conduct experiments with newly published deepcontextualized word representations as well. The experiment results demonstratethat with proper incorporation of the high quality syntactic information, ourmodel achieves a new state-of-the-art performance for the Chinese SRL task onthe CoNLL-2009 dataset.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1910.11204v1"}, "412": {"ID": 412, "title": "Improved Variational Neural Machine Translation by Promoting Mutual  Information", "authors": ["Jiatao Gu", "Xian Li", "Arya D. McCarthy", "Ning Dong"], "published": "2019-09-19T21:16:29Z", "updated": "2019-09-19T21:16:29Z", "abstract": "Posterior collapse plagues VAEs for text, especially for conditional textgeneration with strong autoregressive decoders. In this work, we address thisproblem in variational neural machine translation by explicitly promotingmutual information between the latent variables and the data. Our model extendsthe conditional variational autoencoder (CVAE) with two new ingredients: first,we propose a modified evidence lower bound (ELBO) objective which explicitlypromotes mutual information; second, we regularize the probabilities of thedecoder by mixing an auxiliary factorized distribution which is directlypredicted by the latent variables. We present empirical results on theTransformer architecture and show the proposed model effectively addressedposterior collapse: latent variables are no longer ignored in the presence ofpowerful decoder. As a result, the proposed model yields improved translationquality while demonstrating superior performance in terms of data efficiencyand robustness.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1909.09237v1"}, "413": {"ID": 413, "title": "Neural-Davidsonian Semantic Proto-role Labeling", "authors": ["Benjamin Van Durme", "Ryan Culkin", "Rachel Rudinger", "Adam Teichert", "Sheng Zhang"], "published": "2018-04-21T14:48:56Z", "updated": "2019-08-27T04:12:12Z", "abstract": "We present a model for semantic proto-role labeling (SPRL) using an adaptedbidirectional LSTM encoding strategy that we call \"Neural-Davidsonian\":predicate-argument structure is represented as pairs of hidden statescorresponding to predicate and argument head tokens of the input sequence. Wedemonstrate: (1) state-of-the-art results in SPRL, and (2) that our networknaturally shares parameters between attributes, allowing for learning newattribute types with limited added supervision.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1804.07976v3"}, "414": {"ID": 414, "title": "IIIDYT at IEST 2018: Implicit Emotion Classification With Deep  Contextualized Word Representations", "authors": ["Yutaka Matsuo", "Jorge A. Balazs", "Edison Marrese-Taylor"], "published": "2018-08-27T02:57:42Z", "updated": "2018-09-01T14:37:11Z", "abstract": "In this paper we describe our system designed for the WASSA 2018 ImplicitEmotion Shared Task (IEST), which obtained 2$^{\\text{nd}}$ place out of 26teams with a test macro F1 score of $0.710$. The system is composed of a singlepre-trained ELMo layer for encoding words, a Bidirectional Long-Short MemoryNetwork BiLSTM for enriching word representations with context, a max-poolingoperation for creating sentence representations from said word vectors, and aDense Layer for projecting the sentence representations into label space. Ourofficial submission was obtained by ensembling 6 of these models initializedwith different random seeds. The code for replicating this paper is availableat https://github.com/jabalazs/implicit_emotion.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1808.08672v2"}, "415": {"ID": 415, "title": "NTUA-SLP at SemEval-2018 Task 1: Predicting Affective Content in Tweets  with Deep Attentive RNNs and Transfer Learning", "authors": ["Athanasia Kolovou", "Nikos Athanasiou", "Nikolaos Ellinas", "Alexandros Potamianos", "Alexandra Chronopoulou", "Georgios Paraskevopoulos", "Christos Baziotis", "Shrikanth Narayanan"], "published": "2018-04-18T11:31:06Z", "updated": "2018-04-18T11:31:06Z", "abstract": "In this paper we present deep-learning models that submitted to theSemEval-2018 Task~1 competition: \"Affect in Tweets\". We participated in allsubtasks for English tweets. We propose a Bi-LSTM architecture equipped with amulti-layer self attention mechanism. The attention mechanism improves themodel performance and allows us to identify salient words in tweets, as well asgain insight into the models making them more interpretable. Our model utilizesa set of word2vec word embeddings trained on a large collection of 550 millionTwitter messages, augmented by a set of word affective features. Due to thelimited amount of task-specific training data, we opted for a transfer learningapproach by pretraining the Bi-LSTMs on the dataset of Semeval 2017, Task 4A.The proposed approach ranked 1st in Subtask E \"Multi-Label EmotionClassification\", 2nd in Subtask A \"Emotion Intensity Regression\" and achievedcompetitive results in other subtasks.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1804.06658v1"}, "416": {"ID": 416, "title": "Suggestion Mining from Online Reviews using ULMFiT", "authors": ["Kartik Aggarwal", "Karan Uppal", "Simra Shahid", "Debanjan Mahata", "Laiba Mehnaz", "Rajiv Ratn Shah", "Sarthak Anand", "Yaman Kumar", "Haimin Zhang"], "published": "2019-04-19T04:38:32Z", "updated": "2019-04-19T04:38:32Z", "abstract": "In this paper we present our approach and the system description for Sub TaskA of SemEval 2019 Task 9: Suggestion Mining from Online Reviews and Forums.Given a sentence, the task asks to predict whether the sentence consists of asuggestion or not. Our model is based on Universal Language Model Fine-tuningfor Text Classification. We apply various pre-processing techniques beforetraining the language and the classification model. We further provide detailedanalysis of the results obtained using the trained model. Our team ranked 10thout of 34 participants, achieving an F1 score of 0.7011. We publicly share ourimplementation at https://github.com/isarth/SemEval9_MIDAS", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1904.09076v1"}, "417": {"ID": 417, "title": "MultiWOZ -- A Large-Scale Multi-Domain Wizard-of-Oz Dataset for  Task-Oriented Dialogue Modelling", "authors": ["Tsung-Hsien Wen", "Stefan Ultes", "I\u00f1igo Casanueva", "Pawe\u0142 Budzianowski", "Bo-Hsiang Tseng", "Osman Ramadan", "Milica Ga\u0161i\u0107"], "published": "2018-09-29T23:44:39Z", "updated": "2020-04-20T15:02:43Z", "abstract": "Even though machine learning has become the major scene in dialogue researchcommunity, the real breakthrough has been blocked by the scale of dataavailable. To address this fundamental obstacle, we introduce the Multi-DomainWizard-of-Oz dataset (MultiWOZ), a fully-labeled collection of human-humanwritten conversations spanning over multiple domains and topics. At a size of$10$k dialogues, it is at least one order of magnitude larger than all previousannotated task-oriented corpora. The contribution of this work apart from theopen-sourced dataset labelled with dialogue belief states and dialogue actionsis two-fold: firstly, a detailed description of the data collection procedurealong with a summary of data structure and analysis is provided. The proposeddata-collection pipeline is entirely based on crowd-sourcing without the needof hiring professional annotators; secondly, a set of benchmark results ofbelief tracking, dialogue act and response generation is reported, which showsthe usability of the data and sets a baseline for future studies.", "categories": ["cs.CL"], "arxiv_url": "http://arxiv.org/abs/1810.00278v3"}, "418": {"ID": 418, "title": "Federated Learning for Emoji Prediction in a Mobile Keyboard", "authors": ["Fran\u00e7oise Beaufays", "Rajiv Mathews", "Swaroop Ramaswamy", "Kanishka Rao"], "published": "2019-06-11T00:40:33Z", "updated": "2019-06-11T00:40:33Z", "abstract": "We show that a word-level recurrent neural network can predict emoji fromtext typed on a mobile keyboard. We demonstrate the usefulness of transferlearning for predicting emoji by pretraining the model using a languagemodeling task. We also propose mechanisms to trigger emoji and tune thediversity of candidates. The model is trained using a distributed on-devicelearning framework called federated learning. The federated model is shown toachieve better performance than a server-trained model. This work demonstratesthe feasibility of using federated learning to train production-quality modelsfor natural language understanding tasks while keeping users' data on theirdevices.", "categories": ["cs.CL", "cs.LG"], "arxiv_url": "http://arxiv.org/abs/1906.04329v1"}, "419": {"ID": 419, "title": "VCDM: Leveraging Variational Bi-encoding and Deep Contextualized Word  Representations for Improved Definition Modeling", "authors": ["Machel Reid", "Yutaka Matsuo", "Edison Marrese-Taylor"], "published": "2020-10-07T02:48:44Z", "updated": "2020-10-07T02:48:44Z", "abstract": "In this paper, we tackle the task of definition modeling, where the goal isto learn to generate definitions of words and phrases. Existing approaches forthis task are discriminative, combining distributional and lexical semantics inan implicit rather than direct way. To tackle this issue we propose agenerative model for the task, introducing a continuous latent variable toexplicitly model the underlying relationship between a phrase used within acontext and its definition. We rely on variational inference for estimation andleverage contextualized word embeddings for improved performance. Our approachis evaluated on four existing challenging benchmarks with the addition of twonew datasets, \"Cambridge\" and the first non-English corpus \"Robert\", which werelease to complement our empirical study. Our Variational ContextualDefinition Modeler (VCDM) achieves state-of-the-art performance in terms ofautomatic and human evaluation metrics, demonstrating the effectiveness of ourapproach.", "categories": ["cs.CL", "cs.LG"], "arxiv_url": "http://arxiv.org/abs/2010.03124v1"}, "420": {"ID": 420, "title": "Hierarchical Multi-head Attentive Network for Evidence-aware Fake News  Detection", "authors": ["Kyumin Lee", "Nguyen Vo"], "published": "2021-02-04T15:18:44Z", "updated": "2021-02-04T15:18:44Z", "abstract": "The widespread of fake news and misinformation in various domains rangingfrom politics, economics to public health has posed an urgent need toautomatically fact-check information. A recent trend in fake news detection isto utilize evidence from external sources. However, existing evidence-awarefake news detection methods focused on either only word-level attention orevidence-level attention, which may result in suboptimal performance. In thispaper, we propose a Hierarchical Multi-head Attentive Network to fact-checktextual claims. Our model jointly combines multi-head word-level attention andmulti-head document-level attention, which aid explanation in both word-leveland evidence-level. Experiments on two real-word datasets show that our modeloutperforms seven state-of-the-art baselines. Improvements over baselines arefrom 6\\% to 18\\%. Our source code and datasets are released at\\texttt{\\url{https://github.com/nguyenvo09/EACL2021}}.", "categories": ["cs.AI", "cs.IR"], "arxiv_url": "http://arxiv.org/abs/2102.02680v1"}, "421": {"ID": 421, "title": "Mining the Relationship between Emoji Usage Patterns and Personality", "authors": ["Yuxiao Chen", "Tianran Hu", "Weijian Li", "Jiebo Luo"], "published": "2018-04-14T01:14:28Z", "updated": "2018-04-14T01:14:28Z", "abstract": "Emojis have been widely used in textual communications as a new way to conveynonverbal cues. An interesting observation is the various emoji usage patternsamong different users. In this paper, we investigate the correlation betweenuser personality traits and their emoji usage patterns, particularly on overallamounts and specific preferences. To achieve this goal, we build a largeTwitter dataset which includes 352,245 users and over 1.13 billion tweetsassociated with calculated personality traits and emoji usage patterns. Ourcorrelation and emoji prediction results provide insights into the power ofdiverse personalities that lead to varies emoji usage patterns as well as itspotential in emoji recommendation tasks.", "categories": ["cs.SI"], "arxiv_url": "http://arxiv.org/abs/1804.05143v1"}, "422": {"ID": 422, "title": "An Unsupervised Neural Attention Model for Aspect Extraction", "authors": ["R He", "D Dahlmeier", "HT Ng", "WS Lee"], "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "155", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=7519319500362351570&btnI=1&nossl=1&hl=en&oe=ASCII"}, "423": {"ID": 423, "title": "ASTD: Arabic Sentiment Tweets Dataset", "authors": ["A Atiya", "M Aly", "M Nabil"], "journal": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "136", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9532612034358215906&btnI=1&nossl=1&hl=en&oe=ASCII"}, "424": {"ID": 424, "title": "Reasoning about Quantities in Natural Language", "authors": ["T Vieira", "S Roy", "D Roth"], "journal": "Transactions of the Association for Computational Linguistics 3, 1-13", "citations": "69", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=18301783617107707540&btnI=1&nossl=1&hl=en&oe=ASCII"}, "425": {"ID": 425, "title": "GAKE: Graph Aware Knowledge Embedding.", "authors": ["X Zhu", "J Feng", "M Huang", "Y Yang"], "journal": "COLING, 641-651", "citations": "51", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=15980574807855259662&btnI=1&nossl=1&hl=en&oe=ASCII"}, "426": {"ID": 426, "title": "Convolutional Neural Network for Paraphrase Identification.", "authors": ["W Yin", "H Sch\u00fctze"], "journal": "HLT-NAACL, 901-911", "citations": "182", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6581668875709904765&btnI=1&nossl=1&hl=en&oe=ASCII"}, "427": {"ID": 427, "title": "A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories.", "authors": ["...", "L Vanderwende", "D Parikh", "X He", "N Mostafazadeh", "N Chambers", "D Batra"], "journal": "HLT-NAACL, 839-849", "citations": "172", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=7211914670941662722&btnI=1&nossl=1&hl=en&oe=ASCII"}, "428": {"ID": 428, "title": "NLANGP at SemEval-2016 Task 5: Improving Aspect Based Sentiment Analysis using Neural Network Features", "authors": ["Z Toh", "J Su"], "journal": "Proceedings of the 10th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "81", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=13233354726575074643&btnI=1&nossl=1&hl=en&oe=ASCII"}, "429": {"ID": 429, "title": "Does String-Based Neural MT Learn Source Syntax?", "authors": ["K Knight", "I Padhi", "X Shi"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "185", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=13782051589621719871&btnI=1&nossl=1&hl=en&oe=ASCII"}, "430": {"ID": 430, "title": "Harnessing Context Incongruity for Sarcasm Detection", "authors": ["A Joshi", "P Bhattacharyya", "V Sharma"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "156", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9712143560979450818&btnI=1&nossl=1&hl=en&oe=ASCII"}, "431": {"ID": 431, "title": "Efficient Inference and Structured Learning for Semantic Role Labeling", "authors": ["K Ganchev", "D Das", "O T\u00e4ckstr\u00f6m"], "journal": "Transactions of the Association for Computational Linguistics 3, 29-41", "citations": "72", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=3182097997884702776&btnI=1&nossl=1&hl=en&oe=ASCII"}, "432": {"ID": 432, "title": "Problems in Current Text Simplification Research: New Data Can Help", "authors": ["C Napoles", "W Xu", "C Callison-Burch"], "journal": "Transactions of the Association for Computational Linguistics 3, 283-297", "citations": "148", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9780942806756390190&btnI=1&nossl=1&hl=en&oe=ASCII"}, "433": {"ID": 433, "title": "Hierarchical Attention Networks for Document Classification.", "authors": ["D Yang", "C Dyer", "Z Yang", "AJ Smola", "X He", "EH Hovy"], "journal": "HLT-NAACL, 1480-1489", "citations": "2115", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=803444695585022169&btnI=1&nossl=1&hl=en&oe=ASCII"}, "434": {"ID": 434, "title": "Cross-Lingual Word Embeddings for Low-Resource Language Modeling.", "authors": ["O Adams", "G Neubig", "T Cohn", "S Bird", "AJ Makarucha"], "journal": "EACL (1), 937-947", "citations": "57", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=17999004188807507386&btnI=1&nossl=1&hl=en&oe=ASCII"}, "435": {"ID": 435, "title": "Summarizing Source Code using a Neural Attention Model", "authors": ["I Konstas", "A Cheung", "S Iyer", "L Zettlemoyer"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "174", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=2417292583777494193&btnI=1&nossl=1&hl=en&oe=ASCII"}, "436": {"ID": 436, "title": "Convolutional Neural Networks for Authorship Attribution of Short Texts.", "authors": ["FA Gonz\u00e1lez", "...", "S Sierra", "P Shrestha", "P Rosso", "M Montes-y-G\u00f3mez"], "journal": "EACL (2), 669-674", "citations": "68", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=13209384083187396410&btnI=1&nossl=1&hl=en&oe=ASCII"}, "437": {"ID": 437, "title": "SemEval-2015 Task 4: TimeLine: Cross-Document Event Ordering", "authors": ["...", "I Aldabe", "B Magnini", "M van Erp", "M Speranza", "AL Minard", "E Agirre"], "journal": "Proceedings of the 9th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "66", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=12020250161797671437&btnI=1&nossl=1&hl=en&oe=ASCII"}, "438": {"ID": 438, "title": "Table Filling Multi-Task Recurrent Neural Network for Joint Entity and Relation Extraction.", "authors": ["B Andrassy", "P Gupta", "H Sch\u00fctze"], "journal": "COLING, 2537-2547", "citations": "64", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6609077351347690466&btnI=1&nossl=1&hl=en&oe=ASCII"}, "439": {"ID": 439, "title": "Deep Semantic Role Labeling: What Works and What\u2019s Next", "authors": ["L He", "L Zettlemoyer", "M Lewis", "K Lee"], "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "252", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9859253676765549138&btnI=1&nossl=1&hl=en&oe=ASCII"}, "440": {"ID": 440, "title": "Embeddings for Word Sense Disambiguation: An Evaluation Study", "authors": ["I Iacobacci", "MT Pilehvar", "R Navigli"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "196", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=5242713897441520851&btnI=1&nossl=1&hl=en&oe=ASCII"}, "441": {"ID": 441, "title": "NASARI: a Novel Approach to a Semantically-Aware Representation of Items.", "authors": ["J Camacho-Collados", "MT Pilehvar", "R Navigli"], "journal": "HLT-NAACL, 567-577", "citations": "91", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=16379194240182162231&btnI=1&nossl=1&hl=en&oe=ASCII"}, "442": {"ID": 442, "title": "Natural Questions: A Benchmark for Question Answering Research", "authors": ["A Parikh", "...", "T Kwiatkowski", "J Palomaki", "M Collins", "C Alberti", "O Redfield"], "journal": "Transactions of the Association for Computational Linguistics 7, 453-466", "citations": "149", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=12899990414016819529&btnI=1&nossl=1&hl=en&oe=ASCII"}, "443": {"ID": 443, "title": "A Structural Probe for Finding Syntax in Word Representations.", "authors": ["J Hewitt", "CD Manning"], "journal": "NAACL-HLT (1), 4129-4138", "citations": "130", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=5834204675362303578&btnI=1&nossl=1&hl=en&oe=ASCII"}, "444": {"ID": 444, "title": "Long Short-Term Memory Neural Networks for Chinese Word Segmentation", "authors": ["X Chen", "X Qiu", "C Zhu", "XJ Huang", "P Liu"], "journal": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "197", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9771751330090926827&btnI=1&nossl=1&hl=en&oe=ASCII"}, "445": {"ID": 445, "title": "QCRI: Answer Selection for Community Question Answering-Experiments for Arabic and English", "authors": ["...", "W Gao", "M Nicosia", "H Mubarak", "I Saleh", "S Filice", "A Barr\u00f3n-Cede\u00f1o"], "journal": "Proceedings of the 9th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "56", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14958298988788287379&btnI=1&nossl=1&hl=en&oe=ASCII"}, "446": {"ID": 446, "title": "End-to-end learning of semantic role labeling using recurrent neural networks", "authors": ["W Xu", "J Zhou"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "280", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=13655474225729803169&btnI=1&nossl=1&hl=en&oe=ASCII"}, "447": {"ID": 447, "title": "Sentiment after Translation: A Case-Study on Arabic Social Media Posts.", "authors": ["S Kiritchenko", "S Mohammad", "M Salameh"], "journal": "HLT-NAACL, 767-777", "citations": "95", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4745999733152764522&btnI=1&nossl=1&hl=en&oe=ASCII"}, "448": {"ID": 448, "title": "Symmetric Pattern Based Word Embeddings for Improved Word Similarity Prediction", "authors": ["A Rappoport", "R Schwartz", "R Reichart"], "journal": "Proceedings of the Nineteenth Conference on Computational Natural Language\u00a0\u2026", "citations": "95", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=12034888356372993264&btnI=1&nossl=1&hl=en&oe=ASCII"}, "449": {"ID": 449, "title": "Stanford's Graph-based Neural Dependency Parser at the CoNLL 2017 Shared Task.", "authors": ["T Dozat", "P Qi", "CD Manning"], "journal": "CoNLL Shared Task (2), 20-30", "citations": "125", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4519862694778592405&btnI=1&nossl=1&hl=en&oe=ASCII"}, "450": {"ID": 450, "title": "UWB at SemEval-2016 Task 1: Semantic Textual Similarity using Lexical, Syntactic, and Semantic Information", "authors": ["T Brychc\u00edn", "L Svoboda"], "journal": "Proceedings of the 10th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "53", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4981008408895987365&btnI=1&nossl=1&hl=en&oe=ASCII"}, "451": {"ID": 451, "title": "Webis: An Ensemble for Twitter Sentiment Detection", "authors": ["M B\u00fcchner", "M Potthast", "M Hagen", "B Stein"], "journal": "Proceedings of the 9th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "72", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=16729072362037539395&btnI=1&nossl=1&hl=en&oe=ASCII"}, "452": {"ID": 452, "title": "MultiWOZ-A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling", "authors": ["...", "O Ramadan", "BH Tseng", "TH Wen", "P Budzianowski", "S Ultes", "I Casanueva"], "journal": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "183", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=17669120816018575473&btnI=1&nossl=1&hl=en&oe=ASCII"}, "453": {"ID": 453, "title": "SemEval-2015 Task 10: Sentiment Analysis in Twitter", "authors": ["S Rosenthal", "S Kiritchenko", "P Nakov", "S Mohammad", "V Stoyanov", "A Ritter"], "journal": "Proceedings of the 9th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "368", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8608917526299372820&btnI=1&nossl=1&hl=en&oe=ASCII"}, "454": {"ID": 454, "title": "Not All Contexts Are Created Equal: Better Word Representations with Variable Attention", "authors": ["W Ling", "...", "R Fermandez", "C Dyer", "S Amir", "AW Black", "I Trancoso", "Y Tsvetkov"], "journal": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "113", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=3509334273924700461&btnI=1&nossl=1&hl=en&oe=ASCII"}, "455": {"ID": 455, "title": "Question Answering over Freebase with Multi-Column Convolutional Neural Networks", "authors": ["K Xu", "F Wei", "L Dong", "M Zhou"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "279", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=3253036437378095646&btnI=1&nossl=1&hl=en&oe=ASCII"}, "456": {"ID": 456, "title": "Emergent: a novel data-set for stance classification.", "authors": ["A Vlachos", "W Ferreira"], "journal": "HLT-NAACL, 1163-1168", "citations": "177", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=3335452722201110267&btnI=1&nossl=1&hl=en&oe=ASCII"}, "457": {"ID": 457, "title": "Black Holes and White Rabbits: Metaphor Identification with Visual Features.", "authors": ["D Kiela", "J Maillard", "E Shutova"], "journal": "HLT-NAACL, 160-170", "citations": "93", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=11696824138307344572&btnI=1&nossl=1&hl=en&oe=ASCII"}, "458": {"ID": 458, "title": "Deep multi-task learning with low level tasks supervised at lower layers", "authors": ["A S\u00f8gaard", "Y Goldberg"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "283", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9616702393768893356&btnI=1&nossl=1&hl=en&oe=ASCII"}, "459": {"ID": 459, "title": "Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science", "authors": ["EM Bender", "B Friedman"], "journal": "Transactions of the Association for Computational Linguistics 6, 587-604", "citations": "54", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6068566991664474416&btnI=1&nossl=1&hl=en&oe=ASCII"}, "460": {"ID": 460, "title": "Consensus Attention-based Neural Networks for Chinese Reading Comprehension.", "authors": ["T Liu", "Z Chen", "G Hu", "Y Cui", "S Wang"], "journal": "COLING, 1777-1786", "citations": "56", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=5019621921503344870&btnI=1&nossl=1&hl=en&oe=ASCII"}, "461": {"ID": 461, "title": "Neural Relation Extraction with Selective Attention over Instances", "authors": ["M Sun", "Y Lin", "S Shen", "H Luan", "Z Liu"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "479", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=17165461150433567400&btnI=1&nossl=1&hl=en&oe=ASCII"}, "462": {"ID": 462, "title": "SemEval-2015 Task 13: Multilingual All-Words Sense Disambiguation and Entity Linking", "authors": ["A Moro", "R Navigli"], "journal": "Proceedings of the 9th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "135", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4948575740093984074&btnI=1&nossl=1&hl=en&oe=ASCII"}, "463": {"ID": 463, "title": "A Language-Independent Neural Network for Event Detection", "authors": ["H Ji", "D Tang", "T Liu", "X Feng", "B Qin", "L Huang"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "135", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=15779343314942934483&btnI=1&nossl=1&hl=en&oe=ASCII"}, "464": {"ID": 464, "title": "SemEval-2016 Task 12: Clinical TempEval", "authors": ["L Derczynski", "S Bethard", "WT Chen", "M Verhagen", "G Savova", "J Pustejovsky"], "journal": "Proceedings of the 10th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "130", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=2577052453111278953&btnI=1&nossl=1&hl=en&oe=ASCII"}, "465": {"ID": 465, "title": "Abstractive Document Summarization with a Graph-Based Attentional Neural Model", "authors": ["J Xiao", "X Wan", "J Tan"], "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "157", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9046905308641382519&btnI=1&nossl=1&hl=en&oe=ASCII"}, "466": {"ID": 466, "title": "Leveraging Linguistic Structure For Open Domain Information Extraction", "authors": ["MJJ Premkumar", "G Angeli", "CD Manning"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "354", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=582222775086856675&btnI=1&nossl=1&hl=en&oe=ASCII"}, "467": {"ID": 467, "title": "Phrase-Based & Neural Unsupervised Machine Translation", "authors": ["L Denoyer", "M Ott", "G Lample", "MA Ranzato", "A Conneau"], "journal": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "277", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=17725098892021008539&btnI=1&nossl=1&hl=en&oe=ASCII"}, "468": {"ID": 468, "title": "Gaussian LDA for Topic Models with Word Embeddings", "authors": ["R Das", "M Zaheer", "C Dyer"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "231", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8202222207400087151&btnI=1&nossl=1&hl=en&oe=ASCII"}, "469": {"ID": 469, "title": "Evaluation of Word Vector Representations by Subspace Alignment", "authors": ["M Faruqui", "W Ling", "G Lample", "C Dyer", "Y Tsvetkov"], "journal": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "120", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=491676849345668538&btnI=1&nossl=1&hl=en&oe=ASCII"}, "470": {"ID": 470, "title": "A Latent Variable Recurrent Neural Network for Discourse-Driven Language Models.", "authors": ["J Eisenstein", "Y Ji", "G Haffari"], "journal": "HLT-NAACL, 332-342", "citations": "102", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=13405935404073150268&btnI=1&nossl=1&hl=en&oe=ASCII"}, "471": {"ID": 471, "title": "Imitation Learning of Agenda-based Semantic Parsers", "authors": ["P Liang", "J Berant"], "journal": "Transactions of the Association for Computational Linguistics 3, 545-558", "citations": "71", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=15301741686560154737&btnI=1&nossl=1&hl=en&oe=ASCII"}, "472": {"ID": 472, "title": "Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks", "authors": ["J Zhao", "K Liu", "Y Chen", "D Zeng"], "journal": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "507", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4994001375608323936&btnI=1&nossl=1&hl=en&oe=ASCII"}, "473": {"ID": 473, "title": "Word Embeddings and Convolutional Neural Network for Arabic Sentiment Classification.", "authors": ["A Dahou", "J Zhou", "S Xiong", "P Duan", "MH Haddoud"], "journal": "COLING, 2418-2427", "citations": "82", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=5817504414843329538&btnI=1&nossl=1&hl=en&oe=ASCII"}, "474": {"ID": 474, "title": "SenticNet 4: A Semantic Resource for Sentiment Analysis Based on Conceptual Primitives.", "authors": ["BW Schuller", "R Bajpai", "S Poria", "E Cambria"], "journal": "COLING, 2666-2677", "citations": "221", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=17529121765199549849&btnI=1&nossl=1&hl=en&oe=ASCII"}, "475": {"ID": 475, "title": "SemEval-2018 Task 11: Machine Comprehension Using Commonsense Knowledge", "authors": ["A Modi", "M Roth", "M Pinkal", "S Thater", "S Ostermann"], "journal": "Proceedings of The 12th International Workshop on Semantic Evaluation, 747-757", "citations": "66", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6908697690910145696&btnI=1&nossl=1&hl=en&oe=ASCII"}, "476": {"ID": 476, "title": "Adverse Drug Reaction Classification With Deep Neural Networks.", "authors": ["A Willis", "Y He", "S Rueger", "T Huynh"], "journal": "COLING, 877-887", "citations": "79", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=17343106453321582668&btnI=1&nossl=1&hl=en&oe=ASCII"}, "477": {"ID": 477, "title": "Event Detection and Domain Adaptation with Convolutional Neural Networks", "authors": ["TH Nguyen", "R Grishman"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "182", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=15408191042472834255&btnI=1&nossl=1&hl=en&oe=ASCII"}, "478": {"ID": 478, "title": "Context-Dependent Sentiment Analysis in User-Generated Videos", "authors": ["E Cambria", "A Zadeh", "LP Morency", "D Hazarika", "S Poria", "N Majumder"], "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "192", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14256888481344725999&btnI=1&nossl=1&hl=en&oe=ASCII"}, "479": {"ID": 479, "title": "context2vec: Learning Generic Context Embedding with Bidirectional LSTM.", "authors": ["I Dagan", "O Melamud", "J Goldberger"], "journal": "CoNLL, 51-61", "citations": "263", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=12961772187114263860&btnI=1&nossl=1&hl=en&oe=ASCII"}, "480": {"ID": 480, "title": "Combination of Convolutional and Recurrent Neural Network for Sentiment Analysis of Short Texts.", "authors": ["Z Luo", "W Jiang", "X Wang"], "journal": "COLING, 2428-2437", "citations": "150", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8334982138717532389&btnI=1&nossl=1&hl=en&oe=ASCII"}, "481": {"ID": 481, "title": "Hubness and Pollution: Delving into Cross-Space Mapping for Zero-Shot Learning", "authors": ["G Dinu", "M Baroni", "A Lazaridou"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "139", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=12363258470502766668&btnI=1&nossl=1&hl=en&oe=ASCII"}, "482": {"ID": 482, "title": "Document Modeling with Gated Recurrent Neural Network for Sentiment Classification", "authors": ["B Qin", "T Liu", "D Tang"], "journal": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "996", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=7210529374798616473&btnI=1&nossl=1&hl=en&oe=ASCII"}, "483": {"ID": 483, "title": "Grammatical error correction using neural machine translation.", "authors": ["T Briscoe", "Z Yuan"], "journal": "HLT-NAACL, 380-386", "citations": "110", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=15573422558526233085&btnI=1&nossl=1&hl=en&oe=ASCII"}, "484": {"ID": 484, "title": "Word Sense Disambiguation: A Unified Evaluation Framework and Empirical Comparison.", "authors": ["J Camacho-Collados", "A Raganato", "R Navigli"], "journal": "EACL (1), 99-110", "citations": "130", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=3497844966999632836&btnI=1&nossl=1&hl=en&oe=ASCII"}, "485": {"ID": 485, "title": "Knowledge Graph Embedding via Dynamic Mapping Matrix", "authors": ["G Ji", "L Xu", "K Liu", "J Zhao", "S He"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "489", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=3519879448275453435&btnI=1&nossl=1&hl=en&oe=ASCII"}, "486": {"ID": 486, "title": "SeNTU: Sentiment Analysis of Tweets by Combining a Rule-based Classifier with Supervised Learning", "authors": ["S Poria", "P Chikersal", "E Cambria"], "journal": "Proceedings of the 9th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "107", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10942374473794880636&btnI=1&nossl=1&hl=en&oe=ASCII"}, "487": {"ID": 487, "title": "SemEval-2019 Task 3: EmoContext Contextual Emotion Detection in Text", "authors": ["KN Narahari", "A Chatterjee", "M Joshi", "P Agrawal"], "journal": "Proceedings of the 13th International Workshop on Semantic Evaluation, 39-48", "citations": "62", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10790628531215250131&btnI=1&nossl=1&hl=en&oe=ASCII"}, "488": {"ID": 488, "title": "Lexicon-Free Conversational Speech Recognition with Neural Networks.", "authors": ["Z Xie", "D Jurafsky", "AY Ng", "AL Maas"], "journal": "HLT-NAACL, 345-354", "citations": "131", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=1488373198442828710&btnI=1&nossl=1&hl=en&oe=ASCII"}, "489": {"ID": 489, "title": "Exploiting Source-side Monolingual Data in Neural Machine Translation", "authors": ["J Zhang", "C Zong"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "133", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=170589138700037149&btnI=1&nossl=1&hl=en&oe=ASCII"}, "490": {"ID": 490, "title": "Attention-based LSTM for Aspect-level Sentiment Classification", "authors": ["X Zhu", "L Zhao", "M Huang", "Y Wang"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "666", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=12485671281254249194&btnI=1&nossl=1&hl=en&oe=ASCII"}, "491": {"ID": 491, "title": "Deep Unordered Composition Rivals Syntactic Methods for Text Classification", "authors": ["H Daum\u00e9 III", "M Iyyer", "J Boyd-Graber", "V Manjunatha"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "533", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10909505697816034580&btnI=1&nossl=1&hl=en&oe=ASCII"}, "492": {"ID": 492, "title": "Sentence Compression by Deletion with LSTMs", "authors": ["O Vinyals", "E Alfonseca", "\u0141 Kaiser", "CA Colmenares", "K Filippova"], "journal": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "219", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=7539839444386811330&btnI=1&nossl=1&hl=en&oe=ASCII"}, "493": {"ID": 493, "title": "A Web-Based Interactive Tool for Creating, Inspecting, Editing, and Publishing Etymological Datasets.", "authors": ["JM List"], "journal": "EACL (Software Demonstrations), 9-12", "citations": "45", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14382370984088405627&btnI=1&nossl=1&hl=en&oe=ASCII"}, "494": {"ID": 494, "title": "Semi-Supervised Word Sense Disambiguation Using Word Embeddings in General and Specific Domains.", "authors": ["HT Ng", "K Taghipour"], "journal": "HLT-NAACL, 314-323", "citations": "97", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9876287789496341461&btnI=1&nossl=1&hl=en&oe=ASCII"}, "495": {"ID": 495, "title": "PPDB 2.0: Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification", "authors": ["B Van Durme", "C Callison-Burch", "J Ganitkevitch", "P Rastogi", "E Pavlick"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "201", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10747207053076559326&btnI=1&nossl=1&hl=en&oe=ASCII"}, "496": {"ID": 496, "title": "What you can cram into a single $ &!#* vector: Probing sentence embeddings for linguistic properties", "authors": ["G Lample", "M Baroni", "G Kruszewski", "A Conneau", "L Barrault"], "journal": "Proceedings of the 56th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "229", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=2465733806395845&btnI=1&nossl=1&hl=en&oe=ASCII"}, "497": {"ID": 497, "title": "SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability", "authors": ["D Cer", "...", "W Guo", "C Cardie", "A Gonz\u00e1lez-Agirre", "M Diab", "C Banea", "E Agirre"], "journal": "Proceedings of the 9th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "269", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=5219836858077923170&btnI=1&nossl=1&hl=en&oe=ASCII"}, "498": {"ID": 498, "title": "SensEmbed: Learning Sense Embeddings for Word and Relational Similarity", "authors": ["I Iacobacci", "MT Pilehvar", "R Navigli"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "255", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9447172529404162843&btnI=1&nossl=1&hl=en&oe=ASCII"}, "499": {"ID": 499, "title": "Specializing Word Embeddings for Similarity or Relatedness", "authors": ["F Hill", "D Kiela", "S Clark"], "journal": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "118", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=5106292840608973207&btnI=1&nossl=1&hl=en&oe=ASCII"}, "500": {"ID": 500, "title": "SemEval-2016 Task 8: Meaning Representation Parsing", "authors": ["J May"], "journal": "Proceedings of the 10th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "55", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=2013414945704558388&btnI=1&nossl=1&hl=en&oe=ASCII"}, "501": {"ID": 501, "title": "DLS@ CU: Sentence Similarity from Word Alignment and Semantic Vector Composition", "authors": ["S Bethard", "MA Sultan", "T Sumner"], "journal": "Proceedings of the 9th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "117", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=535193199880126045&btnI=1&nossl=1&hl=en&oe=ASCII"}, "502": {"ID": 502, "title": "A Latent Variable Model Approach to PMI-based Word Embeddings", "authors": ["Y Liang", "Y Li", "S Arora", "T Ma", "A Risteski"], "journal": "Transactions of the Association for Computational Linguistics 4, 385-399", "citations": "149", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6952315206740225376&btnI=1&nossl=1&hl=en&oe=ASCII"}, "503": {"ID": 503, "title": "Multi-Task Learning for Multiple Language Translation", "authors": ["H Wu", "D Dong", "D Yu", "H Wang", "W He"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "293", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6980356795259585193&btnI=1&nossl=1&hl=en&oe=ASCII"}, "504": {"ID": 504, "title": "Frustratingly Easy Neural Domain Adaptation.", "authors": ["R Sarikaya", "YB Kim", "K Stratos"], "journal": "COLING, 387-396", "citations": "62", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8606066058542891678&btnI=1&nossl=1&hl=en&oe=ASCII"}, "505": {"ID": 505, "title": "SemEval-2018 Task 3: Irony Detection in English Tweets", "authors": ["V Hoste", "C Van Hee", "E Lefever"], "journal": "Proceedings of The 12th International Workshop on Semantic Evaluation, 39-50", "citations": "84", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=11645861572300894747&btnI=1&nossl=1&hl=en&oe=ASCII"}, "506": {"ID": 506, "title": "Simple task-specific bilingual word embeddings.", "authors": ["A S\u00f8gaard", "S Gouws"], "journal": "HLT-NAACL, 1386-1390", "citations": "117", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14768678329188575736&btnI=1&nossl=1&hl=en&oe=ASCII"}, "507": {"ID": 507, "title": "Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0 with UDPipe.", "authors": ["M Straka", "J Strakov\u00e1"], "journal": "CoNLL Shared Task (2), 88-99", "citations": "236", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6402658504571684178&btnI=1&nossl=1&hl=en&oe=ASCII"}, "508": {"ID": 508, "title": "Neural Sentiment Classification with User and Product Attention", "authors": ["M Sun", "Y Lin", "Z Liu", "H Chen", "C Tu"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "191", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=18340181053244626351&btnI=1&nossl=1&hl=en&oe=ASCII"}, "509": {"ID": 509, "title": "Transforming Dependency Structures to Logical Forms for Semantic Parsing", "authors": ["M Steedman", "D Das", "...", "O T\u00e4ckstr\u00f6m", "M Collins", "S Reddy", "T Kwiatkowski"], "journal": "Transactions of the Association for Computational Linguistics 4, 127-140", "citations": "123", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=2752963627553509367&btnI=1&nossl=1&hl=en&oe=ASCII"}, "510": {"ID": 510, "title": "CharNER: Character-Level Named Entity Recognition.", "authors": ["O Kuru", "OA Can", "D Yuret"], "journal": "COLING, 911-921", "citations": "63", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=3112297611130757616&btnI=1&nossl=1&hl=en&oe=ASCII"}, "511": {"ID": 511, "title": "The CoNLL-2015 Shared Task on Shallow Discourse Parsing.", "authors": ["R Prasad", "A Rutherford", "S Pradhan", "HT Ng", "C Bryant", "N Xue"], "journal": "CoNLL Shared Task, 1-16", "citations": "111", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=1078714860005877896&btnI=1&nossl=1&hl=en&oe=ASCII"}, "512": {"ID": 512, "title": "Deep Convolutional Neural Network Textual Features and Multiple Kernel Learning for Utterance-level Multimodal Sentiment Analysis", "authors": ["A Gelbukh", "S Poria", "E Cambria"], "journal": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "276", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=7414544956377621956&btnI=1&nossl=1&hl=en&oe=ASCII"}, "513": {"ID": 513, "title": "A Long Short-Term Memory Model for Answer Sentence Selection in Question Answering", "authors": ["E Nyberg", "D Wang"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "196", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14968335177369644588&btnI=1&nossl=1&hl=en&oe=ASCII"}, "514": {"ID": 514, "title": "Multilingual Projection for Parsing Truly Low-Resource Languages", "authors": ["B Plank", "A Johannsen", "HM Alonso", "N Schluter", "A S\u00f8gaard", "\u017d Agi\u0107"], "journal": "Transactions of the Association for Computational Linguistics 4, 301-312", "citations": "77", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8103489822701107718&btnI=1&nossl=1&hl=en&oe=ASCII"}, "515": {"ID": 515, "title": "SemEval 2016 Task 11: Complex Word Identification", "authors": ["L Specia", "G Paetzold"], "journal": "Proceedings of the 10th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "85", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=17631657917613297716&btnI=1&nossl=1&hl=en&oe=ASCII"}, "516": {"ID": 516, "title": "CoNLL 2016 Shared Task on Multilingual Shallow Discourse Parsing.", "authors": ["A Rutherford", "BL Webber", "S Pradhan", "HT Ng", "C Wang", "H Wang", "N Xue"], "journal": "CoNLL Shared Task, 1-19", "citations": "82", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=16103086253674391479&btnI=1&nossl=1&hl=en&oe=ASCII"}, "517": {"ID": 517, "title": "Fine-grained Opinion Mining with Recurrent Neural Networks and Word Embeddings", "authors": ["P Liu", "H Meng", "S Joty"], "journal": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "219", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8947841023158301570&btnI=1&nossl=1&hl=en&oe=ASCII"}, "518": {"ID": 518, "title": "Optimizing Statistical Machine Translation for Text Simplification", "authors": ["C Napoles", "C Callison-Burch", "Q Chen", "W Xu", "E Pavlick"], "journal": "Transactions of the Association for Computational Linguistics 4, 401-415", "citations": "150", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=2746127501372717002&btnI=1&nossl=1&hl=en&oe=ASCII"}, "519": {"ID": 519, "title": "SemEval-2017 Task 5: Fine-Grained Sentiment Analysis on Financial Microblogs and News", "authors": ["...", "K Cortis", "S Handschuh", "T Daudert", "M Huerlimann", "A Freitas", "M Zarrouk"], "journal": "Proceedings of the 11th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "83", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10855180835354539886&btnI=1&nossl=1&hl=en&oe=ASCII"}, "520": {"ID": 520, "title": "Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classification and Information Retrieval.", "authors": ["L Deng", "YY Wang", "J Gao", "K Duh", "X Liu", "X He"], "journal": "HLT-NAACL, 912-921", "citations": "260", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=3794243435491720263&btnI=1&nossl=1&hl=en&oe=ASCII"}, "521": {"ID": 521, "title": "Semantic Clustering and Convolutional Neural Network for Short Text Categorization", "authors": ["J Xu", "H Zhang", "C Liu", "P Wang", "F Wang", "B Xu", "H Hao"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "155", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=7964952564514425521&btnI=1&nossl=1&hl=en&oe=ASCII"}, "522": {"ID": 522, "title": "Stance Classification of Context-Dependent Claims.", "authors": ["R Bar-Haim", "A Saha", "I Bhattacharya", "N Slonim", "F Dinuzzo"], "journal": "EACL (1), 251-261", "citations": "61", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8939664128374957114&btnI=1&nossl=1&hl=en&oe=ASCII"}, "523": {"ID": 523, "title": "Neural Temporal Relation Extraction.", "authors": ["TA Miller", "C Lin", "D Dligach", "S Bethard", "G Savova"], "journal": "EACL (2), 746-751", "citations": "47", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=13470705519950547723&btnI=1&nossl=1&hl=en&oe=ASCII"}, "524": {"ID": 524, "title": "Do Supervised Distributional Methods Really Learn Lexical Inference Relations?", "authors": ["C Biemann", "S Remus", "I Dagan", "O Levy"], "journal": "HLT-NAACL, 970-976", "citations": "162", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=15593115483662961276&btnI=1&nossl=1&hl=en&oe=ASCII"}, "525": {"ID": 525, "title": "Effective Attention Modeling for Aspect-Level Sentiment Classification.", "authors": ["R He", "D Dahlmeier", "HT Ng", "WS Lee"], "journal": "COLING, 1121-1131", "citations": "62", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=7520339772208872303&btnI=1&nossl=1&hl=en&oe=ASCII"}, "526": {"ID": 526, "title": "Improved Representation Learning for Question Answer Matching", "authors": ["B Zhou", "M Tan", "B Xiang", "C dos Santos"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "175", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=12268511895951931058&btnI=1&nossl=1&hl=en&oe=ASCII"}, "527": {"ID": 527, "title": "Attention-based LSTM Network for Cross-Lingual Sentiment Classification", "authors": ["X Zhou", "X Wan", "J Xiao"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "115", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=12399703926930765572&btnI=1&nossl=1&hl=en&oe=ASCII"}, "528": {"ID": 528, "title": "Farasa: A Fast and Furious Segmenter for Arabic.", "authors": ["N Durrani", "K Darwish", "H Mubarak", "A Abdelali"], "journal": "HLT-NAACL Demos, 11-16", "citations": "133", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9685703041207680571&btnI=1&nossl=1&hl=en&oe=ASCII"}, "529": {"ID": 529, "title": "Cross-lingual Dependency Parsing Based on Distributed Representations", "authors": ["J Guo", "T Liu", "H Wang", "W Che", "D Yarowsky"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "135", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10126744622890329804&btnI=1&nossl=1&hl=en&oe=ASCII"}, "530": {"ID": 530, "title": "Entity Disambiguation with Web Links", "authors": ["A Chisholm", "B Hachey"], "journal": "Transactions of the Association for Computational Linguistics 3, 145-156", "citations": "71", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=12282583002598020254&btnI=1&nossl=1&hl=en&oe=ASCII"}, "531": {"ID": 531, "title": "Inner Attention based Recurrent Neural Networks for Answer Selection", "authors": ["K Liu", "J Zhao", "B Wang"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "143", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=3827685161392564456&btnI=1&nossl=1&hl=en&oe=ASCII"}, "532": {"ID": 532, "title": "Representing Text for Joint Embedding of Text and Knowledge Bases", "authors": ["D Chen", "M Gamon", "P Pantel", "H Poon", "P Choudhury", "K Toutanova"], "journal": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "302", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=15975432028718165516&btnI=1&nossl=1&hl=en&oe=ASCII"}, "533": {"ID": 533, "title": "Paraphrasing Revisited with Neural Machine Translation.", "authors": ["M Lapata", "J Mallinson", "R Sennrich"], "journal": "EACL (1), 881-893", "citations": "87", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=3956889909295156179&btnI=1&nossl=1&hl=en&oe=ASCII"}, "534": {"ID": 534, "title": "Semantic Proto-Roles", "authors": ["R Rudinger", "B Van Durme", "F Ferraro", "K Rawlins", "D Reisinger", "C Harman"], "journal": "Transactions of the Association for Computational Linguistics 3, 475-488", "citations": "57", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=13462154377715144179&btnI=1&nossl=1&hl=en&oe=ASCII"}, "535": {"ID": 535, "title": "Unsupervised Morphology Induction Using Word Embeddings.", "authors": ["FJ Och", "R Soricut"], "journal": "HLT-NAACL, 1627-1637", "citations": "103", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10558631520687401106&btnI=1&nossl=1&hl=en&oe=ASCII"}, "536": {"ID": 536, "title": "Bidirectional RNN for Medical Event Detection in Electronic Health Records.", "authors": ["AN Jagannatha", "H Yu"], "journal": "HLT-NAACL, 473-482", "citations": "185", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8712562754546174779&btnI=1&nossl=1&hl=en&oe=ASCII"}, "537": {"ID": 537, "title": "Learning Continuous Word Embedding with Metadata for Question Retrieval in Community Question Answering", "authors": ["G Zhou", "P Hu", "J Zhao", "T He"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "150", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10000991767196272588&btnI=1&nossl=1&hl=en&oe=ASCII"}, "538": {"ID": 538, "title": "Broad-coverage CCG Semantic Parsing with AMR", "authors": ["Y Artzi", "L Zettlemoyer", "K Lee"], "journal": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "133", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4939010191330835095&btnI=1&nossl=1&hl=en&oe=ASCII"}, "539": {"ID": 539, "title": "JAIST: Combining multiple features for Answer Selection in Community Question Answering", "authors": ["QH Tran", "M Le Nguyen", "T Vu", "SB Pham", "DV Tran"], "journal": "Proceedings of the 9th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "64", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=151707112311168985&btnI=1&nossl=1&hl=en&oe=ASCII"}, "540": {"ID": 540, "title": "An Improved Non-monotonic Transition System for Dependency Parsing", "authors": ["M Honnibal", "M Johnson"], "journal": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "288", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=11753634694782997188&btnI=1&nossl=1&hl=en&oe=ASCII"}, "541": {"ID": 541, "title": "Show Me Your Evidence-an Automatic Method for Context Dependent Evidence Detection", "authors": ["L Dankin", "E Aharoni", "R Rinott", "C Alzate", "MM Khapra", "N Slonim"], "journal": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "145", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=5092979475687031429&btnI=1&nossl=1&hl=en&oe=ASCII"}, "542": {"ID": 542, "title": "SemEval-2017 Task 2: Multilingual and Cross-lingual Semantic Word Similarity", "authors": ["J Camacho-Collados", "N Collier", "MT Pilehvar", "R Navigli"], "journal": "Proceedings of the 11th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "97", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=18020833401613962561&btnI=1&nossl=1&hl=en&oe=ASCII"}, "543": {"ID": 543, "title": "Deep Multilingual Correlation for Improved Word Embeddings.", "authors": ["W Wang", "A Lu", "K Gimpel", "K Livescu", "M Bansal"], "journal": "HLT-NAACL, 250-256", "citations": "125", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=12434087318127673264&btnI=1&nossl=1&hl=en&oe=ASCII"}, "544": {"ID": 544, "title": "TransG: A Generative Model for Knowledge Graph Embedding", "authors": ["H Xiao", "X Zhu", "M Huang"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "166", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=11580378703884335297&btnI=1&nossl=1&hl=en&oe=ASCII"}, "545": {"ID": 545, "title": "Computational Argumentation Quality Assessment in Natural Language.", "authors": ["...", "V Prabhakaran", "TA Thijm", "N Naderi", "H Wachsmuth", "Y Bilu", "G Hirst", "Y Hou"], "journal": "EACL (1), 176-187", "citations": "49", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=12476500077198079316&btnI=1&nossl=1&hl=en&oe=ASCII"}, "546": {"ID": 546, "title": "SemEval-2016 Task 1: Semantic Textual Similarity, Monolingual and Cross-Lingual Evaluation", "authors": ["D Cer", "...", "A Gonz\u00e1lez-Agirre", "M Diab", "R Mihalcea", "C Banea", "E Agirre"], "journal": "Proceedings of the 10th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "238", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=947794178468914539&btnI=1&nossl=1&hl=en&oe=ASCII"}, "547": {"ID": 547, "title": "Building a Semantic Parser Overnight", "authors": ["P Liang", "J Berant", "Y Wang"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "193", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=16957824805439705440&btnI=1&nossl=1&hl=en&oe=ASCII"}, "548": {"ID": 548, "title": "Gated End-to-End Memory Networks.", "authors": ["J Perez", "F Liu"], "journal": "EACL (1), 1-10", "citations": "57", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10505024140982687979&btnI=1&nossl=1&hl=en&oe=ASCII"}, "549": {"ID": 549, "title": "Event Extraction via Dynamic Multi-Pooling Convolutional Neural Networks", "authors": ["L Xu", "Y Chen", "K Liu", "J Zhao", "D Zeng"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "280", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=17570998919712496004&btnI=1&nossl=1&hl=en&oe=ASCII"}, "550": {"ID": 550, "title": "NLANGP: Supervised Machine Learning System for Aspect Category Classification and Opinion Target Extraction", "authors": ["Z Toh", "J Su"], "journal": "Proceedings of the 9th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "62", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=5305395485082676970&btnI=1&nossl=1&hl=en&oe=ASCII"}, "551": {"ID": 551, "title": "Adversarial Training for Unsupervised Bilingual Lexicon Induction", "authors": ["M Sun", "M Zhang", "Y Liu", "H Luan"], "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "164", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=1858752500147406961&btnI=1&nossl=1&hl=en&oe=ASCII"}, "552": {"ID": 552, "title": "Bilingual Word Embeddings from Non-Parallel Document-Aligned Data Applied to Bilingual Lexicon Induction", "authors": ["MF Moens", "I Vuli\u0107"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "135", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=2702498797186846435&btnI=1&nossl=1&hl=en&oe=ASCII"}, "553": {"ID": 553, "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD", "authors": ["P Liang", "P Rajpurkar", "R Jia"], "journal": "Proceedings of the 56th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "465", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14360135060414909274&btnI=1&nossl=1&hl=en&oe=ASCII"}, "554": {"ID": 554, "title": "Universal Sentence Encoder for English.", "authors": ["D Cer", "S Kong", "...", "N Limtiaco", "N Hua", "N Constant", "RS John", "Y Yang"], "journal": "EMNLP (Demonstration), 169-174", "citations": "455", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4837247520021520149&btnI=1&nossl=1&hl=en&oe=ASCII"}, "555": {"ID": 555, "title": "SemEval-2015 Task 3: Answer Selection in Community Question Answering", "authors": ["P Nakov", "B Randeree", "A Moschitti", "J Glass", "L M\u00e0rquez", "W Magdy"], "journal": "Proceedings of the 9th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "71", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14144698777367384073&btnI=1&nossl=1&hl=en&oe=ASCII"}, "556": {"ID": 556, "title": "Harnessing Deep Neural Networks with Logic Rules", "authors": ["E Hovy", "E Xing", "X Ma", "Z Liu", "Z Hu"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "298", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=18126586568111983196&btnI=1&nossl=1&hl=en&oe=ASCII"}, "557": {"ID": 557, "title": "\" Why Should I Trust You?\": Explaining the Predictions of Any Classifier.", "authors": ["S Singh", "MT Ribeiro", "C Guestrin"], "journal": "HLT-NAACL Demos, 97-101", "citations": "3256", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=16724302923321127943&btnI=1&nossl=1&hl=en&oe=ASCII"}, "558": {"ID": 558, "title": "Attention-Based Convolutional Neural Network for Semantic Relation Extraction.", "authors": ["X Huang", "Y Shen"], "journal": "COLING, 2526-2536", "citations": "73", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8474037430395034583&btnI=1&nossl=1&hl=en&oe=ASCII"}, "559": {"ID": 559, "title": "SemEval-2016 Task 7: Determining Sentiment Intensity of English and Arabic Phrases", "authors": ["S Kiritchenko", "S Mohammad", "M Salameh"], "journal": "Proceedings of the 10th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "53", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6331880116861600685&btnI=1&nossl=1&hl=en&oe=ASCII"}, "560": {"ID": 560, "title": "Pulling Out the Stops: Rethinking Stopword Removal for Topic Models.", "authors": ["DM Mimno", "M Magnusson", "A Schofield"], "journal": "EACL (2), 432-436", "citations": "63", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=73719129661474772&btnI=1&nossl=1&hl=en&oe=ASCII"}, "561": {"ID": 561, "title": "An analysis of the user occupational class through Twitter content", "authors": ["D Preo\u0163iuc-Pietro", "V Lampos", "N Aletras"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "156", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9850683538726166887&btnI=1&nossl=1&hl=en&oe=ASCII"}, "562": {"ID": 562, "title": "A Neural Approach to Automated Essay Scoring", "authors": ["HT Ng", "K Taghipour"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "154", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9088522007670862124&btnI=1&nossl=1&hl=en&oe=ASCII"}, "563": {"ID": 563, "title": "Hateful Symbols or Hateful People? Predictive Features for Hate Speech Detection on Twitter.", "authors": ["D Hovy", "Z Waseem"], "journal": "SRW@ HLT-NAACL, 88-93", "citations": "421", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=7712687424659615352&btnI=1&nossl=1&hl=en&oe=ASCII"}, "564": {"ID": 564, "title": "Truth of Varying Shades: Analyzing Language in Fake News and Political Fact-Checking", "authors": ["S Volkova", "E Choi", "H Rashkin", "JY Jang", "Y Choi"], "journal": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "195", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=15709466572308496354&btnI=1&nossl=1&hl=en&oe=ASCII"}, "565": {"ID": 565, "title": "Semantic Parsing via Staged Query Graph Generation: Question Answering with Knowledge Base", "authors": ["J Gao", "X He", "W Yih", "MW Chang"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "381", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10767181883547310789&btnI=1&nossl=1&hl=en&oe=ASCII"}, "566": {"ID": 566, "title": "A Transition-based Algorithm for AMR Parsing.", "authors": ["S Pradhan", "C Wang", "N Xue"], "journal": "HLT-NAACL, 366-375", "citations": "128", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=15030949956404637334&btnI=1&nossl=1&hl=en&oe=ASCII"}, "567": {"ID": 567, "title": "CATENA: CAusal and TEmporal relation extraction from NAtural language texts.", "authors": ["S Tonelli", "P Mirza"], "journal": "COLING, 64-75", "citations": "49", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4030450872317521962&btnI=1&nossl=1&hl=en&oe=ASCII"}, "568": {"ID": 568, "title": "SemEval-2015 Task 6: Clinical TempEval", "authors": ["L Derczynski", "S Bethard", "M Verhagen", "G Savova", "J Pustejovsky"], "journal": "Proceedings of the 9th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "123", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=214662230685077344&btnI=1&nossl=1&hl=en&oe=ASCII"}, "569": {"ID": 569, "title": "Authorship Attribution Using Text Distortion.", "authors": ["E Stamatatos"], "journal": "EACL (1), 1138-1149", "citations": "47", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=11124289359908497580&btnI=1&nossl=1&hl=en&oe=ASCII"}, "570": {"ID": 570, "title": "Parsing Algebraic Word Problems into Equations", "authors": ["O Etzioni", "R Koncel-Kedziorski", "SD Ang", "H Hajishirzi", "A Sabharwal"], "journal": "Transactions of the Association for Computational Linguistics 3, 585-597", "citations": "78", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=17789537233043388070&btnI=1&nossl=1&hl=en&oe=ASCII"}, "571": {"ID": 571, "title": "CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies.", "authors": ["J Hajic", "M Popel", "...", "J Nivre", "J Luotolahti", "M Straka", "F Ginter", "D Zeman"], "journal": "CoNLL Shared Task (2), 1-19", "citations": "230", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=12815994839541487485&btnI=1&nossl=1&hl=en&oe=ASCII"}, "572": {"ID": 572, "title": "A Bayesian Model of Diachronic Meaning Change", "authors": ["M Lapata", "L Frermann"], "journal": "Transactions of the Association for Computational Linguistics 4, 31-45", "citations": "83", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=3491741898317917990&btnI=1&nossl=1&hl=en&oe=ASCII"}, "573": {"ID": 573, "title": "Design Challenges for Entity Linking", "authors": ["S Singh", "DS Weld", "X Ling"], "journal": "Transactions of the Association for Computational Linguistics 3, 315-328", "citations": "133", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8988846884802203350&btnI=1&nossl=1&hl=en&oe=ASCII"}, "574": {"ID": 574, "title": "MPQA 3.0: An Entity/Event-Level Sentiment Corpus.", "authors": ["L Deng", "J Wiebe"], "journal": "HLT-NAACL, 1323-1328", "citations": "99", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4829343776822431294&btnI=1&nossl=1&hl=en&oe=ASCII"}, "575": {"ID": 575, "title": "Cross-lingual syntactic variation over age and gender", "authors": ["A Johannsen", "A S\u00f8gaard", "D Hovy"], "journal": "Proceedings of the Nineteenth Conference on Computational Natural Language\u00a0\u2026", "citations": "68", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=3255971166974528015&btnI=1&nossl=1&hl=en&oe=ASCII"}, "576": {"ID": 576, "title": "SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation", "authors": ["D Cer", "I Lopez-Gazpio", "M Diab", "L Specia", "E Agirre"], "journal": "Proceedings of the 11th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "318", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14846717083380854089&btnI=1&nossl=1&hl=en&oe=ASCII"}, "577": {"ID": 577, "title": "Learning to Generate Product Reviews from Attributes.", "authors": ["L Dong", "M Zhou", "F Wei", "M Lapata", "S Huang", "K Xu"], "journal": "EACL (1), 623-632", "citations": "68", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9888005949533882624&btnI=1&nossl=1&hl=en&oe=ASCII"}, "578": {"ID": 578, "title": "Entity-Centric Coreference Resolution with Model Stacking", "authors": ["K Clark", "CD Manning"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "139", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10023837236144411723&btnI=1&nossl=1&hl=en&oe=ASCII"}, "579": {"ID": 579, "title": "Controlling Politeness in Neural Machine Translation via Side Constraints.", "authors": ["B Haddow", "A Birch", "R Sennrich"], "journal": "HLT-NAACL, 35-40", "citations": "142", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=13603295392629577946&btnI=1&nossl=1&hl=en&oe=ASCII"}, "580": {"ID": 580, "title": "Gated Self-Matching Networks for Reading Comprehension and Question Answering", "authors": ["W Wang", "B Chang", "M Zhou", "F Wei", "N Yang"], "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "409", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=11764833516243377392&btnI=1&nossl=1&hl=en&oe=ASCII"}, "581": {"ID": 581, "title": "Improving Distributional Similarity with Lessons Learned from Word Embeddings", "authors": ["Y Goldberg", "I Dagan", "O Levy"], "journal": "Transactions of the Association for Computational Linguistics 3, 211-225", "citations": "1042", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6441605521554204538&btnI=1&nossl=1&hl=en&oe=ASCII"}, "582": {"ID": 582, "title": "SemEval-2019 Task 5: Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter", "authors": ["C Bosco", "...", "V Basile", "V Patti", "D Nozza", "P Rosso", "FMR Pardo", "E Fersini"], "journal": "Proceedings of the 13th International Workshop on Semantic Evaluation, 54-63", "citations": "95", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6488858378370748976&btnI=1&nossl=1&hl=en&oe=ASCII"}, "583": {"ID": 583, "title": "Learning bilingual word embeddings with (almost) no bilingual data", "authors": ["E Agirre", "G Labaka", "M Artetxe"], "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "261", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=17614535864871662614&btnI=1&nossl=1&hl=en&oe=ASCII"}, "584": {"ID": 584, "title": "Ontologically Grounded Multi-sense Representation Learning for Semantic Vector Space Models.", "authors": ["EH Hovy", "SK Jauhar", "C Dyer"], "journal": "HLT-NAACL, 683-693", "citations": "94", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=17996060919796239485&btnI=1&nossl=1&hl=en&oe=ASCII"}, "585": {"ID": 585, "title": "pkudblab at SemEval-2016 Task 6: A Specific Convolutional Neural Network System for Effective Stance Detection", "authors": ["W Chen", "X Zhang", "W Wei", "X Liu", "T Wang"], "journal": "Proceedings of the 10th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "75", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6535371823627043714&btnI=1&nossl=1&hl=en&oe=ASCII"}, "586": {"ID": 586, "title": "SemEval-2015 Task 14: Analysis of Clinical Text", "authors": ["S Gorman", "S Pradhan", "S Manandhar", "N Elhadad", "G Savova", "W Chapman"], "journal": "Proceedings of the 9th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "63", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9136085614997136854&btnI=1&nossl=1&hl=en&oe=ASCII"}, "587": {"ID": 587, "title": "Abstractive Sentence Summarization with Attentive Recurrent Neural Networks.", "authors": ["M Auli", "AM Rush", "S Chopra"], "journal": "HLT-NAACL, 93-98", "citations": "521", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6926588520799541498&btnI=1&nossl=1&hl=en&oe=ASCII"}, "588": {"ID": 588, "title": "DataStories at SemEval-2017 Task 4: Deep LSTM with Attention for Message-level and Topic-based Sentiment Analysis", "authors": ["C Doulkeridis", "C Baziotis", "N Pelekis"], "journal": "Proceedings of the 11th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "200", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=13360522504249968778&btnI=1&nossl=1&hl=en&oe=ASCII"}, "589": {"ID": 589, "title": "Multi-Perspective Sentence Similarity Modeling with Convolutional Neural Networks", "authors": ["J Lin", "K Gimpel", "H He"], "journal": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "288", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14561806959494861865&btnI=1&nossl=1&hl=en&oe=ASCII"}, "590": {"ID": 590, "title": "Learning Composition Models for Phrase Embeddings", "authors": ["M Dredze", "M Yu"], "journal": "Transactions of the Association for Computational Linguistics 3, 227-242", "citations": "66", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9306700245461787707&btnI=1&nossl=1&hl=en&oe=ASCII"}, "591": {"ID": 591, "title": "Unsupervised Lexicon Discovery from Acoustic Input", "authors": ["J Glass", "C Lee", "TJ O\u2019Donnell"], "journal": "Transactions of the Association for Computational Linguistics 3, 389-403", "citations": "68", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=12688544128695762753&btnI=1&nossl=1&hl=en&oe=ASCII"}, "592": {"ID": 592, "title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't.", "authors": ["S Matsuoka", "A Drozd", "A Gladkova"], "journal": "SRW@ HLT-NAACL, 8-15", "citations": "104", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10713156718870048680&btnI=1&nossl=1&hl=en&oe=ASCII"}, "593": {"ID": 593, "title": "SemEval-2018 Task 7: Semantic Relation Extraction and Classification in Scientific Papers", "authors": ["...", "AK Schumann", "H Zargayouna", "K G\u00e1bor", "B QasemiZadeh", "D Buscaldi"], "journal": "Proceedings of The 12th International Workshop on Semantic Evaluation, 679-688", "citations": "69", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4442292598485443620&btnI=1&nossl=1&hl=en&oe=ASCII"}, "594": {"ID": 594, "title": "Position-aware Attention and Supervised Data Improve Slot Filling", "authors": ["D Chen", "Y Zhang", "G Angeli", "CD Manning", "V Zhong"], "journal": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "141", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4754695376351405236&btnI=1&nossl=1&hl=en&oe=ASCII"}, "595": {"ID": 595, "title": "Relation Classification via Multi-Level Attention CNNs", "authors": ["Z Liu", "L Wang", "G de Melo", "Z Cao"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "240", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=7559528633392124839&btnI=1&nossl=1&hl=en&oe=ASCII"}, "596": {"ID": 596, "title": "Relation Extraction with Multi-instance Multi-label Convolutional Neural Networks.", "authors": ["P Li", "Q Wang", "X Jiang", "B Wang"], "journal": "COLING, 1471-1480", "citations": "52", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10385650909010511956&btnI=1&nossl=1&hl=en&oe=ASCII"}, "597": {"ID": 597, "title": "Evaluation methods for unsupervised word embeddings", "authors": ["I Labutov", "T Schnabel", "T Joachims", "D Mimno"], "journal": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "384", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10052969385855870141&btnI=1&nossl=1&hl=en&oe=ASCII"}, "598": {"ID": 598, "title": "Learning Semantic Representations of Users and Products for Document Level Sentiment Classification", "authors": ["B Qin", "T Liu", "D Tang"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "237", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=7360146302972499127&btnI=1&nossl=1&hl=en&oe=ASCII"}, "599": {"ID": 599, "title": "ConvKN at SemEval-2016 Task 3: Answer and Question Selection for Question Answering on Arabic and English Fora", "authors": ["...", "G Da San Martino", "A Moschitti", "D Bonadiman", "A Barr\u00f3n-Cede\u00f1o", "S Joty"], "journal": "Proceedings of the 10th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "59", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14152241895111332316&btnI=1&nossl=1&hl=en&oe=ASCII"}, "600": {"ID": 600, "title": "Dialogue Act Classification in Domain-Independent Conversations Using a Deep Recurrent Neural Network.", "authors": ["H Khanpour", "N Guntakandla", "RD Nielsen"], "journal": "COLING, 2012-2021", "citations": "65", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9992104254453666997&btnI=1&nossl=1&hl=en&oe=ASCII"}, "601": {"ID": 601, "title": "Beyond Binary Labels: Political Ideology Prediction of Twitter Users", "authors": ["L Ungar", "D Hopkins", "D Preo\u0163iuc-Pietro", "Y Liu"], "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "147", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=10528357759706171406&btnI=1&nossl=1&hl=en&oe=ASCII"}, "602": {"ID": 602, "title": "Word Embeddings, Analogies, and Machine Learning: Beyond king-man+ woman= queen.", "authors": ["S Matsuoka", "A Drozd", "A Gladkova"], "journal": "COLING, 3519-3530", "citations": "68", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=15547944015967423244&btnI=1&nossl=1&hl=en&oe=ASCII"}, "603": {"ID": 603, "title": "Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation", "authors": ["W Ling", "...", "L Marujo", "R Fermandez", "AW Black", "C Dyer", "S Amir", "I Trancoso"], "journal": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "500", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=7226918491616250596&btnI=1&nossl=1&hl=en&oe=ASCII"}, "604": {"ID": 604, "title": "Slot-Gated Modeling for Joint Slot Filling and Intent Prediction.", "authors": ["TC Chen", "G Gao", "CL Huo", "YK Hsu", "CW Goo", "KW Hsu", "YN Chen"], "journal": "NAACL-HLT (2), 753-757", "citations": "99", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14054350730330383135&btnI=1&nossl=1&hl=en&oe=ASCII"}, "605": {"ID": 605, "title": "An Empirical Analysis of Formality in Online Communication", "authors": ["J Tetreault", "E Pavlick"], "journal": "Transactions of the Association for Computational Linguistics 4, 61-74", "citations": "65", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=12790598446952918352&btnI=1&nossl=1&hl=en&oe=ASCII"}, "606": {"ID": 606, "title": "Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification", "authors": ["B Li", "B Xu", "Z Qi", "H Hao", "W Shi", "J Tian", "P Zhou"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "650", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=15798784930800340222&btnI=1&nossl=1&hl=en&oe=ASCII"}, "607": {"ID": 607, "title": "Globally Coherent Text Generation with Neural Checklist Models", "authors": ["Y Choi", "C Kiddon", "L Zettlemoyer"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "133", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=3067085892454087884&btnI=1&nossl=1&hl=en&oe=ASCII"}, "608": {"ID": 608, "title": "SemEval-2018 Task 1: Affect in Tweets", "authors": ["S Kiritchenko", "F Bravo-Marquez", "S Mohammad", "M Salameh"], "journal": "Proceedings of The 12th International Workshop on Semantic Evaluation, 1-17", "citations": "210", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=1842299299495928058&btnI=1&nossl=1&hl=en&oe=ASCII"}, "609": {"ID": 609, "title": "Recurrent Attention Network on Memory for Aspect Sentiment Analysis", "authors": ["Z Sun", "W Yang", "L Bing", "P Chen"], "journal": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "244", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=460792871756569481&btnI=1&nossl=1&hl=en&oe=ASCII"}, "610": {"ID": 610, "title": "Named Entity Recognition for Chinese Social Media with Jointly Trained Embeddings", "authors": ["M Dredze", "N Peng"], "journal": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "115", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6967458699373542027&btnI=1&nossl=1&hl=en&oe=ASCII"}, "611": {"ID": 611, "title": "EmoBank: Studying the Impact of Annotation Perspective and Representation Format on Dimensional Emotion Analysis.", "authors": ["U Hahn", "S Buechel"], "journal": "EACL (2), 578-585", "citations": "72", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8283775930935737882&btnI=1&nossl=1&hl=en&oe=ASCII"}, "612": {"ID": 612, "title": "Hierarchical Recurrent Neural Network for Document Modeling", "authors": ["M Li", "M Yang", "M Zhou", "S Li", "S Liu", "R Lin"], "journal": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "117", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=11571171307365616961&btnI=1&nossl=1&hl=en&oe=ASCII"}, "613": {"ID": 613, "title": "WikiQA: A Challenge Dataset for Open-Domain Question Answering", "authors": ["C Meek", "Y Yang", "W Yih"], "journal": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "415", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=3592917354691644031&btnI=1&nossl=1&hl=en&oe=ASCII"}, "614": {"ID": 614, "title": "CANE: Context-Aware Network Embedding for Relation Modeling", "authors": ["M Sun", "Z Liu", "H Liu", "C Tu"], "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "148", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=15120018809349682462&btnI=1&nossl=1&hl=en&oe=ASCII"}, "615": {"ID": 615, "title": "Evaluating the Stability of Embedding-based Word Similarities", "authors": ["D Mimno", "M Antoniak"], "journal": "Transactions of the Association for Computational Linguistics 6, 107-119", "citations": "57", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=17182825607218520979&btnI=1&nossl=1&hl=en&oe=ASCII"}, "616": {"ID": 616, "title": "Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement.", "authors": ["JJ Lin", "H He"], "journal": "HLT-NAACL, 937-948", "citations": "167", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=12141531829092467813&btnI=1&nossl=1&hl=en&oe=ASCII"}, "617": {"ID": 617, "title": "Latent Structures for Coreference Resolution", "authors": ["S Martschat", "M Strube"], "journal": "Transactions of the Association for Computational Linguistics 3, 405-418", "citations": "66", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=5045439245678879637&btnI=1&nossl=1&hl=en&oe=ASCII"}, "618": {"ID": 618, "title": "Normalized Word Embedding and Orthogonal Transform for Bilingual Word Translation.", "authors": ["Y Lin", "C Xing", "C Liu", "D Wang"], "journal": "HLT-NAACL, 1006-1011", "citations": "271", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4009320309746318198&btnI=1&nossl=1&hl=en&oe=ASCII"}, "619": {"ID": 619, "title": "Predicting Polarities of Tweets by Composing Word Embeddings with Long Short-Term Memory", "authors": ["B Wang", "Y Liu", "X Wang", "CJ Sun"], "journal": "Proceedings of the 53rd Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "176", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8903417329639162771&btnI=1&nossl=1&hl=en&oe=ASCII"}, "620": {"ID": 620, "title": "Tweet Sarcasm Detection Using Deep Neural Network.", "authors": ["G Fu", "Y Zhang", "M Zhang"], "journal": "COLING, 2449-2460", "citations": "66", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4465640599517682561&btnI=1&nossl=1&hl=en&oe=ASCII"}, "621": {"ID": 621, "title": "SwissCheese at SemEval-2016 Task 4: Sentiment Classification Using an Ensemble of Convolutional Neural Networks with Distant Supervision", "authors": ["F Uzdilli", "JM Deriu", "A Lucchi", "V De Luca", "M Jaggi", "M Gonzenbach"], "journal": "Proceedings of the 10th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "101", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=5826535112041129523&btnI=1&nossl=1&hl=en&oe=ASCII"}, "622": {"ID": 622, "title": "Deep Pyramid Convolutional Neural Networks for Text Categorization", "authors": ["T Zhang", "R Johnson"], "journal": "Proceedings of the 55th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "167", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=8616931869965463382&btnI=1&nossl=1&hl=en&oe=ASCII"}, "623": {"ID": 623, "title": "UNITN: Training Deep Convolutional Neural Network for Twitter Sentiment Classification", "authors": ["A Severyn", "A Moschitti"], "journal": "Proceedings of the 9th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "182", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=3024113887161912057&btnI=1&nossl=1&hl=en&oe=ASCII"}, "624": {"ID": 624, "title": "Multi-view Response Selection for Human-Computer Conversation", "authors": ["H Wu", "D Dong", "D Yu", "S Zhao", "X Liu", "R Yan", "X Zhou", "H Tian"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "121", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=34677575862246119&btnI=1&nossl=1&hl=en&oe=ASCII"}, "625": {"ID": 625, "title": "SemEval-2016 Task 6: Detecting Stance in Tweets", "authors": ["S Kiritchenko", "S Mohammad", "X Zhu", "P Sobhani", "C Cherry"], "journal": "Proceedings of the 10th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "326", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=6947069929764374242&btnI=1&nossl=1&hl=en&oe=ASCII"}, "626": {"ID": 626, "title": "SemEval-2015 Task 1: Paraphrase and Semantic Similarity in Twitter (PIT)", "authors": ["W Xu", "B Dolan", "C Callison-Burch"], "journal": "Proceedings of the 9th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "106", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=12716057572614221976&btnI=1&nossl=1&hl=en&oe=ASCII"}, "627": {"ID": 627, "title": "Lsislif: CRF and Logistic Regression for Opinion Target Extraction and Sentiment Polarity Analysis", "authors": ["P Bellot", "F Bechet", "H Hamdan"], "journal": "Proceedings of the 9th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "51", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=17403851893770980221&btnI=1&nossl=1&hl=en&oe=ASCII"}, "628": {"ID": 628, "title": "SemEval-2016 Task 13: Taxonomy Extraction Evaluation (TExEval-2)", "authors": ["G Bordea", "P Buitelaar", "E Lefever"], "journal": "Proceedings of the 10th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "91", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=7119036334597361808&btnI=1&nossl=1&hl=en&oe=ASCII"}, "629": {"ID": 629, "title": "Learning principled bilingual mappings of word embeddings while preserving monolingual invariance", "authors": ["E Agirre", "G Labaka", "M Artetxe"], "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "223", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=5308709105842309671&btnI=1&nossl=1&hl=en&oe=ASCII"}, "630": {"ID": 630, "title": "Not All Character N-grams Are Created Equal: A Study in Authorship Attribution.", "authors": ["S Bethard", "U Sapkota", "M Montes-y-G\u00f3mez", "T Solorio"], "journal": "HLT-NAACL, 93-102", "citations": "137", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=16383055081292289119&btnI=1&nossl=1&hl=en&oe=ASCII"}, "631": {"ID": 631, "title": "KeLP at SemEval-2016 Task 3: Learning Semantic Relations between Questions and Answers", "authors": ["R Basili", "A Moschitti", "D Croce", "S Filice"], "journal": "Proceedings of the 10th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "84", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=7794203105897812496&btnI=1&nossl=1&hl=en&oe=ASCII"}, "632": {"ID": 632, "title": "Joint Event Extraction via Recurrent Neural Networks.", "authors": ["TH Nguyen", "K Cho", "R Grishman"], "journal": "HLT-NAACL, 300-309", "citations": "168", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=293752888727069380&btnI=1&nossl=1&hl=en&oe=ASCII"}, "633": {"ID": 633, "title": "Comparing Apples to Apple: The Effects of Stemmers on Topic Models", "authors": ["D Mimno", "A Schofield"], "journal": "Transactions of the Association for Computational Linguistics 4, 287-300", "citations": "60", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=12105369696330456973&btnI=1&nossl=1&hl=en&oe=ASCII"}, "634": {"ID": 634, "title": "Attention Modeling for Targeted Sentiment.", "authors": ["Y Zhang", "J Liu"], "journal": "EACL (2), 572-577", "citations": "90", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=7580758371697896384&btnI=1&nossl=1&hl=en&oe=ASCII"}, "635": {"ID": 635, "title": "Two/Too Simple Adaptations of Word2Vec for Syntax Problems.", "authors": ["W Ling", "I Trancoso", "AW Black", "C Dyer"], "journal": "HLT-NAACL, 1299-1304", "citations": "311", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4859666501614384454&btnI=1&nossl=1&hl=en&oe=ASCII"}, "636": {"ID": 636, "title": "Multitask Learning for Mental Health Conditions with Limited Social Media Data.", "authors": ["D Hovy", "A Benton", "M Mitchell"], "journal": "EACL (1), 152-162", "citations": "65", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=13554850924507478214&btnI=1&nossl=1&hl=en&oe=ASCII"}, "637": {"ID": 637, "title": "SemEval-2015 Task 11: Sentiment Analysis of Figurative Language in Twitter", "authors": ["A Reyes", "E Shutova", "A Ghosh", "J Barnden", "P Rosso", "G Li", "T Veale"], "journal": "Proceedings of the 9th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "169", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4689205609640155759&btnI=1&nossl=1&hl=en&oe=ASCII"}, "638": {"ID": 638, "title": "A Synchronous Hyperedge Replacement Grammar based approach for AMR parsing", "authors": ["L Song", "D Gildea", "X Peng"], "journal": "Proceedings of the Nineteenth Conference on Computational Natural Language\u00a0\u2026", "citations": "68", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=4880625027661102829&btnI=1&nossl=1&hl=en&oe=ASCII"}, "639": {"ID": 639, "title": "Injecting Logical Background Knowledge into Embeddings for Relation Extraction.", "authors": ["S Riedel", "S Singh", "T Rockt\u00e4schel"], "journal": "HLT-NAACL, 1119-1129", "citations": "162", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=17053725938019317222&btnI=1&nossl=1&hl=en&oe=ASCII"}, "640": {"ID": 640, "title": "Constraint-Based Question Answering with Knowledge Graph.", "authors": ["M Zhou", "T Zhao", "Z Yan", "J Bao", "N Duan"], "journal": "COLING, 2503-2514", "citations": "59", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=17928082960021440311&btnI=1&nossl=1&hl=en&oe=ASCII"}, "641": {"ID": 641, "title": "Reading and Thinking: Re-read LSTM Unit for Textual Entailment Recognition.", "authors": ["L Sha", "Z Sui", "B Chang", "S Li"], "journal": "COLING, 2870-2879", "citations": "56", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9883175351890853918&btnI=1&nossl=1&hl=en&oe=ASCII"}, "642": {"ID": 642, "title": "Joint Entity Recognition and Disambiguation", "authors": ["CY Lin", "G Luo", "Z Nie", "X Huang"], "journal": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "210", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=12185841570279487759&btnI=1&nossl=1&hl=en&oe=ASCII"}, "643": {"ID": 643, "title": "Efficient and Expressive Knowledge Base Completion Using Subgraph Feature Extraction", "authors": ["M Gardner", "T Mitchell"], "journal": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language\u00a0\u2026", "citations": "125", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=115946849307202418&btnI=1&nossl=1&hl=en&oe=ASCII"}, "644": {"ID": 644, "title": "Label Embedding for Zero-shot Fine-grained Named Entity Typing.", "authors": ["S Gao", "E Cambria", "Y Ma"], "journal": "COLING, 171-180", "citations": "70", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=17496131336539127769&btnI=1&nossl=1&hl=en&oe=ASCII"}, "645": {"ID": 645, "title": "Semantic Specialization of Distributional Word Vector Spaces using Monolingual and Cross-Lingual Constraints", "authors": ["...", "N Mrk\u0161i\u0107", "I Vuli\u0107", "M Ga\u0161i\u0107", "R Reichart", "I Leviant", "D\u00d3 S\u00e9aghdha"], "journal": "Transactions of the Association for Computational Linguistics 5, 309-324", "citations": "113", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=14042576502028920462&btnI=1&nossl=1&hl=en&oe=ASCII"}, "646": {"ID": 646, "title": "SemEval-2015 Task 12: Aspect Based Sentiment Analysis", "authors": ["H Papageorgiou", "M Pontiki", "S Manandhar", "I Androutsopoulos", "D Galanis"], "journal": "Proceedings of the 9th International Workshop on Semantic Evaluation\u00a0\u2026", "citations": "331", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=1111601259891195586&btnI=1&nossl=1&hl=en&oe=ASCII"}, "647": {"ID": 647, "title": "Dimensional Sentiment Analysis Using a Regional CNN-LSTM Model", "authors": ["LC Yu", "X Zhang", "KR Lai", "J Wang"], "journal": "Proceedings of the 54th Annual Meeting of the Association for Computational\u00a0\u2026", "citations": "219", "gscholar_url": "http://scholar.google.com/scholar?oi=bibs&cluster=9741464634590926318&btnI=1&nossl=1&hl=en&oe=ASCII"}}