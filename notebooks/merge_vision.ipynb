{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "proof-acquisition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import rltk\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "herbal-hampshire",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['semantic_joined_dblp.json.zip',\n",
       " '.DS_Store',\n",
       " 'Merged_graphics.csv',\n",
       " 'semantic_joined_dblp.json',\n",
       " 'Merged_ai.csv',\n",
       " 'output.json',\n",
       " 'semantic_scholar_articles_20000.csv',\n",
       " 'Merged_vision.csv',\n",
       " 'Merged_nlp.csv',\n",
       " 'output_compiled.json']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-trader",
   "metadata": {},
   "source": [
    "## Loading Semantic and DBLP JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "spoken-drunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../data/semantic_joined_dblp.json','r')\n",
    "t=f.read()\n",
    "f.close()\n",
    "js = json.loads(t)\n",
    "df_js = pd.DataFrame(js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "immune-pattern",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>arxivId</th>\n",
       "      <th>authors</th>\n",
       "      <th>citationVelocity</th>\n",
       "      <th>citations</th>\n",
       "      <th>corpusId</th>\n",
       "      <th>doi</th>\n",
       "      <th>influentialCitationCount</th>\n",
       "      <th>paperId</th>\n",
       "      <th>references</th>\n",
       "      <th>title</th>\n",
       "      <th>topics</th>\n",
       "      <th>url</th>\n",
       "      <th>year</th>\n",
       "      <th>dblp_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The artificial neural networks that are used t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'authorId': '1695689', 'name': 'Geoffrey E. ...</td>\n",
       "      <td>150</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '1...</td>\n",
       "      <td>6138085</td>\n",
       "      <td>10.1007/978-3-642-21735-7_6</td>\n",
       "      <td>52</td>\n",
       "      <td>20f0357688876fa4662f806f985779dce6e24f3c</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '5...</td>\n",
       "      <td>Transforming Auto-Encoders</td>\n",
       "      <td>[{'topic': 'Computer vision', 'topicId': '5332...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/20f03576...</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>{'@score': '3', '@id': '3107620', 'info': {'au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to improve the accuracy of capsule ne...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'authorId': '144866658', 'name': 'Xin Ning',...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>215723800</td>\n",
       "      <td>10.1109/ACCESS.2020.2982782</td>\n",
       "      <td>0</td>\n",
       "      <td>4c79e754407d41904979746fbc8090545c0aeec9</td>\n",
       "      <td>[{'arxivId': '1810.10183', 'authors': [{'autho...</td>\n",
       "      <td>BDARS_CapsNet: Bi-Directional Attention Routin...</td>\n",
       "      <td>[{'topic': 'Routing', 'topicId': '1048', 'url'...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c79e754...</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The current dominant paradigm for feature lear...</td>\n",
       "      <td>1505.01596</td>\n",
       "      <td>[{'authorId': '33932184', 'name': 'Pulkit Agra...</td>\n",
       "      <td>96</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '2...</td>\n",
       "      <td>1637703</td>\n",
       "      <td>10.1109/ICCV.2015.13</td>\n",
       "      <td>31</td>\n",
       "      <td>dfbfaaec46d38392f61d683c340ee92a0a66e5d9</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '2...</td>\n",
       "      <td>Learning to See by Moving</td>\n",
       "      <td>[{'topic': 'Visual odometry', 'topicId': '644'...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/dfbfaaec...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>{'@score': '5', '@id': '2026007', 'info': {'au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In this work, we address the problem of improv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'authorId': '3458345', 'name': 'Zhun Sun', '...</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '3...</td>\n",
       "      <td>53872389</td>\n",
       "      <td>10.1109/CVPR.2018.00830</td>\n",
       "      <td>2</td>\n",
       "      <td>51bd5966fc992498cb1147d34527e33656c696bb</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '2...</td>\n",
       "      <td>Feature Quantization for Defending Against Dis...</td>\n",
       "      <td>[{'topic': 'Distortion', 'topicId': '15080', '...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/51bd5966...</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>{'@score': '8', '@id': '1024764', 'info': {'au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We introduce BSD-GAN, a novel multi-branch and...</td>\n",
       "      <td>1803.08467</td>\n",
       "      <td>[{'authorId': '39737792', 'name': 'Zili Yi', '...</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'arxivId': '2009.13311', 'authors': [{'autho...</td>\n",
       "      <td>220666238</td>\n",
       "      <td>10.1109/TIP.2020.3014608</td>\n",
       "      <td>0</td>\n",
       "      <td>70977b9464b94f71c2c4fc68aa03082e5739b6c3</td>\n",
       "      <td>[{'arxivId': '1710.10196', 'authors': [{'autho...</td>\n",
       "      <td>BSD-GAN: Branched Generative Adversarial Netwo...</td>\n",
       "      <td>[{'topic': 'BSD', 'topicId': '10760', 'url': '...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/70977b94...</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>{'@score': '15', '@id': '211133', 'info': {'au...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract     arxivId  \\\n",
       "0  The artificial neural networks that are used t...         NaN   \n",
       "1  In order to improve the accuracy of capsule ne...         NaN   \n",
       "2  The current dominant paradigm for feature lear...  1505.01596   \n",
       "3  In this work, we address the problem of improv...         NaN   \n",
       "4  We introduce BSD-GAN, a novel multi-branch and...  1803.08467   \n",
       "\n",
       "                                             authors  citationVelocity  \\\n",
       "0  [{'authorId': '1695689', 'name': 'Geoffrey E. ...               150   \n",
       "1  [{'authorId': '144866658', 'name': 'Xin Ning',...                 0   \n",
       "2  [{'authorId': '33932184', 'name': 'Pulkit Agra...                96   \n",
       "3  [{'authorId': '3458345', 'name': 'Zhun Sun', '...                 0   \n",
       "4  [{'authorId': '39737792', 'name': 'Zili Yi', '...                 0   \n",
       "\n",
       "                                           citations   corpusId  \\\n",
       "0  [{'arxivId': None, 'authors': [{'authorId': '1...    6138085   \n",
       "1                                                 []  215723800   \n",
       "2  [{'arxivId': None, 'authors': [{'authorId': '2...    1637703   \n",
       "3  [{'arxivId': None, 'authors': [{'authorId': '3...   53872389   \n",
       "4  [{'arxivId': '2009.13311', 'authors': [{'autho...  220666238   \n",
       "\n",
       "                           doi  influentialCitationCount  \\\n",
       "0  10.1007/978-3-642-21735-7_6                        52   \n",
       "1  10.1109/ACCESS.2020.2982782                         0   \n",
       "2         10.1109/ICCV.2015.13                        31   \n",
       "3      10.1109/CVPR.2018.00830                         2   \n",
       "4     10.1109/TIP.2020.3014608                         0   \n",
       "\n",
       "                                    paperId  \\\n",
       "0  20f0357688876fa4662f806f985779dce6e24f3c   \n",
       "1  4c79e754407d41904979746fbc8090545c0aeec9   \n",
       "2  dfbfaaec46d38392f61d683c340ee92a0a66e5d9   \n",
       "3  51bd5966fc992498cb1147d34527e33656c696bb   \n",
       "4  70977b9464b94f71c2c4fc68aa03082e5739b6c3   \n",
       "\n",
       "                                          references  \\\n",
       "0  [{'arxivId': None, 'authors': [{'authorId': '5...   \n",
       "1  [{'arxivId': '1810.10183', 'authors': [{'autho...   \n",
       "2  [{'arxivId': None, 'authors': [{'authorId': '2...   \n",
       "3  [{'arxivId': None, 'authors': [{'authorId': '2...   \n",
       "4  [{'arxivId': '1710.10196', 'authors': [{'autho...   \n",
       "\n",
       "                                               title  \\\n",
       "0                         Transforming Auto-Encoders   \n",
       "1  BDARS_CapsNet: Bi-Directional Attention Routin...   \n",
       "2                          Learning to See by Moving   \n",
       "3  Feature Quantization for Defending Against Dis...   \n",
       "4  BSD-GAN: Branched Generative Adversarial Netwo...   \n",
       "\n",
       "                                              topics  \\\n",
       "0  [{'topic': 'Computer vision', 'topicId': '5332...   \n",
       "1  [{'topic': 'Routing', 'topicId': '1048', 'url'...   \n",
       "2  [{'topic': 'Visual odometry', 'topicId': '644'...   \n",
       "3  [{'topic': 'Distortion', 'topicId': '15080', '...   \n",
       "4  [{'topic': 'BSD', 'topicId': '10760', 'url': '...   \n",
       "\n",
       "                                                 url    year  \\\n",
       "0  https://www.semanticscholar.org/paper/20f03576...  2011.0   \n",
       "1  https://www.semanticscholar.org/paper/4c79e754...  2020.0   \n",
       "2  https://www.semanticscholar.org/paper/dfbfaaec...  2015.0   \n",
       "3  https://www.semanticscholar.org/paper/51bd5966...  2018.0   \n",
       "4  https://www.semanticscholar.org/paper/70977b94...  2020.0   \n",
       "\n",
       "                                           dblp_info  \n",
       "0  {'@score': '3', '@id': '3107620', 'info': {'au...  \n",
       "1                                                NaN  \n",
       "2  {'@score': '5', '@id': '2026007', 'info': {'au...  \n",
       "3  {'@score': '8', '@id': '1024764', 'info': {'au...  \n",
       "4  {'@score': '15', '@id': '211133', 'info': {'au...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_js.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "floppy-wales",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = [i for i in os.listdir('../data') if \"Merged\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "piano-emphasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for i in ls:\n",
    "    df_2 = pd.read_csv('../data/'+i)\n",
    "    df = df.append(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "expressed-robert",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>published</th>\n",
       "      <th>updated</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "      <th>citations</th>\n",
       "      <th>arxiv_url</th>\n",
       "      <th>gscholar_url</th>\n",
       "      <th>journal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Dynamic Graph CNN for Learning on Point Clouds</td>\n",
       "      <td>['Yue Wang', 'Sanjay E. Sarma', 'Justin M. Sol...</td>\n",
       "      <td>2018-01-24T01:14:04Z</td>\n",
       "      <td>2019-06-11T06:11:21Z</td>\n",
       "      <td>Point clouds provide a flexible geometric repr...</td>\n",
       "      <td>['cs.CV']</td>\n",
       "      <td>536.0</td>\n",
       "      <td>http://arxiv.org/abs/1801.07829v2</td>\n",
       "      <td>http://scholar.google.com/scholar?oi=bibs&amp;clus...</td>\n",
       "      <td>ACM Transactions on Graphics (TOG) 38 (5), 1-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Looking to Listen at the Cocktail Party: A Spe...</td>\n",
       "      <td>['Avinatan Hassidim', 'Oran Lang', 'Michael Ru...</td>\n",
       "      <td>2018-04-10T16:28:59Z</td>\n",
       "      <td>2018-08-09T21:22:37Z</td>\n",
       "      <td>We present a joint audio-visual model for isol...</td>\n",
       "      <td>['cs.SD', 'cs.CV', 'eess.AS']</td>\n",
       "      <td>189.0</td>\n",
       "      <td>http://arxiv.org/abs/1804.03619v2</td>\n",
       "      <td>http://scholar.google.com/scholar?oi=bibs&amp;clus...</td>\n",
       "      <td>ACM Transactions on Graphics (TOG) 37 (4), 1-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Convergence of univariate non-stationary subdi...</td>\n",
       "      <td>['Carla Manni', 'Costanza Conti', 'Marie-Laure...</td>\n",
       "      <td>2014-10-10T10:30:14Z</td>\n",
       "      <td>2014-10-10T10:30:14Z</td>\n",
       "      <td>A new equivalence notion between non-stationar...</td>\n",
       "      <td>['math.NA']</td>\n",
       "      <td>34.0</td>\n",
       "      <td>http://arxiv.org/abs/1410.2729v1</td>\n",
       "      <td>http://scholar.google.com/scholar?oi=bibs&amp;clus...</td>\n",
       "      <td>Computer Aided Geometric Design 37, 1-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>HDR image reconstruction from a single exposur...</td>\n",
       "      <td>['Joel Kronander', 'Rafał K. Mantiuk', 'Gyorgy...</td>\n",
       "      <td>2017-10-20T10:48:22Z</td>\n",
       "      <td>2017-10-20T10:48:22Z</td>\n",
       "      <td>Camera sensors can only capture a limited rang...</td>\n",
       "      <td>['cs.CV', 'cs.GR', 'cs.LG']</td>\n",
       "      <td>145.0</td>\n",
       "      <td>http://arxiv.org/abs/1710.07480v1</td>\n",
       "      <td>http://scholar.google.com/scholar?oi=bibs&amp;clus...</td>\n",
       "      <td>ACM Transactions on Graphics (TOG) 36 (6), 1-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Learning Style Similarity for Searching Infogr...</td>\n",
       "      <td>['Aaron Hertzmann', 'Mira Dontcheva', 'Babak S...</td>\n",
       "      <td>2015-05-05T22:59:32Z</td>\n",
       "      <td>2015-05-05T22:59:32Z</td>\n",
       "      <td>Infographics are complex graphic designs integ...</td>\n",
       "      <td>['cs.GR', 'cs.CV', 'cs.HC', 'cs.IR', 'cs.MM']</td>\n",
       "      <td>24.0</td>\n",
       "      <td>http://arxiv.org/abs/1505.01214v1</td>\n",
       "      <td>http://scholar.google.com/scholar?oi=bibs&amp;clus...</td>\n",
       "      <td>Proceedings of the 41st Graphics Interface Con...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                              title  \\\n",
       "0   0     Dynamic Graph CNN for Learning on Point Clouds   \n",
       "1   1  Looking to Listen at the Cocktail Party: A Spe...   \n",
       "2   2  Convergence of univariate non-stationary subdi...   \n",
       "3   3  HDR image reconstruction from a single exposur...   \n",
       "4   4  Learning Style Similarity for Searching Infogr...   \n",
       "\n",
       "                                             authors             published  \\\n",
       "0  ['Yue Wang', 'Sanjay E. Sarma', 'Justin M. Sol...  2018-01-24T01:14:04Z   \n",
       "1  ['Avinatan Hassidim', 'Oran Lang', 'Michael Ru...  2018-04-10T16:28:59Z   \n",
       "2  ['Carla Manni', 'Costanza Conti', 'Marie-Laure...  2014-10-10T10:30:14Z   \n",
       "3  ['Joel Kronander', 'Rafał K. Mantiuk', 'Gyorgy...  2017-10-20T10:48:22Z   \n",
       "4  ['Aaron Hertzmann', 'Mira Dontcheva', 'Babak S...  2015-05-05T22:59:32Z   \n",
       "\n",
       "                updated                                           abstract  \\\n",
       "0  2019-06-11T06:11:21Z  Point clouds provide a flexible geometric repr...   \n",
       "1  2018-08-09T21:22:37Z  We present a joint audio-visual model for isol...   \n",
       "2  2014-10-10T10:30:14Z  A new equivalence notion between non-stationar...   \n",
       "3  2017-10-20T10:48:22Z  Camera sensors can only capture a limited rang...   \n",
       "4  2015-05-05T22:59:32Z  Infographics are complex graphic designs integ...   \n",
       "\n",
       "                                      categories  citations  \\\n",
       "0                                      ['cs.CV']      536.0   \n",
       "1                  ['cs.SD', 'cs.CV', 'eess.AS']      189.0   \n",
       "2                                    ['math.NA']       34.0   \n",
       "3                    ['cs.CV', 'cs.GR', 'cs.LG']      145.0   \n",
       "4  ['cs.GR', 'cs.CV', 'cs.HC', 'cs.IR', 'cs.MM']       24.0   \n",
       "\n",
       "                           arxiv_url  \\\n",
       "0  http://arxiv.org/abs/1801.07829v2   \n",
       "1  http://arxiv.org/abs/1804.03619v2   \n",
       "2   http://arxiv.org/abs/1410.2729v1   \n",
       "3  http://arxiv.org/abs/1710.07480v1   \n",
       "4  http://arxiv.org/abs/1505.01214v1   \n",
       "\n",
       "                                        gscholar_url  \\\n",
       "0  http://scholar.google.com/scholar?oi=bibs&clus...   \n",
       "1  http://scholar.google.com/scholar?oi=bibs&clus...   \n",
       "2  http://scholar.google.com/scholar?oi=bibs&clus...   \n",
       "3  http://scholar.google.com/scholar?oi=bibs&clus...   \n",
       "4  http://scholar.google.com/scholar?oi=bibs&clus...   \n",
       "\n",
       "                                             journal  \n",
       "0    ACM Transactions on Graphics (TOG) 38 (5), 1-12  \n",
       "1    ACM Transactions on Graphics (TOG) 37 (4), 1-11  \n",
       "2            Computer Aided Geometric Design 37, 1-8  \n",
       "3    ACM Transactions on Graphics (TOG) 36 (6), 1-15  \n",
       "4  Proceedings of the 41st Graphics Interface Con...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-source",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-reporter",
   "metadata": {},
   "source": [
    "### Naïve Data Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "opposite-lawyer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\n"
     ]
    }
   ],
   "source": [
    "# How many matches can be found with a naÏve identical string approach?\n",
    "d1 = df_js['title'].values\n",
    "d2 = df['title'].values\n",
    "print(len([1 for w in d1 if w in d2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-count",
   "metadata": {},
   "source": [
    "## RLTK Data Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "divine-formula",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RLTK Tokenizer\n",
    "tokenizer = rltk.CrfTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-lighting",
   "metadata": {},
   "source": [
    "**Arxiv/Google Scholar Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "clear-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_js.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "distant-rugby",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'abstract', 'arxivId', 'authors', 'citationVelocity',\n",
       "       'citations', 'corpusId', 'doi', 'influentialCitationCount', 'paperId',\n",
       "       'references', 'title', 'topics', 'url', 'year', 'dblp_info'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_js.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "studied-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "unlikely-preparation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'ID', 'title', 'authors', 'published', 'updated', 'abstract',\n",
       "       'categories', 'citations', 'arxiv_url', 'gscholar_url', 'journal'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "attractive-envelope",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "final-texture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[{'authorId': '1695689', 'name': 'Geoffrey E. Hinton', 'url': 'https://www.semanticscholar.org/author/1695689'}, {'authorId': '2064160', 'name': 'A. Krizhevsky', 'url': 'https://www.semanticscholar.org/author/2064160'}, {'authorId': '49185042', 'name': 'S. Wang', 'url': 'https://www.semanticscholar.org/author/49185042'}]\""
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_js['authors'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "changing-adoption",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['Yue Wang', 'Sanjay E. Sarma', 'Justin M. Solomon', 'Yongbin Sun', 'Ziwei Liu', 'Michael M. Bronstein']\""
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['authors'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "remarkable-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "supposed-click",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hinton', 'Krizhevsky', 'Wang'}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getPlainString(string):\n",
    "    return ''.join([s for s in string if not s.isnumeric()]).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "informal-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArxivRecord(rltk.Record):\n",
    "    def __init__(self, raw_object):\n",
    "        super().__init__(raw_object)\n",
    "        self.name = 'ArxivRecord'\n",
    "        \n",
    "    @property\n",
    "    def id(self):\n",
    "        return str(self.raw_object['ID'])\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def authors_string(self):\n",
    "        return self.raw_object['authors']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def title_string(self):\n",
    "        return self.raw_object['title']\n",
    "        \n",
    "#     @rltk.cached_property\n",
    "#     def summary_string(self):\n",
    "#         return self.raw_object['summary']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def categories_string(self):\n",
    "        return self.raw_object['categories']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def published_string(self):\n",
    "        return self.raw_object['published']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def updated_string(self):\n",
    "        return self.raw_object['updated']\n",
    "        \n",
    "    @rltk.cached_property\n",
    "    def blocking_author_tokens(self):\n",
    "        return set([getPlainString(s).split(' ')[-1] for s in ast.literal_eval(self.raw_object['authors'])])\n",
    "    \n",
    "#     @rltk.cached_property\n",
    "#     def url_string(self):\n",
    "#         return self.raw_object['url']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def blocking_tokens(self):\n",
    "        tokens = ' '.join([self.title_string])\n",
    "        tokens = re.sub(r'\\bThe\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bthe\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bof\\b', '', tokens)\n",
    "        tokens = re.sub(r\"\\b's\\b\", '', tokens)\n",
    "        tokens = re.sub(r'\\band\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bI\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bA\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bin\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bfor\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bon\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bwith\\b', '', tokens)\n",
    "        return set(tokenizer.tokenize(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "marked-marketing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'rltk.dataset.Dataset'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>authors_string</th>\n",
       "      <th>title_string</th>\n",
       "      <th>categories_string</th>\n",
       "      <th>published_string</th>\n",
       "      <th>updated_string</th>\n",
       "      <th>blocking_author_tokens</th>\n",
       "      <th>blocking_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>['Barret Zoph', 'Kevin Knight']</td>\n",
       "      <td>Multi-Source Neural Translation</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>2016-01-05T00:49:22Z</td>\n",
       "      <td>2016-01-05T00:49:22Z</td>\n",
       "      <td>{Zoph, Knight}</td>\n",
       "      <td>{Source, -, Neural, Multi, Translation}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>['Xiaodong He', 'Asli Celikyilmaz', 'Antoine B...</td>\n",
       "      <td>Deep Communicating Agents for Abstractive Summ...</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>2018-03-27T23:29:23Z</td>\n",
       "      <td>2018-08-15T18:54:22Z</td>\n",
       "      <td>{Choi, Bosselut, Celikyilmaz, He}</td>\n",
       "      <td>{Agents, Communicating, Deep, Summarization, A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>['Furu Wei', 'Ming Zhou', 'Nan Yang', 'Qingyu ...</td>\n",
       "      <td>Selective Encoding for Abstractive Sentence Su...</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>2017-04-24T07:57:37Z</td>\n",
       "      <td>2017-04-24T07:57:37Z</td>\n",
       "      <td>{Yang, Zhou, Wei}</td>\n",
       "      <td>{Selective, Encoding, Sentence, Summarization,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                     authors_string  \\\n",
       "0  0                    ['Barret Zoph', 'Kevin Knight']   \n",
       "1  1  ['Xiaodong He', 'Asli Celikyilmaz', 'Antoine B...   \n",
       "2  2  ['Furu Wei', 'Ming Zhou', 'Nan Yang', 'Qingyu ...   \n",
       "\n",
       "                                        title_string categories_string  \\\n",
       "0                    Multi-Source Neural Translation         ['cs.CL']   \n",
       "1  Deep Communicating Agents for Abstractive Summ...         ['cs.CL']   \n",
       "2  Selective Encoding for Abstractive Sentence Su...         ['cs.CL']   \n",
       "\n",
       "       published_string        updated_string  \\\n",
       "0  2016-01-05T00:49:22Z  2016-01-05T00:49:22Z   \n",
       "1  2018-03-27T23:29:23Z  2018-08-15T18:54:22Z   \n",
       "2  2017-04-24T07:57:37Z  2017-04-24T07:57:37Z   \n",
       "\n",
       "              blocking_author_tokens  \\\n",
       "0                     {Zoph, Knight}   \n",
       "1  {Choi, Bosselut, Celikyilmaz, He}   \n",
       "2                  {Yang, Zhou, Wei}   \n",
       "\n",
       "                                     blocking_tokens  \n",
       "0            {Source, -, Neural, Multi, Translation}  \n",
       "1  {Agents, Communicating, Deep, Summarization, A...  \n",
       "2  {Selective, Encoding, Sentence, Summarization,...  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_1 = rltk.Dataset(reader=rltk.DataFrameReader(df), record_class=ArxivRecord, adapter=rltk.MemoryKeyValueAdapter())\n",
    "print(type(ds_1))\n",
    "ds_1.generate_dataframe().head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-steps",
   "metadata": {},
   "source": [
    "**Google Scholar Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "august-sellers",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticRecord(rltk.Record):\n",
    "    def __init__(self, raw_object):\n",
    "        super().__init__(raw_object)\n",
    "        self.name = 'SemanticRecord'\n",
    "        \n",
    "    @property\n",
    "    def id(self):\n",
    "        return str(self.raw_object['index'])\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def authors_string(self):\n",
    "        return self.raw_object['authors']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def title_string(self):\n",
    "        return self.raw_object['title']\n",
    "        \n",
    "#     @rltk.cached_property\n",
    "#     def journal_string(self):\n",
    "#         return self.raw_object['journal']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def citations_string(self):\n",
    "        return self.raw_object['citations']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def year_string(self):\n",
    "        return self.raw_object['year']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def url_string(self):\n",
    "        return self.raw_object['url']\n",
    "    \n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def blocking_author_tokens(self):\n",
    "        return set([getPlainString(s['name']).split(' ')[-1] for s in ast.literal_eval((self.raw_object['authors']))])\n",
    "\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def blocking_tokens(self):\n",
    "        tokens = ' '.join([self.title_string])\n",
    "        tokens = re.sub(r'\\bThe\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bthe\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bof\\b', '', tokens)\n",
    "        tokens = re.sub(r\"\\b's\\b\", '', tokens)\n",
    "        tokens = re.sub(r'\\band\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bI\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bA\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bin\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bfor\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bon\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bwith\\b', '', tokens)\n",
    "        return set(tokenizer.tokenize(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "embedded-tournament",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'rltk.dataset.Dataset'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>authors_string</th>\n",
       "      <th>title_string</th>\n",
       "      <th>citations_string</th>\n",
       "      <th>year_string</th>\n",
       "      <th>url_string</th>\n",
       "      <th>blocking_author_tokens</th>\n",
       "      <th>blocking_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'authorId': '1695689', 'name': 'Geoffrey E. ...</td>\n",
       "      <td>Transforming Auto-Encoders</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '1...</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>https://www.semanticscholar.org/paper/20f03576...</td>\n",
       "      <td>{Krizhevsky, Wang, Hinton}</td>\n",
       "      <td>{Auto, Encoders, -, Transforming}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'authorId': '144866658', 'name': 'Xin Ning',...</td>\n",
       "      <td>BDARS_CapsNet: Bi-Directional Attention Routin...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c79e754...</td>\n",
       "      <td>{Nie, Ning, Sun, Chen, Tian, Li, Lu}</td>\n",
       "      <td>{Directional, _, -, Attention, Sausage, Capsul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[{'authorId': '33932184', 'name': 'Pulkit Agra...</td>\n",
       "      <td>Learning to See by Moving</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '2...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>https://www.semanticscholar.org/paper/dfbfaaec...</td>\n",
       "      <td>{Agrawal, Malik, Carreira}</td>\n",
       "      <td>{Moving, Learning, by, See, to}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                     authors_string  \\\n",
       "0  0  [{'authorId': '1695689', 'name': 'Geoffrey E. ...   \n",
       "1  1  [{'authorId': '144866658', 'name': 'Xin Ning',...   \n",
       "2  2  [{'authorId': '33932184', 'name': 'Pulkit Agra...   \n",
       "\n",
       "                                        title_string  \\\n",
       "0                         Transforming Auto-Encoders   \n",
       "1  BDARS_CapsNet: Bi-Directional Attention Routin...   \n",
       "2                          Learning to See by Moving   \n",
       "\n",
       "                                    citations_string  year_string  \\\n",
       "0  [{'arxivId': None, 'authors': [{'authorId': '1...       2011.0   \n",
       "1                                                 []       2020.0   \n",
       "2  [{'arxivId': None, 'authors': [{'authorId': '2...       2015.0   \n",
       "\n",
       "                                          url_string  \\\n",
       "0  https://www.semanticscholar.org/paper/20f03576...   \n",
       "1  https://www.semanticscholar.org/paper/4c79e754...   \n",
       "2  https://www.semanticscholar.org/paper/dfbfaaec...   \n",
       "\n",
       "                 blocking_author_tokens  \\\n",
       "0            {Krizhevsky, Wang, Hinton}   \n",
       "1  {Nie, Ning, Sun, Chen, Tian, Li, Lu}   \n",
       "2            {Agrawal, Malik, Carreira}   \n",
       "\n",
       "                                     blocking_tokens  \n",
       "0                  {Auto, Encoders, -, Transforming}  \n",
       "1  {Directional, _, -, Attention, Sausage, Capsul...  \n",
       "2                    {Moving, Learning, by, See, to}  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_2 = rltk.Dataset(reader=rltk.DataFrameReader(df_js), record_class=SemanticRecord, adapter=rltk.MemoryKeyValueAdapter())\n",
    "print(type(ds_2))\n",
    "ds_2.generate_dataframe().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "worth-anaheim",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-fraud",
   "metadata": {},
   "source": [
    "### Blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "dramatic-disabled",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'rltk.blocking.block.Block'>\n"
     ]
    }
   ],
   "source": [
    "# Generate blocks from tokens\n",
    "token_blocker = rltk.TokenBlockGenerator()\n",
    "blocks = token_blocker.generate(\n",
    "    token_blocker.block(ds_1, property_='blocking_author_tokens'),\n",
    "    token_blocker.block(ds_2, property_='blocking_author_tokens'))\n",
    "print(type(blocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "fifth-dispatch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction Ratio: 0.93040\n"
     ]
    }
   ],
   "source": [
    "# Extract all record pairs from the block\n",
    "record_pairs = rltk.get_record_pairs(ds_1, ds_2, block=blocks)\n",
    "\n",
    "# Get the total number of record pairs generated\n",
    "compared_pairs = len(list(record_pairs))\n",
    "\n",
    "# Get the number of elements in each rltk.Dataset\n",
    "tally_imdb = ds_1.generate_dataframe().shape[0]\n",
    "tally_tmd = ds_2.generate_dataframe().shape[0]\n",
    "\n",
    "# Calculate the total number of pairs if both datasets were to be compared without any blocking (eg: a double for loop)\n",
    "tally_unblocked = tally_imdb * tally_tmd\n",
    "\n",
    "# Calculate how much smaller the blocked pairings are\n",
    "reduction_ratio = compared_pairs / tally_unblocked\n",
    "\n",
    "# Calculate the reduction ratio (the inverse of the )\n",
    "reduction_ratio = 1 - reduction_ratio\n",
    "print(f'Reduction Ratio: {reduction_ratio:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "innovative-consistency",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3036085"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compared_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-public",
   "metadata": {},
   "source": [
    "### Matching Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "automated-penny",
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_similarity(arxiv_tuple, gscholar_tuple):\n",
    "    arxiv_title = arxiv_tuple.title_string.strip().lower()\n",
    "    gscholar_title = gscholar_tuple.title_string.strip().lower()\n",
    "    similarity = SequenceMatcher(None, arxiv_title, gscholar_title).ratio()\n",
    "\n",
    "    penalties = sum([len(arxiv_title)<=6,\n",
    "                     len(gscholar_title)<=6])\n",
    "\n",
    "    return similarity * (0.9**penalties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "together-terminology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def author_similarity(arxiv_tuple, gscholar_tuple):\n",
    "    arxiv_author = ' '.join(arxiv_tuple.authors_string).strip().lower()\n",
    "    gscholar_author = ' '.join(gscholar_tuple.authors_string).strip().lower()\n",
    "    similarity = SequenceMatcher(None, arxiv_author, gscholar_author).ratio() \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "wooden-powder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_similarity(arxiv_tuple, gscholar_tuple):\n",
    "#     arxiv_year = int(float(arxiv_tuple.updated_string[0:4]))\n",
    "#     print(arxiv_tuple.published_string)\n",
    "    if str(arxiv_tuple.published_string) == \"nan\":\n",
    "        return 0\n",
    "    arxiv_year = datetime.datetime.strptime(arxiv_tuple.published_string, '%Y-%m-%dT%H:%M:%SZ').year\n",
    "\n",
    "    gscholar_year = int(float(gscholar_tuple.year_string))\n",
    "    similarity = 1 /(1 + abs(arxiv_year-gscholar_year))\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "wireless-tenant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elementwise_similarity(arxiv_tuple, gscholar_tuple, match_threshold=0.75):\n",
    "    sim_title = title_similarity(arxiv_tuple, gscholar_tuple)\n",
    "    sim_author = author_similarity(arxiv_tuple, gscholar_tuple)\n",
    "    sim_year = year_similarity(arxiv_tuple, gscholar_tuple)\n",
    "\n",
    "    element_similarity = (0.70 * sim_title) + (0.15 * sim_author) + (0.15 * sim_year)\n",
    "\n",
    "    return element_similarity > match_threshold, element_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-vertex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict matches for all pairs in the blocked data \n",
    "print(f'Arxiv samples: {df.shape[0]}')\n",
    "print(f'Semantic samples: {df_js.shape[0]}')\n",
    "\n",
    "summary_df = pd.DataFrame()\n",
    "THRESHOLDS = [T/100 for T in range(65, 101, 5)]\n",
    "c=0\n",
    "# Iterate through various thresholds to find the most matches without any duplicates\n",
    "for T in tqdm(THRESHOLDS):\n",
    "\n",
    "    # Set to store pairs of IDs matched\n",
    "    ids_matched = set()\n",
    "    \n",
    "    # Iterate through candidates on the block\n",
    "    for block_id, arxiv_id, gscholar_id in blocks.pairwise(ds_1, ds_2):\n",
    "        \n",
    "        # Find similarity at a given threshold\n",
    "        match , similarity = elementwise_similarity(ds_1.get_record(arxiv_id),\n",
    "                                                    ds_2.get_record(gscholar_id),\n",
    "                                                    match_threshold=T)\n",
    "        # If a match is found, add to the set of matches\n",
    "        if match:\n",
    "            ids_matched.add((arxiv_id, gscholar_id))\n",
    "        c=c+1\n",
    "        if c>1000:\n",
    "            break\n",
    "    \n",
    "    # Count the number of unique elements derived from each source\n",
    "    set_a = set()\n",
    "    set_b = set()\n",
    "    for tp in ids_matched:\n",
    "        set_a.add(tp[0])\n",
    "        set_b.add(tp[1])\n",
    "    \n",
    "    summary_df.at[T, 'Matches'] = int(len(ids_matched))\n",
    "    summary_df.at[T, 'Set_A Size'] = int(len(set_a))\n",
    "    summary_df.at[T, 'Set_B Size'] = int(len(ids_matched))\n",
    "    summary_df.at[T, 'Duplicates'] = int((len(ids_matched)-len(set_a)) + (len(ids_matched)-len(set_b)))\n",
    "    \n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "powered-nursing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2700133"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids_matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-anaheim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the lowest threshold which gives no duplicates\n",
    "optimal_threshold = summary_df[summary_df['Duplicates']==0].index[0]\n",
    "optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-blowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually set threshold to 0.85 to trade 104 extra True Positives for 3 extra False Positive\n",
    "# optimal_threshold = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-florence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate matches based on the optimal (no-duplicate) threshold\n",
    "print(f'Arxiv samples: {df_arxiv.shape[0]}')\n",
    "print(f'GScholar samples: {df_gscholar.shape[0]}')\n",
    "\n",
    "# Store tuples of matches IDs, as well as singletons witouth a match\n",
    "ids_matched = set()\n",
    "singles_arxiv = set()\n",
    "singles_gscholar = set()\n",
    "\n",
    "# Write matches (and non-matches) to a CSV\n",
    "with open(f'Matches_{TOPIC}.csv', 'w') as predictions_full:\n",
    "    for block_id, arxiv_id, gscholar_id in blocks.pairwise(ds_arxiv, ds_gscholar):\n",
    "\n",
    "        match , similarity = elementwise_similarity(ds_arxiv.get_record(arxiv_id),\n",
    "                                                    ds_gscholar.get_record(gscholar_id),\n",
    "                                                    match_threshold=optimal_threshold)\n",
    "\n",
    "        if match:\n",
    "            ids_matched.add((arxiv_id, gscholar_id))\n",
    "        else:\n",
    "            singles_arxiv.add(arxiv_id)\n",
    "            singles_gscholar.add(gscholar_id)\n",
    "    \n",
    "    # After finding all matches, write them to a csv\n",
    "    for match_pair in ids_matched:\n",
    "        predictions_full.write(f'{match_pair[0]},{match_pair[1]},1\\n')\n",
    "        # And ensure that no item in the matches is counted as a single\n",
    "        try:\n",
    "            singles_arxiv.remove(match_pair[0])\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            singles_gscholar.remove(match_pair[1])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Then write all the singles which didn't find a match\n",
    "    NULL = None\n",
    "    for arxiv_id in singles_arxiv:\n",
    "        predictions_full.write(f'{arxiv_id},{NULL},0\\n')\n",
    "    for gscholar_id in singles_gscholar:\n",
    "        predictions_full.write(f'{NULL},{gscholar_id},0\\n')        \n",
    "        \n",
    "print()\n",
    "print(f'Matches: {len(ids_matched)}')\n",
    "print(f'Non-Matches Arxiv: {len(singles_arxiv)}')\n",
    "print(f'Non-Matches GScholar: {len(singles_gscholar)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-clause",
   "metadata": {},
   "source": [
    "### Create Merged Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-banking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib import URIRef, Literal, Namespace\n",
    "from rdflib.namespace import RDF, RDFS, XSD\n",
    "\n",
    "MYNS = Namespace('http://inf558.org/myfakenamespace#')\n",
    "SCHEMA = Namespace(\"https://schema.org/\")\n",
    "\n",
    "# Initliaze the graph\n",
    "g = rdflib.Graph()\n",
    "\n",
    "# Bind namespace and prefixes\n",
    "g.bind('my_ns', MYNS)\n",
    "g.bind('schema', SCHEMA)\n",
    "g.bind('rdf', RDF)\n",
    "g.bind('rdfs', RDFS)\n",
    "g.bind('xsd', XSD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-delta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictions to be used in populating the RDF\n",
    "predictions_df = pd.read_csv(f'Matches_{TOPIC}.csv', header=None, names=['ARXIV_ID', 'GSCHOLAR_ID', 'LABEL'])\n",
    "print(f'predictions_df.shape: {predictions_df.shape}')\n",
    "predicted_matches = predictions_df['LABEL'].sum()\n",
    "print(f'predicted matches: {predicted_matches}  [{100*predicted_matches/predictions_df.shape[0]:.2f} %]')\n",
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-azerbaijan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe to store the merged datasets\n",
    "df_merged = pd.DataFrame(columns = ['ID', 'title', 'authors', 'published', 'updated', \n",
    "                                    'abstract', 'categories', 'citations', 'arxiv_url', 'gscholar_url'],\n",
    "                         dtype='object')\n",
    "json_merged = {}\n",
    "\n",
    "NEW_ID = 0\n",
    "\n",
    "# Populate the RDF with predictions with a positive (1) label\n",
    "for idx, row in tqdm(predictions_df.iterrows(), total=predictions_df.shape[0]):\n",
    "    \n",
    "    # Populate the json object\n",
    "    json_merged[NEW_ID] = {}\n",
    "    \n",
    "    ### URI ###\n",
    "    node_uri = URIRef(str(NEW_ID))\n",
    "    g.add((node_uri, RDF.type, SCHEMA.ScholarlyArticle))\n",
    "    df_merged.at[NEW_ID, 'ID'] = NEW_ID\n",
    "    json_merged[NEW_ID]['ID'] = NEW_ID\n",
    "\n",
    "    \n",
    "    ### Title ###\n",
    "    try:\n",
    "        title_arxiv = str(df_arxiv[df_arxiv['ID'] == str(row['ARXIV_ID'])]['title'].values[0])\n",
    "    except:\n",
    "        title_arxiv = '<___>'\n",
    "    try:\n",
    "        title_gscholar = str(df_gscholar[df_gscholar['ID'] == str(row['GSCHOLAR_ID'])]['title'].values[0])\n",
    "    except:\n",
    "        title_gscholar = '<___>'\n",
    "    title = title_arxiv if title_arxiv != '<___>' else title_gscholar if title_gscholar != '<___>' else None\n",
    "    g.add((node_uri, SCHEMA.headline, Literal(title, datatype=SCHEMA.Text)))\n",
    "    df_merged.at[NEW_ID, 'title'] = title\n",
    "    json_merged[NEW_ID]['title'] = title\n",
    "\n",
    "    \n",
    "    ### Author(s) ###\n",
    "    try:\n",
    "        author_arxiv = df_arxiv[df_arxiv['ID'] == str(row['ARXIV_ID'])]['authors'].values[0]\n",
    "        author_arxiv = [name.strip() for name in author_arxiv if name != '<___>']\n",
    "    except:\n",
    "        author_arxiv = '<___>'\n",
    "    try:\n",
    "        author_gscholar = df_gscholar[df_gscholar['ID'] == str(row['GSCHOLAR_ID'])]['authors'].values[0]\n",
    "        author_gscholar = [name.strip() for name in author_gscholar if name != '<___>']\n",
    "    except:\n",
    "        author_gscholar = '<___>'\n",
    "    if author_arxiv != '<___>':\n",
    "        authors = list(set(author_arxiv))\n",
    "    else:\n",
    "        authors = list(set(author_gscholar))           \n",
    "    [g.add((node_uri, SCHEMA.author, Literal(author, datatype=SCHEMA.Person))) for author in authors]\n",
    "    df_merged.at[NEW_ID, 'authors'] = authors\n",
    "    json_merged[NEW_ID]['authors'] = authors\n",
    "                       \n",
    "    ### Published ###\n",
    "    try:\n",
    "        published = str(df_arxiv[df_arxiv['ID'] == str(row['ARXIV_ID'])]['published'].values[0])\n",
    "        g.add((node_uri, SCHEMA.datePublished, Literal(published, datatype=SCHEMA.DateTime)))\n",
    "        df_merged.at[NEW_ID, 'published'] = published\n",
    "        json_merged[NEW_ID]['published'] = published\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "                       \n",
    "    ### Updated ###\n",
    "    try:\n",
    "        updated = str(df_arxiv[df_arxiv['ID'] == str(row['ARXIV_ID'])]['updated'].values[0])\n",
    "        g.add((node_uri, SCHEMA.dateModified, Literal(updated, datatype=SCHEMA.DateTime)))\n",
    "        df_merged.at[NEW_ID, 'updated'] = updated\n",
    "        json_merged[NEW_ID]['updated'] = updated\n",
    "    except:\n",
    "        pass\n",
    "          \n",
    "                       \n",
    "    ### Abstract ###\n",
    "    try:\n",
    "        abstract = str(df_arxiv[df_arxiv['ID'] == str(row['ARXIV_ID'])]['summary'].values[0]).strip()\n",
    "        g.add((node_uri, SCHEMA.abstract, Literal(abstract, datatype=SCHEMA.Text)))\n",
    "        df_merged.at[NEW_ID, 'abstract'] = abstract\n",
    "        json_merged[NEW_ID]['abstract'] = abstract\n",
    "    except:\n",
    "        pass\n",
    "       \n",
    "                       \n",
    "    ### Categories ###\n",
    "    try:\n",
    "        categories = df_arxiv[df_arxiv['ID'] == str(row['ARXIV_ID'])]['categories'].values[0]\n",
    "        categories = [name.strip() for name in categories if name != '<___>']\n",
    "        [g.add((node_uri, SCHEMA.genre, Literal(category, datatype=SCHEMA.Text))) for category in categories]\n",
    "        df_merged.at[NEW_ID, 'categories'] = categories\n",
    "        json_merged[NEW_ID]['categories'] = categories\n",
    "    except:\n",
    "        pass\n",
    "          \n",
    "                       \n",
    "    ### Journal ###\n",
    "    try:\n",
    "        journal = str(df_gscholar[df_gscholar['ID'] == str(row['GSCHOLAR_ID'])]['journal'].values[0])\n",
    "        g.add((node_uri, SCHEMA.publisher, Literal(journal, datatype=SCHEMA.Periodical))) #datatype=SCHEMA.Organisation\n",
    "        df_merged.at[NEW_ID, 'journal'] = journal\n",
    "        json_merged[NEW_ID]['journal'] = journal\n",
    "    except:\n",
    "        pass\n",
    "     \n",
    "                       \n",
    "    ### Citations ###\n",
    "    try:\n",
    "        citations = str(df_gscholar[df_gscholar['ID'] == str(row['GSCHOLAR_ID'])]['citations'].values[0])\n",
    "        g.add((node_uri, SCHEMA.commentCount, Literal(citations, datatype=SCHEMA.Integer)))\n",
    "        df_merged.at[NEW_ID, 'citations'] = citations\n",
    "        json_merged[NEW_ID]['citations'] = citations\n",
    "    except:\n",
    "        pass\n",
    "            \n",
    "                       \n",
    "    ### Arxiv URL ###\n",
    "    try:\n",
    "        arxiv_url = str(df_arxiv[df_arxiv['ID'] == str(row['ARXIV_ID'])]['url'].values[0])\n",
    "        g.add((node_uri, SCHEMA.url, Literal(arxiv_url, datatype=SCHEMA.URL)))\n",
    "        df_merged.at[NEW_ID, 'arxiv_url'] = arxiv_url    \n",
    "        json_merged[NEW_ID]['arxiv_url'] = arxiv_url\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "                       \n",
    "    ### Google Scholar URL ###\n",
    "    try:\n",
    "        gscholar_url = str(df_gscholar[df_gscholar['ID'] == str(row['GSCHOLAR_ID'])]['url'].values[0])\n",
    "        g.add((node_uri, SCHEMA.url, Literal(gscholar_url, datatype=SCHEMA.URL)))\n",
    "        df_merged.at[NEW_ID, 'gscholar_url'] = gscholar_url   \n",
    "        json_merged[NEW_ID]['gscholar_url'] = gscholar_url\n",
    "    except:\n",
    "        pass\n",
    "             \n",
    "                       \n",
    "    NEW_ID += 1\n",
    "    \n",
    "# Save to disk using turtle format\n",
    "g.serialize(f'Triples_{TOPIC}.ttl.', format=\"turtle\")\n",
    "\n",
    "# And save the merged DataFrame as CSV\n",
    "df_merged.to_csv(f'Merged_{TOPIC}.csv', index=False)\n",
    "\n",
    "# Also save as Json, just because\n",
    "with open(f'Json_{TOPIC}.json', 'w') as fout:\n",
    "    json.dump(json_merged, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-skill",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-workstation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-samoa",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finsent",
   "language": "python",
   "name": "finsent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
