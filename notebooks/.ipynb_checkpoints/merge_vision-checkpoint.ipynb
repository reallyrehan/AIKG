{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "proof-acquisition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import rltk\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bridal-sympathy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['semantic_joined_dblp.json.zip',\n",
       " '.DS_Store',\n",
       " 'Merged_graphics.csv',\n",
       " 'semantic_joined_dblp.json',\n",
       " 'Merged_ai.csv',\n",
       " 'output.json',\n",
       " 'semantic_scholar_articles_20000.csv',\n",
       " 'Merged_vision.csv',\n",
       " 'Merged_nlp.csv',\n",
       " 'output_compiled.json']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-craft",
   "metadata": {},
   "source": [
    "## Loading Semantic and DBLP JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "internal-shanghai",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../data/semantic_joined_dblp.json','r')\n",
    "t=f.read()\n",
    "f.close()\n",
    "js = json.loads(t)\n",
    "df_js = pd.DataFrame(js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "international-difference",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>arxivId</th>\n",
       "      <th>authors</th>\n",
       "      <th>citationVelocity</th>\n",
       "      <th>citations</th>\n",
       "      <th>corpusId</th>\n",
       "      <th>doi</th>\n",
       "      <th>influentialCitationCount</th>\n",
       "      <th>paperId</th>\n",
       "      <th>references</th>\n",
       "      <th>title</th>\n",
       "      <th>topics</th>\n",
       "      <th>url</th>\n",
       "      <th>year</th>\n",
       "      <th>dblp_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The artificial neural networks that are used t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'authorId': '1695689', 'name': 'Geoffrey E. ...</td>\n",
       "      <td>150</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '1...</td>\n",
       "      <td>6138085</td>\n",
       "      <td>10.1007/978-3-642-21735-7_6</td>\n",
       "      <td>52</td>\n",
       "      <td>20f0357688876fa4662f806f985779dce6e24f3c</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '5...</td>\n",
       "      <td>Transforming Auto-Encoders</td>\n",
       "      <td>[{'topic': 'Computer vision', 'topicId': '5332...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/20f03576...</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>{'@score': '3', '@id': '3107620', 'info': {'au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to improve the accuracy of capsule ne...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'authorId': '144866658', 'name': 'Xin Ning',...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>215723800</td>\n",
       "      <td>10.1109/ACCESS.2020.2982782</td>\n",
       "      <td>0</td>\n",
       "      <td>4c79e754407d41904979746fbc8090545c0aeec9</td>\n",
       "      <td>[{'arxivId': '1810.10183', 'authors': [{'autho...</td>\n",
       "      <td>BDARS_CapsNet: Bi-Directional Attention Routin...</td>\n",
       "      <td>[{'topic': 'Routing', 'topicId': '1048', 'url'...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c79e754...</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The current dominant paradigm for feature lear...</td>\n",
       "      <td>1505.01596</td>\n",
       "      <td>[{'authorId': '33932184', 'name': 'Pulkit Agra...</td>\n",
       "      <td>96</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '2...</td>\n",
       "      <td>1637703</td>\n",
       "      <td>10.1109/ICCV.2015.13</td>\n",
       "      <td>31</td>\n",
       "      <td>dfbfaaec46d38392f61d683c340ee92a0a66e5d9</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '2...</td>\n",
       "      <td>Learning to See by Moving</td>\n",
       "      <td>[{'topic': 'Visual odometry', 'topicId': '644'...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/dfbfaaec...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>{'@score': '5', '@id': '2026007', 'info': {'au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In this work, we address the problem of improv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'authorId': '3458345', 'name': 'Zhun Sun', '...</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '3...</td>\n",
       "      <td>53872389</td>\n",
       "      <td>10.1109/CVPR.2018.00830</td>\n",
       "      <td>2</td>\n",
       "      <td>51bd5966fc992498cb1147d34527e33656c696bb</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '2...</td>\n",
       "      <td>Feature Quantization for Defending Against Dis...</td>\n",
       "      <td>[{'topic': 'Distortion', 'topicId': '15080', '...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/51bd5966...</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>{'@score': '8', '@id': '1024764', 'info': {'au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We introduce BSD-GAN, a novel multi-branch and...</td>\n",
       "      <td>1803.08467</td>\n",
       "      <td>[{'authorId': '39737792', 'name': 'Zili Yi', '...</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'arxivId': '2009.13311', 'authors': [{'autho...</td>\n",
       "      <td>220666238</td>\n",
       "      <td>10.1109/TIP.2020.3014608</td>\n",
       "      <td>0</td>\n",
       "      <td>70977b9464b94f71c2c4fc68aa03082e5739b6c3</td>\n",
       "      <td>[{'arxivId': '1710.10196', 'authors': [{'autho...</td>\n",
       "      <td>BSD-GAN: Branched Generative Adversarial Netwo...</td>\n",
       "      <td>[{'topic': 'BSD', 'topicId': '10760', 'url': '...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/70977b94...</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>{'@score': '15', '@id': '211133', 'info': {'au...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract     arxivId  \\\n",
       "0  The artificial neural networks that are used t...         NaN   \n",
       "1  In order to improve the accuracy of capsule ne...         NaN   \n",
       "2  The current dominant paradigm for feature lear...  1505.01596   \n",
       "3  In this work, we address the problem of improv...         NaN   \n",
       "4  We introduce BSD-GAN, a novel multi-branch and...  1803.08467   \n",
       "\n",
       "                                             authors  citationVelocity  \\\n",
       "0  [{'authorId': '1695689', 'name': 'Geoffrey E. ...               150   \n",
       "1  [{'authorId': '144866658', 'name': 'Xin Ning',...                 0   \n",
       "2  [{'authorId': '33932184', 'name': 'Pulkit Agra...                96   \n",
       "3  [{'authorId': '3458345', 'name': 'Zhun Sun', '...                 0   \n",
       "4  [{'authorId': '39737792', 'name': 'Zili Yi', '...                 0   \n",
       "\n",
       "                                           citations   corpusId  \\\n",
       "0  [{'arxivId': None, 'authors': [{'authorId': '1...    6138085   \n",
       "1                                                 []  215723800   \n",
       "2  [{'arxivId': None, 'authors': [{'authorId': '2...    1637703   \n",
       "3  [{'arxivId': None, 'authors': [{'authorId': '3...   53872389   \n",
       "4  [{'arxivId': '2009.13311', 'authors': [{'autho...  220666238   \n",
       "\n",
       "                           doi  influentialCitationCount  \\\n",
       "0  10.1007/978-3-642-21735-7_6                        52   \n",
       "1  10.1109/ACCESS.2020.2982782                         0   \n",
       "2         10.1109/ICCV.2015.13                        31   \n",
       "3      10.1109/CVPR.2018.00830                         2   \n",
       "4     10.1109/TIP.2020.3014608                         0   \n",
       "\n",
       "                                    paperId  \\\n",
       "0  20f0357688876fa4662f806f985779dce6e24f3c   \n",
       "1  4c79e754407d41904979746fbc8090545c0aeec9   \n",
       "2  dfbfaaec46d38392f61d683c340ee92a0a66e5d9   \n",
       "3  51bd5966fc992498cb1147d34527e33656c696bb   \n",
       "4  70977b9464b94f71c2c4fc68aa03082e5739b6c3   \n",
       "\n",
       "                                          references  \\\n",
       "0  [{'arxivId': None, 'authors': [{'authorId': '5...   \n",
       "1  [{'arxivId': '1810.10183', 'authors': [{'autho...   \n",
       "2  [{'arxivId': None, 'authors': [{'authorId': '2...   \n",
       "3  [{'arxivId': None, 'authors': [{'authorId': '2...   \n",
       "4  [{'arxivId': '1710.10196', 'authors': [{'autho...   \n",
       "\n",
       "                                               title  \\\n",
       "0                         Transforming Auto-Encoders   \n",
       "1  BDARS_CapsNet: Bi-Directional Attention Routin...   \n",
       "2                          Learning to See by Moving   \n",
       "3  Feature Quantization for Defending Against Dis...   \n",
       "4  BSD-GAN: Branched Generative Adversarial Netwo...   \n",
       "\n",
       "                                              topics  \\\n",
       "0  [{'topic': 'Computer vision', 'topicId': '5332...   \n",
       "1  [{'topic': 'Routing', 'topicId': '1048', 'url'...   \n",
       "2  [{'topic': 'Visual odometry', 'topicId': '644'...   \n",
       "3  [{'topic': 'Distortion', 'topicId': '15080', '...   \n",
       "4  [{'topic': 'BSD', 'topicId': '10760', 'url': '...   \n",
       "\n",
       "                                                 url    year  \\\n",
       "0  https://www.semanticscholar.org/paper/20f03576...  2011.0   \n",
       "1  https://www.semanticscholar.org/paper/4c79e754...  2020.0   \n",
       "2  https://www.semanticscholar.org/paper/dfbfaaec...  2015.0   \n",
       "3  https://www.semanticscholar.org/paper/51bd5966...  2018.0   \n",
       "4  https://www.semanticscholar.org/paper/70977b94...  2020.0   \n",
       "\n",
       "                                           dblp_info  \n",
       "0  {'@score': '3', '@id': '3107620', 'info': {'au...  \n",
       "1                                                NaN  \n",
       "2  {'@score': '5', '@id': '2026007', 'info': {'au...  \n",
       "3  {'@score': '8', '@id': '1024764', 'info': {'au...  \n",
       "4  {'@score': '15', '@id': '211133', 'info': {'au...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_js.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "closed-order",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = [i for i in os.listdir('../data') if \"Merged\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "musical-understanding",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for i in ls:\n",
    "    df_2 = pd.read_csv('../data/'+i)\n",
    "    df = df.append(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "funny-trout",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>published</th>\n",
       "      <th>updated</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "      <th>citations</th>\n",
       "      <th>arxiv_url</th>\n",
       "      <th>gscholar_url</th>\n",
       "      <th>journal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Dynamic Graph CNN for Learning on Point Clouds</td>\n",
       "      <td>['Yue Wang', 'Sanjay E. Sarma', 'Justin M. Sol...</td>\n",
       "      <td>2018-01-24T01:14:04Z</td>\n",
       "      <td>2019-06-11T06:11:21Z</td>\n",
       "      <td>Point clouds provide a flexible geometric repr...</td>\n",
       "      <td>['cs.CV']</td>\n",
       "      <td>536.0</td>\n",
       "      <td>http://arxiv.org/abs/1801.07829v2</td>\n",
       "      <td>http://scholar.google.com/scholar?oi=bibs&amp;clus...</td>\n",
       "      <td>ACM Transactions on Graphics (TOG) 38 (5), 1-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Looking to Listen at the Cocktail Party: A Spe...</td>\n",
       "      <td>['Avinatan Hassidim', 'Oran Lang', 'Michael Ru...</td>\n",
       "      <td>2018-04-10T16:28:59Z</td>\n",
       "      <td>2018-08-09T21:22:37Z</td>\n",
       "      <td>We present a joint audio-visual model for isol...</td>\n",
       "      <td>['cs.SD', 'cs.CV', 'eess.AS']</td>\n",
       "      <td>189.0</td>\n",
       "      <td>http://arxiv.org/abs/1804.03619v2</td>\n",
       "      <td>http://scholar.google.com/scholar?oi=bibs&amp;clus...</td>\n",
       "      <td>ACM Transactions on Graphics (TOG) 37 (4), 1-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Convergence of univariate non-stationary subdi...</td>\n",
       "      <td>['Carla Manni', 'Costanza Conti', 'Marie-Laure...</td>\n",
       "      <td>2014-10-10T10:30:14Z</td>\n",
       "      <td>2014-10-10T10:30:14Z</td>\n",
       "      <td>A new equivalence notion between non-stationar...</td>\n",
       "      <td>['math.NA']</td>\n",
       "      <td>34.0</td>\n",
       "      <td>http://arxiv.org/abs/1410.2729v1</td>\n",
       "      <td>http://scholar.google.com/scholar?oi=bibs&amp;clus...</td>\n",
       "      <td>Computer Aided Geometric Design 37, 1-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>HDR image reconstruction from a single exposur...</td>\n",
       "      <td>['Joel Kronander', 'Rafał K. Mantiuk', 'Gyorgy...</td>\n",
       "      <td>2017-10-20T10:48:22Z</td>\n",
       "      <td>2017-10-20T10:48:22Z</td>\n",
       "      <td>Camera sensors can only capture a limited rang...</td>\n",
       "      <td>['cs.CV', 'cs.GR', 'cs.LG']</td>\n",
       "      <td>145.0</td>\n",
       "      <td>http://arxiv.org/abs/1710.07480v1</td>\n",
       "      <td>http://scholar.google.com/scholar?oi=bibs&amp;clus...</td>\n",
       "      <td>ACM Transactions on Graphics (TOG) 36 (6), 1-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Learning Style Similarity for Searching Infogr...</td>\n",
       "      <td>['Aaron Hertzmann', 'Mira Dontcheva', 'Babak S...</td>\n",
       "      <td>2015-05-05T22:59:32Z</td>\n",
       "      <td>2015-05-05T22:59:32Z</td>\n",
       "      <td>Infographics are complex graphic designs integ...</td>\n",
       "      <td>['cs.GR', 'cs.CV', 'cs.HC', 'cs.IR', 'cs.MM']</td>\n",
       "      <td>24.0</td>\n",
       "      <td>http://arxiv.org/abs/1505.01214v1</td>\n",
       "      <td>http://scholar.google.com/scholar?oi=bibs&amp;clus...</td>\n",
       "      <td>Proceedings of the 41st Graphics Interface Con...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                              title  \\\n",
       "0   0     Dynamic Graph CNN for Learning on Point Clouds   \n",
       "1   1  Looking to Listen at the Cocktail Party: A Spe...   \n",
       "2   2  Convergence of univariate non-stationary subdi...   \n",
       "3   3  HDR image reconstruction from a single exposur...   \n",
       "4   4  Learning Style Similarity for Searching Infogr...   \n",
       "\n",
       "                                             authors             published  \\\n",
       "0  ['Yue Wang', 'Sanjay E. Sarma', 'Justin M. Sol...  2018-01-24T01:14:04Z   \n",
       "1  ['Avinatan Hassidim', 'Oran Lang', 'Michael Ru...  2018-04-10T16:28:59Z   \n",
       "2  ['Carla Manni', 'Costanza Conti', 'Marie-Laure...  2014-10-10T10:30:14Z   \n",
       "3  ['Joel Kronander', 'Rafał K. Mantiuk', 'Gyorgy...  2017-10-20T10:48:22Z   \n",
       "4  ['Aaron Hertzmann', 'Mira Dontcheva', 'Babak S...  2015-05-05T22:59:32Z   \n",
       "\n",
       "                updated                                           abstract  \\\n",
       "0  2019-06-11T06:11:21Z  Point clouds provide a flexible geometric repr...   \n",
       "1  2018-08-09T21:22:37Z  We present a joint audio-visual model for isol...   \n",
       "2  2014-10-10T10:30:14Z  A new equivalence notion between non-stationar...   \n",
       "3  2017-10-20T10:48:22Z  Camera sensors can only capture a limited rang...   \n",
       "4  2015-05-05T22:59:32Z  Infographics are complex graphic designs integ...   \n",
       "\n",
       "                                      categories  citations  \\\n",
       "0                                      ['cs.CV']      536.0   \n",
       "1                  ['cs.SD', 'cs.CV', 'eess.AS']      189.0   \n",
       "2                                    ['math.NA']       34.0   \n",
       "3                    ['cs.CV', 'cs.GR', 'cs.LG']      145.0   \n",
       "4  ['cs.GR', 'cs.CV', 'cs.HC', 'cs.IR', 'cs.MM']       24.0   \n",
       "\n",
       "                           arxiv_url  \\\n",
       "0  http://arxiv.org/abs/1801.07829v2   \n",
       "1  http://arxiv.org/abs/1804.03619v2   \n",
       "2   http://arxiv.org/abs/1410.2729v1   \n",
       "3  http://arxiv.org/abs/1710.07480v1   \n",
       "4  http://arxiv.org/abs/1505.01214v1   \n",
       "\n",
       "                                        gscholar_url  \\\n",
       "0  http://scholar.google.com/scholar?oi=bibs&clus...   \n",
       "1  http://scholar.google.com/scholar?oi=bibs&clus...   \n",
       "2  http://scholar.google.com/scholar?oi=bibs&clus...   \n",
       "3  http://scholar.google.com/scholar?oi=bibs&clus...   \n",
       "4  http://scholar.google.com/scholar?oi=bibs&clus...   \n",
       "\n",
       "                                             journal  \n",
       "0    ACM Transactions on Graphics (TOG) 38 (5), 1-12  \n",
       "1    ACM Transactions on Graphics (TOG) 37 (4), 1-11  \n",
       "2            Computer Aided Geometric Design 37, 1-8  \n",
       "3    ACM Transactions on Graphics (TOG) 36 (6), 1-15  \n",
       "4  Proceedings of the 41st Graphics Interface Con...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-source",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-reporter",
   "metadata": {},
   "source": [
    "### Naïve Data Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "opposite-lawyer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\n"
     ]
    }
   ],
   "source": [
    "# How many matches can be found with a naÏve identical string approach?\n",
    "d1 = df_js['title'].values\n",
    "d2 = df['title'].values\n",
    "print(len([1 for w in d1 if w in d2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-count",
   "metadata": {},
   "source": [
    "## RLTK Data Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "divine-formula",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RLTK Tokenizer\n",
    "tokenizer = rltk.CrfTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-lighting",
   "metadata": {},
   "source": [
    "**Arxiv/Google Scholar Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "acoustic-antarctica",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_js.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "valuable-mistress",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'abstract', 'arxivId', 'authors', 'citationVelocity',\n",
       "       'citations', 'corpusId', 'doi', 'influentialCitationCount', 'paperId',\n",
       "       'references', 'title', 'topics', 'url', 'year', 'dblp_info'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_js.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "descending-atlantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "massive-dylan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'ID', 'title', 'authors', 'published', 'updated', 'abstract',\n",
       "       'categories', 'citations', 'arxiv_url', 'gscholar_url', 'journal'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "victorian-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "perceived-island",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[{'authorId': '1695689', 'name': 'Geoffrey E. Hinton', 'url': 'https://www.semanticscholar.org/author/1695689'}, {'authorId': '2064160', 'name': 'A. Krizhevsky', 'url': 'https://www.semanticscholar.org/author/2064160'}, {'authorId': '49185042', 'name': 'S. Wang', 'url': 'https://www.semanticscholar.org/author/49185042'}]\""
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_js['authors'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "hawaiian-prerequisite",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['Yue Wang', 'Sanjay E. Sarma', 'Justin M. Solomon', 'Yongbin Sun', 'Ziwei Liu', 'Michael M. Bronstein']\""
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['authors'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "rapid-tradition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "wrapped-jefferson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hinton', 'Krizhevsky', 'Wang'}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getPlainString(string):\n",
    "    return ''.join([s for s in string if not s.isnumeric()]).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "informal-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArxivRecord(rltk.Record):\n",
    "    def __init__(self, raw_object):\n",
    "        super().__init__(raw_object)\n",
    "        self.name = 'ArxivRecord'\n",
    "        \n",
    "    @property\n",
    "    def id(self):\n",
    "        return str(self.raw_object['ID'])\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def authors_string(self):\n",
    "        return self.raw_object['authors']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def title_string(self):\n",
    "        return self.raw_object['title']\n",
    "        \n",
    "#     @rltk.cached_property\n",
    "#     def summary_string(self):\n",
    "#         return self.raw_object['summary']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def categories_string(self):\n",
    "        return self.raw_object['categories']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def published_string(self):\n",
    "        return self.raw_object['published']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def updated_string(self):\n",
    "        return self.raw_object['updated']\n",
    "        \n",
    "    @rltk.cached_property\n",
    "    def blocking_author_tokens(self):\n",
    "        return set([getPlainString(s).split(' ')[-1] for s in ast.literal_eval(self.raw_object['authors'])])\n",
    "    \n",
    "#     @rltk.cached_property\n",
    "#     def url_string(self):\n",
    "#         return self.raw_object['url']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def blocking_tokens(self):\n",
    "        tokens = ' '.join([self.title_string])\n",
    "        tokens = re.sub(r'\\bThe\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bthe\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bof\\b', '', tokens)\n",
    "        tokens = re.sub(r\"\\b's\\b\", '', tokens)\n",
    "        tokens = re.sub(r'\\band\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bI\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bA\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bin\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bfor\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bon\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bwith\\b', '', tokens)\n",
    "        return set(tokenizer.tokenize(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "marked-marketing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'rltk.dataset.Dataset'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>authors_string</th>\n",
       "      <th>title_string</th>\n",
       "      <th>categories_string</th>\n",
       "      <th>published_string</th>\n",
       "      <th>updated_string</th>\n",
       "      <th>blocking_author_tokens</th>\n",
       "      <th>blocking_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>['Barret Zoph', 'Kevin Knight']</td>\n",
       "      <td>Multi-Source Neural Translation</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>2016-01-05T00:49:22Z</td>\n",
       "      <td>2016-01-05T00:49:22Z</td>\n",
       "      <td>{Zoph, Knight}</td>\n",
       "      <td>{Source, -, Neural, Multi, Translation}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>['Xiaodong He', 'Asli Celikyilmaz', 'Antoine B...</td>\n",
       "      <td>Deep Communicating Agents for Abstractive Summ...</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>2018-03-27T23:29:23Z</td>\n",
       "      <td>2018-08-15T18:54:22Z</td>\n",
       "      <td>{Choi, Bosselut, Celikyilmaz, He}</td>\n",
       "      <td>{Agents, Communicating, Deep, Summarization, A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>['Furu Wei', 'Ming Zhou', 'Nan Yang', 'Qingyu ...</td>\n",
       "      <td>Selective Encoding for Abstractive Sentence Su...</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>2017-04-24T07:57:37Z</td>\n",
       "      <td>2017-04-24T07:57:37Z</td>\n",
       "      <td>{Yang, Zhou, Wei}</td>\n",
       "      <td>{Selective, Encoding, Sentence, Summarization,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                     authors_string  \\\n",
       "0  0                    ['Barret Zoph', 'Kevin Knight']   \n",
       "1  1  ['Xiaodong He', 'Asli Celikyilmaz', 'Antoine B...   \n",
       "2  2  ['Furu Wei', 'Ming Zhou', 'Nan Yang', 'Qingyu ...   \n",
       "\n",
       "                                        title_string categories_string  \\\n",
       "0                    Multi-Source Neural Translation         ['cs.CL']   \n",
       "1  Deep Communicating Agents for Abstractive Summ...         ['cs.CL']   \n",
       "2  Selective Encoding for Abstractive Sentence Su...         ['cs.CL']   \n",
       "\n",
       "       published_string        updated_string  \\\n",
       "0  2016-01-05T00:49:22Z  2016-01-05T00:49:22Z   \n",
       "1  2018-03-27T23:29:23Z  2018-08-15T18:54:22Z   \n",
       "2  2017-04-24T07:57:37Z  2017-04-24T07:57:37Z   \n",
       "\n",
       "              blocking_author_tokens  \\\n",
       "0                     {Zoph, Knight}   \n",
       "1  {Choi, Bosselut, Celikyilmaz, He}   \n",
       "2                  {Yang, Zhou, Wei}   \n",
       "\n",
       "                                     blocking_tokens  \n",
       "0            {Source, -, Neural, Multi, Translation}  \n",
       "1  {Agents, Communicating, Deep, Summarization, A...  \n",
       "2  {Selective, Encoding, Sentence, Summarization,...  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_1 = rltk.Dataset(reader=rltk.DataFrameReader(df), record_class=ArxivRecord, adapter=rltk.MemoryKeyValueAdapter())\n",
    "print(type(ds_1))\n",
    "ds_1.generate_dataframe().head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-steps",
   "metadata": {},
   "source": [
    "**Google Scholar Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "august-sellers",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticRecord(rltk.Record):\n",
    "    def __init__(self, raw_object):\n",
    "        super().__init__(raw_object)\n",
    "        self.name = 'SemanticRecord'\n",
    "        \n",
    "    @property\n",
    "    def id(self):\n",
    "        return str(self.raw_object['index'])\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def authors_string(self):\n",
    "        return self.raw_object['authors']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def title_string(self):\n",
    "        return self.raw_object['title']\n",
    "        \n",
    "#     @rltk.cached_property\n",
    "#     def journal_string(self):\n",
    "#         return self.raw_object['journal']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def citations_string(self):\n",
    "        return self.raw_object['citations']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def year_string(self):\n",
    "        return self.raw_object['year']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def url_string(self):\n",
    "        return self.raw_object['url']\n",
    "    \n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def blocking_author_tokens(self):\n",
    "        return set([getPlainString(s['name']).split(' ')[-1] for s in ast.literal_eval((self.raw_object['authors']))])\n",
    "\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def blocking_tokens(self):\n",
    "        tokens = ' '.join([self.title_string])\n",
    "        tokens = re.sub(r'\\bThe\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bthe\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bof\\b', '', tokens)\n",
    "        tokens = re.sub(r\"\\b's\\b\", '', tokens)\n",
    "        tokens = re.sub(r'\\band\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bI\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bA\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bin\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bfor\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bon\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bwith\\b', '', tokens)\n",
    "        return set(tokenizer.tokenize(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "embedded-tournament",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'rltk.dataset.Dataset'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>authors_string</th>\n",
       "      <th>title_string</th>\n",
       "      <th>citations_string</th>\n",
       "      <th>year_string</th>\n",
       "      <th>url_string</th>\n",
       "      <th>blocking_author_tokens</th>\n",
       "      <th>blocking_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'authorId': '1695689', 'name': 'Geoffrey E. ...</td>\n",
       "      <td>Transforming Auto-Encoders</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '1...</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>https://www.semanticscholar.org/paper/20f03576...</td>\n",
       "      <td>{Krizhevsky, Wang, Hinton}</td>\n",
       "      <td>{Auto, Encoders, -, Transforming}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'authorId': '144866658', 'name': 'Xin Ning',...</td>\n",
       "      <td>BDARS_CapsNet: Bi-Directional Attention Routin...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c79e754...</td>\n",
       "      <td>{Nie, Ning, Sun, Chen, Tian, Li, Lu}</td>\n",
       "      <td>{Directional, _, -, Attention, Sausage, Capsul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[{'authorId': '33932184', 'name': 'Pulkit Agra...</td>\n",
       "      <td>Learning to See by Moving</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '2...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>https://www.semanticscholar.org/paper/dfbfaaec...</td>\n",
       "      <td>{Agrawal, Malik, Carreira}</td>\n",
       "      <td>{Moving, Learning, by, See, to}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                     authors_string  \\\n",
       "0  0  [{'authorId': '1695689', 'name': 'Geoffrey E. ...   \n",
       "1  1  [{'authorId': '144866658', 'name': 'Xin Ning',...   \n",
       "2  2  [{'authorId': '33932184', 'name': 'Pulkit Agra...   \n",
       "\n",
       "                                        title_string  \\\n",
       "0                         Transforming Auto-Encoders   \n",
       "1  BDARS_CapsNet: Bi-Directional Attention Routin...   \n",
       "2                          Learning to See by Moving   \n",
       "\n",
       "                                    citations_string  year_string  \\\n",
       "0  [{'arxivId': None, 'authors': [{'authorId': '1...       2011.0   \n",
       "1                                                 []       2020.0   \n",
       "2  [{'arxivId': None, 'authors': [{'authorId': '2...       2015.0   \n",
       "\n",
       "                                          url_string  \\\n",
       "0  https://www.semanticscholar.org/paper/20f03576...   \n",
       "1  https://www.semanticscholar.org/paper/4c79e754...   \n",
       "2  https://www.semanticscholar.org/paper/dfbfaaec...   \n",
       "\n",
       "                 blocking_author_tokens  \\\n",
       "0            {Krizhevsky, Wang, Hinton}   \n",
       "1  {Nie, Ning, Sun, Chen, Tian, Li, Lu}   \n",
       "2            {Agrawal, Malik, Carreira}   \n",
       "\n",
       "                                     blocking_tokens  \n",
       "0                  {Auto, Encoders, -, Transforming}  \n",
       "1  {Directional, _, -, Attention, Sausage, Capsul...  \n",
       "2                    {Moving, Learning, by, See, to}  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_2 = rltk.Dataset(reader=rltk.DataFrameReader(df_js), record_class=SemanticRecord, adapter=rltk.MemoryKeyValueAdapter())\n",
    "print(type(ds_2))\n",
    "ds_2.generate_dataframe().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cognitive-yorkshire",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-fraud",
   "metadata": {},
   "source": [
    "### Blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "dramatic-disabled",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'rltk.blocking.block.Block'>\n"
     ]
    }
   ],
   "source": [
    "# Generate blocks from tokens\n",
    "token_blocker = rltk.TokenBlockGenerator()\n",
    "blocks = token_blocker.generate(\n",
    "    token_blocker.block(ds_1, property_='blocking_author_tokens'),\n",
    "    token_blocker.block(ds_2, property_='blocking_author_tokens'))\n",
    "print(type(blocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "fifth-dispatch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction Ratio: 0.93040\n"
     ]
    }
   ],
   "source": [
    "# Extract all record pairs from the block\n",
    "record_pairs = rltk.get_record_pairs(ds_1, ds_2, block=blocks)\n",
    "\n",
    "# Get the total number of record pairs generated\n",
    "compared_pairs = len(list(record_pairs))\n",
    "\n",
    "# Get the number of elements in each rltk.Dataset\n",
    "tally_imdb = ds_1.generate_dataframe().shape[0]\n",
    "tally_tmd = ds_2.generate_dataframe().shape[0]\n",
    "\n",
    "# Calculate the total number of pairs if both datasets were to be compared without any blocking (eg: a double for loop)\n",
    "tally_unblocked = tally_imdb * tally_tmd\n",
    "\n",
    "# Calculate how much smaller the blocked pairings are\n",
    "reduction_ratio = compared_pairs / tally_unblocked\n",
    "\n",
    "# Calculate the reduction ratio (the inverse of the )\n",
    "reduction_ratio = 1 - reduction_ratio\n",
    "print(f'Reduction Ratio: {reduction_ratio:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "blank-aggregate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3036085"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compared_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-public",
   "metadata": {},
   "source": [
    "### Matching Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "automated-penny",
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_similarity(arxiv_tuple, gscholar_tuple):\n",
    "    arxiv_title = arxiv_tuple.title_string.strip().lower()\n",
    "    gscholar_title = gscholar_tuple.title_string.strip().lower()\n",
    "    similarity = SequenceMatcher(None, arxiv_title, gscholar_title).ratio()\n",
    "\n",
    "    penalties = sum([len(arxiv_title)<=6,\n",
    "                     len(gscholar_title)<=6])\n",
    "\n",
    "    return similarity * (0.9**penalties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "together-terminology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def author_similarity(arxiv_tuple, gscholar_tuple):\n",
    "    arxiv_author = ' '.join(arxiv_tuple.authors_string).strip().lower()\n",
    "    gscholar_author = ' '.join(gscholar_tuple.authors_string).strip().lower()\n",
    "    similarity = SequenceMatcher(None, arxiv_author, gscholar_author).ratio() \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "wooden-powder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_similarity(arxiv_tuple, gscholar_tuple):\n",
    "#     arxiv_year = int(float(arxiv_tuple.updated_string[0:4]))\n",
    "#     print(arxiv_tuple.published_string)\n",
    "    if str(arxiv_tuple.published_string) == \"nan\":\n",
    "        return 0\n",
    "    arxiv_year = datetime.datetime.strptime(arxiv_tuple.published_string, '%Y-%m-%dT%H:%M:%SZ').year\n",
    "\n",
    "    gscholar_year = int(float(gscholar_tuple.year_string))\n",
    "    similarity = 1 /(1 + abs(arxiv_year-gscholar_year))\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "wireless-tenant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elementwise_similarity(arxiv_tuple, gscholar_tuple, match_threshold=0.75):\n",
    "    sim_title = title_similarity(arxiv_tuple, gscholar_tuple)\n",
    "    sim_author = author_similarity(arxiv_tuple, gscholar_tuple)\n",
    "    sim_year = year_similarity(arxiv_tuple, gscholar_tuple)\n",
    "\n",
    "    element_similarity = (0.70 * sim_title) + (0.15 * sim_author) + (0.15 * sim_year)\n",
    "\n",
    "    return element_similarity > match_threshold, element_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "straight-vertex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arxiv samples: 4646\n",
      "Semantic samples: 20000\n",
      "10000\n",
      "7.284152\n",
      "20000\n",
      "14.271344\n",
      "30000\n",
      "21.127184\n",
      "40000\n",
      "57.158214\n",
      "50000\n",
      "64.671015\n",
      "60000\n",
      "71.706392\n",
      "70000\n",
      "81.137466\n",
      "80000\n",
      "88.420146\n",
      "90000\n",
      "94.897969\n",
      "100000\n",
      "102.849485\n",
      "110000\n",
      "110.056087\n",
      "120000\n",
      "117.790662\n",
      "130000\n",
      "125.296907\n",
      "140000\n",
      "131.97089\n",
      "150000\n",
      "138.974103\n",
      "160000\n",
      "146.407909\n",
      "170000\n",
      "154.458258\n",
      "180000\n",
      "161.863257\n",
      "190000\n",
      "169.634482\n",
      "200000\n",
      "175.831923\n",
      "210000\n",
      "183.043679\n",
      "220000\n",
      "190.056504\n",
      "230000\n",
      "202.94599\n",
      "240000\n",
      "209.471111\n",
      "250000\n",
      "217.605227\n",
      "260000\n",
      "224.855609\n",
      "270000\n",
      "232.219422\n",
      "280000\n",
      "239.216955\n",
      "290000\n",
      "245.238374\n",
      "300000\n",
      "252.28544\n",
      "310000\n",
      "259.382405\n",
      "320000\n",
      "269.411767\n",
      "330000\n",
      "276.211361\n",
      "340000\n",
      "282.816828\n",
      "350000\n",
      "289.249735\n",
      "360000\n",
      "296.968307\n",
      "370000\n",
      "304.009623\n",
      "380000\n",
      "310.439445\n",
      "390000\n",
      "319.551906\n",
      "400000\n",
      "328.329245\n",
      "410000\n",
      "336.315853\n",
      "420000\n",
      "343.388986\n",
      "430000\n",
      "350.132081\n",
      "440000\n",
      "355.988518\n",
      "450000\n",
      "412.284076\n",
      "460000\n",
      "418.76381\n",
      "470000\n",
      "427.449864\n",
      "480000\n",
      "433.662697\n",
      "490000\n",
      "439.765901\n",
      "500000\n",
      "447.759576\n",
      "510000\n",
      "455.710945\n",
      "520000\n",
      "463.370302\n",
      "530000\n",
      "469.723784\n",
      "540000\n",
      "476.614581\n",
      "550000\n",
      "483.675219\n",
      "560000\n",
      "490.218859\n",
      "570000\n",
      "497.212427\n",
      "580000\n",
      "503.304049\n",
      "590000\n",
      "509.995991\n",
      "600000\n",
      "517.299754\n",
      "610000\n",
      "524.509443\n",
      "620000\n",
      "531.471303\n",
      "630000\n",
      "538.495006\n",
      "640000\n",
      "546.914005\n",
      "650000\n",
      "553.687244\n",
      "660000\n",
      "560.500347\n",
      "670000\n",
      "566.919521\n",
      "680000\n",
      "574.471122\n",
      "690000\n",
      "581.093091\n",
      "700000\n",
      "590.338978\n",
      "710000\n",
      "596.762921\n",
      "720000\n",
      "603.518805\n",
      "730000\n",
      "610.640519\n",
      "740000\n",
      "617.682299\n",
      "750000\n",
      "624.133656\n",
      "760000\n",
      "630.474523\n",
      "770000\n",
      "636.445194\n",
      "780000\n",
      "642.847296\n",
      "790000\n",
      "648.868247\n",
      "800000\n",
      "655.095241\n",
      "810000\n",
      "664.336085\n",
      "820000\n",
      "671.810622\n",
      "830000\n",
      "679.180912\n",
      "840000\n",
      "687.100897\n",
      "850000\n",
      "694.375066\n",
      "860000\n",
      "700.824188\n",
      "870000\n",
      "707.314236\n",
      "880000\n",
      "714.497909\n",
      "890000\n",
      "721.766494\n",
      "900000\n",
      "728.53005\n",
      "910000\n",
      "736.042896\n",
      "920000\n",
      "741.920812\n",
      "930000\n",
      "748.145624\n",
      "940000\n",
      "754.618568\n",
      "950000\n",
      "763.154552\n",
      "960000\n",
      "769.451624\n",
      "970000\n",
      "777.303321\n",
      "980000\n",
      "783.301061\n",
      "990000\n",
      "789.54256\n",
      "1000000\n",
      "796.606284\n",
      "1010000\n",
      "804.081102\n",
      "1020000\n",
      "810.348145\n",
      "1030000\n",
      "817.502002\n",
      "1040000\n",
      "824.881272\n",
      "1050000\n",
      "831.928864\n",
      "1060000\n",
      "839.969602\n",
      "1070000\n",
      "846.720809\n",
      "1080000\n",
      "852.899305\n",
      "1090000\n",
      "859.571425\n",
      "1100000\n",
      "868.094825\n",
      "1110000\n",
      "876.243067\n",
      "1120000\n",
      "884.29608\n",
      "1130000\n",
      "890.782417\n",
      "1140000\n",
      "897.080359\n",
      "1150000\n",
      "905.310215\n",
      "1160000\n",
      "912.055992\n",
      "1170000\n",
      "918.665941\n",
      "1180000\n",
      "925.480789\n",
      "1190000\n",
      "932.745232\n",
      "1200000\n",
      "939.462361\n",
      "1210000\n",
      "947.013373\n",
      "1220000\n",
      "955.157511\n",
      "1230000\n",
      "965.569362\n",
      "1240000\n",
      "973.000956\n",
      "1250000\n",
      "980.404773\n",
      "1260000\n",
      "987.482557\n",
      "1270000\n",
      "995.139252\n",
      "1280000\n",
      "1002.450711\n",
      "1290000\n",
      "1009.024045\n",
      "1300000\n",
      "1015.3307\n",
      "1310000\n",
      "1073.276642\n",
      "1320000\n",
      "1080.091523\n",
      "1330000\n",
      "1087.192409\n",
      "1340000\n",
      "1095.4504\n",
      "1350000\n",
      "1102.787509\n",
      "1360000\n",
      "1109.106608\n",
      "1370000\n",
      "1116.742076\n",
      "1380000\n",
      "1123.315174\n",
      "1390000\n",
      "1130.570158\n",
      "1400000\n",
      "1136.920399\n",
      "1410000\n",
      "1143.057991\n",
      "1420000\n",
      "1149.552941\n",
      "1430000\n",
      "1156.591511\n",
      "1440000\n",
      "1164.412447\n",
      "1450000\n",
      "1170.413687\n",
      "1460000\n",
      "1176.586634\n",
      "1470000\n",
      "1182.584559\n",
      "1480000\n",
      "1189.038936\n",
      "1490000\n",
      "1196.754343\n",
      "1500000\n",
      "1204.548681\n",
      "1510000\n",
      "1213.492388\n",
      "1520000\n",
      "1220.014896\n",
      "1530000\n",
      "1226.853553\n",
      "1540000\n",
      "1235.283114\n",
      "1550000\n",
      "1244.2385\n",
      "1560000\n",
      "1251.37394\n",
      "1570000\n",
      "1258.758418\n",
      "1580000\n",
      "1265.387066\n",
      "1590000\n",
      "1272.414975\n",
      "1600000\n",
      "1279.433991\n",
      "1610000\n",
      "1286.090785\n",
      "1620000\n",
      "1295.372062\n",
      "1630000\n",
      "1301.93807\n",
      "1640000\n",
      "1314.498164\n",
      "1650000\n",
      "1321.766604\n",
      "1660000\n",
      "1355.407614\n",
      "1670000\n",
      "1371.445874\n",
      "1680000\n",
      "1378.588961\n",
      "1690000\n",
      "1386.136178\n",
      "1700000\n",
      "1392.669988\n",
      "1710000\n",
      "1400.547836\n",
      "1720000\n",
      "1407.472527\n",
      "1730000\n",
      "1415.958996\n",
      "1740000\n",
      "1423.6087\n",
      "1750000\n",
      "1430.596418\n",
      "1760000\n",
      "1440.203075\n",
      "1770000\n",
      "1447.319961\n",
      "1780000\n",
      "1454.839748\n",
      "1790000\n",
      "1462.476811\n",
      "1800000\n",
      "1468.934219\n",
      "1810000\n",
      "1478.229816\n",
      "1820000\n",
      "1486.574225\n",
      "1830000\n",
      "1496.547127\n",
      "1840000\n",
      "1505.508869\n",
      "1850000\n",
      "1514.471903\n",
      "1860000\n",
      "1521.637602\n",
      "1870000\n",
      "1528.378465\n",
      "1880000\n",
      "1534.651284\n",
      "1890000\n",
      "1540.980365\n",
      "1900000\n",
      "1548.981372\n",
      "1910000\n",
      "1559.001916\n",
      "1920000\n",
      "1565.489668\n",
      "1930000\n",
      "1572.87659\n",
      "1940000\n",
      "1578.505695\n",
      "1950000\n",
      "1585.096281\n",
      "1960000\n",
      "1591.678093\n",
      "1970000\n",
      "1601.521233\n",
      "1980000\n",
      "1608.35015\n",
      "1990000\n",
      "1614.173054\n",
      "2000000\n",
      "1620.255893\n",
      "2010000\n",
      "1627.069564\n",
      "2020000\n",
      "1636.344493\n",
      "2030000\n",
      "1643.322097\n",
      "2040000\n",
      "1649.29011\n",
      "2050000\n",
      "1665.09506\n",
      "2060000\n",
      "1672.5019\n",
      "2070000\n",
      "1679.718918\n",
      "2080000\n",
      "1687.711642\n",
      "2090000\n",
      "1694.269635\n",
      "2100000\n",
      "1701.779726\n",
      "2110000\n",
      "1709.45914\n",
      "2120000\n",
      "1717.108094\n",
      "2130000\n",
      "1724.202057\n",
      "2140000\n",
      "1730.974367\n",
      "2150000\n",
      "1738.494527\n",
      "2160000\n",
      "1745.126561\n",
      "2170000\n",
      "1751.781031\n",
      "2180000\n",
      "1760.264207\n",
      "2190000\n",
      "1768.049862\n",
      "2200000\n",
      "1775.163848\n",
      "2210000\n",
      "1781.647181\n",
      "2220000\n",
      "1787.926242\n",
      "2230000\n",
      "1796.276573\n",
      "2240000\n",
      "1804.244902\n",
      "2250000\n",
      "1812.137442\n",
      "2260000\n",
      "1819.445305\n",
      "2270000\n",
      "1825.065927\n",
      "2280000\n",
      "1830.854683\n",
      "2290000\n",
      "1836.729655\n",
      "2300000\n",
      "1844.043333\n",
      "2310000\n",
      "1851.87451\n",
      "2320000\n",
      "1858.063315\n",
      "2330000\n",
      "1866.212323\n",
      "2340000\n",
      "1873.937304\n",
      "2350000\n",
      "1879.98522\n",
      "2360000\n",
      "1886.560714\n",
      "2370000\n",
      "1893.297066\n",
      "2380000\n",
      "1900.071231\n",
      "2390000\n",
      "1909.496652\n",
      "2400000\n",
      "1916.413657\n",
      "2410000\n",
      "1923.894197\n",
      "2420000\n",
      "1931.903492\n",
      "2430000\n",
      "1940.021054\n",
      "2440000\n",
      "1947.529194\n",
      "2450000\n",
      "1954.45443\n",
      "2460000\n",
      "1961.658183\n",
      "2470000\n",
      "1987.155132\n",
      "2480000\n",
      "1994.081907\n",
      "2490000\n",
      "2002.315952\n",
      "2500000\n",
      "2010.631021\n",
      "2510000\n",
      "2018.514642\n",
      "2520000\n",
      "2026.133635\n",
      "2530000\n",
      "2033.016178\n",
      "2540000\n",
      "2040.44473\n",
      "2550000\n",
      "2048.043038\n",
      "2560000\n",
      "2055.329403\n",
      "2570000\n",
      "2062.808779\n",
      "2580000\n",
      "2073.236214\n",
      "2590000\n",
      "2082.491648\n",
      "2600000\n",
      "2090.982929\n",
      "2610000\n",
      "2099.329773\n",
      "2620000\n",
      "2107.293837\n",
      "2630000\n",
      "2114.401715\n",
      "2640000\n",
      "2122.837446\n",
      "2650000\n",
      "2130.167498\n",
      "2660000\n",
      "2138.977424\n",
      "2670000\n",
      "2156.12732\n",
      "2680000\n",
      "2163.20146\n",
      "2690000\n",
      "2170.434365\n",
      "2700000\n",
      "2178.182487\n",
      "2710000\n",
      "2185.039042\n",
      "2720000\n",
      "2192.605793\n",
      "2730000\n",
      "2200.308129\n",
      "2740000\n",
      "2208.008963\n",
      "2750000\n",
      "2216.367249\n",
      "2760000\n",
      "2223.79525\n",
      "2770000\n",
      "2230.969749\n",
      "2780000\n",
      "2238.212603\n",
      "2790000\n",
      "2253.721549\n",
      "2800000\n",
      "2262.110619\n",
      "2810000\n",
      "2269.807557\n",
      "2820000\n",
      "2279.153742\n",
      "2830000\n",
      "2286.841635\n",
      "2840000\n",
      "2293.977103\n",
      "2850000\n",
      "2305.658097\n",
      "2860000\n",
      "2313.152597\n",
      "2870000\n",
      "2319.772829\n",
      "2880000\n",
      "2327.919417\n",
      "2890000\n",
      "2335.458012\n",
      "2900000\n",
      "2345.155335\n",
      "2910000\n",
      "2352.466999\n",
      "2920000\n",
      "2360.136736\n",
      "2930000\n",
      "2370.244499\n",
      "2940000\n",
      "2379.844547\n",
      "2950000\n",
      "2388.409383\n",
      "2960000\n",
      "2404.173807\n",
      "2970000\n",
      "2420.964805\n",
      "2980000\n",
      "2428.71202\n",
      "2990000\n",
      "2437.099382\n",
      "3000000\n",
      "2444.914994\n",
      "3010000\n",
      "2454.364327\n",
      "3020000\n",
      "2462.626815\n",
      "3030000\n",
      "2483.287025\n"
     ]
    }
   ],
   "source": [
    "# Predict matches for all pairs in the blocked data \n",
    "print(f'Arxiv samples: {df.shape[0]}')\n",
    "print(f'Semantic samples: {df_js.shape[0]}')\n",
    "st = datetime.datetime.now()\n",
    "summary_df = pd.DataFrame()\n",
    "THRESHOLDS = [T/100 for T in range(0, 101, 5)]\n",
    "c=0\n",
    "# Iterate through various thresholds to find the most matches without any duplicates\n",
    "# for T in tqdm(THRESHOLDS):\n",
    "\n",
    "    # Set to store pairs of IDs matched\n",
    "#     ids_matched = set()\n",
    "    \n",
    "    # Iterate through candidates on the block\n",
    "c=0\n",
    "for block_id, arxiv_id, gscholar_id in blocks.pairwise(ds_1, ds_2):\n",
    "\n",
    "    # Find similarity at a given threshold\n",
    "    match , similarity = elementwise_similarity(ds_1.get_record(arxiv_id),\n",
    "                                                ds_2.get_record(gscholar_id),\n",
    "                                                match_threshold=T)\n",
    "    # If a match is found, add to the set of matches\n",
    "    if match:\n",
    "        ids_matched.add((arxiv_id, gscholar_id))\n",
    "    c=c+1\n",
    "    if c%10000==0:\n",
    "        et = datetime.datetime.now()\n",
    "        print(c)\n",
    "        print((et-st).total_seconds())\n",
    "    \n",
    "#     # Count the number of unique elements derived from each source\n",
    "#     set_a = set()\n",
    "#     set_b = set()\n",
    "#     for tp in ids_matched:\n",
    "#         set_a.add(tp[0])\n",
    "#         set_b.add(tp[1])\n",
    "    \n",
    "#     summary_df.at[T, 'Matches'] = int(len(ids_matched))\n",
    "#     summary_df.at[T, 'Set_A Size'] = int(len(set_a))\n",
    "#     summary_df.at[T, 'Set_B Size'] = int(len(ids_matched))\n",
    "#     summary_df.at[T, 'Duplicates'] = int((len(ids_matched)-len(set_a)) + (len(ids_matched)-len(set_b)))\n",
    "    \n",
    "# summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "scenic-captain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2700133"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids_matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-anaheim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the lowest threshold which gives no duplicates\n",
    "optimal_threshold = summary_df[summary_df['Duplicates']==0].index[0]\n",
    "optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-blowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually set threshold to 0.85 to trade 104 extra True Positives for 3 extra False Positive\n",
    "optimal_threshold = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-florence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate matches based on the optimal (no-duplicate) threshold\n",
    "print(f'Arxiv samples: {df_arxiv.shape[0]}')\n",
    "print(f'GScholar samples: {df_gscholar.shape[0]}')\n",
    "\n",
    "# Store tuples of matches IDs, as well as singletons witouth a match\n",
    "ids_matched = set()\n",
    "singles_arxiv = set()\n",
    "singles_gscholar = set()\n",
    "\n",
    "# Write matches (and non-matches) to a CSV\n",
    "with open(f'Matches_{TOPIC}.csv', 'w') as predictions_full:\n",
    "    for block_id, arxiv_id, gscholar_id in blocks.pairwise(ds_arxiv, ds_gscholar):\n",
    "\n",
    "        match , similarity = elementwise_similarity(ds_arxiv.get_record(arxiv_id),\n",
    "                                                    ds_gscholar.get_record(gscholar_id),\n",
    "                                                    match_threshold=optimal_threshold)\n",
    "\n",
    "        if match:\n",
    "            ids_matched.add((arxiv_id, gscholar_id))\n",
    "        else:\n",
    "            singles_arxiv.add(arxiv_id)\n",
    "            singles_gscholar.add(gscholar_id)\n",
    "    \n",
    "    # After finding all matches, write them to a csv\n",
    "    for match_pair in ids_matched:\n",
    "        predictions_full.write(f'{match_pair[0]},{match_pair[1]},1\\n')\n",
    "        # And ensure that no item in the matches is counted as a single\n",
    "        try:\n",
    "            singles_arxiv.remove(match_pair[0])\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            singles_gscholar.remove(match_pair[1])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Then write all the singles which didn't find a match\n",
    "    NULL = None\n",
    "    for arxiv_id in singles_arxiv:\n",
    "        predictions_full.write(f'{arxiv_id},{NULL},0\\n')\n",
    "    for gscholar_id in singles_gscholar:\n",
    "        predictions_full.write(f'{NULL},{gscholar_id},0\\n')        \n",
    "        \n",
    "print()\n",
    "print(f'Matches: {len(ids_matched)}')\n",
    "print(f'Non-Matches Arxiv: {len(singles_arxiv)}')\n",
    "print(f'Non-Matches GScholar: {len(singles_gscholar)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-clause",
   "metadata": {},
   "source": [
    "### Create Merged Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-banking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib import URIRef, Literal, Namespace\n",
    "from rdflib.namespace import RDF, RDFS, XSD\n",
    "\n",
    "MYNS = Namespace('http://inf558.org/myfakenamespace#')\n",
    "SCHEMA = Namespace(\"https://schema.org/\")\n",
    "\n",
    "# Initliaze the graph\n",
    "g = rdflib.Graph()\n",
    "\n",
    "# Bind namespace and prefixes\n",
    "g.bind('my_ns', MYNS)\n",
    "g.bind('schema', SCHEMA)\n",
    "g.bind('rdf', RDF)\n",
    "g.bind('rdfs', RDFS)\n",
    "g.bind('xsd', XSD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-delta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictions to be used in populating the RDF\n",
    "predictions_df = pd.read_csv(f'Matches_{TOPIC}.csv', header=None, names=['ARXIV_ID', 'GSCHOLAR_ID', 'LABEL'])\n",
    "print(f'predictions_df.shape: {predictions_df.shape}')\n",
    "predicted_matches = predictions_df['LABEL'].sum()\n",
    "print(f'predicted matches: {predicted_matches}  [{100*predicted_matches/predictions_df.shape[0]:.2f} %]')\n",
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-azerbaijan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe to store the merged datasets\n",
    "df_merged = pd.DataFrame(columns = ['ID', 'title', 'authors', 'published', 'updated', \n",
    "                                    'abstract', 'categories', 'citations', 'arxiv_url', 'gscholar_url'],\n",
    "                         dtype='object')\n",
    "json_merged = {}\n",
    "\n",
    "NEW_ID = 0\n",
    "\n",
    "# Populate the RDF with predictions with a positive (1) label\n",
    "for idx, row in tqdm(predictions_df.iterrows(), total=predictions_df.shape[0]):\n",
    "    \n",
    "    # Populate the json object\n",
    "    json_merged[NEW_ID] = {}\n",
    "    \n",
    "    ### URI ###\n",
    "    node_uri = URIRef(str(NEW_ID))\n",
    "    g.add((node_uri, RDF.type, SCHEMA.ScholarlyArticle))\n",
    "    df_merged.at[NEW_ID, 'ID'] = NEW_ID\n",
    "    json_merged[NEW_ID]['ID'] = NEW_ID\n",
    "\n",
    "    \n",
    "    ### Title ###\n",
    "    try:\n",
    "        title_arxiv = str(df_arxiv[df_arxiv['ID'] == str(row['ARXIV_ID'])]['title'].values[0])\n",
    "    except:\n",
    "        title_arxiv = '<___>'\n",
    "    try:\n",
    "        title_gscholar = str(df_gscholar[df_gscholar['ID'] == str(row['GSCHOLAR_ID'])]['title'].values[0])\n",
    "    except:\n",
    "        title_gscholar = '<___>'\n",
    "    title = title_arxiv if title_arxiv != '<___>' else title_gscholar if title_gscholar != '<___>' else None\n",
    "    g.add((node_uri, SCHEMA.headline, Literal(title, datatype=SCHEMA.Text)))\n",
    "    df_merged.at[NEW_ID, 'title'] = title\n",
    "    json_merged[NEW_ID]['title'] = title\n",
    "\n",
    "    \n",
    "    ### Author(s) ###\n",
    "    try:\n",
    "        author_arxiv = df_arxiv[df_arxiv['ID'] == str(row['ARXIV_ID'])]['authors'].values[0]\n",
    "        author_arxiv = [name.strip() for name in author_arxiv if name != '<___>']\n",
    "    except:\n",
    "        author_arxiv = '<___>'\n",
    "    try:\n",
    "        author_gscholar = df_gscholar[df_gscholar['ID'] == str(row['GSCHOLAR_ID'])]['authors'].values[0]\n",
    "        author_gscholar = [name.strip() for name in author_gscholar if name != '<___>']\n",
    "    except:\n",
    "        author_gscholar = '<___>'\n",
    "    if author_arxiv != '<___>':\n",
    "        authors = list(set(author_arxiv))\n",
    "    else:\n",
    "        authors = list(set(author_gscholar))           \n",
    "    [g.add((node_uri, SCHEMA.author, Literal(author, datatype=SCHEMA.Person))) for author in authors]\n",
    "    df_merged.at[NEW_ID, 'authors'] = authors\n",
    "    json_merged[NEW_ID]['authors'] = authors\n",
    "                       \n",
    "    ### Published ###\n",
    "    try:\n",
    "        published = str(df_arxiv[df_arxiv['ID'] == str(row['ARXIV_ID'])]['published'].values[0])\n",
    "        g.add((node_uri, SCHEMA.datePublished, Literal(published, datatype=SCHEMA.DateTime)))\n",
    "        df_merged.at[NEW_ID, 'published'] = published\n",
    "        json_merged[NEW_ID]['published'] = published\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "                       \n",
    "    ### Updated ###\n",
    "    try:\n",
    "        updated = str(df_arxiv[df_arxiv['ID'] == str(row['ARXIV_ID'])]['updated'].values[0])\n",
    "        g.add((node_uri, SCHEMA.dateModified, Literal(updated, datatype=SCHEMA.DateTime)))\n",
    "        df_merged.at[NEW_ID, 'updated'] = updated\n",
    "        json_merged[NEW_ID]['updated'] = updated\n",
    "    except:\n",
    "        pass\n",
    "          \n",
    "                       \n",
    "    ### Abstract ###\n",
    "    try:\n",
    "        abstract = str(df_arxiv[df_arxiv['ID'] == str(row['ARXIV_ID'])]['summary'].values[0]).strip()\n",
    "        g.add((node_uri, SCHEMA.abstract, Literal(abstract, datatype=SCHEMA.Text)))\n",
    "        df_merged.at[NEW_ID, 'abstract'] = abstract\n",
    "        json_merged[NEW_ID]['abstract'] = abstract\n",
    "    except:\n",
    "        pass\n",
    "       \n",
    "                       \n",
    "    ### Categories ###\n",
    "    try:\n",
    "        categories = df_arxiv[df_arxiv['ID'] == str(row['ARXIV_ID'])]['categories'].values[0]\n",
    "        categories = [name.strip() for name in categories if name != '<___>']\n",
    "        [g.add((node_uri, SCHEMA.genre, Literal(category, datatype=SCHEMA.Text))) for category in categories]\n",
    "        df_merged.at[NEW_ID, 'categories'] = categories\n",
    "        json_merged[NEW_ID]['categories'] = categories\n",
    "    except:\n",
    "        pass\n",
    "          \n",
    "                       \n",
    "    ### Journal ###\n",
    "    try:\n",
    "        journal = str(df_gscholar[df_gscholar['ID'] == str(row['GSCHOLAR_ID'])]['journal'].values[0])\n",
    "        g.add((node_uri, SCHEMA.publisher, Literal(journal, datatype=SCHEMA.Periodical))) #datatype=SCHEMA.Organisation\n",
    "        df_merged.at[NEW_ID, 'journal'] = journal\n",
    "        json_merged[NEW_ID]['journal'] = journal\n",
    "    except:\n",
    "        pass\n",
    "     \n",
    "                       \n",
    "    ### Citations ###\n",
    "    try:\n",
    "        citations = str(df_gscholar[df_gscholar['ID'] == str(row['GSCHOLAR_ID'])]['citations'].values[0])\n",
    "        g.add((node_uri, SCHEMA.commentCount, Literal(citations, datatype=SCHEMA.Integer)))\n",
    "        df_merged.at[NEW_ID, 'citations'] = citations\n",
    "        json_merged[NEW_ID]['citations'] = citations\n",
    "    except:\n",
    "        pass\n",
    "            \n",
    "                       \n",
    "    ### Arxiv URL ###\n",
    "    try:\n",
    "        arxiv_url = str(df_arxiv[df_arxiv['ID'] == str(row['ARXIV_ID'])]['url'].values[0])\n",
    "        g.add((node_uri, SCHEMA.url, Literal(arxiv_url, datatype=SCHEMA.URL)))\n",
    "        df_merged.at[NEW_ID, 'arxiv_url'] = arxiv_url    \n",
    "        json_merged[NEW_ID]['arxiv_url'] = arxiv_url\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "                       \n",
    "    ### Google Scholar URL ###\n",
    "    try:\n",
    "        gscholar_url = str(df_gscholar[df_gscholar['ID'] == str(row['GSCHOLAR_ID'])]['url'].values[0])\n",
    "        g.add((node_uri, SCHEMA.url, Literal(gscholar_url, datatype=SCHEMA.URL)))\n",
    "        df_merged.at[NEW_ID, 'gscholar_url'] = gscholar_url   \n",
    "        json_merged[NEW_ID]['gscholar_url'] = gscholar_url\n",
    "    except:\n",
    "        pass\n",
    "             \n",
    "                       \n",
    "    NEW_ID += 1\n",
    "    \n",
    "# Save to disk using turtle format\n",
    "g.serialize(f'Triples_{TOPIC}.ttl.', format=\"turtle\")\n",
    "\n",
    "# And save the merged DataFrame as CSV\n",
    "df_merged.to_csv(f'Merged_{TOPIC}.csv', index=False)\n",
    "\n",
    "# Also save as Json, just because\n",
    "with open(f'Json_{TOPIC}.json', 'w') as fout:\n",
    "    json.dump(json_merged, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-skill",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-workstation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-samoa",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finsent",
   "language": "python",
   "name": "finsent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
