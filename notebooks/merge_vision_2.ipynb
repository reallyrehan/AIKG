{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "private-representative",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import rltk\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "special-officer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['semantic_joined_dblp.json.zip',\n",
       " '.DS_Store',\n",
       " 'Merged_graphics.csv',\n",
       " 'semantic_joined_dblp.json',\n",
       " 'Merged_ai.csv',\n",
       " 'output.json',\n",
       " 'semantic_scholar_articles_20000.csv',\n",
       " 'Merged_vision.csv',\n",
       " 'Merged_nlp.csv',\n",
       " 'output_compiled.json']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-crack",
   "metadata": {},
   "source": [
    "## Loading Semantic and DBLP JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "canadian-harris",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../data/semantic_joined_dblp.json','r')\n",
    "t=f.read()\n",
    "f.close()\n",
    "js = json.loads(t)\n",
    "df_js = pd.DataFrame(js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "attractive-storm",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>arxivId</th>\n",
       "      <th>authors</th>\n",
       "      <th>citationVelocity</th>\n",
       "      <th>citations</th>\n",
       "      <th>corpusId</th>\n",
       "      <th>doi</th>\n",
       "      <th>influentialCitationCount</th>\n",
       "      <th>paperId</th>\n",
       "      <th>references</th>\n",
       "      <th>title</th>\n",
       "      <th>topics</th>\n",
       "      <th>url</th>\n",
       "      <th>year</th>\n",
       "      <th>dblp_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The artificial neural networks that are used t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'authorId': '1695689', 'name': 'Geoffrey E. ...</td>\n",
       "      <td>150</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '1...</td>\n",
       "      <td>6138085</td>\n",
       "      <td>10.1007/978-3-642-21735-7_6</td>\n",
       "      <td>52</td>\n",
       "      <td>20f0357688876fa4662f806f985779dce6e24f3c</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '5...</td>\n",
       "      <td>Transforming Auto-Encoders</td>\n",
       "      <td>[{'topic': 'Computer vision', 'topicId': '5332...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/20f03576...</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>{'@score': '3', '@id': '3107620', 'info': {'au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to improve the accuracy of capsule ne...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'authorId': '144866658', 'name': 'Xin Ning',...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>215723800</td>\n",
       "      <td>10.1109/ACCESS.2020.2982782</td>\n",
       "      <td>0</td>\n",
       "      <td>4c79e754407d41904979746fbc8090545c0aeec9</td>\n",
       "      <td>[{'arxivId': '1810.10183', 'authors': [{'autho...</td>\n",
       "      <td>BDARS_CapsNet: Bi-Directional Attention Routin...</td>\n",
       "      <td>[{'topic': 'Routing', 'topicId': '1048', 'url'...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c79e754...</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The current dominant paradigm for feature lear...</td>\n",
       "      <td>1505.01596</td>\n",
       "      <td>[{'authorId': '33932184', 'name': 'Pulkit Agra...</td>\n",
       "      <td>96</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '2...</td>\n",
       "      <td>1637703</td>\n",
       "      <td>10.1109/ICCV.2015.13</td>\n",
       "      <td>31</td>\n",
       "      <td>dfbfaaec46d38392f61d683c340ee92a0a66e5d9</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '2...</td>\n",
       "      <td>Learning to See by Moving</td>\n",
       "      <td>[{'topic': 'Visual odometry', 'topicId': '644'...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/dfbfaaec...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>{'@score': '5', '@id': '2026007', 'info': {'au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In this work, we address the problem of improv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'authorId': '3458345', 'name': 'Zhun Sun', '...</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '3...</td>\n",
       "      <td>53872389</td>\n",
       "      <td>10.1109/CVPR.2018.00830</td>\n",
       "      <td>2</td>\n",
       "      <td>51bd5966fc992498cb1147d34527e33656c696bb</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '2...</td>\n",
       "      <td>Feature Quantization for Defending Against Dis...</td>\n",
       "      <td>[{'topic': 'Distortion', 'topicId': '15080', '...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/51bd5966...</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>{'@score': '8', '@id': '1024764', 'info': {'au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We introduce BSD-GAN, a novel multi-branch and...</td>\n",
       "      <td>1803.08467</td>\n",
       "      <td>[{'authorId': '39737792', 'name': 'Zili Yi', '...</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'arxivId': '2009.13311', 'authors': [{'autho...</td>\n",
       "      <td>220666238</td>\n",
       "      <td>10.1109/TIP.2020.3014608</td>\n",
       "      <td>0</td>\n",
       "      <td>70977b9464b94f71c2c4fc68aa03082e5739b6c3</td>\n",
       "      <td>[{'arxivId': '1710.10196', 'authors': [{'autho...</td>\n",
       "      <td>BSD-GAN: Branched Generative Adversarial Netwo...</td>\n",
       "      <td>[{'topic': 'BSD', 'topicId': '10760', 'url': '...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/70977b94...</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>{'@score': '15', '@id': '211133', 'info': {'au...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract     arxivId  \\\n",
       "0  The artificial neural networks that are used t...         NaN   \n",
       "1  In order to improve the accuracy of capsule ne...         NaN   \n",
       "2  The current dominant paradigm for feature lear...  1505.01596   \n",
       "3  In this work, we address the problem of improv...         NaN   \n",
       "4  We introduce BSD-GAN, a novel multi-branch and...  1803.08467   \n",
       "\n",
       "                                             authors  citationVelocity  \\\n",
       "0  [{'authorId': '1695689', 'name': 'Geoffrey E. ...               150   \n",
       "1  [{'authorId': '144866658', 'name': 'Xin Ning',...                 0   \n",
       "2  [{'authorId': '33932184', 'name': 'Pulkit Agra...                96   \n",
       "3  [{'authorId': '3458345', 'name': 'Zhun Sun', '...                 0   \n",
       "4  [{'authorId': '39737792', 'name': 'Zili Yi', '...                 0   \n",
       "\n",
       "                                           citations   corpusId  \\\n",
       "0  [{'arxivId': None, 'authors': [{'authorId': '1...    6138085   \n",
       "1                                                 []  215723800   \n",
       "2  [{'arxivId': None, 'authors': [{'authorId': '2...    1637703   \n",
       "3  [{'arxivId': None, 'authors': [{'authorId': '3...   53872389   \n",
       "4  [{'arxivId': '2009.13311', 'authors': [{'autho...  220666238   \n",
       "\n",
       "                           doi  influentialCitationCount  \\\n",
       "0  10.1007/978-3-642-21735-7_6                        52   \n",
       "1  10.1109/ACCESS.2020.2982782                         0   \n",
       "2         10.1109/ICCV.2015.13                        31   \n",
       "3      10.1109/CVPR.2018.00830                         2   \n",
       "4     10.1109/TIP.2020.3014608                         0   \n",
       "\n",
       "                                    paperId  \\\n",
       "0  20f0357688876fa4662f806f985779dce6e24f3c   \n",
       "1  4c79e754407d41904979746fbc8090545c0aeec9   \n",
       "2  dfbfaaec46d38392f61d683c340ee92a0a66e5d9   \n",
       "3  51bd5966fc992498cb1147d34527e33656c696bb   \n",
       "4  70977b9464b94f71c2c4fc68aa03082e5739b6c3   \n",
       "\n",
       "                                          references  \\\n",
       "0  [{'arxivId': None, 'authors': [{'authorId': '5...   \n",
       "1  [{'arxivId': '1810.10183', 'authors': [{'autho...   \n",
       "2  [{'arxivId': None, 'authors': [{'authorId': '2...   \n",
       "3  [{'arxivId': None, 'authors': [{'authorId': '2...   \n",
       "4  [{'arxivId': '1710.10196', 'authors': [{'autho...   \n",
       "\n",
       "                                               title  \\\n",
       "0                         Transforming Auto-Encoders   \n",
       "1  BDARS_CapsNet: Bi-Directional Attention Routin...   \n",
       "2                          Learning to See by Moving   \n",
       "3  Feature Quantization for Defending Against Dis...   \n",
       "4  BSD-GAN: Branched Generative Adversarial Netwo...   \n",
       "\n",
       "                                              topics  \\\n",
       "0  [{'topic': 'Computer vision', 'topicId': '5332...   \n",
       "1  [{'topic': 'Routing', 'topicId': '1048', 'url'...   \n",
       "2  [{'topic': 'Visual odometry', 'topicId': '644'...   \n",
       "3  [{'topic': 'Distortion', 'topicId': '15080', '...   \n",
       "4  [{'topic': 'BSD', 'topicId': '10760', 'url': '...   \n",
       "\n",
       "                                                 url    year  \\\n",
       "0  https://www.semanticscholar.org/paper/20f03576...  2011.0   \n",
       "1  https://www.semanticscholar.org/paper/4c79e754...  2020.0   \n",
       "2  https://www.semanticscholar.org/paper/dfbfaaec...  2015.0   \n",
       "3  https://www.semanticscholar.org/paper/51bd5966...  2018.0   \n",
       "4  https://www.semanticscholar.org/paper/70977b94...  2020.0   \n",
       "\n",
       "                                           dblp_info  \n",
       "0  {'@score': '3', '@id': '3107620', 'info': {'au...  \n",
       "1                                                NaN  \n",
       "2  {'@score': '5', '@id': '2026007', 'info': {'au...  \n",
       "3  {'@score': '8', '@id': '1024764', 'info': {'au...  \n",
       "4  {'@score': '15', '@id': '211133', 'info': {'au...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_js.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "numeric-difference",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = [i for i in os.listdir('../data') if \"Merged\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "turned-novelty",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for i in ls:\n",
    "    df_2 = pd.read_csv('../data/'+i)\n",
    "    df = df.append(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "interstate-essence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>published</th>\n",
       "      <th>updated</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "      <th>citations</th>\n",
       "      <th>arxiv_url</th>\n",
       "      <th>gscholar_url</th>\n",
       "      <th>journal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Dynamic Graph CNN for Learning on Point Clouds</td>\n",
       "      <td>['Yue Wang', 'Sanjay E. Sarma', 'Justin M. Sol...</td>\n",
       "      <td>2018-01-24T01:14:04Z</td>\n",
       "      <td>2019-06-11T06:11:21Z</td>\n",
       "      <td>Point clouds provide a flexible geometric repr...</td>\n",
       "      <td>['cs.CV']</td>\n",
       "      <td>536.0</td>\n",
       "      <td>http://arxiv.org/abs/1801.07829v2</td>\n",
       "      <td>http://scholar.google.com/scholar?oi=bibs&amp;clus...</td>\n",
       "      <td>ACM Transactions on Graphics (TOG) 38 (5), 1-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Looking to Listen at the Cocktail Party: A Spe...</td>\n",
       "      <td>['Avinatan Hassidim', 'Oran Lang', 'Michael Ru...</td>\n",
       "      <td>2018-04-10T16:28:59Z</td>\n",
       "      <td>2018-08-09T21:22:37Z</td>\n",
       "      <td>We present a joint audio-visual model for isol...</td>\n",
       "      <td>['cs.SD', 'cs.CV', 'eess.AS']</td>\n",
       "      <td>189.0</td>\n",
       "      <td>http://arxiv.org/abs/1804.03619v2</td>\n",
       "      <td>http://scholar.google.com/scholar?oi=bibs&amp;clus...</td>\n",
       "      <td>ACM Transactions on Graphics (TOG) 37 (4), 1-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Convergence of univariate non-stationary subdi...</td>\n",
       "      <td>['Carla Manni', 'Costanza Conti', 'Marie-Laure...</td>\n",
       "      <td>2014-10-10T10:30:14Z</td>\n",
       "      <td>2014-10-10T10:30:14Z</td>\n",
       "      <td>A new equivalence notion between non-stationar...</td>\n",
       "      <td>['math.NA']</td>\n",
       "      <td>34.0</td>\n",
       "      <td>http://arxiv.org/abs/1410.2729v1</td>\n",
       "      <td>http://scholar.google.com/scholar?oi=bibs&amp;clus...</td>\n",
       "      <td>Computer Aided Geometric Design 37, 1-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>HDR image reconstruction from a single exposur...</td>\n",
       "      <td>['Joel Kronander', 'Rafał K. Mantiuk', 'Gyorgy...</td>\n",
       "      <td>2017-10-20T10:48:22Z</td>\n",
       "      <td>2017-10-20T10:48:22Z</td>\n",
       "      <td>Camera sensors can only capture a limited rang...</td>\n",
       "      <td>['cs.CV', 'cs.GR', 'cs.LG']</td>\n",
       "      <td>145.0</td>\n",
       "      <td>http://arxiv.org/abs/1710.07480v1</td>\n",
       "      <td>http://scholar.google.com/scholar?oi=bibs&amp;clus...</td>\n",
       "      <td>ACM Transactions on Graphics (TOG) 36 (6), 1-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Learning Style Similarity for Searching Infogr...</td>\n",
       "      <td>['Aaron Hertzmann', 'Mira Dontcheva', 'Babak S...</td>\n",
       "      <td>2015-05-05T22:59:32Z</td>\n",
       "      <td>2015-05-05T22:59:32Z</td>\n",
       "      <td>Infographics are complex graphic designs integ...</td>\n",
       "      <td>['cs.GR', 'cs.CV', 'cs.HC', 'cs.IR', 'cs.MM']</td>\n",
       "      <td>24.0</td>\n",
       "      <td>http://arxiv.org/abs/1505.01214v1</td>\n",
       "      <td>http://scholar.google.com/scholar?oi=bibs&amp;clus...</td>\n",
       "      <td>Proceedings of the 41st Graphics Interface Con...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                              title  \\\n",
       "0   0     Dynamic Graph CNN for Learning on Point Clouds   \n",
       "1   1  Looking to Listen at the Cocktail Party: A Spe...   \n",
       "2   2  Convergence of univariate non-stationary subdi...   \n",
       "3   3  HDR image reconstruction from a single exposur...   \n",
       "4   4  Learning Style Similarity for Searching Infogr...   \n",
       "\n",
       "                                             authors             published  \\\n",
       "0  ['Yue Wang', 'Sanjay E. Sarma', 'Justin M. Sol...  2018-01-24T01:14:04Z   \n",
       "1  ['Avinatan Hassidim', 'Oran Lang', 'Michael Ru...  2018-04-10T16:28:59Z   \n",
       "2  ['Carla Manni', 'Costanza Conti', 'Marie-Laure...  2014-10-10T10:30:14Z   \n",
       "3  ['Joel Kronander', 'Rafał K. Mantiuk', 'Gyorgy...  2017-10-20T10:48:22Z   \n",
       "4  ['Aaron Hertzmann', 'Mira Dontcheva', 'Babak S...  2015-05-05T22:59:32Z   \n",
       "\n",
       "                updated                                           abstract  \\\n",
       "0  2019-06-11T06:11:21Z  Point clouds provide a flexible geometric repr...   \n",
       "1  2018-08-09T21:22:37Z  We present a joint audio-visual model for isol...   \n",
       "2  2014-10-10T10:30:14Z  A new equivalence notion between non-stationar...   \n",
       "3  2017-10-20T10:48:22Z  Camera sensors can only capture a limited rang...   \n",
       "4  2015-05-05T22:59:32Z  Infographics are complex graphic designs integ...   \n",
       "\n",
       "                                      categories  citations  \\\n",
       "0                                      ['cs.CV']      536.0   \n",
       "1                  ['cs.SD', 'cs.CV', 'eess.AS']      189.0   \n",
       "2                                    ['math.NA']       34.0   \n",
       "3                    ['cs.CV', 'cs.GR', 'cs.LG']      145.0   \n",
       "4  ['cs.GR', 'cs.CV', 'cs.HC', 'cs.IR', 'cs.MM']       24.0   \n",
       "\n",
       "                           arxiv_url  \\\n",
       "0  http://arxiv.org/abs/1801.07829v2   \n",
       "1  http://arxiv.org/abs/1804.03619v2   \n",
       "2   http://arxiv.org/abs/1410.2729v1   \n",
       "3  http://arxiv.org/abs/1710.07480v1   \n",
       "4  http://arxiv.org/abs/1505.01214v1   \n",
       "\n",
       "                                        gscholar_url  \\\n",
       "0  http://scholar.google.com/scholar?oi=bibs&clus...   \n",
       "1  http://scholar.google.com/scholar?oi=bibs&clus...   \n",
       "2  http://scholar.google.com/scholar?oi=bibs&clus...   \n",
       "3  http://scholar.google.com/scholar?oi=bibs&clus...   \n",
       "4  http://scholar.google.com/scholar?oi=bibs&clus...   \n",
       "\n",
       "                                             journal  \n",
       "0    ACM Transactions on Graphics (TOG) 38 (5), 1-12  \n",
       "1    ACM Transactions on Graphics (TOG) 37 (4), 1-11  \n",
       "2            Computer Aided Geometric Design 37, 1-8  \n",
       "3    ACM Transactions on Graphics (TOG) 36 (6), 1-15  \n",
       "4  Proceedings of the 41st Graphics Interface Con...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-gospel",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-stations",
   "metadata": {},
   "source": [
    "### Naïve Data Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "executed-commander",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\n"
     ]
    }
   ],
   "source": [
    "# How many matches can be found with a naÏve identical string approach?\n",
    "d1 = df_js['title'].values\n",
    "d2 = df['title'].values\n",
    "print(len([1 for w in d1 if w in d2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-welding",
   "metadata": {},
   "source": [
    "## RLTK Data Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "middle-deadline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RLTK Tokenizer\n",
    "tokenizer = rltk.CrfTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-tenant",
   "metadata": {},
   "source": [
    "**Arxiv/Google Scholar Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "theoretical-finding",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_js.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cathedral-reservoir",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'abstract', 'arxivId', 'authors', 'citationVelocity',\n",
       "       'citations', 'corpusId', 'doi', 'influentialCitationCount', 'paperId',\n",
       "       'references', 'title', 'topics', 'url', 'year', 'dblp_info'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_js.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "micro-magic",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "multiple-bubble",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'ID', 'title', 'authors', 'published', 'updated', 'abstract',\n",
       "       'categories', 'citations', 'arxiv_url', 'gscholar_url', 'journal'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "after-bhutan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "turkish-customer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[{'authorId': '1695689', 'name': 'Geoffrey E. Hinton', 'url': 'https://www.semanticscholar.org/author/1695689'}, {'authorId': '2064160', 'name': 'A. Krizhevsky', 'url': 'https://www.semanticscholar.org/author/2064160'}, {'authorId': '49185042', 'name': 'S. Wang', 'url': 'https://www.semanticscholar.org/author/49185042'}]\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_js['authors'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "alpha-tonight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['Yue Wang', 'Sanjay E. Sarma', 'Justin M. Solomon', 'Yongbin Sun', 'Ziwei Liu', 'Michael M. Bronstein']\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['authors'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "forbidden-steal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "periodic-flesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPlainString(string):\n",
    "    return ''.join([s for s in string if not s.isnumeric()]).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "continued-sussex",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArxivRecord(rltk.Record):\n",
    "    def __init__(self, raw_object):\n",
    "        super().__init__(raw_object)\n",
    "        self.name = 'ArxivRecord'\n",
    "        \n",
    "    @property\n",
    "    def id(self):\n",
    "        return str(self.raw_object['ID'])\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def authors_string(self):\n",
    "        return self.raw_object['authors']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def title_string(self):\n",
    "        return self.raw_object['title']\n",
    "        \n",
    "#     @rltk.cached_property\n",
    "#     def summary_string(self):\n",
    "#         return self.raw_object['summary']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def categories_string(self):\n",
    "        return self.raw_object['categories']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def published_string(self):\n",
    "        return self.raw_object['published']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def updated_string(self):\n",
    "        return self.raw_object['updated']\n",
    "        \n",
    "    @rltk.cached_property\n",
    "    def blocking_author_tokens(self):\n",
    "        return set([getPlainString(s).split(' ')[-1] for s in ast.literal_eval(self.raw_object['authors'])])\n",
    "    \n",
    "#     @rltk.cached_property\n",
    "#     def url_string(self):\n",
    "#         return self.raw_object['url']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def blocking_tokens(self):\n",
    "        tokens = ' '.join([self.title_string])\n",
    "        tokens = re.sub(r'\\bThe\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bthe\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bof\\b', '', tokens)\n",
    "        tokens = re.sub(r\"\\b's\\b\", '', tokens)\n",
    "        tokens = re.sub(r'\\band\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bI\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bA\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bin\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bfor\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bon\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bwith\\b', '', tokens)\n",
    "        return set(tokenizer.tokenize(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "accurate-investigation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'rltk.dataset.Dataset'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>authors_string</th>\n",
       "      <th>title_string</th>\n",
       "      <th>categories_string</th>\n",
       "      <th>published_string</th>\n",
       "      <th>updated_string</th>\n",
       "      <th>blocking_author_tokens</th>\n",
       "      <th>blocking_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>['Barret Zoph', 'Kevin Knight']</td>\n",
       "      <td>Multi-Source Neural Translation</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>2016-01-05T00:49:22Z</td>\n",
       "      <td>2016-01-05T00:49:22Z</td>\n",
       "      <td>{Zoph, Knight}</td>\n",
       "      <td>{-, Neural, Multi, Source, Translation}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>['Xiaodong He', 'Asli Celikyilmaz', 'Antoine B...</td>\n",
       "      <td>Deep Communicating Agents for Abstractive Summ...</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>2018-03-27T23:29:23Z</td>\n",
       "      <td>2018-08-15T18:54:22Z</td>\n",
       "      <td>{Celikyilmaz, He, Choi, Bosselut}</td>\n",
       "      <td>{Summarization, Abstractive, Agents, Communica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>['Furu Wei', 'Ming Zhou', 'Nan Yang', 'Qingyu ...</td>\n",
       "      <td>Selective Encoding for Abstractive Sentence Su...</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>2017-04-24T07:57:37Z</td>\n",
       "      <td>2017-04-24T07:57:37Z</td>\n",
       "      <td>{Yang, Wei, Zhou}</td>\n",
       "      <td>{Encoding, Summarization, Abstractive, Sentenc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                     authors_string  \\\n",
       "0  0                    ['Barret Zoph', 'Kevin Knight']   \n",
       "1  1  ['Xiaodong He', 'Asli Celikyilmaz', 'Antoine B...   \n",
       "2  2  ['Furu Wei', 'Ming Zhou', 'Nan Yang', 'Qingyu ...   \n",
       "\n",
       "                                        title_string categories_string  \\\n",
       "0                    Multi-Source Neural Translation         ['cs.CL']   \n",
       "1  Deep Communicating Agents for Abstractive Summ...         ['cs.CL']   \n",
       "2  Selective Encoding for Abstractive Sentence Su...         ['cs.CL']   \n",
       "\n",
       "       published_string        updated_string  \\\n",
       "0  2016-01-05T00:49:22Z  2016-01-05T00:49:22Z   \n",
       "1  2018-03-27T23:29:23Z  2018-08-15T18:54:22Z   \n",
       "2  2017-04-24T07:57:37Z  2017-04-24T07:57:37Z   \n",
       "\n",
       "              blocking_author_tokens  \\\n",
       "0                     {Zoph, Knight}   \n",
       "1  {Celikyilmaz, He, Choi, Bosselut}   \n",
       "2                  {Yang, Wei, Zhou}   \n",
       "\n",
       "                                     blocking_tokens  \n",
       "0            {-, Neural, Multi, Source, Translation}  \n",
       "1  {Summarization, Abstractive, Agents, Communica...  \n",
       "2  {Encoding, Summarization, Abstractive, Sentenc...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_1 = rltk.Dataset(reader=rltk.DataFrameReader(df), record_class=ArxivRecord, adapter=rltk.MemoryKeyValueAdapter())\n",
    "print(type(ds_1))\n",
    "ds_1.generate_dataframe().head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-bahrain",
   "metadata": {},
   "source": [
    "**Google Scholar Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "peripheral-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticRecord(rltk.Record):\n",
    "    def __init__(self, raw_object):\n",
    "        super().__init__(raw_object)\n",
    "        self.name = 'SemanticRecord'\n",
    "        \n",
    "    @property\n",
    "    def id(self):\n",
    "        return str(self.raw_object['index'])\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def authors_string(self):\n",
    "        return self.raw_object['authors']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def title_string(self):\n",
    "        return self.raw_object['title']\n",
    "        \n",
    "#     @rltk.cached_property\n",
    "#     def journal_string(self):\n",
    "#         return self.raw_object['journal']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def citations_string(self):\n",
    "        return self.raw_object['citations']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def year_string(self):\n",
    "        return self.raw_object['year']\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def url_string(self):\n",
    "        return self.raw_object['url']\n",
    "    \n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def blocking_author_tokens(self):\n",
    "        return set([getPlainString(s['name']).split(' ')[-1] for s in ast.literal_eval((self.raw_object['authors']))])\n",
    "\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def blocking_tokens(self):\n",
    "        tokens = ' '.join([self.title_string])\n",
    "        tokens = re.sub(r'\\bThe\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bthe\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bof\\b', '', tokens)\n",
    "        tokens = re.sub(r\"\\b's\\b\", '', tokens)\n",
    "        tokens = re.sub(r'\\band\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bI\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bA\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bin\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bfor\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bon\\b', '', tokens)\n",
    "        tokens = re.sub(r'\\bwith\\b', '', tokens)\n",
    "        return set(tokenizer.tokenize(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "skilled-navigation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'rltk.dataset.Dataset'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>authors_string</th>\n",
       "      <th>title_string</th>\n",
       "      <th>citations_string</th>\n",
       "      <th>year_string</th>\n",
       "      <th>url_string</th>\n",
       "      <th>blocking_author_tokens</th>\n",
       "      <th>blocking_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'authorId': '1695689', 'name': 'Geoffrey E. ...</td>\n",
       "      <td>Transforming Auto-Encoders</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '1...</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>https://www.semanticscholar.org/paper/20f03576...</td>\n",
       "      <td>{Wang, Hinton, Krizhevsky}</td>\n",
       "      <td>{Auto, -, Transforming, Encoders}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'authorId': '144866658', 'name': 'Xin Ning',...</td>\n",
       "      <td>BDARS_CapsNet: Bi-Directional Attention Routin...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c79e754...</td>\n",
       "      <td>{Li, Tian, Ning, Sun, Nie, Chen, Lu}</td>\n",
       "      <td>{Network, Directional, Capsule, _, CapsNet, -,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[{'authorId': '33932184', 'name': 'Pulkit Agra...</td>\n",
       "      <td>Learning to See by Moving</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '2...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>https://www.semanticscholar.org/paper/dfbfaaec...</td>\n",
       "      <td>{Agrawal, Malik, Carreira}</td>\n",
       "      <td>{to, See, by, Moving, Learning}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                     authors_string  \\\n",
       "0  0  [{'authorId': '1695689', 'name': 'Geoffrey E. ...   \n",
       "1  1  [{'authorId': '144866658', 'name': 'Xin Ning',...   \n",
       "2  2  [{'authorId': '33932184', 'name': 'Pulkit Agra...   \n",
       "\n",
       "                                        title_string  \\\n",
       "0                         Transforming Auto-Encoders   \n",
       "1  BDARS_CapsNet: Bi-Directional Attention Routin...   \n",
       "2                          Learning to See by Moving   \n",
       "\n",
       "                                    citations_string  year_string  \\\n",
       "0  [{'arxivId': None, 'authors': [{'authorId': '1...       2011.0   \n",
       "1                                                 []       2020.0   \n",
       "2  [{'arxivId': None, 'authors': [{'authorId': '2...       2015.0   \n",
       "\n",
       "                                          url_string  \\\n",
       "0  https://www.semanticscholar.org/paper/20f03576...   \n",
       "1  https://www.semanticscholar.org/paper/4c79e754...   \n",
       "2  https://www.semanticscholar.org/paper/dfbfaaec...   \n",
       "\n",
       "                 blocking_author_tokens  \\\n",
       "0            {Wang, Hinton, Krizhevsky}   \n",
       "1  {Li, Tian, Ning, Sun, Nie, Chen, Lu}   \n",
       "2            {Agrawal, Malik, Carreira}   \n",
       "\n",
       "                                     blocking_tokens  \n",
       "0                  {Auto, -, Transforming, Encoders}  \n",
       "1  {Network, Directional, Capsule, _, CapsNet, -,...  \n",
       "2                    {to, See, by, Moving, Learning}  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_2 = rltk.Dataset(reader=rltk.DataFrameReader(df_js), record_class=SemanticRecord, adapter=rltk.MemoryKeyValueAdapter())\n",
    "print(type(ds_2))\n",
    "ds_2.generate_dataframe().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-building",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-concern",
   "metadata": {},
   "source": [
    "### Blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "mathematical-filing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'rltk.blocking.block.Block'>\n"
     ]
    }
   ],
   "source": [
    "# Generate blocks from tokens\n",
    "token_blocker = rltk.TokenBlockGenerator()\n",
    "blocks = token_blocker.generate(\n",
    "    token_blocker.block(ds_1, property_='blocking_author_tokens'),\n",
    "    token_blocker.block(ds_2, property_='blocking_author_tokens'))\n",
    "print(type(blocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "minor-auditor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction Ratio: 0.93040\n"
     ]
    }
   ],
   "source": [
    "# Extract all record pairs from the block\n",
    "record_pairs = rltk.get_record_pairs(ds_1, ds_2, block=blocks)\n",
    "\n",
    "# Get the total number of record pairs generated\n",
    "compared_pairs = len(list(record_pairs))\n",
    "\n",
    "# Get the number of elements in each rltk.Dataset\n",
    "tally_imdb = ds_1.generate_dataframe().shape[0]\n",
    "tally_tmd = ds_2.generate_dataframe().shape[0]\n",
    "\n",
    "# Calculate the total number of pairs if both datasets were to be compared without any blocking (eg: a double for loop)\n",
    "tally_unblocked = tally_imdb * tally_tmd\n",
    "\n",
    "# Calculate how much smaller the blocked pairings are\n",
    "reduction_ratio = compared_pairs / tally_unblocked\n",
    "\n",
    "3036085/(1-0.93040)\n",
    "\n",
    "# Calculate the reduction ratio (the inverse of the )\n",
    "reduction_ratio = 1 - reduction_ratio\n",
    "print(f'Reduction Ratio: {reduction_ratio:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-japanese",
   "metadata": {},
   "outputs": [],
   "source": [
    "Total Pairs = 43621910.9\n",
    "Compared Pairs = 3036085\n",
    "Reduction Ratio = 0.93040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "corrected-seminar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3036085"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compared_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-occurrence",
   "metadata": {},
   "source": [
    "### Matching Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "exact-cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_similarity(arxiv_tuple, gscholar_tuple):\n",
    "    arxiv_title = arxiv_tuple.title_string.strip().lower()\n",
    "    gscholar_title = gscholar_tuple.title_string.strip().lower()\n",
    "    similarity = SequenceMatcher(None, arxiv_title, gscholar_title).ratio()\n",
    "\n",
    "    penalties = sum([len(arxiv_title)<=6,\n",
    "                     len(gscholar_title)<=6])\n",
    "\n",
    "    return similarity * (0.9**penalties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "biological-montreal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def author_similarity(arxiv_tuple, gscholar_tuple):\n",
    "    arxiv_author = ' '.join(arxiv_tuple.authors_string).strip().lower()\n",
    "    gscholar_author = ' '.join(gscholar_tuple.authors_string).strip().lower()\n",
    "    similarity = SequenceMatcher(None, arxiv_author, gscholar_author).ratio() \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "welcome-amplifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_similarity(arxiv_tuple, gscholar_tuple):\n",
    "#     arxiv_year = int(float(arxiv_tuple.updated_string[0:4]))\n",
    "#     print(arxiv_tuple.published_string)\n",
    "    if str(arxiv_tuple.published_string) == \"nan\":\n",
    "        return 0\n",
    "    arxiv_year = datetime.datetime.strptime(arxiv_tuple.published_string, '%Y-%m-%dT%H:%M:%SZ').year\n",
    "\n",
    "    gscholar_year = int(float(gscholar_tuple.year_string))\n",
    "    similarity = 1 /(1 + abs(arxiv_year-gscholar_year))\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "signal-sense",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elementwise_similarity(arxiv_tuple, gscholar_tuple, match_threshold=0.75):\n",
    "    sim_title = title_similarity(arxiv_tuple, gscholar_tuple)\n",
    "    sim_author = author_similarity(arxiv_tuple, gscholar_tuple)\n",
    "    sim_year = year_similarity(arxiv_tuple, gscholar_tuple)\n",
    "\n",
    "    element_similarity = (0.70 * sim_title) + (0.15 * sim_author) + (0.15 * sim_year)\n",
    "\n",
    "    return element_similarity > match_threshold, element_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "japanese-whole",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arxiv samples: 4646\n",
      "Semantic samples: 20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 8/8 [12:54:11<00:00, 5806.49s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Matches</th>\n",
       "      <th>Set_A Size</th>\n",
       "      <th>Set_B Size</th>\n",
       "      <th>Duplicates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.65</th>\n",
       "      <td>247.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>115.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>225.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>191.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.80</th>\n",
       "      <td>135.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.85</th>\n",
       "      <td>127.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.95</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Matches  Set_A Size  Set_B Size  Duplicates\n",
       "0.65    247.0       141.0       247.0       115.0\n",
       "0.70    225.0       127.0       225.0       105.0\n",
       "0.75    191.0       103.0       191.0        89.0\n",
       "0.80    135.0        65.0       135.0        70.0\n",
       "0.85    127.0        60.0       127.0        67.0\n",
       "0.90      0.0         0.0         0.0         0.0\n",
       "0.95      0.0         0.0         0.0         0.0\n",
       "1.00      0.0         0.0         0.0         0.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict matches for all pairs in the blocked data \n",
    "print(f'Arxiv samples: {df.shape[0]}')\n",
    "print(f'Semantic samples: {df_js.shape[0]}')\n",
    "\n",
    "summary_df = pd.DataFrame()\n",
    "THRESHOLDS = [T/100 for T in range(65, 101, 5)]\n",
    "\n",
    "# Iterate through various thresholds to find the most matches without any duplicates\n",
    "for T in tqdm(THRESHOLDS):\n",
    "#     c=0\n",
    "\n",
    "    # Set to store pairs of IDs matched\n",
    "    ids_matched = set()\n",
    "    \n",
    "    # Iterate through candidates on the block\n",
    "    for block_id, arxiv_id, gscholar_id in blocks.pairwise(ds_1, ds_2):\n",
    "        \n",
    "        # Find similarity at a given threshold\n",
    "        match , similarity = elementwise_similarity(ds_1.get_record(arxiv_id),\n",
    "                                                    ds_2.get_record(gscholar_id),\n",
    "                                                    match_threshold=T)\n",
    "        # If a match is found, add to the set of matches\n",
    "        if match:\n",
    "            ids_matched.add((arxiv_id, gscholar_id))\n",
    "            \n",
    "#         c=c+1\n",
    "#         if c>1000:\n",
    "#             break\n",
    "    # Count the number of unique elements derived from each source\n",
    "    set_a = set()\n",
    "    set_b = set()\n",
    "    for tp in ids_matched:\n",
    "        set_a.add(tp[0])\n",
    "        set_b.add(tp[1])\n",
    "    \n",
    "    summary_df.at[T, 'Matches'] = int(len(ids_matched))\n",
    "    summary_df.at[T, 'Set_A Size'] = int(len(set_a))\n",
    "    summary_df.at[T, 'Set_B Size'] = int(len(ids_matched))\n",
    "    summary_df.at[T, 'Duplicates'] = int((len(ids_matched)-len(set_a)) + (len(ids_matched)-len(set_b)))\n",
    "    \n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the lowest threshold which gives no duplicates\n",
    "optimal_threshold = summary_df[summary_df['Duplicates']==0].index[0]\n",
    "optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "stable-jesus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually set threshold to 0.85 to trade 104 extra True Positives for 3 extra False Positive\n",
    "optimal_threshold = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "formal-tradition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arxiv samples: 4646\n",
      "GScholar samples: 20000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-150-44fd1fc03350>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         match , similarity = elementwise_similarity(ds_1.get_record(arxiv_id),\n\u001b[1;32m     15\u001b[0m                                                     \u001b[0mds_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgscholar_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                                                     match_threshold=optimal_threshold)\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-5768feacaabe>\u001b[0m in \u001b[0;36melementwise_similarity\u001b[0;34m(arxiv_tuple, gscholar_tuple, match_threshold)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0melementwise_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marxiv_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgscholar_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msim_title\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtitle_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marxiv_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgscholar_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msim_author\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauthor_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marxiv_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgscholar_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msim_year\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myear_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marxiv_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgscholar_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-b4f7918d9339>\u001b[0m in \u001b[0;36mauthor_similarity\u001b[0;34m(arxiv_tuple, gscholar_tuple)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0marxiv_author\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marxiv_tuple\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthors_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mgscholar_author\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgscholar_tuple\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthors_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequenceMatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marxiv_author\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgscholar_author\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/finsent/lib/python3.6/difflib.py\u001b[0m in \u001b[0;36mratio\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    642\u001b[0m         \"\"\"\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtriple\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_matching_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_calculate_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/finsent/lib/python3.6/difflib.py\u001b[0m in \u001b[0;36mget_matching_blocks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0malo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mahi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbhi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m             \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_longest_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mahi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbhi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m             \u001b[0;31m# a[alo:i] vs b[blo:j] unknown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0;31m# a[i:i+k] same as b[j:j+k]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/finsent/lib/python3.6/difflib.py\u001b[0m in \u001b[0;36mfind_longest_match\u001b[0;34m(self, alo, ahi, blo, bhi)\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mbhi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m                 \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewj2len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj2lenget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbestsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m                     \u001b[0mbesti\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbestj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbestsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate matches based on the optimal (no-duplicate) threshold\n",
    "print(f'Arxiv samples: {df.shape[0]}')\n",
    "print(f'GScholar samples: {df_js.shape[0]}')\n",
    "\n",
    "# Store tuples of matches IDs, as well as singletons witouth a match\n",
    "ids_matched = set()\n",
    "singles_arxiv = set()\n",
    "singles_gscholar = set()\n",
    "\n",
    "# Write matches (and non-matches) to a CSV\n",
    "with open('Matches_test.csv', 'w') as predictions_full:\n",
    "    for block_id, arxiv_id, gscholar_id in blocks.pairwise(ds_1, ds_2):\n",
    "\n",
    "        match , similarity = elementwise_similarity(ds_1.get_record(arxiv_id),\n",
    "                                                    ds_2.get_record(gscholar_id),\n",
    "                                                    match_threshold=optimal_threshold)\n",
    "\n",
    "        if match:\n",
    "            break\n",
    "            ids_matched.add((arxiv_id, gscholar_id))\n",
    "        else:\n",
    "            singles_arxiv.add(arxiv_id)\n",
    "            singles_gscholar.add(gscholar_id)\n",
    "    \n",
    "    # After finding all matches, write them to a csv\n",
    "    for match_pair in ids_matched:\n",
    "        predictions_full.write(f'{match_pair[0]},{match_pair[1]},1\\n')\n",
    "        # And ensure that no item in the matches is counted as a single\n",
    "        try:\n",
    "            singles_arxiv.remove(match_pair[0])\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            singles_gscholar.remove(match_pair[1])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Then write all the singles which didn't find a match\n",
    "    NULL = None\n",
    "    for arxiv_id in singles_arxiv:\n",
    "        predictions_full.write(f'{arxiv_id},{NULL},0\\n')\n",
    "    for gscholar_id in singles_gscholar:\n",
    "        predictions_full.write(f'{NULL},{gscholar_id},0\\n')        \n",
    "        \n",
    "print()\n",
    "print(f'Matches: {len(ids_matched)}')\n",
    "print(f'Non-Matches Arxiv: {len(singles_arxiv)}')\n",
    "print(f'Non-Matches GScholar: {len(singles_gscholar)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-commitment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "restricted-execution",
   "metadata": {},
   "source": [
    "### Create Merged Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "composed-norway",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib import URIRef, Literal, Namespace\n",
    "from rdflib.namespace import RDF, RDFS, XSD\n",
    "\n",
    "MYNS = Namespace('http://inf558.org/myfakenamespace#')\n",
    "SCHEMA = Namespace(\"https://schema.org/\")\n",
    "\n",
    "# Initliaze the graph\n",
    "g = rdflib.Graph()\n",
    "\n",
    "# Bind namespace and prefixes\n",
    "g.bind('my_ns', MYNS)\n",
    "g.bind('schema', SCHEMA)\n",
    "g.bind('rdf', RDF)\n",
    "g.bind('rdfs', RDFS)\n",
    "g.bind('xsd', XSD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "balanced-diesel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions_df.shape: (17227, 3)\n",
      "predicted matches: 127  [0.74 %]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARXIV_ID</th>\n",
       "      <th>SEMANTIC_ID</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>468</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>238</td>\n",
       "      <td>13831</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>268</td>\n",
       "      <td>11329</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>475</td>\n",
       "      <td>574</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>450</td>\n",
       "      <td>3042</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ARXIV_ID SEMANTIC_ID  LABEL\n",
       "0      468           2      1\n",
       "1      238       13831      1\n",
       "2      268       11329      1\n",
       "3      475         574      1\n",
       "4      450        3042      1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load predictions to be used in populating the RDF\n",
    "predictions_df = pd.read_csv(f'Matches.csv', header=None, names=['ARXIV_ID', 'SEMANTIC_ID', 'LABEL'])\n",
    "print(f'predictions_df.shape: {predictions_df.shape}')\n",
    "predicted_matches = predictions_df['LABEL'].sum()\n",
    "print(f'predicted matches: {predicted_matches}  [{100*predicted_matches/predictions_df.shape[0]:.2f} %]')\n",
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "decent-setup",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'ID', 'title', 'authors', 'published', 'updated', 'abstract',\n",
       "       'categories', 'citations', 'arxiv_url', 'gscholar_url', 'journal'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "permanent-yellow",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'abstract', 'arxivId', 'authors', 'citationVelocity',\n",
       "       'citations', 'corpusId', 'doi', 'influentialCitationCount', 'paperId',\n",
       "       'references', 'title', 'topics', 'url', 'year', 'dblp_info'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_js.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "lovely-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.DataFrame(columns = ['ID', 'title', 'authors', 'published', 'updated', \n",
    "                                    'abstract', 'categories', 'citations', 'arxiv_url', 'gscholar_url',\n",
    "                                    'index', 'abstract', 'arxivId', 'authors', 'citationVelocity','citations', 'corpusId', 'doi', 'influentialCitationCount', 'paperId','references', 'title', 'topics', 'url', 'year', 'dblp_info'\n",
    "                                   ],\n",
    "                         dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "answering-scene",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ARXIV_ID       468\n",
      "SEMANTIC_ID      2\n",
      "LABEL            1\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Dynamic Projection Mapping onto Deforming Non-Rigid Surface Using Deformable Dot Cluster Marker'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ccount = 0\n",
    "for i,row in predictions_df.iterrows():\n",
    "    print(i,row) \n",
    "    break\n",
    "for c1 in df.columns():\n",
    "    df_merged.at[ccount,c1] =     df.at[468,'title']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "blessed-referral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366 15298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('NTUA-SLP at SemEval-2018 Task 3: Tracking Ironic Tweets using Ensembles  of Word and Character Level Attentive RNNs',\n",
       " 'Multi-task Self-Supervised Visual Learning')"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=25\n",
    "s1,s2 = predictions_df.iloc[x]['ARXIV_ID'],predictions_df.iloc[x]['SEMANTIC_ID']\n",
    "print(s1,s2)\n",
    "ts[ts['id']==s1]['title_string'].iloc[0],ts2[ts2['id']==s2]['title_string'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-presentation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "ultimate-profile",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in df_js.iterrows():\n",
    "    if row['title'] in df['title']:\n",
    "        print(row['title'])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "correct-thickness",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dimensional Sentiment Analysis Using a Regional CNN-LSTM Model'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row['title']\n",
    "df_js['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-bangladesh",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "attractive-prompt",
   "metadata": {},
   "source": [
    "# Start Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "narrow-hardware",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "opening-vietnamese",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import rltk\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import rltk\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "from rltk.similarity.needleman import needleman_wunsch_score\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "serial-phenomenon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['semantic_joined_dblp.json.zip',\n",
       " 'complete_merged.pkl',\n",
       " '.DS_Store',\n",
       " 'Merged_graphics.csv',\n",
       " 'semantic_joined_dblp.json',\n",
       " 'Merged_ai.csv',\n",
       " 'output.json',\n",
       " 'semantic_scholar_articles_20000.csv',\n",
       " 'Merged_vision.csv',\n",
       " 'Merged_nlp.csv',\n",
       " 'output_compiled.json',\n",
       " 'complete_merged.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-nevada",
   "metadata": {},
   "source": [
    "## Loading Semantic and DBLP JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "colored-marine",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../data/semantic_joined_dblp.json','r')\n",
    "t=f.read()\n",
    "f.close()\n",
    "js = json.loads(t)\n",
    "df_js = pd.DataFrame(js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "lyric-organization",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "special-skirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = [i for i in os.listdir('../data') if \"Merged\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eligible-command",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sharp-thumbnail",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for i in ls:\n",
    "    df_2 = pd.read_csv('../data/'+i)\n",
    "    df = df.append(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "compact-above",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4646"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-scholar",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-brick",
   "metadata": {},
   "source": [
    "### Naïve Data Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "breathing-jefferson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\n"
     ]
    }
   ],
   "source": [
    "# How many matches can be found with a naÏve identical string approach?\n",
    "d1 = df_js['title'].values\n",
    "d2 = df['title'].values\n",
    "print(len([1 for w in d1 if w in d2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "chronic-variety",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAuthorNames(a):\n",
    "    return sorted([s['name'] for s in ast.literal_eval(a)],key=lambda s:s.split(\" \")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "micro-transition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAuthorNames_2(b):\n",
    "    return sorted(ast.literal_eval(b),key=lambda s:s.split(\" \")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "worthy-blowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_js['authors'] = list(map(getAuthorNames,df_js['authors']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "resistant-influence",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['authors'] = list(map(getAuthorNames_2,df['authors']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "severe-extent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def authorComparison(s_authors,j_authors,year):\n",
    "    score = []\n",
    "#     thres = 9 - (2.5*year)\n",
    "    thres = 6\n",
    "    \n",
    "    if len(s_authors)==0 and len(j_authors)==0:\n",
    "        return 0 \n",
    "    if len(s_authors)==len(j_authors):\n",
    "        for a1,a2 in zip(s_authors,j_authors):\n",
    "            score.append(needleman_wunsch_score(a1, a2, match=2, mismatch=-1, gap=-0.5, score_table=None))\n",
    "#             print(score)\n",
    "        if sum(score)/len(score)>thres:\n",
    "            return 1\n",
    "    return 0\n",
    "def getPlainString(string):\n",
    "    return ''.join([s for s in string if not s.isnumeric()]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "spatial-mexico",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/finsent/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_value\u001b[0;34m(self, index, col, value, takeable)\u001b[0m\n\u001b[1;32m   3147\u001b[0m             \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3148\u001b[0;31m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3149\u001b[0m             \u001b[0mvalidate_numeric_casting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 9544",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2e0c15f093a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mdf_merged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"b_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/finsent/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2087\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2089\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/finsent/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2038\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Not enough indexers for scalar access (setting)!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2040\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/finsent/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_value\u001b[0;34m(self, index, col, value, takeable)\u001b[0m\n\u001b[1;32m   3157\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3158\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3159\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3160\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_item_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/finsent/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0miloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"iloc\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m         \u001b[0miloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/finsent/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m   1609\u001b[0m                     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m                     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1611\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1612\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/finsent/lib/python3.6/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mkind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOSITIONAL_OR_KEYWORD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/finsent/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mreindex\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4034\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"axis\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4035\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4036\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4038\u001b[0m     def drop(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/finsent/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mreindex\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4462\u001b[0m         \u001b[0;31m# perform the reindex on the axes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4463\u001b[0m         return self._reindex_axes(\n\u001b[0;32m-> 4464\u001b[0;31m             \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4465\u001b[0m         ).__finalize__(self, method=\"reindex\")\n\u001b[1;32m   4466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/finsent/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_reindex_axes\u001b[0;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[1;32m   3881\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3882\u001b[0m             frame = frame._reindex_index(\n\u001b[0;32m-> 3883\u001b[0;31m                 \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3884\u001b[0m             )\n\u001b[1;32m   3885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/finsent/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_reindex_index\u001b[0;34m(self, new_index, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[1;32m   3903\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3904\u001b[0m             \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3905\u001b[0;31m             \u001b[0mallow_dups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3906\u001b[0m         )\n\u001b[1;32m   3907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/finsent/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_reindex_with_indexers\u001b[0;34m(self, reindexers, fill_value, copy, allow_dups)\u001b[0m\n\u001b[1;32m   4530\u001b[0m                 \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4531\u001b[0m                 \u001b[0mallow_dups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_dups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4532\u001b[0;31m                 \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4533\u001b[0m             )\n\u001b[1;32m   4534\u001b[0m             \u001b[0;31m# If we've made a copy once, no need to make another one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/finsent/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate)\u001b[0m\n\u001b[1;32m   1299\u001b[0m                     ),\n\u001b[1;32m   1300\u001b[0m                 )\n\u001b[0;32m-> 1301\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1302\u001b[0m             ]\n\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/finsent/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1299\u001b[0m                     ),\n\u001b[1;32m   1300\u001b[0m                 )\n\u001b[0;32m-> 1301\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1302\u001b[0m             ]\n\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/finsent/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m         new_values = algos.take_nd(\n\u001b[0;32m-> 1256\u001b[0;31m             \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1257\u001b[0m         )\n\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/finsent/lib/python3.6/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, out, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     func = _get_take_nd_function(\n\u001b[0;32m-> 1735\u001b[0;31m         \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1736\u001b[0m     )\n\u001b[1;32m   1737\u001b[0m     \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/finsent/lib/python3.6/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36m_get_take_nd_function\u001b[0;34m(ndim, arr_dtype, out_dtype, axis, mask_info)\u001b[0m\n\u001b[1;32m   1513\u001b[0m ):\n\u001b[1;32m   1514\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1515\u001b[0;31m         \u001b[0mtup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marr_dtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_take_1d_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/finsent/lib/python3.6/site-packages/numpy/core/_dtype.py\u001b[0m in \u001b[0;36m_name_get\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_name_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m     \u001b[0;31m# provides dtype.name.__get__, documented as returning a \"bit name\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "matches = []\n",
    "c = -1\n",
    "df_merged = pd.DataFrame(columns = [\"a_\"+i for i in list(df.keys())]+[\"b_\"+i for i in list(df_js.keys())],\n",
    "                         dtype='object')\n",
    "\n",
    "for index,row1 in df_js.iterrows():\n",
    "#     print(index)\n",
    "#     print(row)\n",
    "    if row1['doi'] in df_merged['b_doi'].values:\n",
    "        continue\n",
    "    \n",
    "    c=c+1\n",
    "    row2 = df[df['title']==row1['title']]\n",
    "    \n",
    "    if len(row2)>0:\n",
    "        score = authorComparison(row1['authors'],row2['authors'].iloc[0],1)\n",
    "        if score==1:\n",
    "            for i in row1.keys():\n",
    "                df_merged.at[c,\"b_\"+i] = row1[i]\n",
    "            for i in row2.keys():\n",
    "                df_merged.at[c,\"a_\"+i] = row2[i].iloc[0]\n",
    "            continue\n",
    "    for i in row1.keys():\n",
    "        \n",
    "        df_merged.at[c,\"b_\"+i] = row1[i]\n",
    "    \n",
    "    \n",
    "    if c%1000==0:\n",
    "        print(c)\n",
    "#     for i in row2.keys():\n",
    "#         df_merged.at[c,\"a_\"+i] = row2[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-modification",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_dup = df_merged.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "id": "willing-taylor",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = df_merged_dup['a_ID'].values\n",
    "co = 0\n",
    "# c=19999\n",
    "for index,row in df.iterrows():\n",
    "    if row['ID'] not in ls:\n",
    "        co=co+1\n",
    "        c = c+1\n",
    "        for i in row.keys():\n",
    "            df_merged_dup.at[c,\"a_\"+i] = row[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "id": "invisible-performer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20534"
      ]
     },
     "execution_count": 651,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_merged_dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "id": "particular-carry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_merged_dup.to_pickle('../data/complete_merged.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "id": "rental-translator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_merged_dup.to_csv('../data/complete_merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "empirical-trail",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame(columns = ['id','title','authors','yearPublished','yearUpdated','abstract','categories','journal','arxivUrl','gscholarUrl','semanticUrl','citationVelocity','influentialCitationCount','citations','references'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-wildlife",
   "metadata": {},
   "source": [
    "### Creating DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "killing-disclaimer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.360121\n",
      "20534\n",
      "100\n",
      "19.281095\n",
      "20534\n",
      "200\n",
      "36.48304\n",
      "20534\n",
      "300\n",
      "46.033151\n",
      "20534\n",
      "400\n",
      "63.794891\n",
      "20534\n",
      "500\n",
      "73.962508\n",
      "20534\n",
      "600\n",
      "81.571125\n",
      "20534\n",
      "700\n",
      "88.012834\n",
      "20534\n",
      "800\n",
      "100.325919\n",
      "20534\n",
      "900\n",
      "113.99596\n",
      "20534\n",
      "1000\n",
      "127.987083\n",
      "20534\n",
      "1100\n",
      "135.499374\n",
      "20534\n",
      "1200\n",
      "147.206104\n",
      "20534\n",
      "1300\n",
      "167.818165\n",
      "20534\n",
      "1400\n",
      "178.855763\n",
      "20534\n",
      "1500\n",
      "186.100404\n",
      "20534\n",
      "1600\n",
      "197.174888\n",
      "20534\n",
      "1700\n",
      "203.21058\n",
      "20534\n",
      "1800\n",
      "207.858666\n",
      "20534\n",
      "1900\n",
      "215.00275\n",
      "20534\n",
      "2000\n",
      "222.653097\n",
      "20534\n",
      "2100\n",
      "229.621485\n",
      "20534\n",
      "2200\n",
      "236.397039\n",
      "20534\n",
      "2300\n",
      "243.324421\n",
      "20534\n",
      "2400\n",
      "250.787283\n",
      "20534\n",
      "2500\n",
      "256.62082\n",
      "20534\n",
      "2600\n",
      "261.913084\n",
      "20534\n",
      "2700\n",
      "271.390982\n",
      "20534\n",
      "2800\n",
      "278.735798\n",
      "20534\n",
      "2900\n",
      "289.24742\n",
      "20534\n",
      "3000\n",
      "303.261372\n",
      "20534\n",
      "3100\n",
      "314.277622\n",
      "20534\n",
      "3200\n",
      "321.996457\n",
      "20534\n",
      "3300\n",
      "327.676616\n",
      "20534\n",
      "3400\n",
      "340.876159\n",
      "20534\n",
      "3500\n",
      "349.205355\n",
      "20534\n",
      "3600\n",
      "353.757299\n",
      "20534\n",
      "3700\n",
      "361.214629\n",
      "20534\n",
      "3800\n",
      "369.109771\n",
      "20534\n",
      "3900\n",
      "379.531903\n",
      "20534\n",
      "4000\n",
      "383.952506\n",
      "20534\n",
      "4100\n",
      "392.724554\n",
      "20534\n",
      "4200\n",
      "400.482516\n",
      "20534\n",
      "4300\n",
      "410.516747\n",
      "20534\n",
      "4400\n",
      "418.427245\n",
      "20534\n",
      "4500\n",
      "428.553467\n",
      "20534\n",
      "4600\n",
      "439.848592\n",
      "20534\n",
      "4700\n",
      "446.838125\n",
      "20534\n",
      "4800\n",
      "454.42905\n",
      "20534\n",
      "4900\n",
      "463.573714\n",
      "20534\n",
      "5000\n",
      "475.951761\n",
      "20534\n",
      "5100\n",
      "486.901385\n",
      "20534\n",
      "5200\n",
      "494.76384\n",
      "20534\n",
      "5300\n",
      "505.016635\n",
      "20534\n",
      "5400\n",
      "513.658023\n",
      "20534\n",
      "5500\n",
      "522.678891\n",
      "20534\n",
      "5600\n",
      "536.383825\n",
      "20534\n",
      "5700\n",
      "545.443157\n",
      "20534\n",
      "5800\n",
      "556.222206\n",
      "20534\n",
      "5900\n",
      "577.287077\n",
      "20534\n",
      "6000\n",
      "584.393938\n",
      "20534\n",
      "6100\n",
      "595.432445\n",
      "20534\n",
      "6200\n",
      "603.464691\n",
      "20534\n",
      "6300\n",
      "615.048842\n",
      "20534\n",
      "6400\n",
      "620.272255\n",
      "20534\n",
      "6500\n",
      "629.759236\n",
      "20534\n",
      "6600\n",
      "637.564302\n",
      "20534\n",
      "6700\n",
      "646.335312\n",
      "20534\n",
      "6800\n",
      "653.870159\n",
      "20534\n",
      "6900\n",
      "664.512481\n",
      "20534\n",
      "7000\n",
      "675.522975\n",
      "20534\n",
      "7100\n",
      "683.425528\n",
      "20534\n",
      "7200\n",
      "690.84092\n",
      "20534\n",
      "7300\n",
      "699.165455\n",
      "20534\n",
      "7400\n",
      "714.866635\n",
      "20534\n",
      "7500\n",
      "723.021519\n",
      "20534\n",
      "7600\n",
      "730.067064\n",
      "20534\n",
      "7700\n",
      "740.354714\n",
      "20534\n",
      "7800\n",
      "748.958404\n",
      "20534\n",
      "7900\n",
      "756.842261\n",
      "20534\n",
      "8000\n",
      "768.321329\n",
      "20534\n",
      "8100\n",
      "790.632085\n",
      "20534\n",
      "8200\n",
      "800.070665\n",
      "20534\n",
      "8300\n",
      "808.628824\n",
      "20534\n",
      "8400\n",
      "821.865566\n",
      "20534\n",
      "8500\n",
      "830.005634\n",
      "20534\n",
      "8600\n",
      "837.400233\n",
      "20534\n",
      "8700\n",
      "843.474856\n",
      "20534\n",
      "8800\n",
      "852.679261\n",
      "20534\n",
      "8900\n",
      "859.320759\n",
      "20534\n",
      "9000\n",
      "867.113806\n",
      "20534\n",
      "9100\n",
      "872.833781\n",
      "20534\n",
      "9200\n",
      "879.260197\n",
      "20534\n",
      "9300\n",
      "887.367508\n",
      "20534\n",
      "9400\n",
      "896.000488\n",
      "20534\n",
      "9500\n",
      "902.21135\n",
      "20534\n",
      "9600\n",
      "909.251225\n",
      "20534\n",
      "9700\n",
      "915.790497\n",
      "20534\n",
      "9800\n",
      "923.211979\n",
      "20534\n",
      "9900\n",
      "930.872669\n",
      "20534\n",
      "10000\n",
      "936.018908\n",
      "20534\n",
      "10100\n",
      "941.849621\n",
      "20534\n",
      "10200\n",
      "946.82986\n",
      "20534\n",
      "10300\n",
      "951.950378\n",
      "20534\n",
      "10400\n",
      "958.902154\n",
      "20534\n",
      "10500\n",
      "967.345387\n",
      "20534\n",
      "10600\n",
      "973.804974\n",
      "20534\n",
      "10700\n",
      "985.760467\n",
      "20534\n",
      "10800\n",
      "991.702348\n",
      "20534\n",
      "10900\n",
      "1003.859238\n",
      "20534\n",
      "11000\n",
      "1015.183722\n",
      "20534\n",
      "11100\n",
      "1027.448429\n",
      "20534\n",
      "11200\n",
      "1037.433812\n",
      "20534\n",
      "11300\n",
      "1042.903986\n",
      "20534\n",
      "11400\n",
      "1050.579581\n",
      "20534\n",
      "11500\n",
      "1058.066138\n",
      "20534\n",
      "11600\n",
      "1064.986269\n",
      "20534\n",
      "11700\n",
      "1072.863271\n",
      "20534\n",
      "11800\n",
      "1079.894993\n",
      "20534\n",
      "11900\n",
      "1088.383942\n",
      "20534\n",
      "12000\n",
      "1096.715115\n",
      "20534\n",
      "12100\n",
      "1102.906865\n",
      "20534\n",
      "12200\n",
      "1109.637867\n",
      "20534\n",
      "12300\n",
      "1117.93657\n",
      "20534\n",
      "12400\n",
      "1125.837382\n",
      "20534\n",
      "12500\n",
      "1133.62992\n",
      "20534\n",
      "12600\n",
      "1139.716645\n",
      "20534\n",
      "12700\n",
      "1151.044691\n",
      "20534\n",
      "12800\n",
      "1158.159058\n",
      "20534\n",
      "12900\n",
      "1164.732728\n",
      "20534\n",
      "13000\n",
      "1172.075648\n",
      "20534\n",
      "13100\n",
      "1179.132776\n",
      "20534\n",
      "13200\n",
      "1185.853001\n",
      "20534\n",
      "13300\n",
      "1191.403005\n",
      "20534\n",
      "13400\n",
      "1205.152263\n",
      "20534\n",
      "13500\n",
      "1211.486957\n",
      "20534\n",
      "13600\n",
      "1218.69909\n",
      "20534\n",
      "13700\n",
      "1226.679926\n",
      "20534\n",
      "13800\n",
      "1235.238585\n",
      "20534\n",
      "13900\n",
      "1241.138162\n",
      "20534\n",
      "14000\n",
      "1248.066815\n",
      "20534\n",
      "14100\n",
      "1255.301255\n",
      "20534\n",
      "14200\n",
      "1263.163975\n",
      "20534\n",
      "14300\n",
      "1268.219385\n",
      "20534\n",
      "14400\n",
      "1274.616691\n",
      "20534\n",
      "14500\n",
      "1281.44237\n",
      "20534\n",
      "14600\n",
      "1287.427657\n",
      "20534\n",
      "14700\n",
      "1295.970912\n",
      "20534\n",
      "14800\n",
      "1301.300226\n",
      "20534\n",
      "14900\n",
      "1309.567178\n",
      "20534\n",
      "15000\n",
      "1318.523342\n",
      "20534\n",
      "15100\n",
      "1324.258512\n",
      "20534\n",
      "15200\n",
      "1331.551346\n",
      "20534\n",
      "15300\n",
      "1338.110416\n",
      "20534\n",
      "15400\n",
      "1342.486384\n",
      "20534\n",
      "15500\n",
      "1348.466138\n",
      "20534\n",
      "15600\n",
      "1354.995264\n",
      "20534\n",
      "15700\n",
      "1363.124193\n",
      "20534\n",
      "15800\n",
      "1369.800691\n",
      "20534\n",
      "15900\n",
      "1375.839727\n",
      "20534\n",
      "16000\n",
      "1381.307189\n",
      "20534\n",
      "16100\n",
      "1389.197969\n",
      "20534\n",
      "16200\n",
      "1397.195342\n",
      "20534\n",
      "16300\n",
      "1405.201604\n",
      "20534\n",
      "16400\n",
      "1413.179549\n",
      "20534\n",
      "16500\n",
      "1421.161733\n",
      "20534\n",
      "16600\n",
      "1429.154389\n",
      "20534\n",
      "16700\n",
      "1437.152552\n",
      "20534\n",
      "16800\n",
      "1445.152774\n",
      "20534\n",
      "16900\n",
      "1453.198944\n",
      "20534\n",
      "17000\n",
      "1461.209039\n",
      "20534\n",
      "17100\n",
      "1469.216007\n",
      "20534\n",
      "17200\n",
      "1477.279575\n",
      "20534\n",
      "17300\n",
      "1485.287592\n",
      "20534\n",
      "17400\n",
      "1493.321928\n",
      "20534\n",
      "17500\n",
      "1501.399523\n",
      "20534\n",
      "17600\n",
      "1509.428834\n",
      "20534\n",
      "17700\n",
      "1517.456839\n",
      "20534\n",
      "17800\n",
      "1525.480494\n",
      "20534\n",
      "17900\n",
      "1533.516192\n",
      "20534\n",
      "18000\n",
      "1541.55219\n",
      "20534\n",
      "18100\n",
      "1549.591963\n",
      "20534\n",
      "18200\n",
      "1557.63445\n",
      "20534\n",
      "18300\n",
      "1565.716613\n",
      "20534\n",
      "18400\n",
      "1573.75015\n",
      "20534\n",
      "18500\n",
      "1581.78418\n",
      "20534\n",
      "18600\n",
      "1589.829634\n",
      "20534\n",
      "18700\n",
      "1597.885653\n",
      "20534\n",
      "18800\n",
      "1605.947389\n",
      "20534\n",
      "18900\n",
      "1614.035263\n",
      "20534\n",
      "19000\n",
      "1622.201478\n",
      "20534\n",
      "19100\n",
      "1946.96466\n",
      "20534\n",
      "19200\n",
      "1955.021455\n",
      "20534\n",
      "19300\n",
      "1963.042118\n",
      "20534\n",
      "19400\n",
      "1971.079105\n",
      "20534\n",
      "19500\n",
      "1979.13827\n",
      "20534\n",
      "19600\n",
      "1987.220885\n",
      "20534\n",
      "19700\n",
      "1995.303409\n",
      "20534\n",
      "19800\n",
      "2003.424523\n",
      "20534\n",
      "19900\n",
      "2011.507378\n",
      "20534\n",
      "20000\n",
      "2019.596965\n",
      "20534\n",
      "20100\n",
      "2027.730799\n",
      "20534\n",
      "20200\n",
      "2035.890027\n",
      "20534\n",
      "20300\n",
      "2043.973534\n",
      "20534\n",
      "20400\n",
      "2052.046797\n",
      "20534\n",
      "20500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# NEW_ID = 0 \n",
    "c_c = 0\n",
    "st = datetime.now()\n",
    "for index,row in df.iterrows():\n",
    "#     print(index)\n",
    "#     print(row)\n",
    "\n",
    "\n",
    "    # row=df.iloc[2]\n",
    "\n",
    "    NEW_ID = index\n",
    "\n",
    "    ### URI ###\n",
    "#     node_uri = URIRef(str(NEW_ID))\n",
    "#     g.add((node_uri, RDF.type, SCHEMA.ScholarlyArticle))\n",
    "    df_final.at[NEW_ID,'id'] = NEW_ID\n",
    "    \n",
    "    if  str(row['a_title'])!=\"nan\":\n",
    "        title = row['a_title']\n",
    "    else:\n",
    "        title = row['b_title']\n",
    "    if title:\n",
    "        df_final.at[NEW_ID,'title'] = title\n",
    "#         g.add((node_uri, SCHEMA.headline, Literal(title, datatype=SCHEMA.Text)))\n",
    "\n",
    "    if str(row['a_authors'])!=\"nan\":\n",
    "        authors = row['a_authors']\n",
    "    else:\n",
    "        authors = row['b_authors']\n",
    "    if authors:\n",
    "        df_final.at[NEW_ID,'authors'] = authors\n",
    "#         [g.add((node_uri, SCHEMA.author, Literal(author, datatype=SCHEMA.Person))) for author in authors]\n",
    "\n",
    "    published = None\n",
    "    if str(row['b_year'])!=\"nan\":\n",
    "        published = int(row['b_year'])\n",
    "    else:\n",
    "        pub =row[\"a_published\"]\n",
    "        if str(pub)!=\"nan\":\n",
    "            \n",
    "            published = datetime.strptime(pub ,\"%Y-%m-%dT%H:%M:%SZ\").year\n",
    "        \n",
    "    if published:\n",
    "        df_final.at[NEW_ID,'yearPublished'] = published\n",
    "#         g.add((node_uri, SCHEMA.datePublished, Literal(published, datatype=SCHEMA.DateTime)))\n",
    "\n",
    "    try:\n",
    "        if str(row['a_updated'])!=\"nan\":\n",
    "            updated = datetime.strptime(row[\"a_updated\"],\"%Y-%m-%dT%H:%M:%SZ\").year\n",
    "            df_final.at[NEW_ID,'yearUpdated'] = updated\n",
    "#             g.add((node_uri, SCHEMA.dateModified, Literal(updated, datatype=SCHEMA.DateTime)))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    try:\n",
    "        if str(row['b_abstract'])!=\"nan\":\n",
    "            abstract = row['b_abstract']\n",
    "        else:\n",
    "            abstract = row['a_abstract']\n",
    "        df_final.at[NEW_ID,'abstract'] = abstract\n",
    "#         g.add((node_uri, SCHEMA.abstract, Literal(abstract, datatype=SCHEMA.Text)))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    try:\n",
    "        if str(row['a_categories'])!=\"nan\":\n",
    "            categories = ast.literal_eval(row['a_categories'])\n",
    "            df_final.at[NEW_ID,'categories'] = categories\n",
    "#             for category in categories:\n",
    "#                 g.add((node_uri, SCHEMA.genre, Literal(category, datatype=SCHEMA.Text)))\n",
    "    except:\n",
    "        pass\n",
    "    if str(row['a_journal'])!=\"nan\":\n",
    "        journal = row['a_journal']\n",
    "        df_final.at[NEW_ID,'journal'] = journal\n",
    "#         g.add((node_uri, SCHEMA.publisher, Literal(journal, datatype=SCHEMA.Text))) #datatype=SCHEMA.Organisation\n",
    "\n",
    "\n",
    "    if str(row['a_arxiv_url'])!=\"nan\":\n",
    "        arxiv_url = row['a_arxiv_url']\n",
    "        df_final.at[NEW_ID,'arxivUrl'] = arxiv_url\n",
    "#         g.add((node_uri, SCHEMA.url, Literal(arxiv_url, datatype=SCHEMA.URL)))\n",
    "\n",
    "\n",
    "    if str(row['a_gscholar_url'])!=\"nan\":\n",
    "        g_scholar_url = row['a_gscholar_url']\n",
    "        df_final.at[NEW_ID,'gscholarUrl'] = g_scholar_url\n",
    "#         g.add((node_uri, SCHEMA.url, Literal(g_scholar_url, datatype=SCHEMA.URL)))\n",
    "\n",
    "    if str(row['b_url'])!=\"nan\":\n",
    "        g_scholar_url = row['b_url']\n",
    "        df_final.at[NEW_ID,'semanticUrl'] = g_scholar_url\n",
    "#         g.add((node_uri, SCHEMA.url, Literal(g_scholar_url, datatype=SCHEMA.URL)))\n",
    "\n",
    "    if str(row['b_citationVelocity'])!=\"nan\":\n",
    "        num = row['b_citationVelocity']\n",
    "        df_final.at[NEW_ID,'citationVelocity'] = num\n",
    "#         g.add((node_uri, MYNS.citationVelocity, Literal(num))) #datatype=SCHEMA.Organisation\n",
    "\n",
    "    if str(row['b_influentialCitationCount'])!=\"nan\":\n",
    "        num = row['b_influentialCitationCount']\n",
    "        df_final.at[NEW_ID,'influentialCitationCount'] = num\n",
    "#         g.add((node_uri, MYNS.b_influentialCitationCount, Literal(num))) #datatype=SCHEMA.Organisation\n",
    "\n",
    "    try:\n",
    "        citations_1 = ast.literal_eval(row['b_citations'])\n",
    "    except:\n",
    "        citations_1 = []\n",
    "    try:\n",
    "        citations_2 = row['b_dblp_info']['references_opencitations']\n",
    "    except:\n",
    "        citations_2 = []\n",
    "    citations_1.extend(citations_2)\n",
    "\n",
    "    c_final_list = []\n",
    "    \n",
    "    for c in citations_1:\n",
    "\n",
    "        if 'citing' in c:\n",
    "            c_doi = c['citing'].split(\" => \")[-1]\n",
    "            citing_row = df[df['b_doi']==c_doi]\n",
    "            if len(citing_row)>0:\n",
    "                citing_id = citing_row.index[0]\n",
    "                cite_uri = URIRef(str(citing_id))\n",
    "                c_final_list.append(cite_uri)\n",
    "#                 g.add((node_uri, SCHEMA.citation, cite_uri))\n",
    "            else:\n",
    "                cite_uri = c_doi\n",
    "#                 len_df = len(df)\n",
    "#                 cite_uri = URIRef(str(len_df))\n",
    "#                 df.at[len_df,'b_doi'] = c_doi\n",
    "#                 try:\n",
    "#                     df.at[len_df,'b_year'] = float(c['creation'].split(\" => \")[-1])\n",
    "#                 except:\n",
    "#                     pass\n",
    "                c_final_list.append(cite_uri)\n",
    "#                 g.add((node_uri, SCHEMA.citation, Literal(cite_uri)))\n",
    "        else:\n",
    "            try:\n",
    "                c_doi = c['doi']\n",
    "            except:\n",
    "                c_doi = None\n",
    "            if c_doi:\n",
    "                citing_row = df[df['b_doi']==c_doi]\n",
    "#                 if c_doi:\n",
    "#                     citing_row = df[df['b_doi']==c_doi]\n",
    "#                 else:\n",
    "#                     citing_row = df[df['b_title']==c_title]\n",
    "\n",
    "                if len(citing_row)>0:\n",
    "                    citing_id = citing_row.index[0]\n",
    "                    cite_uri = URIRef(str(citing_id))\n",
    "                    c_final_list.append(cite_uri)\n",
    "#                     g.add((node_uri, SCHEMA.citation, cite_uri))\n",
    "                else:\n",
    "                    cite_uri = c_doi\n",
    "#                     len_df = len(df)\n",
    "#                     cite_uri = URIRef(str(len_df))\n",
    "#                     df.at[len_df,'b_doi'] = c_doi\n",
    "#                     if c['arxivId']:\n",
    "#                         df.at[len_df,'b_arxivId'] = c['arxivId']\n",
    "#                     if c['authors']:\n",
    "#                         df.at[len_df,'b_authors'] = [i['name'] for i in c['authors']]\n",
    "#                     if c['title']:\n",
    "#                         df.at[len_df,'b_title'] = c['title']\n",
    "#                     if c['url']:\n",
    "#                         df.at[len_df,'b_url'] = c['url']\n",
    "#                     if c['year']:\n",
    "#                         df.at[len_df,'b_year'] = c['year'] \n",
    "                    c_final_list.append(cite_uri)\n",
    "#                     g.add((node_uri, SCHEMA.citation, Literal(cite_uri)))\n",
    "                \n",
    "\n",
    "    #     else:\n",
    "    #         g.add((node_uri, SCHEMA.citation,Literal(c_doi, datatype=SCHEMA.Text)))\n",
    "\n",
    "    df_final.at[NEW_ID,'citations'] = list(set(c_final_list))\n",
    "    \n",
    "    try:\n",
    "        references_1 = ast.literal_eval(row['b_references'])\n",
    "    except:\n",
    "        referneces_1 = []\n",
    "    try:\n",
    "        references_2 = row['b_dblp_info']['references_crossref']\n",
    "    except:\n",
    "        references_2 = []\n",
    "    references_1.extend(references_2)   \n",
    "\n",
    "    ref_final_list = []\n",
    "    \n",
    "    for c in references_1:\n",
    "        if 'key' in c:\n",
    "            citing_row = []\n",
    "            c_doi = None\n",
    "\n",
    "            if \"DOI\" in c:\n",
    "                c_doi = c[\"DOI\"]\n",
    "                citing_row = df[df['b_doi']==c_doi]\n",
    "\n",
    "            elif \"article-title\" in c:\n",
    "                c_title = c['article-title']\n",
    "                citing_row = df[(df['b_title']==c_title) | (df['a_title']==c_title)]\n",
    "\n",
    "\n",
    "            if len(citing_row)>0:\n",
    "                citing_id = citing_row.index[0]\n",
    "                cite_uri = URIRef(str(citing_id))\n",
    "                ref_final_list.append(cite_uri)\n",
    "#                 g.add((node_uri, MYNS.references, cite_uri))\n",
    "            else:\n",
    "                cite_uri = c_doi \n",
    "                \n",
    "#                 len_df = len(df)\n",
    "#                 cite_uri = URIRef(str(len_df))\n",
    "\n",
    "#                 try:\n",
    "#                     df.at[len_df,'b_doi'] = c_doi\n",
    "#                 except:\n",
    "#                     pass\n",
    "#                 try:\n",
    "#                     df.at[len_df,'b_year'] = float(c['year'])\n",
    "#                 except:\n",
    "#                     pass\n",
    "#                 try:\n",
    "#                     df.at[len_df,'b_author'] = c['author']\n",
    "#                 except:\n",
    "#                     pass\n",
    "#                 try:\n",
    "#                     df.at[len_df,'b_title'] = c['article-title']\n",
    "#                 except:\n",
    "#                     pass\n",
    "                ref_final_list.append(cite_uri)\n",
    "#                 g.add((node_uri, MYNS.references, Literal(cite_uri)))\n",
    "        else:\n",
    "            try:\n",
    "                c_doi = c['doi']\n",
    "            except:\n",
    "                c_doi = None\n",
    "                pass\n",
    "            if c_doi:\n",
    "                citing_row = df[df['b_doi']==c_doi]\n",
    "#                 if c_doi:\n",
    "#                     citing_row = df[df['b_doi']==c_doi]\n",
    "#                 else:\n",
    "#                     citing_row = df[df['b_title']==c_title]\n",
    "\n",
    "                if len(citing_row)>0:\n",
    "                    citing_id = citing_row.index[0]\n",
    "                    cite_uri = URIRef(str(citing_id))\n",
    "                    ref_final_list.append(cite_uri)\n",
    "#                     g.add((node_uri, MYNS.references, cite_uri))\n",
    "                else:\n",
    "                    cite_uri = c_doi\n",
    "#                     len_df = len(df)\n",
    "#                     cite_uri = URIRef(str(len_df))\n",
    "#                     df.at[len_df,'b_doi'] = c_doi\n",
    "#                     if c['arxivId']:\n",
    "#                         df.at[len_df,'b_arxivId'] = c['arxivId']\n",
    "#                     if c['authors']:\n",
    "#                         df.at[len_df,'b_authors'] = [i['name'] for i in c['authors']]\n",
    "#                     if c['title']:\n",
    "#                         df.at[len_df,'b_title'] = c['title']\n",
    "#                     if c['url']:\n",
    "#                         df.at[len_df,'b_url'] = c['url']\n",
    "#                     if c['year']:\n",
    "#                         df.at[len_df,'b_year'] = c['year']  \n",
    "#                     g.add((node_uri, MYNS.references, Literal(cite_uri)))\n",
    "                    ref_final_list.append(cite_uri)\n",
    "\n",
    "    #     else:\n",
    "    #         g.add((node_uri, SCHEMA.citation,Literal(c_doi, datatype=SCHEMA.Text)))\n",
    "    df_final.at[NEW_ID,'references'] = list(set(ref_final_list))\n",
    "    c_c = c_c+1\n",
    "    if c_c%100==0:\n",
    "        et = datetime.now()\n",
    "        print((et-st).total_seconds())\n",
    "        print(len(df))\n",
    "        print(c_c)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "annoying-horizontal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_final.to_csv('final_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "described-timer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_final.to_pickle('final_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "national-template",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "contained-correspondence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib import URIRef, Literal, Namespace\n",
    "from rdflib.namespace import RDF, RDFS, XSD\n",
    "from datetime import datetime\n",
    "import math\n",
    "import ast\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "raised-stanford",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_pickle('final_dataset.pkl')\n",
    "df = pd.read_pickle('../data/complete_merged.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "hybrid-archive",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_authors = []\n",
    "[all_authors.extend(i) for i in df_final['authors'] if isinstance(i,list)]\n",
    "all_authors = list(set(all_authors))\n",
    "author_dict = {i:c for i,c in zip(all_authors,range(1000000,1000000+len(all_authors)))}\n",
    "# SCHEMA PERSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "acoustic-plain",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_categories = []\n",
    "[all_categories.extend(i) for i in df_final['categories'] if isinstance(i,list)]\n",
    "all_categories = list(set(all_categories))\n",
    "category_dict = {i:c for i,c in zip(all_categories,range(2000000,2000000+len(all_categories)))}\n",
    "# , datatype=SCHEMA.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "drawn-williams",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_journals = []\n",
    "all_journals = list(df_final['journal'][df_final['journal'].notnull()])\n",
    "all_journals = list(map(lambda s:''.join([i for i in s if i.isalpha() or i==\" \"]).strip(),all_journals))\n",
    "all_journals = list(set(all_journals))\n",
    "journal_dict = {i:c for i,c in zip(all_journals,range(3000000,3000000+len(all_journals)))}\n",
    "# , datatype=SCHEMA.Text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "enclosed-maintenance",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "MYNS = Namespace('http://inf558.org/myfakenamespace#')\n",
    "SCHEMA = Namespace(\"https://schema.org/\")\n",
    "\n",
    "# Initliaze the graph\n",
    "g = rdflib.Graph()\n",
    "\n",
    "# Bind namespace and prefixes\n",
    "g.bind('my_ns', MYNS)\n",
    "g.bind('schema', SCHEMA)\n",
    "g.bind('rdf', RDF)\n",
    "g.bind('rdfs', RDFS)\n",
    "g.bind('xsd', XSD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "supposed-australian",
   "metadata": {},
   "outputs": [],
   "source": [
    "doi_dict = {i:c for (i,c) in zip(df['b_doi'],range(len(df['b_doi'])))}\n",
    "doi_dict_count = 20534"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "amazing-holmes",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "extended-source",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:00, 124.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10010it [02:14, 103.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20087it [03:31, 572.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20534it [03:32, 96.71it/s] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# NEW_ID = 0 \n",
    "c_c = 0\n",
    "st = datetime.now()\n",
    "for index,row in tqdm(df.iterrows()):\n",
    "#     print(index)\n",
    "#     print(row)\n",
    "    if index%10000==0:\n",
    "        print(len(doi_dict))\n",
    "    \n",
    "\n",
    "    # row=df.iloc[2]\n",
    "\n",
    "    NEW_ID = index\n",
    "\n",
    "    ### URI ###\n",
    "    node_uri = URIRef(str(NEW_ID))\n",
    "    g.add((node_uri, RDF.type, SCHEMA.ScholarlyArticle))\n",
    "    \n",
    "    if  str(row['b_doi'])!=\"nan\":\n",
    "        b_doi = row['b_doi']\n",
    "        g.add((node_uri, MYNS.doi, Literal(b_doi, datatype=SCHEMA.Text)))\n",
    "\n",
    "    \n",
    "    if  str(row['a_title'])!=\"nan\":\n",
    "        title = row['a_title']\n",
    "    else:\n",
    "        title = row['b_title']\n",
    "    if title:\n",
    "        g.add((node_uri, SCHEMA.headline, Literal(title, datatype=SCHEMA.Text)))\n",
    "\n",
    "\n",
    "    if str(row['a_authors'])!=\"nan\":\n",
    "        authors = row['a_authors']\n",
    "    else:\n",
    "        authors = row['b_authors']\n",
    "    if authors:\n",
    "#         [g.add((node_uri, SCHEMA.author, Literal(author, datatype=SCHEMA.Person))) for author in authors]\n",
    "        for author in authors:\n",
    "            g.add((node_uri, SCHEMA.author, URIRef(str(author_dict[author])))) \n",
    "            \n",
    "    published = None\n",
    "    if str(row['b_year'])!=\"nan\":\n",
    "        published = int(row['b_year'])\n",
    "    else:\n",
    "        pub =row[\"a_published\"]\n",
    "        if str(pub)!=\"nan\":\n",
    "            published = datetime.strptime(pub ,\"%Y-%m-%dT%H:%M:%SZ\").year\n",
    "        \n",
    "    if published:\n",
    "        g.add((node_uri, SCHEMA.datePublished, Literal(published, datatype=SCHEMA.DateTime)))\n",
    "\n",
    "    try:\n",
    "        if str(row['a_updated'])!=\"nan\":\n",
    "            updated = datetime.strptime(row[\"a_updated\"],\"%Y-%m-%dT%H:%M:%SZ\").year\n",
    "            g.add((node_uri, SCHEMA.dateModified, Literal(updated, datatype=SCHEMA.DateTime)))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    try:\n",
    "        if str(row['b_abstract'])!=\"nan\":\n",
    "            abstract = row['b_abstract']\n",
    "        else:\n",
    "            abstract = row['a_abstract']\n",
    "        g.add((node_uri, SCHEMA.abstract, Literal(abstract, datatype=SCHEMA.Text)))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    try:\n",
    "        if str(row['a_categories'])!=\"nan\":\n",
    "            categories = ast.literal_eval(row['a_categories'])\n",
    "            for category in categories:\n",
    "                g.add((node_uri, SCHEMA.genre, URIRef(str(category_dict[category]))))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if str(row['a_journal'])!=\"nan\":\n",
    "        s = row['a_journal']\n",
    "        journal = ''.join([i for i in s if i.isalpha() or i==\" \"]).strip()\n",
    "        \n",
    "        g.add((node_uri, SCHEMA.publisher, URIRef(str(journal_dict[journal])))) #datatype=SCHEMA.Organisation\n",
    "\n",
    "\n",
    "    if str(row['a_arxiv_url'])!=\"nan\":\n",
    "        arxiv_url = row['a_arxiv_url']\n",
    "\n",
    "        g.add((node_uri, SCHEMA.url, Literal(arxiv_url, datatype=SCHEMA.URL)))\n",
    "\n",
    "\n",
    "    if str(row['a_gscholar_url'])!=\"nan\":\n",
    "        g_scholar_url = row['a_gscholar_url']\n",
    "\n",
    "        g.add((node_uri, SCHEMA.url, Literal(g_scholar_url, datatype=SCHEMA.URL)))\n",
    "\n",
    "    if str(row['b_url'])!=\"nan\":\n",
    "        g_scholar_url = row['b_url']\n",
    "\n",
    "        g.add((node_uri, SCHEMA.url, Literal(g_scholar_url, datatype=SCHEMA.URL)))\n",
    "\n",
    "    if str(row['b_citationVelocity'])!=\"nan\":\n",
    "        num = row['b_citationVelocity']\n",
    "        g.add((node_uri, MYNS.citationVelocity, Literal(num))) #datatype=SCHEMA.Organisation\n",
    "\n",
    "    if str(row['b_influentialCitationCount'])!=\"nan\":\n",
    "        num = row['b_influentialCitationCount']\n",
    "        g.add((node_uri, MYNS.b_influentialCitationCount, Literal(num))) #datatype=SCHEMA.Organisation\n",
    "\n",
    "    try:\n",
    "        citations_1 = ast.literal_eval(row['b_citations'])\n",
    "    except:\n",
    "        citations_1 = []\n",
    "    try:\n",
    "        citations_2 = row['b_dblp_info']['references_opencitations']\n",
    "    except:\n",
    "        citations_2 = []\n",
    "    citations_1.extend(citations_2)\n",
    "\n",
    "    for c in citations_1:\n",
    "\n",
    "        if 'citing' in c:\n",
    "            c_doi = c['citing'].split(\" => \")[-1]\n",
    "#             citing_row = df[df['b_doi']==c_doi]\n",
    "            citing_id=-1\n",
    "            try:\n",
    "                citing_id = doi_dict[c_doi]\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            if citing_id>-1:\n",
    "#                 citing_id = citing_row.index[0]\n",
    "                cite_uri = URIRef(str(citing_id))\n",
    "                g.add((node_uri, SCHEMA.citation, cite_uri))\n",
    "            else:\n",
    "#                 len_df = len(df)\n",
    "                cite_uri = URIRef(str(doi_dict_count))\n",
    "                doi_dict[c_doi] = doi_dict_count\n",
    "                doi_dict_count+=1\n",
    "\n",
    "#                 df.at[len_df,'b_doi'] = c_doi\n",
    "#                 try:\n",
    "#                     df.at[len_df,'b_year'] = float(c['creation'].split(\" => \")[-1])\n",
    "#                 except:\n",
    "#                     pass\n",
    "                \n",
    "                g.add((node_uri, SCHEMA.citation, cite_uri))\n",
    "        else:\n",
    "            try:\n",
    "                c_doi = c['doi']\n",
    "            except:\n",
    "                c_doi = None\n",
    "            if c_doi:\n",
    "                citing_id=-1\n",
    "                try:\n",
    "                    citing_id = doi_dict[c_doi]\n",
    "                except:\n",
    "                    pass\n",
    "            #                 if c_doi:\n",
    "#                     citing_row = df[df['b_doi']==c_doi]\n",
    "#                 else:\n",
    "#                     citing_row = df[df['b_title']==c_title]\n",
    "\n",
    "                if citing_id>-1:\n",
    "#                     citing_id = citing_row.index[0]\n",
    "                    cite_uri = URIRef(str(citing_id))\n",
    "                    g.add((node_uri, SCHEMA.citation, cite_uri))\n",
    "                else:\n",
    "                    cite_uri = URIRef(str(doi_dict_count))\n",
    "                    doi_dict[c_doi] = doi_dict_count\n",
    "                    doi_dict_count+=1\n",
    "                    #                     len_df = len(df)\n",
    "#                     cite_uri = URIRef(str(len_df))\n",
    "#                     df.at[len_df,'b_doi'] = c_doi\n",
    "#                     if c['arxivId']:\n",
    "#                         df.at[len_df,'b_arxivId'] = c['arxivId']\n",
    "#                     if c['authors']:\n",
    "#                         df.at[len_df,'b_authors'] = [i['name'] for i in c['authors']]\n",
    "#                     if c['title']:\n",
    "#                         df.at[len_df,'b_title'] = c['title']\n",
    "#                     if c['url']:\n",
    "#                         df.at[len_df,'b_url'] = c['url']\n",
    "#                     if c['year']:\n",
    "#                         df.at[len_df,'b_year'] = c['year']            \n",
    "                    g.add((node_uri, SCHEMA.citation, cite_uri))\n",
    "                \n",
    "\n",
    "    #     else:\n",
    "    #         g.add((node_uri, SCHEMA.citation,Literal(c_doi, datatype=SCHEMA.Text)))\n",
    "\n",
    "    try:\n",
    "        references_1 = ast.literal_eval(row['b_references'])\n",
    "    except:\n",
    "        referneces_1 = []\n",
    "    try:\n",
    "        references_2 = row['b_dblp_info']['references_crossref']\n",
    "    except:\n",
    "        references_2 = []\n",
    "    references_1.extend(references_2)   \n",
    "\n",
    "    for c in references_1:\n",
    "        if 'key' in c:\n",
    "            citing_row = []\n",
    "            c_doi = None\n",
    "\n",
    "            if \"DOI\" in c:\n",
    "                c_doi = c[\"DOI\"]\n",
    "#                 citing_row = df[df['b_doi']==c_doi]\n",
    "\n",
    "#             elif \"article-title\" in c:\n",
    "#                 c_title = c['article-title']\n",
    "#                 citing_row = df[(df['b_title']==c_title) | (df['a_title']==c_title)]\n",
    "            if c_doi:\n",
    "                citing_id=-1\n",
    "                try:\n",
    "                    citing_id = doi_dict[c_doi]\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                if citing_id>-1:\n",
    "#                     citing_id = citing_row.index[0]\n",
    "                    cite_uri = URIRef(str(citing_id))\n",
    "                    g.add((node_uri, MYNS.references, cite_uri))\n",
    "                else:\n",
    "                    cite_uri = URIRef(str(doi_dict_count))\n",
    "                    doi_dict[c_doi] = doi_dict_count\n",
    "                    doi_dict_count+=1\n",
    "    #                 len_df = len(df)\n",
    "    #                 cite_uri = URIRef(str(len_df))\n",
    "\n",
    "    #                 try:\n",
    "    #                     df.at[len_df,'b_doi'] = c_doi\n",
    "    #                 except:\n",
    "    #                     pass\n",
    "    #                 try:\n",
    "    #                     df.at[len_df,'b_year'] = float(c['year'])\n",
    "    #                 except:\n",
    "    #                     pass\n",
    "    #                 try:\n",
    "    #                     df.at[len_df,'b_author'] = c['author']\n",
    "    #                 except:\n",
    "    #                     pass\n",
    "    #                 try:\n",
    "    #                     df.at[len_df,'b_title'] = c['article-title']\n",
    "    #                 except:\n",
    "    #                     pass\n",
    "\n",
    "                    g.add((node_uri, MYNS.references, cite_uri))\n",
    "        else:\n",
    "            try:\n",
    "                c_doi = c['doi']\n",
    "            except:\n",
    "                c_doi = None\n",
    "                pass\n",
    "            if c_doi:\n",
    "                citing_id=-1\n",
    "                try:\n",
    "                    citing_id = doi_dict[c_doi]\n",
    "                except:\n",
    "                    pass#                 if c_doi:\n",
    "#                     citing_row = df[df['b_doi']==c_doi]\n",
    "#                 else:\n",
    "#                     citing_row = df[df['b_title']==c_title]\n",
    "\n",
    "                if citing_id>-1:\n",
    "#                     citing_id = citing_row.index[0]\n",
    "                    cite_uri = URIRef(str(citing_id))\n",
    "                    g.add((node_uri, MYNS.references, cite_uri))\n",
    "                else:\n",
    "                    cite_uri = URIRef(str(doi_dict_count))\n",
    "                    doi_dict[c_doi] = doi_dict_count\n",
    "                    doi_dict_count+=1\n",
    "                    #                     len_df = len(df)\n",
    "#                     cite_uri = URIRef(str(len_df))\n",
    "#                     df.at[len_df,'b_doi'] = c_doi\n",
    "#                     if c['arxivId']:\n",
    "#                         df.at[len_df,'b_arxivId'] = c['arxivId']\n",
    "#                     if c['authors']:\n",
    "#                         df.at[len_df,'b_authors'] = [i['name'] for i in c['authors']]\n",
    "#                     if c['title']:\n",
    "#                         df.at[len_df,'b_title'] = c['title']\n",
    "#                     if c['url']:\n",
    "#                         df.at[len_df,'b_url'] = c['url']\n",
    "#                     if c['year']:\n",
    "#                         df.at[len_df,'b_year'] = c['year']  \n",
    "                    g.add((node_uri, MYNS.references, cite_uri))\n",
    "\n",
    "    #     else:\n",
    "    #         g.add((node_uri, SCHEMA.citation,Literal(c_doi, datatype=SCHEMA.Text)))\n",
    "#     c_c = c_c+1\n",
    "#     if c_c%100==0:\n",
    "#         et = datetime.now()\n",
    "#         print((et-st).total_seconds())\n",
    "#         print(len(df))\n",
    "#         print(c_c)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "frequent-prefix",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "453463"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doi_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "fifth-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "doi_dict_inverse = {i:c for c,i in zip(doi_dict,range(0,len(doi_dict))) if i > 20534}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "statistical-breed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 432928/432928 [00:21<00:00, 19848.57it/s]\n"
     ]
    }
   ],
   "source": [
    "for d in tqdm(doi_dict_inverse):\n",
    "    node_uri = URIRef(str(d))\n",
    "    g.add((node_uri, RDF.type, SCHEMA.ScholarlyArticle))\n",
    "    b_doi = doi_dict_inverse[d]\n",
    "    g.add((node_uri, MYNS.doi, Literal(b_doi, datatype=SCHEMA.Text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "honest-billion",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50677/50677 [00:03<00:00, 14960.68it/s]\n"
     ]
    }
   ],
   "source": [
    "for d in tqdm(author_dict):\n",
    "    node_uri = URIRef(str(author_dict[d]))\n",
    "    g.add((node_uri, RDF.type, SCHEMA.Person))\n",
    "    g.add((node_uri, SCHEMA.name, Literal(d, datatype=SCHEMA.Text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "metric-marketplace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86/86 [00:00<00:00, 6250.93it/s]\n"
     ]
    }
   ],
   "source": [
    "for d in tqdm(journal_dict):\n",
    "    node_uri = URIRef(str(journal_dict[d]))\n",
    "    g.add((node_uri, RDF.type, SCHEMA.Publisher))\n",
    "    g.add((node_uri, SCHEMA.name, Literal(d, datatype=SCHEMA.Text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "generous-yorkshire",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [00:00<00:00, 12791.47it/s]\n"
     ]
    }
   ],
   "source": [
    "for d in tqdm(category_dict):\n",
    "    node_uri = URIRef(str(category_dict[d]))\n",
    "    g.add((node_uri, RDF.type, SCHEMA.genre))\n",
    "    g.add((node_uri, SCHEMA.name, Literal(d, datatype=SCHEMA.Text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "infrared-workstation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(g.serialize(format=\"turtle\").decode()[0:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "impossible-reliance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "basic-links",
   "metadata": {},
   "outputs": [],
   "source": [
    "# author_splits = {}\n",
    "# for i in all_authors:\n",
    "#     sp = i.split(\" \")[-1]\n",
    "#     if sp not in author_splits:\n",
    "#         author_splits[sp] = [i]\n",
    "#     else:\n",
    "#         author_splits[sp].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "binding-sculpture",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50677/50677 [03:46<00:00, 223.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# author_dict = {}\n",
    "# for i in tqdm(all_authors):\n",
    "# #     print(i)\n",
    "#     if i not in author_dict:\n",
    "#         sp = i.split(\" \")[-1]\n",
    "#         for j in author_splits[sp]:\n",
    "#             score = needleman_wunsch_score(i, j, match=2, mismatch=-1, gap=-0.5, score_table=None)\n",
    "#             if score>20:\n",
    "#                 author_dict[i] = j\n",
    "#                 break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aggregate-azerbaijan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29411"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(author_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-cradle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-juice",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "extra-orientation",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    citations_1 = ast.literal_eval(row['b_citations'])\n",
    "except:\n",
    "    citations_1 = []\n",
    "try:\n",
    "    citations_2 = row['b_dblp_info']['references_opencitations']\n",
    "except:\n",
    "    citations_2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "inappropriate-vacuum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.iloc[0]['b_dblp_info']['references_opencitations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1082,
   "id": "frequent-firewall",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 1082,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[1][\"b_dblp_info\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1080,
   "id": "square-helena",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dblp_info'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/finsent/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dblp_info'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1080-86de5bd02211>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dblp_info\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/finsent/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/finsent/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/finsent/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dblp_info'"
     ]
    }
   ],
   "source": [
    "len(df.iloc[1][\"dblp_info\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1066,
   "id": "purple-postage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_journal\n",
    "# # b_arxivId\n",
    "# b_citationVelocity\n",
    "# b_influentialCitationCount\n",
    "# b_topics\n",
    "# b_url - semantic url "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1049,
   "id": "frequent-england",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417\n",
      "108\n"
     ]
    }
   ],
   "source": [
    "print(len(citations_1))\n",
    "print(len(citations_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1050,
   "id": "southeast-gabriel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1051,
   "id": "experienced-ethnic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "id": "identified-crest",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "id": "awful-initial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 1053,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(references_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "id": "interior-serial",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1055,
   "id": "greatest-pleasure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@prefix my_ns: <http://inf558.org/myfakenamespace#> .\n",
      "@prefix schema: <https://schema.org/> .\n",
      "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
      "\n",
      "<0> a schema:ScholarlyArticle ;\n",
      "    my_ns:b_influentialCitationCount 52 ;\n",
      "    my_ns:citationVelocity 150 ;\n",
      "    my_ns:references <0>,\n",
      "        <20580>,\n",
      "        <20581>,\n",
      "        <20582>,\n",
      "        <20583>,\n",
      "        <20584>,\n",
      "        <20585>,\n",
      "        <20586>,\n",
      "        <20587>,\n",
      "        <20588>,\n",
      "        <20589>,\n",
      "        <20590>,\n",
      "        <20591>,\n",
      "        <20592>,\n",
      "        <20593>,\n",
      "        <20594>,\n",
      "        <20595>,\n",
      "        <20596>,\n",
      "        <20597>,\n",
      "        <20598>,\n",
      "        <20599>,\n",
      "        <20600>,\n",
      "        <20601>,\n",
      "        <20602>,\n",
      "        <20603>,\n",
      "        <20604>,\n",
      "        <20605>,\n",
      "        <20606>,\n",
      "        <20607>,\n",
      "        <20608>,\n",
      "        <20609>,\n",
      "        <20610>,\n",
      "        <20611>,\n",
      "        <20612>,\n",
      "        <20613>,\n",
      "        <20614>,\n",
      "        <20615>,\n",
      "        <20616>,\n",
      "        <20617>,\n",
      "        <20618>,\n",
      "        <20619>,\n",
      "        <20620>,\n",
      "        <20621>,\n",
      "        <20622>,\n",
      "        <20623>,\n",
      "        <20624>,\n",
      "        <20625>,\n",
      "        <20626>,\n",
      "        <20627>,\n",
      "        <20628>,\n",
      "        <20629>,\n",
      "        <20630>,\n",
      "        <20631>,\n",
      "        <20632>,\n",
      "        <20634>,\n",
      "        <20637>,\n",
      "        <20639>,\n",
      "        <20641>,\n",
      "        <20642>,\n",
      "        <20643>,\n",
      "        <20644>,\n",
      "        <20645>,\n",
      "        <20646>,\n",
      "        <20813>,\n",
      "        <20814>,\n",
      "        <20815>,\n",
      "        <20816>,\n",
      "        <20817>,\n",
      "        <20818>,\n",
      "        <20824>,\n",
      "        <20825>,\n",
      "        <20826>,\n",
      "        <20827>,\n",
      "        <20828>,\n",
      "        <270>,\n",
      "        <412>,\n",
      "        <7059>,\n",
      "        <87> ;\n",
      "    schema:abstract \"The artificial neural networks that are used to recognize shapes typically use one or more layers of learned feature detectors that produce scalar outputs. By contrast, the computer vision community uses complicated, hand-engineered features, like SIFT [6], that produce a whole vector of outputs including an explicit representation of the pose of the feature. We show how neural networks can be used to learn features that output a whole vector of instantiation parameters and we argue that this is a much more promising way of dealing with variations in position, orientation, scale and lighting than the methods currently employed in the neural networks community. It is also more promising than the hand-engineered features currently used in computer vision because it provides an efficient way of adapting the features to the domain.\"^^schema:Text ;\n",
      "    schema:author \"A. Krizhevsky\"^^schema:Person,\n",
      "        \"Geoffrey E. Hinton\"^^schema:Person,\n",
      "        \"S. Wang\"^^schema:Person ;\n",
      "    schema:citation <117>,\n",
      "        <139>,\n",
      "        <160>,\n",
      "        <174>,\n",
      "        <18>,\n",
      "        <20534>,\n",
      "        <20535>,\n",
      "        <20536>,\n",
      "        <20537>,\n",
      "        <20538>,\n",
      "        <20539>,\n",
      "        <20540>,\n",
      "        <20541>,\n",
      "        <20542>,\n",
      "        <20543>,\n",
      "        <20544>,\n",
      "        <20545>,\n",
      "        <20546>,\n",
      "        <20547>,\n",
      "        <20548>,\n",
      "        <20549>,\n",
      "        <20550>,\n",
      "        <20551>,\n",
      "        <20552>,\n",
      "        <20553>,\n",
      "        <20554>,\n",
      "        <20555>,\n",
      "        <20556>,\n",
      "        <20557>,\n",
      "        <20558>,\n",
      "        <20559>,\n",
      "        <20560>,\n",
      "        <20561>,\n",
      "        <20562>,\n",
      "        <20563>,\n",
      "        <20564>,\n",
      "        <20565>,\n",
      "        <20566>,\n",
      "        <20567>,\n",
      "        <20568>,\n",
      "        <20569>,\n",
      "        <20570>,\n",
      "        <20571>,\n",
      "        <20572>,\n",
      "        <20573>,\n",
      "        <20574>,\n",
      "        <20575>,\n",
      "        <20576>,\n",
      "        <20577>,\n",
      "        <20578>,\n",
      "        <20579>,\n",
      "        <20652>,\n",
      "        <20653>,\n",
      "        <20654>,\n",
      "        <20655>,\n",
      "        <20656>,\n",
      "        <20657>,\n",
      "        <20658>,\n",
      "        <20659>,\n",
      "        <20660>,\n",
      "        <20661>,\n",
      "        <20662>,\n",
      "        <20663>,\n",
      "        <20664>,\n",
      "        <20665>,\n",
      "        <20666>,\n",
      "        <20667>,\n",
      "        <20668>,\n",
      "        <20669>,\n",
      "        <20670>,\n",
      "        <20671>,\n",
      "        <20672>,\n",
      "        <20673>,\n",
      "        <20674>,\n",
      "        <20675>,\n",
      "        <20676>,\n",
      "        <20677>,\n",
      "        <20678>,\n",
      "        <20679>,\n",
      "        <20680>,\n",
      "        <20681>,\n",
      "        <20682>,\n",
      "        <20683>,\n",
      "        <20684>,\n",
      "        <20685>,\n",
      "        <20686>,\n",
      "        <20687>,\n",
      "        <20688>,\n",
      "        <20689>,\n",
      "        <20690>,\n",
      "        <20691>,\n",
      "        <20692>,\n",
      "        <20693>,\n",
      "        <20694>,\n",
      "        <20695>,\n",
      "        <20696>,\n",
      "        <20697>,\n",
      "        <20698>,\n",
      "        <20699>,\n",
      "        <20700>,\n",
      "        <20701>,\n",
      "        <20702>,\n",
      "        <20703>,\n",
      "        <20704>,\n",
      "        <20705>,\n",
      "        <20706>,\n",
      "        <20707>,\n",
      "        <20708>,\n",
      "        <20709>,\n",
      "        <20710>,\n",
      "        <20711>,\n",
      "        <20712>,\n",
      "        <20713>,\n",
      "        <20714>,\n",
      "        <20715>,\n",
      "        <20716>,\n",
      "        <20717>,\n",
      "        <20718>,\n",
      "        <20719>,\n",
      "        <20720>,\n",
      "        <20721>,\n",
      "        <20722>,\n",
      "        <20723>,\n",
      "        <20724>,\n",
      "        <20725>,\n",
      "        <20726>,\n",
      "        <20727>,\n",
      "        <20728>,\n",
      "        <20729>,\n",
      "        <20730>,\n",
      "        <20731>,\n",
      "        <20732>,\n",
      "        <20733>,\n",
      "        <20734>,\n",
      "        <20735>,\n",
      "        <20736>,\n",
      "        <20737>,\n",
      "        <20738>,\n",
      "        <20739>,\n",
      "        <20740>,\n",
      "        <20741>,\n",
      "        <20742>,\n",
      "        <20743>,\n",
      "        <20744>,\n",
      "        <20745>,\n",
      "        <20746>,\n",
      "        <20747>,\n",
      "        <20748>,\n",
      "        <20749>,\n",
      "        <20750>,\n",
      "        <20751>,\n",
      "        <20752>,\n",
      "        <20753>,\n",
      "        <20754>,\n",
      "        <20755>,\n",
      "        <20756>,\n",
      "        <20757>,\n",
      "        <20758>,\n",
      "        <20759>,\n",
      "        <20760>,\n",
      "        <20761>,\n",
      "        <20762>,\n",
      "        <20763>,\n",
      "        <20764>,\n",
      "        <20765>,\n",
      "        <20766>,\n",
      "        <20767>,\n",
      "        <20768>,\n",
      "        <20769>,\n",
      "        <20770>,\n",
      "        <20771>,\n",
      "        <20772>,\n",
      "        <20773>,\n",
      "        <20774>,\n",
      "        <20775>,\n",
      "        <20776>,\n",
      "        <20777>,\n",
      "        <20778>,\n",
      "        <20779>,\n",
      "        <20780>,\n",
      "        <20781>,\n",
      "        <20782>,\n",
      "        <20783>,\n",
      "        <20784>,\n",
      "        <20785>,\n",
      "        <20786>,\n",
      "        <20787>,\n",
      "        <20788>,\n",
      "        <20789>,\n",
      "        <20790>,\n",
      "        <20791>,\n",
      "        <20792>,\n",
      "        <20793>,\n",
      "        <20794>,\n",
      "        <20795>,\n",
      "        <20796>,\n",
      "        <20797>,\n",
      "        <20798>,\n",
      "        <20799>,\n",
      "        <20800>,\n",
      "        <20801>,\n",
      "        <20802>,\n",
      "        <20803>,\n",
      "        <20804>,\n",
      "        <20805>,\n",
      "        <20806>,\n",
      "        <20807>,\n",
      "        <20808>,\n",
      "        <20809>,\n",
      "        <20810>,\n",
      "        <20811>,\n",
      "        <20812>,\n",
      "        <232>,\n",
      "        <246>,\n",
      "        <250>,\n",
      "        <252>,\n",
      "        <282>,\n",
      "        <324>,\n",
      "        <325>,\n",
      "        <326>,\n",
      "        <327>,\n",
      "        <328>,\n",
      "        <329>,\n",
      "        <330>,\n",
      "        <331>,\n",
      "        <332>,\n",
      "        <333>,\n",
      "        <334>,\n",
      "        <335>,\n",
      "        <336>,\n",
      "        <337>,\n",
      "        <338>,\n",
      "        <339>,\n",
      "        <340>,\n",
      "        <341>,\n",
      "        <342>,\n",
      "        <343>,\n",
      "        <344>,\n",
      "        <345>,\n",
      "        <346>,\n",
      "        <347>,\n",
      "        <348>,\n",
      "        <349>,\n",
      "        <350>,\n",
      "        <351>,\n",
      "        <352>,\n",
      "        <353>,\n",
      "        <354>,\n",
      "        <355>,\n",
      "        <356>,\n",
      "        <357>,\n",
      "        <358>,\n",
      "        <359>,\n",
      "        <360>,\n",
      "        <361>,\n",
      "        <362>,\n",
      "        <363>,\n",
      "        <364>,\n",
      "        <365>,\n",
      "        <366>,\n",
      "        <367>,\n",
      "        <368>,\n",
      "        <369>,\n",
      "        <370>,\n",
      "        <371>,\n",
      "        <372>,\n",
      "        <373>,\n",
      "        <374>,\n",
      "        <375>,\n",
      "        <376>,\n",
      "        <377>,\n",
      "        <378>,\n",
      "        <379>,\n",
      "        <380>,\n",
      "        <381>,\n",
      "        <382>,\n",
      "        <383>,\n",
      "        <384>,\n",
      "        <385>,\n",
      "        <386>,\n",
      "        <387>,\n",
      "        <388>,\n",
      "        <389>,\n",
      "        <390>,\n",
      "        <391>,\n",
      "        <392>,\n",
      "        <393>,\n",
      "        <394>,\n",
      "        <395>,\n",
      "        <396>,\n",
      "        <397>,\n",
      "        <398>,\n",
      "        <399>,\n",
      "        <400>,\n",
      "        <401>,\n",
      "        <402>,\n",
      "        <403>,\n",
      "        <404>,\n",
      "        <405>,\n",
      "        <406>,\n",
      "        <407>,\n",
      "        <408>,\n",
      "        <409>,\n",
      "        <410>,\n",
      "        <411>,\n",
      "        <412>,\n",
      "        <413>,\n",
      "        <414>,\n",
      "        <415>,\n",
      "        <416>,\n",
      "        <417>,\n",
      "        <418>,\n",
      "        <419>,\n",
      "        <420>,\n",
      "        <421>,\n",
      "        <422>,\n",
      "        <423>,\n",
      "        <424>,\n",
      "        <425>,\n",
      "        <426>,\n",
      "        <427>,\n",
      "        <428>,\n",
      "        <429>,\n",
      "        <430>,\n",
      "        <431>,\n",
      "        <432>,\n",
      "        <433>,\n",
      "        <434>,\n",
      "        <435>,\n",
      "        <436>,\n",
      "        <437>,\n",
      "        <438>,\n",
      "        <439>,\n",
      "        <440>,\n",
      "        <441>,\n",
      "        <442>,\n",
      "        <443>,\n",
      "        <444>,\n",
      "        <445>,\n",
      "        <446>,\n",
      "        <447>,\n",
      "        <448>,\n",
      "        <449>,\n",
      "        <450>,\n",
      "        <451>,\n",
      "        <452>,\n",
      "        <453>,\n",
      "        <454>,\n",
      "        <455>,\n",
      "        <456>,\n",
      "        <457>,\n",
      "        <458>,\n",
      "        <459>,\n",
      "        <460>,\n",
      "        <461>,\n",
      "        <462>,\n",
      "        <463>,\n",
      "        <464>,\n",
      "        <465>,\n",
      "        <466>,\n",
      "        <467>,\n",
      "        <468>,\n",
      "        <469>,\n",
      "        <470>,\n",
      "        <471>,\n",
      "        <472>,\n",
      "        <473>,\n",
      "        <474>,\n",
      "        <475>,\n",
      "        <476>,\n",
      "        <477>,\n",
      "        <478>,\n",
      "        <479>,\n",
      "        <480>,\n",
      "        <481>,\n",
      "        <482>,\n",
      "        <483>,\n",
      "        <484>,\n",
      "        <485>,\n",
      "        <486>,\n",
      "        <487>,\n",
      "        <488>,\n",
      "        <489>,\n",
      "        <490>,\n",
      "        <491>,\n",
      "        <492>,\n",
      "        <493>,\n",
      "        <494>,\n",
      "        <495>,\n",
      "        <496>,\n",
      "        <497>,\n",
      "        <498>,\n",
      "        <499>,\n",
      "        <500>,\n",
      "        <501>,\n",
      "        <502>,\n",
      "        <503>,\n",
      "        <504>,\n",
      "        <505>,\n",
      "        <506>,\n",
      "        <507>,\n",
      "        <508>,\n",
      "        <509>,\n",
      "        <510>,\n",
      "        <511>,\n",
      "        <512>,\n",
      "        <513>,\n",
      "        <514>,\n",
      "        <515>,\n",
      "        <516>,\n",
      "        <517>,\n",
      "        <518>,\n",
      "        <519>,\n",
      "        <520>,\n",
      "        <521>,\n",
      "        <522>,\n",
      "        <523>,\n",
      "        <524>,\n",
      "        <525>,\n",
      "        <526>,\n",
      "        <527>,\n",
      "        <528>,\n",
      "        <529>,\n",
      "        <530>,\n",
      "        <531>,\n",
      "        <532>,\n",
      "        <533>,\n",
      "        <534>,\n",
      "        <535>,\n",
      "        <536>,\n",
      "        <537>,\n",
      "        <538>,\n",
      "        <539>,\n",
      "        <540>,\n",
      "        <541>,\n",
      "        <542>,\n",
      "        <543>,\n",
      "        <544>,\n",
      "        <545>,\n",
      "        <546>,\n",
      "        <547>,\n",
      "        <548>,\n",
      "        <549>,\n",
      "        <550>,\n",
      "        <551>,\n",
      "        <552>,\n",
      "        <553>,\n",
      "        <554>,\n",
      "        <555>,\n",
      "        <556>,\n",
      "        <557>,\n",
      "        <558>,\n",
      "        <559>,\n",
      "        <560>,\n",
      "        <561>,\n",
      "        <562>,\n",
      "        <563>,\n",
      "        <74>,\n",
      "        <77>,\n",
      "        <87>,\n",
      "        <98> ;\n",
      "    schema:datePublished \"2011\"^^schema:DateTime ;\n",
      "    schema:headline \"Transforming Auto-Encoders\"^^schema:Text ;\n",
      "    schema:url \"https://www.semanticscholar.org/paper/20f0357688876fa4662f806f985779dce6e24f3c\"^^schema:URL .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-treatment",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1030,
   "id": "national-brake",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Proceedings of the IEEE International Conference on Computer Vision, 37-45'"
      ]
     },
     "execution_count": 1030,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[2]['a_journal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-sensitivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "if str(row['a_journal'])!=\"nan\":\n",
    "    journal = row['a_journal']\n",
    "\n",
    "    [g.add((node_uri, SCHEMA.genre, Literal(category, datatype=SCHEMA.Text))) for category in categories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "id": "prerequisite-princeton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10.1007/s41745-019-0099-3'"
      ]
     },
     "execution_count": 1013,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[347]['b_doi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1019,
   "id": "driven-somewhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in citations_1:\n",
    "    if (i['doi']==df.iloc[347]['b_doi']):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "id": "flexible-motivation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10.1007/s41745-019-0099-3'"
      ]
     },
     "execution_count": 1020,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i['doi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-polyester",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "id": "brazilian-boston",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20534"
      ]
     },
     "execution_count": 921,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "id": "english-mixture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10.1007/978-3-030-01246-5_18'"
      ]
     },
     "execution_count": 881,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citations_1[0]['doi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "treated-burton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        10.1007/978-3-642-21735-7_6\n",
       "1        10.1109/ACCESS.2020.2982782\n",
       "2               10.1109/ICCV.2015.13\n",
       "3            10.1109/CVPR.2018.00830\n",
       "4           10.1109/TIP.2020.3014608\n",
       "                    ...             \n",
       "24400                            NaN\n",
       "24401                            NaN\n",
       "24402                            NaN\n",
       "24403                            NaN\n",
       "24404                            NaN\n",
       "Name: b_doi, Length: 24405, dtype: object"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['b_doi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "amber-blanket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_index</th>\n",
       "      <th>a_ID</th>\n",
       "      <th>a_title</th>\n",
       "      <th>a_authors</th>\n",
       "      <th>a_published</th>\n",
       "      <th>a_updated</th>\n",
       "      <th>a_abstract</th>\n",
       "      <th>a_categories</th>\n",
       "      <th>a_citations</th>\n",
       "      <th>a_arxiv_url</th>\n",
       "      <th>...</th>\n",
       "      <th>b_corpusId</th>\n",
       "      <th>b_doi</th>\n",
       "      <th>b_influentialCitationCount</th>\n",
       "      <th>b_paperId</th>\n",
       "      <th>b_references</th>\n",
       "      <th>b_title</th>\n",
       "      <th>b_topics</th>\n",
       "      <th>b_url</th>\n",
       "      <th>b_year</th>\n",
       "      <th>b_dblp_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>7469546</td>\n",
       "      <td>10.1007/978-3-319-46475-6_1</td>\n",
       "      <td>3</td>\n",
       "      <td>d93ececdc44e7700cd84fc75ca069125022d8c9d</td>\n",
       "      <td>[{'arxivId': '1312.6114', 'authors': [{'author...</td>\n",
       "      <td>The Curious Robot: Learning Visual Representat...</td>\n",
       "      <td>[{'topic': 'Fundamental interaction', 'topicId...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/d93ececd...</td>\n",
       "      <td>2016</td>\n",
       "      <td>{'@score': '9', '@id': '1695087', 'info': {'au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11347</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>7469546</td>\n",
       "      <td>10.1007/978-3-319-46475-6_1</td>\n",
       "      <td>3</td>\n",
       "      <td>d93ececdc44e7700cd84fc75ca069125022d8c9d</td>\n",
       "      <td>[{'arxivId': '1312.6114', 'authors': [{'author...</td>\n",
       "      <td>The Curious Robot: Learning Visual Representat...</td>\n",
       "      <td>[{'topic': 'Fundamental interaction', 'topicId...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/d93ececd...</td>\n",
       "      <td>2016</td>\n",
       "      <td>{'@score': '9', '@id': '1696642', 'info': {'au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18299</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>7469546</td>\n",
       "      <td>10.1007/978-3-319-46475-6_1</td>\n",
       "      <td>3</td>\n",
       "      <td>d93ececdc44e7700cd84fc75ca069125022d8c9d</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '1...</td>\n",
       "      <td>The Curious Robot: Learning Visual Representat...</td>\n",
       "      <td>[{'topic': 'Fundamental interaction', 'topicId...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/d93ececd...</td>\n",
       "      <td>2016</td>\n",
       "      <td>{'@score': '9', '@id': '1696642', 'info': {'au...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      a_index a_ID a_title a_authors a_published a_updated a_abstract  \\\n",
       "526       NaN  NaN     NaN       NaN         NaN       NaN        NaN   \n",
       "11347     NaN  NaN     NaN       NaN         NaN       NaN        NaN   \n",
       "18299     NaN  NaN     NaN       NaN         NaN       NaN        NaN   \n",
       "\n",
       "      a_categories a_citations a_arxiv_url  ... b_corpusId  \\\n",
       "526            NaN         NaN         NaN  ...    7469546   \n",
       "11347          NaN         NaN         NaN  ...    7469546   \n",
       "18299          NaN         NaN         NaN  ...    7469546   \n",
       "\n",
       "                             b_doi b_influentialCitationCount  \\\n",
       "526    10.1007/978-3-319-46475-6_1                          3   \n",
       "11347  10.1007/978-3-319-46475-6_1                          3   \n",
       "18299  10.1007/978-3-319-46475-6_1                          3   \n",
       "\n",
       "                                      b_paperId  \\\n",
       "526    d93ececdc44e7700cd84fc75ca069125022d8c9d   \n",
       "11347  d93ececdc44e7700cd84fc75ca069125022d8c9d   \n",
       "18299  d93ececdc44e7700cd84fc75ca069125022d8c9d   \n",
       "\n",
       "                                            b_references  \\\n",
       "526    [{'arxivId': '1312.6114', 'authors': [{'author...   \n",
       "11347  [{'arxivId': '1312.6114', 'authors': [{'author...   \n",
       "18299  [{'arxivId': None, 'authors': [{'authorId': '1...   \n",
       "\n",
       "                                                 b_title  \\\n",
       "526    The Curious Robot: Learning Visual Representat...   \n",
       "11347  The Curious Robot: Learning Visual Representat...   \n",
       "18299  The Curious Robot: Learning Visual Representat...   \n",
       "\n",
       "                                                b_topics  \\\n",
       "526    [{'topic': 'Fundamental interaction', 'topicId...   \n",
       "11347  [{'topic': 'Fundamental interaction', 'topicId...   \n",
       "18299  [{'topic': 'Fundamental interaction', 'topicId...   \n",
       "\n",
       "                                                   b_url b_year  \\\n",
       "526    https://www.semanticscholar.org/paper/d93ececd...   2016   \n",
       "11347  https://www.semanticscholar.org/paper/d93ececd...   2016   \n",
       "18299  https://www.semanticscholar.org/paper/d93ececd...   2016   \n",
       "\n",
       "                                             b_dblp_info  \n",
       "526    {'@score': '9', '@id': '1695087', 'info': {'au...  \n",
       "11347  {'@score': '9', '@id': '1696642', 'info': {'au...  \n",
       "18299  {'@score': '9', '@id': '1696642', 'info': {'au...  \n",
       "\n",
       "[3 rows x 28 columns]"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['b_doi']==citations_2[99]['citing'].split(\" => \")[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "chief-warrant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2281"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df['b_doi'].value_counts()>1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "actual-patrol",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot mask with non-boolean array containing NA / NaN values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-601-34679c37ff52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b_doi'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/finsent/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2895\u001b[0m         \u001b[0;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2896\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2897\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/finsent/lib/python3.6/site-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mis_bool_indexer\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0mna_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Cannot mask with non-boolean array containing NA / NaN values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mna_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot mask with non-boolean array containing NA / NaN values"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "informal-zoning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_index</th>\n",
       "      <th>a_ID</th>\n",
       "      <th>a_title</th>\n",
       "      <th>a_authors</th>\n",
       "      <th>a_published</th>\n",
       "      <th>a_updated</th>\n",
       "      <th>a_abstract</th>\n",
       "      <th>a_categories</th>\n",
       "      <th>a_citations</th>\n",
       "      <th>a_arxiv_url</th>\n",
       "      <th>...</th>\n",
       "      <th>b_corpusId</th>\n",
       "      <th>b_doi</th>\n",
       "      <th>b_influentialCitationCount</th>\n",
       "      <th>b_paperId</th>\n",
       "      <th>b_references</th>\n",
       "      <th>b_title</th>\n",
       "      <th>b_topics</th>\n",
       "      <th>b_url</th>\n",
       "      <th>b_year</th>\n",
       "      <th>b_dblp_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020</td>\n",
       "      <td>{'@score': '11', '@id': '793611', 'info': {'au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020</td>\n",
       "      <td>{'@score': '11', '@id': '793611', 'info': {'au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11360</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020</td>\n",
       "      <td>{'@score': '11', '@id': '795166', 'info': {'au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13358</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020</td>\n",
       "      <td>{'@score': '11', '@id': '795166', 'info': {'au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13388</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020</td>\n",
       "      <td>{'@score': '11', '@id': '795166', 'info': {'au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13558</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020</td>\n",
       "      <td>{'@score': '11', '@id': '795166', 'info': {'au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13761</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020</td>\n",
       "      <td>{'@score': '11', '@id': '795166', 'info': {'au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13973</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020</td>\n",
       "      <td>{'@score': '11', '@id': '795166', 'info': {'au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14571</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020</td>\n",
       "      <td>{'@score': '11', '@id': '795166', 'info': {'au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14907</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020</td>\n",
       "      <td>{'@score': '11', '@id': '795166', 'info': {'au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15021</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020</td>\n",
       "      <td>{'@score': '11', '@id': '795166', 'info': {'au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15041</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020</td>\n",
       "      <td>{'@score': '11', '@id': '795166', 'info': {'au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15345</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020</td>\n",
       "      <td>{'@score': '11', '@id': '795166', 'info': {'au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15442</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020</td>\n",
       "      <td>{'@score': '11', '@id': '795166', 'info': {'au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15493</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020</td>\n",
       "      <td>{'@score': '11', '@id': '795166', 'info': {'au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16772</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020</td>\n",
       "      <td>{'@score': '11', '@id': '795166', 'info': {'au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18008</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020</td>\n",
       "      <td>{'@score': '11', '@id': '795166', 'info': {'au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18535</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020</td>\n",
       "      <td>{'@score': '11', '@id': '795166', 'info': {'au...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      a_index a_ID a_title a_authors a_published a_updated a_abstract  \\\n",
       "414       NaN  NaN     NaN       NaN         NaN       NaN        NaN   \n",
       "724       NaN  NaN     NaN       NaN         NaN       NaN        NaN   \n",
       "11360     NaN  NaN     NaN       NaN         NaN       NaN        NaN   \n",
       "13358     NaN  NaN     NaN       NaN         NaN       NaN        NaN   \n",
       "13388     NaN  NaN     NaN       NaN         NaN       NaN        NaN   \n",
       "13558     NaN  NaN     NaN       NaN         NaN       NaN        NaN   \n",
       "13761     NaN  NaN     NaN       NaN         NaN       NaN        NaN   \n",
       "13973     NaN  NaN     NaN       NaN         NaN       NaN        NaN   \n",
       "14571     NaN  NaN     NaN       NaN         NaN       NaN        NaN   \n",
       "14907     NaN  NaN     NaN       NaN         NaN       NaN        NaN   \n",
       "15021     NaN  NaN     NaN       NaN         NaN       NaN        NaN   \n",
       "15041     NaN  NaN     NaN       NaN         NaN       NaN        NaN   \n",
       "15345     NaN  NaN     NaN       NaN         NaN       NaN        NaN   \n",
       "15442     NaN  NaN     NaN       NaN         NaN       NaN        NaN   \n",
       "15493     NaN  NaN     NaN       NaN         NaN       NaN        NaN   \n",
       "16772     NaN  NaN     NaN       NaN         NaN       NaN        NaN   \n",
       "18008     NaN  NaN     NaN       NaN         NaN       NaN        NaN   \n",
       "18535     NaN  NaN     NaN       NaN         NaN       NaN        NaN   \n",
       "\n",
       "      a_categories a_citations a_arxiv_url  ... b_corpusId  \\\n",
       "414            NaN         NaN         NaN  ...   62841734   \n",
       "724            NaN         NaN         NaN  ...   62841734   \n",
       "11360          NaN         NaN         NaN  ...   62841734   \n",
       "13358          NaN         NaN         NaN  ...   62841734   \n",
       "13388          NaN         NaN         NaN  ...   62841734   \n",
       "13558          NaN         NaN         NaN  ...   62841734   \n",
       "13761          NaN         NaN         NaN  ...   62841734   \n",
       "13973          NaN         NaN         NaN  ...   62841734   \n",
       "14571          NaN         NaN         NaN  ...   62841734   \n",
       "14907          NaN         NaN         NaN  ...   62841734   \n",
       "15021          NaN         NaN         NaN  ...   62841734   \n",
       "15041          NaN         NaN         NaN  ...   62841734   \n",
       "15345          NaN         NaN         NaN  ...   62841734   \n",
       "15442          NaN         NaN         NaN  ...   62841734   \n",
       "15493          NaN         NaN         NaN  ...   62841734   \n",
       "16772          NaN         NaN         NaN  ...   62841734   \n",
       "18008          NaN         NaN         NaN  ...   62841734   \n",
       "18535          NaN         NaN         NaN  ...   62841734   \n",
       "\n",
       "                            b_doi b_influentialCitationCount  \\\n",
       "414    10.1109/tpami.2020.2992393                         11   \n",
       "724    10.1109/tpami.2020.2992393                         11   \n",
       "11360  10.1109/tpami.2020.2992393                         11   \n",
       "13358  10.1109/tpami.2020.2992393                         11   \n",
       "13388  10.1109/tpami.2020.2992393                         11   \n",
       "13558  10.1109/tpami.2020.2992393                         11   \n",
       "13761  10.1109/tpami.2020.2992393                         11   \n",
       "13973  10.1109/tpami.2020.2992393                         11   \n",
       "14571  10.1109/tpami.2020.2992393                         11   \n",
       "14907  10.1109/tpami.2020.2992393                         11   \n",
       "15021  10.1109/tpami.2020.2992393                         11   \n",
       "15041  10.1109/tpami.2020.2992393                         11   \n",
       "15345  10.1109/tpami.2020.2992393                         11   \n",
       "15442  10.1109/tpami.2020.2992393                         11   \n",
       "15493  10.1109/tpami.2020.2992393                         11   \n",
       "16772  10.1109/tpami.2020.2992393                         11   \n",
       "18008  10.1109/tpami.2020.2992393                         11   \n",
       "18535  10.1109/tpami.2020.2992393                         11   \n",
       "\n",
       "                                      b_paperId  \\\n",
       "414    4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "724    4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "11360  4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "13358  4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "13388  4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "13558  4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "13761  4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "13973  4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "14571  4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "14907  4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "15021  4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "15041  4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "15345  4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "15442  4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "15493  4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "16772  4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "18008  4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "18535  4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "\n",
       "                                            b_references  \\\n",
       "414    [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "724    [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "11360  [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "13358  [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "13388  [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "13558  [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "13761  [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "13973  [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "14571  [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "14907  [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "15021  [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "15041  [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "15345  [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "15442  [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "15493  [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "16772  [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "18008  [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "18535  [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "\n",
       "                                                 b_title  \\\n",
       "414    Self-supervised Visual Feature Learning with D...   \n",
       "724    Self-supervised Visual Feature Learning with D...   \n",
       "11360  Self-supervised Visual Feature Learning with D...   \n",
       "13358  Self-supervised Visual Feature Learning with D...   \n",
       "13388  Self-supervised Visual Feature Learning with D...   \n",
       "13558  Self-supervised Visual Feature Learning with D...   \n",
       "13761  Self-supervised Visual Feature Learning with D...   \n",
       "13973  Self-supervised Visual Feature Learning with D...   \n",
       "14571  Self-supervised Visual Feature Learning with D...   \n",
       "14907  Self-supervised Visual Feature Learning with D...   \n",
       "15021  Self-supervised Visual Feature Learning with D...   \n",
       "15041  Self-supervised Visual Feature Learning with D...   \n",
       "15345  Self-supervised Visual Feature Learning with D...   \n",
       "15442  Self-supervised Visual Feature Learning with D...   \n",
       "15493  Self-supervised Visual Feature Learning with D...   \n",
       "16772  Self-supervised Visual Feature Learning with D...   \n",
       "18008  Self-supervised Visual Feature Learning with D...   \n",
       "18535  Self-supervised Visual Feature Learning with D...   \n",
       "\n",
       "                                                b_topics  \\\n",
       "414    [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "724    [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "11360  [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "13358  [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "13388  [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "13558  [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "13761  [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "13973  [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "14571  [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "14907  [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "15021  [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "15041  [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "15345  [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "15442  [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "15493  [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "16772  [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "18008  [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "18535  [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "\n",
       "                                                   b_url b_year  \\\n",
       "414    https://www.semanticscholar.org/paper/4c94ee7d...   2020   \n",
       "724    https://www.semanticscholar.org/paper/4c94ee7d...   2020   \n",
       "11360  https://www.semanticscholar.org/paper/4c94ee7d...   2020   \n",
       "13358  https://www.semanticscholar.org/paper/4c94ee7d...   2020   \n",
       "13388  https://www.semanticscholar.org/paper/4c94ee7d...   2020   \n",
       "13558  https://www.semanticscholar.org/paper/4c94ee7d...   2020   \n",
       "13761  https://www.semanticscholar.org/paper/4c94ee7d...   2020   \n",
       "13973  https://www.semanticscholar.org/paper/4c94ee7d...   2020   \n",
       "14571  https://www.semanticscholar.org/paper/4c94ee7d...   2020   \n",
       "14907  https://www.semanticscholar.org/paper/4c94ee7d...   2020   \n",
       "15021  https://www.semanticscholar.org/paper/4c94ee7d...   2020   \n",
       "15041  https://www.semanticscholar.org/paper/4c94ee7d...   2020   \n",
       "15345  https://www.semanticscholar.org/paper/4c94ee7d...   2020   \n",
       "15442  https://www.semanticscholar.org/paper/4c94ee7d...   2020   \n",
       "15493  https://www.semanticscholar.org/paper/4c94ee7d...   2020   \n",
       "16772  https://www.semanticscholar.org/paper/4c94ee7d...   2020   \n",
       "18008  https://www.semanticscholar.org/paper/4c94ee7d...   2020   \n",
       "18535  https://www.semanticscholar.org/paper/4c94ee7d...   2020   \n",
       "\n",
       "                                             b_dblp_info  \n",
       "414    {'@score': '11', '@id': '793611', 'info': {'au...  \n",
       "724    {'@score': '11', '@id': '793611', 'info': {'au...  \n",
       "11360  {'@score': '11', '@id': '795166', 'info': {'au...  \n",
       "13358  {'@score': '11', '@id': '795166', 'info': {'au...  \n",
       "13388  {'@score': '11', '@id': '795166', 'info': {'au...  \n",
       "13558  {'@score': '11', '@id': '795166', 'info': {'au...  \n",
       "13761  {'@score': '11', '@id': '795166', 'info': {'au...  \n",
       "13973  {'@score': '11', '@id': '795166', 'info': {'au...  \n",
       "14571  {'@score': '11', '@id': '795166', 'info': {'au...  \n",
       "14907  {'@score': '11', '@id': '795166', 'info': {'au...  \n",
       "15021  {'@score': '11', '@id': '795166', 'info': {'au...  \n",
       "15041  {'@score': '11', '@id': '795166', 'info': {'au...  \n",
       "15345  {'@score': '11', '@id': '795166', 'info': {'au...  \n",
       "15442  {'@score': '11', '@id': '795166', 'info': {'au...  \n",
       "15493  {'@score': '11', '@id': '795166', 'info': {'au...  \n",
       "16772  {'@score': '11', '@id': '795166', 'info': {'au...  \n",
       "18008  {'@score': '11', '@id': '795166', 'info': {'au...  \n",
       "18535  {'@score': '11', '@id': '795166', 'info': {'au...  \n",
       "\n",
       "[18 rows x 28 columns]"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-yellow",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "adjusted-albania",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot convert the series to <class 'float'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-595-b91e63037fd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b_doi'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/finsent/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"cannot convert the series to {converter}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"__{converter.__name__}__\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot convert the series to <class 'float'>"
     ]
    }
   ],
   "source": [
    "doi_ls = df['b_doi'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "id": "surrounded-neighbor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24405"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['b_doi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "selected-shuttle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    references_1 = ast.literal_eval(df.iloc[ic]['b_references'])\n",
    "except:\n",
    "    references_1 = []\n",
    "try:\n",
    "    references_2 = df.iloc[ic]['b_dblp_info']['references_crossref']\n",
    "except:\n",
    "    references_2 = []\n",
    "print(len(references_1))\n",
    "print(len(references_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "contemporary-providence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "39\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "yellow-pipeline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a_ID',\n",
       " 'a_abstract',\n",
       " 'b_abstract',\n",
       " 'b_arxivId',\n",
       " 'a_authors',\n",
       " 'b_authors',\n",
       " 'a_categories',\n",
       " 'b_citationVelocity',\n",
       " 'a_citations',\n",
       " 'b_citations',\n",
       " 'b_corpusId',\n",
       " 'b_doi',\n",
       " 'a_index',\n",
       " 'b_index',\n",
       " 'b_influentialCitationCount',\n",
       " 'b_dblp_info',\n",
       " 'a_journal',\n",
       " 'b_paperId',\n",
       " 'a_published',\n",
       " 'b_references',\n",
       " 'a_title',\n",
       " 'b_title',\n",
       " 'b_topics',\n",
       " 'a_updated',\n",
       " 'a_arxiv_url',\n",
       " 'a_gscholar_url',\n",
       " 'b_url',\n",
       " 'b_year']"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(df.keys()),key=lambda s:s.split(\"_\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "auburn-traveler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@prefix schema: <https://schema.org/> .\n",
      "\n",
      "<0> a schema:ScholarlyArticle ;\n",
      "    schema:abstract \"The artificial neural networks that are used to recognize shapes typically use one or more layers of learned feature detectors that produce scalar outputs. By contrast, the computer vision community uses complicated, hand-engineered features, like SIFT [6], that produce a whole vector of outputs including an explicit representation of the pose of the feature. We show how neural networks can be used to learn features that output a whole vector of instantiation parameters and we argue that this is a much more promising way of dealing with variations in position, orientation, scale and lighting than the methods currently employed in the neural networks community. It is also more promising than the hand-engineered features currently used in computer vision because it provides an efficient way of adapting the features to the domain.\"^^schema:Text ;\n",
      "    schema:author \"A. Krizhevsky\"^^schema:Person,\n",
      "        \"Geoffrey E. Hinton\"^^schema:Person,\n",
      "        \"S. Wang\"^^schema:Person ;\n",
      "    schema:datePublished \"2011\"^^schema:DateTime ;\n",
      "    schema:headline \"Transforming Auto-Encoders\"^^schema:Text .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(g.serialize(format='turtle').decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-holly",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "formed-arbor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdflib.term.URIRef('0')"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "### Author(s) ###\n",
    "try:\n",
    "    author_arxiv = df_arxiv[df_arxiv['ID'] == str(row['ARXIV_ID'])]['authors'].values[0]\n",
    "    author_arxiv = [name.strip() for name in author_arxiv if name != '<___>']\n",
    "except:\n",
    "    author_arxiv = '<___>'\n",
    "try:\n",
    "    author_gscholar = df_gscholar[df_gscholar['ID'] == str(row['GSCHOLAR_ID'])]['authors'].values[0]\n",
    "    author_gscholar = [name.strip() for name in author_gscholar if name != '<___>']\n",
    "except:\n",
    "    author_gscholar = '<___>'\n",
    "if author_arxiv != '<___>':\n",
    "    authors = list(set(author_arxiv))\n",
    "else:\n",
    "    authors = list(set(author_gscholar))           \n",
    "[g.add((node_uri, SCHEMA.author, Literal(author, datatype=SCHEMA.Person))) for author in authors]\n",
    "df_merged.at[NEW_ID, 'authors'] = authors\n",
    "json_merged[NEW_ID]['authors'] = authors\n",
    "\n",
    "### Published ###\n",
    "try:\n",
    "    published = str(df_arxiv[df_arxiv['ID'] == str(row['ARXIV_ID'])]['published'].values[0])\n",
    "    g.add((node_uri, SCHEMA.datePublished, Literal(published, datatype=SCHEMA.DateTime)))\n",
    "    df_merged.at[NEW_ID, 'published'] = published\n",
    "    json_merged[NEW_ID]['published'] = published\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "### Updated ###\n",
    "try:\n",
    "    updated = str(df_arxiv[df_arxiv['ID'] == str(row['ARXIV_ID'])]['updated'].values[0])\n",
    "    g.add((node_uri, SCHEMA.dateModified, Literal(updated, datatype=SCHEMA.DateTime)))\n",
    "    df_merged.at[NEW_ID, 'updated'] = updated\n",
    "    json_merged[NEW_ID]['updated'] = updated\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "### Abstract ###\n",
    "try:\n",
    "    abstract = str(df_arxiv[df_arxiv['ID'] == str(row['ARXIV_ID'])]['summary'].values[0]).strip()\n",
    "    g.add((node_uri, SCHEMA.abstract, Literal(abstract, datatype=SCHEMA.Text)))\n",
    "    df_merged.at[NEW_ID, 'abstract'] = abstract\n",
    "    json_merged[NEW_ID]['abstract'] = abstract\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "### Categories ###\n",
    "try:\n",
    "    categories = df_arxiv[df_arxiv['ID'] == str(row['ARXIV_ID'])]['categories'].values[0]\n",
    "    categories = [name.strip() for name in categories if name != '<___>']\n",
    "    [g.add((node_uri, SCHEMA.genre, Literal(category, datatype=SCHEMA.Text))) for category in categories]\n",
    "    df_merged.at[NEW_ID, 'categories'] = categories\n",
    "    json_merged[NEW_ID]['categories'] = categories\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "### Journal ###\n",
    "try:\n",
    "    journal = str(df_gscholar[df_gscholar['ID'] == str(row['GSCHOLAR_ID'])]['journal'].values[0])\n",
    "    g.add((node_uri, SCHEMA.publisher, Literal(journal, datatype=SCHEMA.Periodical))) #datatype=SCHEMA.Organisation\n",
    "    df_merged.at[NEW_ID, 'journal'] = journal\n",
    "    json_merged[NEW_ID]['journal'] = journal\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "### Citations ###\n",
    "try:\n",
    "    citations = str(df_gscholar[df_gscholar['ID'] == str(row['GSCHOLAR_ID'])]['citations'].values[0])\n",
    "    g.add((node_uri, SCHEMA.commentCount, Literal(citations, datatype=SCHEMA.Integer)))\n",
    "    df_merged.at[NEW_ID, 'citations'] = citations\n",
    "    json_merged[NEW_ID]['citations'] = citations\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "### Arxiv URL ###\n",
    "try:\n",
    "    arxiv_url = str(df_arxiv[df_arxiv['ID'] == str(row['ARXIV_ID'])]['url'].values[0])\n",
    "    g.add((node_uri, SCHEMA.url, Literal(arxiv_url, datatype=SCHEMA.URL)))\n",
    "    df_merged.at[NEW_ID, 'arxiv_url'] = arxiv_url    \n",
    "    json_merged[NEW_ID]['arxiv_url'] = arxiv_url\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "### Google Scholar URL ###\n",
    "try:\n",
    "    gscholar_url = str(df_gscholar[df_gscholar['ID'] == str(row['GSCHOLAR_ID'])]['url'].values[0])\n",
    "    g.add((node_uri, SCHEMA.url, Literal(gscholar_url, datatype=SCHEMA.URL)))\n",
    "    df_merged.at[NEW_ID, 'gscholar_url'] = gscholar_url   \n",
    "    json_merged[NEW_ID]['gscholar_url'] = gscholar_url\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "NEW_ID += 1\n",
    "\n",
    "# Save to disk using turtle format\n",
    "g.serialize(f'Triples_{TOPIC}.ttl.', format=\"turtle\")\n",
    "\n",
    "# And save the merged DataFrame as CSV\n",
    "df_merged.to_csv(f'Merged_{TOPIC}.csv', index=False)\n",
    "\n",
    "# Also save as Json, just because\n",
    "with open(f'Json_{TOPIC}.json', 'w') as fout:\n",
    "json.dump(json_merged, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "inappropriate-brunei",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a_ID',\n",
       " 'a_abstract',\n",
       " 'b_abstract',\n",
       " 'b_arxivId',\n",
       " 'a_authors',\n",
       " 'b_authors',\n",
       " 'a_categories',\n",
       " 'b_citationVelocity',\n",
       " 'a_citations',\n",
       " 'b_citations',\n",
       " 'b_corpusId',\n",
       " 'b_doi',\n",
       " 'a_index',\n",
       " 'b_index',\n",
       " 'b_influentialCitationCount',\n",
       " 'b_dblp_info',\n",
       " 'a_journal',\n",
       " 'b_paperId',\n",
       " 'a_published',\n",
       " 'b_references',\n",
       " 'a_title',\n",
       " 'b_title',\n",
       " 'b_topics',\n",
       " 'a_updated',\n",
       " 'a_arxiv_url',\n",
       " 'a_gscholar_url',\n",
       " 'b_url',\n",
       " 'b_year']"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "shared-marker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.isnan(row['a_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "upper-librarian",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'isnan'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-417-340357fdbd0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a_index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'isnan'"
     ]
    }
   ],
   "source": [
    ".isnan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "adjacent-finance",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.add((node_uri, RDF.type, SCHEMA.ScholarlyArticle))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "former-ethiopia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N0fb1bac5551443b6b8085f491c0eb81f (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-connecticut",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-conditioning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe to store the merged datasets\n",
    "df_merged = pd.DataFrame(columns = ['ID', 'title', 'authors', 'published', 'updated', \n",
    "                                    'abstract', 'categories', 'citations', 'arxiv_url', 'gscholar_url'],\n",
    "                         dtype='object')\n",
    "json_merged = {}\n",
    "\n",
    "NEW_ID = 0\n",
    "\n",
    "# Populate the RDF with predictions with a positive (1) label\n",
    "for idx, row in tqdm(predictions_df.iterrows(), total=predictions_df.shape[0]):\n",
    "    \n",
    "    # Populate the json object\n",
    "    json_merged[NEW_ID] = {}\n",
    "    \n",
    "    ### URI ###\n",
    "    node_uri = URIRef(str(NEW_ID))\n",
    "    g.add((node_uri, RDF.type, SCHEMA.ScholarlyArticle))\n",
    "    df_merged.at[NEW_ID, 'ID'] = NEW_ID\n",
    "    json_merged[NEW_ID]['ID'] = NEW_ID\n",
    "\n",
    "    \n",
    "    ### Title ###\n",
    "    try:\n",
    "        title_arxiv = str(df_arxiv[df_arxiv['ID'] == str(row['ARXIV_ID'])]['title'].values[0])\n",
    "    except:\n",
    "        title_arxiv = '<___>'\n",
    "    try:\n",
    "        title_gscholar = str(df_gscholar[df_gscholar['ID'] == str(row['GSCHOLAR_ID'])]['title'].values[0])\n",
    "    except:\n",
    "        title_gscholar = '<___>'\n",
    "    title = title_arxiv if title_arxiv != '<___>' else title_gscholar if title_gscholar != '<___>' else None\n",
    "    g.add((node_uri, SCHEMA.headline, Literal(title, datatype=SCHEMA.Text)))\n",
    "    df_merged.at[NEW_ID, 'title'] = title\n",
    "    json_merged[NEW_ID]['title'] = title\n",
    "\n",
    "    \n",
    "    ### Author(s) ###\n",
    "    try:\n",
    "        author_arxiv = df_arxiv[df_arxiv['ID'] == str(row['ARXIV_ID'])]['authors'].values[0]\n",
    "        author_arxiv = [name.strip() for name in author_arxiv if name != '<___>']\n",
    "    except:\n",
    "        author_arxiv = '<___>'\n",
    "    try:\n",
    "        author_gscholar = df_gscholar[df_gscholar['ID'] == str(row['GSCHOLAR_ID'])]['authors'].values[0]\n",
    "        author_gscholar = [name.strip() for name in author_gscholar if name != '<___>']\n",
    "    except:\n",
    "        author_gscholar = '<___>'\n",
    "    if author_arxiv != '<___>':\n",
    "        authors = list(set(author_arxiv))\n",
    "    else:\n",
    "        authors = list(set(author_gscholar))           \n",
    "    [g.add((node_uri, SCHEMA.author, Literal(author, datatype=SCHEMA.Person))) for author in authors]\n",
    "    df_merged.at[NEW_ID, 'authors'] = authors\n",
    "    json_merged[NEW_ID]['authors'] = authors\n",
    "                       \n",
    "    ### Published ###\n",
    "    try:\n",
    "        published = str(df_arxiv[df_arxiv['ID'] == str(row['ARXIV_ID'])]['published'].values[0])\n",
    "        g.add((node_uri, SCHEMA.datePublished, Literal(published, datatype=SCHEMA.DateTime)))\n",
    "        df_merged.at[NEW_ID, 'published'] = published\n",
    "        json_merged[NEW_ID]['published'] = published\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "                       \n",
    "    ### Updated ###\n",
    "    try:\n",
    "        updated = str(df_arxiv[df_arxiv['ID'] == str(row['ARXIV_ID'])]['updated'].values[0])\n",
    "        g.add((node_uri, SCHEMA.dateModified, Literal(updated, datatype=SCHEMA.DateTime)))\n",
    "        df_merged.at[NEW_ID, 'updated'] = updated\n",
    "        json_merged[NEW_ID]['updated'] = updated\n",
    "    except:\n",
    "        pass\n",
    "          \n",
    "                       \n",
    "    ### Abstract ###\n",
    "    try:\n",
    "        abstract = str(df_arxiv[df_arxiv['ID'] == str(row['ARXIV_ID'])]['summary'].values[0]).strip()\n",
    "        g.add((node_uri, SCHEMA.abstract, Literal(abstract, datatype=SCHEMA.Text)))\n",
    "        df_merged.at[NEW_ID, 'abstract'] = abstract\n",
    "        json_merged[NEW_ID]['abstract'] = abstract\n",
    "    except:\n",
    "        pass\n",
    "       \n",
    "                       \n",
    "    ### Categories ###\n",
    "    try:\n",
    "        categories = df_arxiv[df_arxiv['ID'] == str(row['ARXIV_ID'])]['categories'].values[0]\n",
    "        categories = [name.strip() for name in categories if name != '<___>']\n",
    "        [g.add((node_uri, SCHEMA.genre, Literal(category, datatype=SCHEMA.Text))) for category in categories]\n",
    "        df_merged.at[NEW_ID, 'categories'] = categories\n",
    "        json_merged[NEW_ID]['categories'] = categories\n",
    "    except:\n",
    "        pass\n",
    "          \n",
    "                       \n",
    "    ### Journal ###\n",
    "    try:\n",
    "        journal = str(df_gscholar[df_gscholar['ID'] == str(row['GSCHOLAR_ID'])]['journal'].values[0])\n",
    "        g.add((node_uri, SCHEMA.publisher, Literal(journal, datatype=SCHEMA.Periodical))) #datatype=SCHEMA.Organisation\n",
    "        df_merged.at[NEW_ID, 'journal'] = journal\n",
    "        json_merged[NEW_ID]['journal'] = journal\n",
    "    except:\n",
    "        pass\n",
    "     \n",
    "                       \n",
    "    ### Citations ###\n",
    "    try:\n",
    "        citations = str(df_gscholar[df_gscholar['ID'] == str(row['GSCHOLAR_ID'])]['citations'].values[0])\n",
    "        g.add((node_uri, SCHEMA.commentCount, Literal(citations, datatype=SCHEMA.Integer)))\n",
    "        df_merged.at[NEW_ID, 'citations'] = citations\n",
    "        json_merged[NEW_ID]['citations'] = citations\n",
    "    except:\n",
    "        pass\n",
    "            \n",
    "                       \n",
    "    ### Arxiv URL ###\n",
    "    try:\n",
    "        arxiv_url = str(df_arxiv[df_arxiv['ID'] == str(row['ARXIV_ID'])]['url'].values[0])\n",
    "        g.add((node_uri, SCHEMA.url, Literal(arxiv_url, datatype=SCHEMA.URL)))\n",
    "        df_merged.at[NEW_ID, 'arxiv_url'] = arxiv_url    \n",
    "        json_merged[NEW_ID]['arxiv_url'] = arxiv_url\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "                       \n",
    "    ### Google Scholar URL ###\n",
    "    try:\n",
    "        gscholar_url = str(df_gscholar[df_gscholar['ID'] == str(row['GSCHOLAR_ID'])]['url'].values[0])\n",
    "        g.add((node_uri, SCHEMA.url, Literal(gscholar_url, datatype=SCHEMA.URL)))\n",
    "        df_merged.at[NEW_ID, 'gscholar_url'] = gscholar_url   \n",
    "        json_merged[NEW_ID]['gscholar_url'] = gscholar_url\n",
    "    except:\n",
    "        pass\n",
    "             \n",
    "                       \n",
    "    NEW_ID += 1\n",
    "    \n",
    "# Save to disk using turtle format\n",
    "g.serialize(f'Triples_{TOPIC}.ttl.', format=\"turtle\")\n",
    "\n",
    "# And save the merged DataFrame as CSV\n",
    "df_merged.to_csv(f'Merged_{TOPIC}.csv', index=False)\n",
    "\n",
    "# Also save as Json, just because\n",
    "with open(f'Json_{TOPIC}.json', 'w') as fout:\n",
    "    json.dump(json_merged, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-recommendation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-scenario",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-birth",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-bundle",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-vietnam",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "cheap-young",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20534"
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(df['b_doi']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "infectious-technique",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../data/semantic_scholar_articles_20000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-damage",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(test_df['doi']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "enhanced-hotel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19660"
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "temporal-retro",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>arxivId</th>\n",
       "      <th>authors</th>\n",
       "      <th>citationVelocity</th>\n",
       "      <th>citations</th>\n",
       "      <th>corpusId</th>\n",
       "      <th>doi</th>\n",
       "      <th>influentialCitationCount</th>\n",
       "      <th>paperId</th>\n",
       "      <th>references</th>\n",
       "      <th>title</th>\n",
       "      <th>topics</th>\n",
       "      <th>url</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>Large-scale labeled data are generally require...</td>\n",
       "      <td>1902.06162</td>\n",
       "      <td>[{'authorId': '29724362', 'name': 'Longlong Ji...</td>\n",
       "      <td>61</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '4...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>Large-scale labeled data are generally require...</td>\n",
       "      <td>1902.06162</td>\n",
       "      <td>[{'authorId': '29724362', 'name': 'Longlong Ji...</td>\n",
       "      <td>61</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '4...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11360</th>\n",
       "      <td>Large-scale labeled data are generally require...</td>\n",
       "      <td>1902.06162</td>\n",
       "      <td>[{'authorId': '29724362', 'name': 'Longlong Ji...</td>\n",
       "      <td>61</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '4...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13358</th>\n",
       "      <td>Large-scale labeled data are generally require...</td>\n",
       "      <td>1902.06162</td>\n",
       "      <td>[{'authorId': '29724362', 'name': 'Longlong Ji...</td>\n",
       "      <td>61</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '4...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13388</th>\n",
       "      <td>Large-scale labeled data are generally require...</td>\n",
       "      <td>1902.06162</td>\n",
       "      <td>[{'authorId': '29724362', 'name': 'Longlong Ji...</td>\n",
       "      <td>61</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '4...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13558</th>\n",
       "      <td>Large-scale labeled data are generally require...</td>\n",
       "      <td>1902.06162</td>\n",
       "      <td>[{'authorId': '29724362', 'name': 'Longlong Ji...</td>\n",
       "      <td>61</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '4...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13761</th>\n",
       "      <td>Large-scale labeled data are generally require...</td>\n",
       "      <td>1902.06162</td>\n",
       "      <td>[{'authorId': '29724362', 'name': 'Longlong Ji...</td>\n",
       "      <td>61</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '4...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13973</th>\n",
       "      <td>Large-scale labeled data are generally require...</td>\n",
       "      <td>1902.06162</td>\n",
       "      <td>[{'authorId': '29724362', 'name': 'Longlong Ji...</td>\n",
       "      <td>61</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '4...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14571</th>\n",
       "      <td>Large-scale labeled data are generally require...</td>\n",
       "      <td>1902.06162</td>\n",
       "      <td>[{'authorId': '29724362', 'name': 'Longlong Ji...</td>\n",
       "      <td>61</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '4...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14907</th>\n",
       "      <td>Large-scale labeled data are generally require...</td>\n",
       "      <td>1902.06162</td>\n",
       "      <td>[{'authorId': '29724362', 'name': 'Longlong Ji...</td>\n",
       "      <td>61</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '4...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15021</th>\n",
       "      <td>Large-scale labeled data are generally require...</td>\n",
       "      <td>1902.06162</td>\n",
       "      <td>[{'authorId': '29724362', 'name': 'Longlong Ji...</td>\n",
       "      <td>61</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '4...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15041</th>\n",
       "      <td>Large-scale labeled data are generally require...</td>\n",
       "      <td>1902.06162</td>\n",
       "      <td>[{'authorId': '29724362', 'name': 'Longlong Ji...</td>\n",
       "      <td>61</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '4...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15345</th>\n",
       "      <td>Large-scale labeled data are generally require...</td>\n",
       "      <td>1902.06162</td>\n",
       "      <td>[{'authorId': '29724362', 'name': 'Longlong Ji...</td>\n",
       "      <td>61</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '4...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15442</th>\n",
       "      <td>Large-scale labeled data are generally require...</td>\n",
       "      <td>1902.06162</td>\n",
       "      <td>[{'authorId': '29724362', 'name': 'Longlong Ji...</td>\n",
       "      <td>61</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '4...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15493</th>\n",
       "      <td>Large-scale labeled data are generally require...</td>\n",
       "      <td>1902.06162</td>\n",
       "      <td>[{'authorId': '29724362', 'name': 'Longlong Ji...</td>\n",
       "      <td>61</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '4...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16772</th>\n",
       "      <td>Large-scale labeled data are generally require...</td>\n",
       "      <td>1902.06162</td>\n",
       "      <td>[{'authorId': '29724362', 'name': 'Longlong Ji...</td>\n",
       "      <td>61</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '4...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18008</th>\n",
       "      <td>Large-scale labeled data are generally require...</td>\n",
       "      <td>1902.06162</td>\n",
       "      <td>[{'authorId': '29724362', 'name': 'Longlong Ji...</td>\n",
       "      <td>61</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '4...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18535</th>\n",
       "      <td>Large-scale labeled data are generally require...</td>\n",
       "      <td>1902.06162</td>\n",
       "      <td>[{'authorId': '29724362', 'name': 'Longlong Ji...</td>\n",
       "      <td>61</td>\n",
       "      <td>[{'arxivId': None, 'authors': [{'authorId': '4...</td>\n",
       "      <td>62841734</td>\n",
       "      <td>10.1109/tpami.2020.2992393</td>\n",
       "      <td>11</td>\n",
       "      <td>4c94ee7df6bc2bfcac76703be4f059a79010f7e5</td>\n",
       "      <td>[{'arxivId': '1803.02276', 'authors': [{'autho...</td>\n",
       "      <td>Self-supervised Visual Feature Learning with D...</td>\n",
       "      <td>[{'topic': 'Feature learning', 'topicId': '205...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/4c94ee7d...</td>\n",
       "      <td>2020.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                abstract     arxivId  \\\n",
       "414    Large-scale labeled data are generally require...  1902.06162   \n",
       "724    Large-scale labeled data are generally require...  1902.06162   \n",
       "11360  Large-scale labeled data are generally require...  1902.06162   \n",
       "13358  Large-scale labeled data are generally require...  1902.06162   \n",
       "13388  Large-scale labeled data are generally require...  1902.06162   \n",
       "13558  Large-scale labeled data are generally require...  1902.06162   \n",
       "13761  Large-scale labeled data are generally require...  1902.06162   \n",
       "13973  Large-scale labeled data are generally require...  1902.06162   \n",
       "14571  Large-scale labeled data are generally require...  1902.06162   \n",
       "14907  Large-scale labeled data are generally require...  1902.06162   \n",
       "15021  Large-scale labeled data are generally require...  1902.06162   \n",
       "15041  Large-scale labeled data are generally require...  1902.06162   \n",
       "15345  Large-scale labeled data are generally require...  1902.06162   \n",
       "15442  Large-scale labeled data are generally require...  1902.06162   \n",
       "15493  Large-scale labeled data are generally require...  1902.06162   \n",
       "16772  Large-scale labeled data are generally require...  1902.06162   \n",
       "18008  Large-scale labeled data are generally require...  1902.06162   \n",
       "18535  Large-scale labeled data are generally require...  1902.06162   \n",
       "\n",
       "                                                 authors  citationVelocity  \\\n",
       "414    [{'authorId': '29724362', 'name': 'Longlong Ji...                61   \n",
       "724    [{'authorId': '29724362', 'name': 'Longlong Ji...                61   \n",
       "11360  [{'authorId': '29724362', 'name': 'Longlong Ji...                61   \n",
       "13358  [{'authorId': '29724362', 'name': 'Longlong Ji...                61   \n",
       "13388  [{'authorId': '29724362', 'name': 'Longlong Ji...                61   \n",
       "13558  [{'authorId': '29724362', 'name': 'Longlong Ji...                61   \n",
       "13761  [{'authorId': '29724362', 'name': 'Longlong Ji...                61   \n",
       "13973  [{'authorId': '29724362', 'name': 'Longlong Ji...                61   \n",
       "14571  [{'authorId': '29724362', 'name': 'Longlong Ji...                61   \n",
       "14907  [{'authorId': '29724362', 'name': 'Longlong Ji...                61   \n",
       "15021  [{'authorId': '29724362', 'name': 'Longlong Ji...                61   \n",
       "15041  [{'authorId': '29724362', 'name': 'Longlong Ji...                61   \n",
       "15345  [{'authorId': '29724362', 'name': 'Longlong Ji...                61   \n",
       "15442  [{'authorId': '29724362', 'name': 'Longlong Ji...                61   \n",
       "15493  [{'authorId': '29724362', 'name': 'Longlong Ji...                61   \n",
       "16772  [{'authorId': '29724362', 'name': 'Longlong Ji...                61   \n",
       "18008  [{'authorId': '29724362', 'name': 'Longlong Ji...                61   \n",
       "18535  [{'authorId': '29724362', 'name': 'Longlong Ji...                61   \n",
       "\n",
       "                                               citations  corpusId  \\\n",
       "414    [{'arxivId': None, 'authors': [{'authorId': '4...  62841734   \n",
       "724    [{'arxivId': None, 'authors': [{'authorId': '4...  62841734   \n",
       "11360  [{'arxivId': None, 'authors': [{'authorId': '4...  62841734   \n",
       "13358  [{'arxivId': None, 'authors': [{'authorId': '4...  62841734   \n",
       "13388  [{'arxivId': None, 'authors': [{'authorId': '4...  62841734   \n",
       "13558  [{'arxivId': None, 'authors': [{'authorId': '4...  62841734   \n",
       "13761  [{'arxivId': None, 'authors': [{'authorId': '4...  62841734   \n",
       "13973  [{'arxivId': None, 'authors': [{'authorId': '4...  62841734   \n",
       "14571  [{'arxivId': None, 'authors': [{'authorId': '4...  62841734   \n",
       "14907  [{'arxivId': None, 'authors': [{'authorId': '4...  62841734   \n",
       "15021  [{'arxivId': None, 'authors': [{'authorId': '4...  62841734   \n",
       "15041  [{'arxivId': None, 'authors': [{'authorId': '4...  62841734   \n",
       "15345  [{'arxivId': None, 'authors': [{'authorId': '4...  62841734   \n",
       "15442  [{'arxivId': None, 'authors': [{'authorId': '4...  62841734   \n",
       "15493  [{'arxivId': None, 'authors': [{'authorId': '4...  62841734   \n",
       "16772  [{'arxivId': None, 'authors': [{'authorId': '4...  62841734   \n",
       "18008  [{'arxivId': None, 'authors': [{'authorId': '4...  62841734   \n",
       "18535  [{'arxivId': None, 'authors': [{'authorId': '4...  62841734   \n",
       "\n",
       "                              doi  influentialCitationCount  \\\n",
       "414    10.1109/tpami.2020.2992393                        11   \n",
       "724    10.1109/tpami.2020.2992393                        11   \n",
       "11360  10.1109/tpami.2020.2992393                        11   \n",
       "13358  10.1109/tpami.2020.2992393                        11   \n",
       "13388  10.1109/tpami.2020.2992393                        11   \n",
       "13558  10.1109/tpami.2020.2992393                        11   \n",
       "13761  10.1109/tpami.2020.2992393                        11   \n",
       "13973  10.1109/tpami.2020.2992393                        11   \n",
       "14571  10.1109/tpami.2020.2992393                        11   \n",
       "14907  10.1109/tpami.2020.2992393                        11   \n",
       "15021  10.1109/tpami.2020.2992393                        11   \n",
       "15041  10.1109/tpami.2020.2992393                        11   \n",
       "15345  10.1109/tpami.2020.2992393                        11   \n",
       "15442  10.1109/tpami.2020.2992393                        11   \n",
       "15493  10.1109/tpami.2020.2992393                        11   \n",
       "16772  10.1109/tpami.2020.2992393                        11   \n",
       "18008  10.1109/tpami.2020.2992393                        11   \n",
       "18535  10.1109/tpami.2020.2992393                        11   \n",
       "\n",
       "                                        paperId  \\\n",
       "414    4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "724    4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "11360  4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "13358  4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "13388  4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "13558  4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "13761  4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "13973  4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "14571  4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "14907  4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "15021  4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "15041  4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "15345  4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "15442  4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "15493  4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "16772  4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "18008  4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "18535  4c94ee7df6bc2bfcac76703be4f059a79010f7e5   \n",
       "\n",
       "                                              references  \\\n",
       "414    [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "724    [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "11360  [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "13358  [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "13388  [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "13558  [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "13761  [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "13973  [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "14571  [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "14907  [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "15021  [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "15041  [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "15345  [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "15442  [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "15493  [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "16772  [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "18008  [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "18535  [{'arxivId': '1803.02276', 'authors': [{'autho...   \n",
       "\n",
       "                                                   title  \\\n",
       "414    Self-supervised Visual Feature Learning with D...   \n",
       "724    Self-supervised Visual Feature Learning with D...   \n",
       "11360  Self-supervised Visual Feature Learning with D...   \n",
       "13358  Self-supervised Visual Feature Learning with D...   \n",
       "13388  Self-supervised Visual Feature Learning with D...   \n",
       "13558  Self-supervised Visual Feature Learning with D...   \n",
       "13761  Self-supervised Visual Feature Learning with D...   \n",
       "13973  Self-supervised Visual Feature Learning with D...   \n",
       "14571  Self-supervised Visual Feature Learning with D...   \n",
       "14907  Self-supervised Visual Feature Learning with D...   \n",
       "15021  Self-supervised Visual Feature Learning with D...   \n",
       "15041  Self-supervised Visual Feature Learning with D...   \n",
       "15345  Self-supervised Visual Feature Learning with D...   \n",
       "15442  Self-supervised Visual Feature Learning with D...   \n",
       "15493  Self-supervised Visual Feature Learning with D...   \n",
       "16772  Self-supervised Visual Feature Learning with D...   \n",
       "18008  Self-supervised Visual Feature Learning with D...   \n",
       "18535  Self-supervised Visual Feature Learning with D...   \n",
       "\n",
       "                                                  topics  \\\n",
       "414    [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "724    [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "11360  [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "13358  [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "13388  [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "13558  [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "13761  [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "13973  [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "14571  [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "14907  [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "15021  [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "15041  [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "15345  [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "15442  [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "15493  [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "16772  [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "18008  [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "18535  [{'topic': 'Feature learning', 'topicId': '205...   \n",
       "\n",
       "                                                     url    year  \n",
       "414    https://www.semanticscholar.org/paper/4c94ee7d...  2020.0  \n",
       "724    https://www.semanticscholar.org/paper/4c94ee7d...  2020.0  \n",
       "11360  https://www.semanticscholar.org/paper/4c94ee7d...  2020.0  \n",
       "13358  https://www.semanticscholar.org/paper/4c94ee7d...  2020.0  \n",
       "13388  https://www.semanticscholar.org/paper/4c94ee7d...  2020.0  \n",
       "13558  https://www.semanticscholar.org/paper/4c94ee7d...  2020.0  \n",
       "13761  https://www.semanticscholar.org/paper/4c94ee7d...  2020.0  \n",
       "13973  https://www.semanticscholar.org/paper/4c94ee7d...  2020.0  \n",
       "14571  https://www.semanticscholar.org/paper/4c94ee7d...  2020.0  \n",
       "14907  https://www.semanticscholar.org/paper/4c94ee7d...  2020.0  \n",
       "15021  https://www.semanticscholar.org/paper/4c94ee7d...  2020.0  \n",
       "15041  https://www.semanticscholar.org/paper/4c94ee7d...  2020.0  \n",
       "15345  https://www.semanticscholar.org/paper/4c94ee7d...  2020.0  \n",
       "15442  https://www.semanticscholar.org/paper/4c94ee7d...  2020.0  \n",
       "15493  https://www.semanticscholar.org/paper/4c94ee7d...  2020.0  \n",
       "16772  https://www.semanticscholar.org/paper/4c94ee7d...  2020.0  \n",
       "18008  https://www.semanticscholar.org/paper/4c94ee7d...  2020.0  \n",
       "18535  https://www.semanticscholar.org/paper/4c94ee7d...  2020.0  "
      ]
     },
     "execution_count": 605,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[test_df['doi']==\"10.1109/tpami.2020.2992393\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "annoying-clerk",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "spread-mouth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SemanticScholar_DBLP_Linkage.ipynb',\n",
       " 'Matches_test.csv',\n",
       " '.DS_Store',\n",
       " 'merge_vision.ipynb',\n",
       " 'papers.ttl',\n",
       " 'merge_vision_2.ipynb',\n",
       " 'Matches.csv',\n",
       " 'Parsing_Citation.ipynb',\n",
       " '.ipynb_checkpoints',\n",
       " 'Citation Scraper.ipynb',\n",
       " 'semantic_short.csv',\n",
       " 'output.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "polished-neutral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rdfpandas\n",
      "  Downloading rdfpandas-1.1.0-py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: rdflib>=5.0.0 in /Users/rehanahmed/opt/anaconda3/envs/finsent/lib/python3.6/site-packages (from rdfpandas) (5.0.0)\n",
      "  Downloading rdfpandas-1.0.0-py3-none-any.whl (5.2 kB)\n",
      "Installing collected packages: rdfpandas\n",
      "Successfully installed rdfpandas-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install rdfpandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "southern-representation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N8112c80df5854c32ba2b5ecb7a2499a9 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rdflib import Graph\n",
    "\n",
    "g = Graph()\n",
    "g.parse(\"papers.ttl\", format=\"turtle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "secondary-relationship",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'to_dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b67903475cc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdflib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'papers.ttl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ttl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# df.to_csv('test.csv', index = True, index_label = \"@id\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'to_dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "import rdfpandas.graph\n",
    "import pandas as pd\n",
    "import rdflib\n",
    "\n",
    "g = rdflib.Graph()\n",
    "g.parse('papers.ttl', format = 'ttl')\n",
    "# df.to_csv('test.csv', index = True, index_label = \"@id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "academic-hospital",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdfpandas.graph import to_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "intermediate-melissa",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-30a83500110c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/finsent/lib/python3.6/site-packages/rdfpandas/graph.py\u001b[0m in \u001b[0;36mto_dataframe\u001b[0;34m(g)\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0ms_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \u001b[0mlast_seen_subject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubject_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlast_seen_subject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                             \u001b[0ms_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/finsent/lib/python3.6/site-packages/rdflib/term.py\u001b[0m in \u001b[0;36m__lt__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    877\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__gt__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/finsent/lib/python3.6/site-packages/rdflib/term.py\u001b[0m in \u001b[0;36meq\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1051\u001b[0m                     raise TypeError(\n\u001b[1;32m   1052\u001b[0m                         'I cannot know that these two lexical forms do not map to the same value: %s and %s' % (self, other))\n\u001b[0;32m-> 1053\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1054\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = to_dataframe(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "molecular-glasgow",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-00cf07b74dcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NEW_ID = 0 \n",
    "c_c = 0\n",
    "st = datetime.now()\n",
    "for index,row in df.iterrows():\n",
    "#     print(index)\n",
    "#     print(row)\n",
    "    if index>10:\n",
    "        break\n",
    "    \n",
    "\n",
    "    # row=df.iloc[2]\n",
    "\n",
    "    NEW_ID = index\n",
    "\n",
    "    ### URI ###\n",
    "    node_uri = URIRef(str(NEW_ID))\n",
    "    g.add((node_uri, RDF.type, SCHEMA.ScholarlyArticle))\n",
    "    \n",
    "    if  str(row['b_doi'])!=\"nan\":\n",
    "        b_doi = row['b_doi']\n",
    "        g.add((node_uri, MYNS.doi, Literal(b_doi, datatype=SCHEMA.Text)))\n",
    "\n",
    "    \n",
    "    if  str(row['a_title'])!=\"nan\":\n",
    "        title = row['a_title']\n",
    "    else:\n",
    "        title = row['b_title']\n",
    "    if title:\n",
    "        g.add((node_uri, SCHEMA.headline, Literal(title, datatype=SCHEMA.Text)))\n",
    "\n",
    "\n",
    "    if str(row['a_authors'])!=\"nan\":\n",
    "        authors = row['a_authors']\n",
    "    else:\n",
    "        authors = row['b_authors']\n",
    "    if authors:\n",
    "#         [g.add((node_uri, SCHEMA.author, Literal(author, datatype=SCHEMA.Person))) for author in authors]\n",
    "        for author in authors:\n",
    "            g.add((node_uri, SCHEMA.author, URIRef(str(author_dict[author])))) \n",
    "            \n",
    "    published = None\n",
    "    if str(row['b_year'])!=\"nan\":\n",
    "        published = int(row['b_year'])\n",
    "    else:\n",
    "        pub =row[\"a_published\"]\n",
    "        if str(pub)!=\"nan\":\n",
    "            published = datetime.strptime(pub ,\"%Y-%m-%dT%H:%M:%SZ\").year\n",
    "        \n",
    "    if published:\n",
    "        g.add((node_uri, SCHEMA.datePublished, Literal(published, datatype=SCHEMA.DateTime)))\n",
    "\n",
    "    try:\n",
    "        if str(row['a_updated'])!=\"nan\":\n",
    "            updated = datetime.strptime(row[\"a_updated\"],\"%Y-%m-%dT%H:%M:%SZ\").year\n",
    "            g.add((node_uri, SCHEMA.dateModified, Literal(updated, datatype=SCHEMA.DateTime)))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    try:\n",
    "        if str(row['b_abstract'])!=\"nan\":\n",
    "            abstract = row['b_abstract']\n",
    "        else:\n",
    "            abstract = row['a_abstract']\n",
    "        g.add((node_uri, SCHEMA.abstract, Literal(abstract, datatype=SCHEMA.Text)))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    try:\n",
    "        if str(row['a_categories'])!=\"nan\":\n",
    "            categories = ast.literal_eval(row['a_categories'])\n",
    "            for category in categories:\n",
    "                g.add((node_uri, SCHEMA.genre, URIRef(str(category_dict[category]))))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if str(row['a_journal'])!=\"nan\":\n",
    "        s = row['a_journal']\n",
    "        journal = ''.join([i for i in s if i.isalpha() or i==\" \"]).strip()\n",
    "        \n",
    "        g.add((node_uri, SCHEMA.publisher, URIRef(str(journal_dict[journal])))) #datatype=SCHEMA.Organisation\n",
    "\n",
    "\n",
    "    if str(row['a_arxiv_url'])!=\"nan\":\n",
    "        arxiv_url = row['a_arxiv_url']\n",
    "\n",
    "        g.add((node_uri, SCHEMA.url, Literal(arxiv_url, datatype=SCHEMA.URL)))\n",
    "\n",
    "\n",
    "    if str(row['a_gscholar_url'])!=\"nan\":\n",
    "        g_scholar_url = row['a_gscholar_url']\n",
    "\n",
    "        g.add((node_uri, SCHEMA.url, Literal(g_scholar_url, datatype=SCHEMA.URL)))\n",
    "\n",
    "    if str(row['b_url'])!=\"nan\":\n",
    "        g_scholar_url = row['b_url']\n",
    "\n",
    "        g.add((node_uri, SCHEMA.url, Literal(g_scholar_url, datatype=SCHEMA.URL)))\n",
    "\n",
    "    if str(row['b_citationVelocity'])!=\"nan\":\n",
    "        num = row['b_citationVelocity']\n",
    "        g.add((node_uri, MYNS.citationVelocity, Literal(num))) #datatype=SCHEMA.Organisation\n",
    "\n",
    "    if str(row['b_influentialCitationCount'])!=\"nan\":\n",
    "        num = row['b_influentialCitationCount']\n",
    "        g.add((node_uri, MYNS.b_influentialCitationCount, Literal(num))) #datatype=SCHEMA.Organisation\n",
    "\n",
    "    try:\n",
    "        citations_1 = ast.literal_eval(row['b_citations'])\n",
    "    except:\n",
    "        citations_1 = []\n",
    "    try:\n",
    "        citations_2 = row['b_dblp_info']['references_opencitations']\n",
    "    except:\n",
    "        citations_2 = []\n",
    "    citations_1.extend(citations_2)\n",
    "\n",
    "    for c in citations_1:\n",
    "\n",
    "        if 'citing' in c:\n",
    "            c_doi = c['citing'].split(\" => \")[-1]\n",
    "            citing_row = df[df['b_doi']==c_doi]\n",
    "            if len(citing_row)>0:\n",
    "                citing_id = citing_row.index[0]\n",
    "                cite_uri = URIRef(str(citing_id))\n",
    "                g.add((node_uri, SCHEMA.citation, cite_uri))\n",
    "            else:\n",
    "                cite_uri = c_doi\n",
    "#                 len_df = len(df)\n",
    "#                 cite_uri = URIRef(str(len_df))\n",
    "#                 df.at[len_df,'b_doi'] = c_doi\n",
    "#                 try:\n",
    "#                     df.at[len_df,'b_year'] = float(c['creation'].split(\" => \")[-1])\n",
    "#                 except:\n",
    "#                     pass\n",
    "                \n",
    "                g.add((node_uri, SCHEMA.citation, Literal(cite_uri)))\n",
    "        else:\n",
    "            try:\n",
    "                c_doi = c['doi']\n",
    "            except:\n",
    "                c_doi = None\n",
    "            if c_doi:\n",
    "                citing_row = df[df['b_doi']==c_doi]\n",
    "#                 if c_doi:\n",
    "#                     citing_row = df[df['b_doi']==c_doi]\n",
    "#                 else:\n",
    "#                     citing_row = df[df['b_title']==c_title]\n",
    "\n",
    "                if len(citing_row)>0:\n",
    "                    citing_id = citing_row.index[0]\n",
    "                    cite_uri = URIRef(str(citing_id))\n",
    "                    g.add((node_uri, SCHEMA.citation, cite_uri))\n",
    "                else:\n",
    "                    cite_uri = c_doi\n",
    "#                     len_df = len(df)\n",
    "#                     cite_uri = URIRef(str(len_df))\n",
    "#                     df.at[len_df,'b_doi'] = c_doi\n",
    "#                     if c['arxivId']:\n",
    "#                         df.at[len_df,'b_arxivId'] = c['arxivId']\n",
    "#                     if c['authors']:\n",
    "#                         df.at[len_df,'b_authors'] = [i['name'] for i in c['authors']]\n",
    "#                     if c['title']:\n",
    "#                         df.at[len_df,'b_title'] = c['title']\n",
    "#                     if c['url']:\n",
    "#                         df.at[len_df,'b_url'] = c['url']\n",
    "#                     if c['year']:\n",
    "#                         df.at[len_df,'b_year'] = c['year']            \n",
    "                    g.add((node_uri, SCHEMA.citation, Literal(cite_uri)))\n",
    "                \n",
    "\n",
    "    #     else:\n",
    "    #         g.add((node_uri, SCHEMA.citation,Literal(c_doi, datatype=SCHEMA.Text)))\n",
    "\n",
    "    try:\n",
    "        references_1 = ast.literal_eval(row['b_references'])\n",
    "    except:\n",
    "        referneces_1 = []\n",
    "    try:\n",
    "        references_2 = row['b_dblp_info']['references_crossref']\n",
    "    except:\n",
    "        references_2 = []\n",
    "    references_1.extend(references_2)   \n",
    "\n",
    "    for c in references_1:\n",
    "        if 'key' in c:\n",
    "            citing_row = []\n",
    "            c_doi = None\n",
    "\n",
    "            if \"DOI\" in c:\n",
    "                c_doi = c[\"DOI\"]\n",
    "                citing_row = df[df['b_doi']==c_doi]\n",
    "\n",
    "            elif \"article-title\" in c:\n",
    "                c_title = c['article-title']\n",
    "                citing_row = df[(df['b_title']==c_title) | (df['a_title']==c_title)]\n",
    "\n",
    "\n",
    "            if len(citing_row)>0:\n",
    "                citing_id = citing_row.index[0]\n",
    "                cite_uri = URIRef(str(citing_id))\n",
    "                g.add((node_uri, MYNS.references, cite_uri))\n",
    "            else:\n",
    "                cite_uri = c_doi \n",
    "                \n",
    "#                 len_df = len(df)\n",
    "#                 cite_uri = URIRef(str(len_df))\n",
    "\n",
    "#                 try:\n",
    "#                     df.at[len_df,'b_doi'] = c_doi\n",
    "#                 except:\n",
    "#                     pass\n",
    "#                 try:\n",
    "#                     df.at[len_df,'b_year'] = float(c['year'])\n",
    "#                 except:\n",
    "#                     pass\n",
    "#                 try:\n",
    "#                     df.at[len_df,'b_author'] = c['author']\n",
    "#                 except:\n",
    "#                     pass\n",
    "#                 try:\n",
    "#                     df.at[len_df,'b_title'] = c['article-title']\n",
    "#                 except:\n",
    "#                     pass\n",
    "\n",
    "                g.add((node_uri, MYNS.references, Literal(cite_uri)))\n",
    "        else:\n",
    "            try:\n",
    "                c_doi = c['doi']\n",
    "            except:\n",
    "                c_doi = None\n",
    "                pass\n",
    "            if c_doi:\n",
    "                citing_row = df[df['b_doi']==c_doi]\n",
    "#                 if c_doi:\n",
    "#                     citing_row = df[df['b_doi']==c_doi]\n",
    "#                 else:\n",
    "#                     citing_row = df[df['b_title']==c_title]\n",
    "\n",
    "                if len(citing_row)>0:\n",
    "                    citing_id = citing_row.index[0]\n",
    "                    cite_uri = URIRef(str(citing_id))\n",
    "                    g.add((node_uri, MYNS.references, cite_uri))\n",
    "                else:\n",
    "                    cite_uri = c_doi\n",
    "#                     len_df = len(df)\n",
    "#                     cite_uri = URIRef(str(len_df))\n",
    "#                     df.at[len_df,'b_doi'] = c_doi\n",
    "#                     if c['arxivId']:\n",
    "#                         df.at[len_df,'b_arxivId'] = c['arxivId']\n",
    "#                     if c['authors']:\n",
    "#                         df.at[len_df,'b_authors'] = [i['name'] for i in c['authors']]\n",
    "#                     if c['title']:\n",
    "#                         df.at[len_df,'b_title'] = c['title']\n",
    "#                     if c['url']:\n",
    "#                         df.at[len_df,'b_url'] = c['url']\n",
    "#                     if c['year']:\n",
    "#                         df.at[len_df,'b_year'] = c['year']  \n",
    "                    g.add((node_uri, MYNS.references, Literal(cite_uri)))\n",
    "\n",
    "    #     else:\n",
    "    #         g.add((node_uri, SCHEMA.citation,Literal(c_doi, datatype=SCHEMA.Text)))\n",
    "    c_c = c_c+1\n",
    "    if c_c%100==0:\n",
    "        et = datetime.now()\n",
    "        print((et-st).total_seconds())\n",
    "        print(len(df))\n",
    "        print(c_c)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-catering",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "documentary-encounter",
   "metadata": {},
   "source": [
    "# Author Sparql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "arctic-silence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparql_query_or = \"\"\"\n",
    "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
    "PREFIX dbo:<http://dbpedia.org/ontology/>\n",
    "\n",
    "SELECT ?lbl ?sub ?a ?b ?c ?d WHERE {\n",
    "  ?sub a foaf:Person .\n",
    "  ?sub rdfs:label ?lbl .\n",
    "  ?lbl bif:contains \"__TERM__\" .\n",
    "  filter(langMatches(lang(?lbl), \"en\"))\n",
    "OPTIONAL {?sub dbo:birthName ?a}\n",
    "OPTIONAL {?sub dbo:birthPlace ?b}\n",
    "OPTIONAL {?sub dbo:birthDate ?c}\n",
    "?sub dbo:academicDiscipline ?d\n",
    "\n",
    "} \n",
    "limit 100\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-blackjack",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('authors.txt','r') as f:\n",
    "  t = f.read()\n",
    "  ls = t.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-rebate",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json,pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-squad",
   "metadata": {},
   "outputs": [],
   "source": [
    "c=0\n",
    "ans_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-track",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for r in ls[c:]:\n",
    "  c+=1\n",
    "    \n",
    "  if c%500==0:\n",
    "    with open('ans_dict.pickle', 'wb') as handle:\n",
    "      print(c)\n",
    "      pickle.dump(ans_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "  line = r.split(',')\n",
    "\n",
    "  name_res = line[0].split(\" \")\n",
    "  name_res = \"Geoffrey Hinton\".split(\" \")\n",
    "  search_name = \"\"\n",
    "  search_bool = True\n",
    "\n",
    "  if len(name_res)>1:\n",
    "      for i in name_res:\n",
    "          search_name = search_name +i+' AND '\n",
    "          if len(i)<2 or i[1]==\".\":\n",
    "              search_bool = False\n",
    "\n",
    "      if search_bool:\n",
    "          sparql_query = sparql_query_or[:]\n",
    "          sparql_query = sparql_query.replace('__TERM__',search_name[:-5])\n",
    "          result = sparql.query('http://dbpedia.org/sparql', sparql_query)\n",
    "\n",
    "          ans_list = []\n",
    "          for row in result:\n",
    "              values = sparql.unpack_row(row)\n",
    "              ans_list.append(values)\n",
    "          if len(ans_list)>0:\n",
    "              ans_dict[line[1]] = ans_list\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-determination",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = pickle.load(open('ans_dict.pickle','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib import URIRef, Literal, Namespace\n",
    "from rdflib.namespace import RDF, RDFS, XSD\n",
    "\n",
    "MYNS = Namespace('http://inf558.org/myfakenamespace#')\n",
    "SCHEMA = Namespace(\"https://schema.org/\")\n",
    "\n",
    "# Initliaze the graph\n",
    "g = rdflib.Graph()\n",
    "\n",
    "# Bind namespace and prefixes\n",
    "g.bind('my_ns', MYNS)\n",
    "g.bind('schema', SCHEMA)\n",
    "g.bind('rdf', RDF)\n",
    "g.bind('rdfs', RDFS)\n",
    "g.bind('xsd', XSD)\n",
    "\n",
    "for d in tqdm(authors):\n",
    "    node_uri = URIRef(d.split(\"/\")[-1])\n",
    "    g.add((node_uri, RDF.type, SCHEMA.Person))\n",
    "    g.add((node_uri, SCHEMA.name, Literal(authors[d][0][2], datatype=SCHEMA.Text)))\n",
    "    try:\n",
    "        g.add((node_uri, SCHEMA.birthDate, Literal(authors[d][0][1], datatype=SCHEMA.Date)))\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        g.add((node_uri, SCHEMA.url, Literal(authors[d][0][4], datatype=SCHEMA.URL)))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-trance",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.serialize('authors.ttl', format=\"turtle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
